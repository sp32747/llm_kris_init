{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmMW7OsPT_Eq"
      },
      "source": [
        "#Self Attention Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxdmjEZyUG6V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXdiDPuNTAjq"
      },
      "source": [
        "## Step-1 : Basic Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGkfCUy9SO2j"
      },
      "outputs": [],
      "source": [
        "# Add contextual embeddings for Example 1 (Riverbank)\n",
        "'''\n",
        "The farmer planted a row of seeds near the bank of the river,\n",
        "hoping they would bear fruit by the summer.\n",
        "'''\n",
        "\n",
        "embeddings_example1 = {\n",
        "    \"Bank\": np.array([0.1, 0.2, 0.3]),\n",
        "    \"Bear\": np.array([0.4, 0.1, 0.6]),\n",
        "    \"Row\": np.array([0.3, 0.5, 0.2]),\n",
        "    \"River\": np.array([0.2, 0.3, 0.5]),\n",
        "    \"Seeds\": np.array([0.3, 0.1, 0.4]),\n",
        "    \"Summer\": np.array([0.6, 0.2, 0.1]),\n",
        "}\n",
        "\n",
        "# Add contextual embeddings for Example 2 (Financial institution)\n",
        "'''\n",
        "The row between the bank and investors grew heated when they argued\n",
        "about whether the bear market would affect the company's financial stability.\n",
        "'''\n",
        "embeddings_example2 = {\n",
        "    \"Bank\": np.array([0.1, 0.2, 0.3]),\n",
        "    \"Bear\": np.array([0.4, 0.1, 0.6]),\n",
        "    \"Row\": np.array([0.3, 0.5, 0.2]),\n",
        "    \"Investors\": np.array([0.7, 0.3, 0.4]),\n",
        "    \"Market\": np.array([0.2, 0.8, 0.5]),\n",
        "    \"Dispute\": np.array([0.4, 0.6, 0.7]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdDaPH-aTHnn"
      },
      "source": [
        "## Step-2: Attention Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfFciq2NUCxR"
      },
      "outputs": [],
      "source": [
        "# Step 2: Define the Scaled Dot-Product Attention Function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Q: Query matrix\n",
        "    K: Key matrix\n",
        "    V: Value matrix\n",
        "    \"\"\"\n",
        "    # Step 2.1: Compute dot products between Query and Key\n",
        "    scores = np.dot(Q, K.T)  # Shape: (n_words, n_words)\n",
        "\n",
        "    # Step 2.2: Scale the scores by sqrt(d_k) where d_k is the embedding size\n",
        "    d_k = K.shape[1]\n",
        "    scaled_scores = scores / math.sqrt(d_k)\n",
        "\n",
        "    # Step 2.3: Apply softmax to get attention weights\n",
        "    attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)\n",
        "\n",
        "    # Step 2.4: Compute the weighted sum of values\n",
        "    output = np.dot(attention_weights, V)\n",
        "\n",
        "    return attention_weights, output\n",
        "\n",
        "def calculate_attention(embeddings):\n",
        "    words = list(embeddings.keys())\n",
        "    embedding_matrix = np.array(list(embeddings.values()))\n",
        "\n",
        "    # Use the same embeddings for Q, K, and V in self-attention\n",
        "    Q = embedding_matrix  # Query\n",
        "    K = embedding_matrix  # Key\n",
        "    V = embedding_matrix  # Value\n",
        "\n",
        "    # Compute self-attention\n",
        "    attention_weights, attention_output = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "    return words, attention_weights, attention_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCThPBjPTSyT"
      },
      "source": [
        "## Step-3 : Calculate Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izgdTgXZUC4l"
      },
      "outputs": [],
      "source": [
        "# Calculate Attention for Example 1\n",
        "print(\"Example 1: Riverbank, Produce, Arrangement\")\n",
        "words1, attention_weights1, output1 = calculate_attention(embeddings_example1)\n",
        "\n",
        "#print(\"Words:\", words1)\n",
        "#print(\"Attention Weights:\\n\", attention_weights1)\n",
        "#print(\"Attention Output (weighted sum of values):\\n\", output1)\n",
        "for i in range(len(words1)):\n",
        "  print(f\"Attention values for {words1[i]}: {output1[i]}\")\n",
        "\n",
        "\n",
        "# Calculate Attention for Example 2\n",
        "print(\"\\nExample 2: Financial Institution, Stock Market, Dispute\")\n",
        "words2, attention_weights2, output2 = calculate_attention(embeddings_example2)\n",
        "\n",
        "#print(\"Words:\", words2)\n",
        "#print(\"Attention Weights:\\n\", attention_weights2)\n",
        "for i in range(len(words2)):\n",
        "  print(f\"Attention values for {words2[i]}: {output2[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-ccTpSSQeww"
      },
      "source": [
        "# English-to-Spanish translation model with Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC9ld7TsQew2"
      },
      "source": [
        "## Background\n",
        "In this example, we'll build a sequence-to-sequence Transformer model, which\n",
        "we'll train on an English-to-Spanish machine translation task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x2mgsQvQew3"
      },
      "source": [
        "## Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad0SHQULQew3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "#The below package is a custom transformer package stored on my github\n",
        "\n",
        "import Transformer\n",
        "from Transformer import TransformerEncoder, TransformerDecoder, PositionalEmbedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miofjQaeQew5"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "We'll be working with an English-to-Spanish translation dataset\n",
        "provided by [Anki](https://www.manythings.org/anki/). Let's download it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGgBBeZLmzu9"
      },
      "outputs": [],
      "source": [
        "datafile_location = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "eng_spa_raw = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=datafile_location,\n",
        "    extract=True,\n",
        ")\n",
        "eng_spa_raw = pathlib.Path(eng_spa_raw).parent / \"spa-eng\" / \"spa.txt\"\n",
        "'''\n",
        "### Parsing the data\n",
        "\n",
        "Each line contains an English sentence and its corresponding Spanish sentence.\n",
        "The English sentence is the source sequence and Spanish one is the target sequence.\n",
        "We attach the tokens [start] and [end] to the Spanish sentence.\n",
        "'''\n",
        "with open(eng_spa_raw) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    text_pairs.append((eng, spa))\n",
        "\n",
        "print(\"Sample Data Points\")\n",
        "for i in random.sample(range(1, 80000), 5):\n",
        "    print(text_pairs[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1qpncmzBvgW"
      },
      "source": [
        "## Split the Data into Train and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoAFF_B-Qew7"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1hENIgqQew8"
      },
      "outputs": [],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5p8e_DkQew8"
      },
      "source": [
        "## Vectorizing the text data\n",
        "\n",
        "We'll use two instances of the `TextVectorization` layer to vectorize the text\n",
        "data (one for English and one for Spanish),\n",
        "that is to say, to turn the original strings into integer sequences\n",
        "where each integer represents the index of a word in a vocabulary.\n",
        "\n",
        "The English layer will use the default string standardization (strip punctuation characters)\n",
        "and splitting scheme (split on whitespace), while\n",
        "the Spanish layer will use a custom standardization, where we add the character\n",
        "`\"¿\"` to the set of punctuation characters to be stripped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE-MXvV4Qew8"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf_strings.lower(input_string)\n",
        "    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)\n",
        "\n",
        "#print vectorized text\n",
        "for i in random.sample(range(1, 100000), 3):\n",
        "  print(train_eng_texts[i],'\\n',eng_vectorization(train_eng_texts[i]))\n",
        "  print(train_spa_texts[i],'\\n',spa_vectorization(train_spa_texts[i]))\n",
        "  print(\"=============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23z9Q1zpC3cd"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scY5TiVHGTMW"
      },
      "source": [
        "At each training step, the model will seek to predict target words N+1 (and beyond)\n",
        "using the source sentence and the target words 0 to N.\n",
        "\n",
        "As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n",
        "\n",
        "- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n",
        "`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n",
        "that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n",
        "- `target` is the target sentence offset by one step:\n",
        "it provides the next words in the target sentence -- what the model will try to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD7UvsneQew9"
      },
      "outputs": [],
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": spa[:, :-1],\n",
        "        },\n",
        "        spa[:, 1:],\n",
        "    )\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.cache().shuffle(2048).prefetch(16)\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR2dkC2uYaEN"
      },
      "outputs": [],
      "source": [
        "#shape of train_ds\n",
        "#dataset=train_ds.take(3)\n",
        "#list(dataset.as_numpy_iterator())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE8mF9VcQew9"
      },
      "source": [
        "Let's take a quick look at the sequence shapes\n",
        "(we have batches of 64 pairs, and all sequences are 20 steps long):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Z6qojDQew9"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obqcqr6DQew9"
      },
      "source": [
        "## Model Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj8ZPUIlQew-"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048 # Nodes in the Dense Layer over the multi head attention layer\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)([x, encoder_outputs])\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "transformer = keras.Model(\n",
        "    {\"encoder_inputs\": encoder_inputs, \"decoder_inputs\": decoder_inputs},\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")\n",
        "\n",
        "#transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\",\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=0),\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWunCcfgQew_"
      },
      "source": [
        "## Training our model\n",
        "\n",
        "We'll use accuracy as a quick way to monitor training progress on the validation data.\n",
        "Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n",
        "\n",
        "Here we only train for 1 epoch, but to get the model to actually converge\n",
        "you should train for at least 30 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgkk4pLYQew_"
      },
      "outputs": [],
      "source": [
        "epochs = 2  # This should be at least 30 for convergence\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8KqbMVHdHXe"
      },
      "source": [
        "## Save and Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyrfLmG4dG3Q"
      },
      "outputs": [],
      "source": [
        "transformer.save_weights(\"eng_spa_2epochs.weights.h5\")\n",
        "transformer.load_weights(\"eng_spa_2epochs.weights.h5\")\n",
        "transformer.fit(train_ds, epochs=1, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv4bBIS0JUeW"
      },
      "outputs": [],
      "source": [
        "#Download the model weights file\n",
        "!gdown 'https://drive.google.com/uc?export=download&id=1jMLFnlXPQXlRRVmXfr2sIOuxUO4VGuKo' -O eng_spa_50epochs.weights.h5\n",
        "\n",
        "# Load the model weights\n",
        "transformer.load_weights(\"eng_spa_50epochs.weights.h5\")\n",
        "transformer.fit(train_ds, epochs=1, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP6tBhARQew_"
      },
      "source": [
        "## Making Predictions\n",
        "\n",
        "Finally, let's demonstrate how to translate brand new English sentences.\n",
        "We simply feed into the model the vectorized English sentence\n",
        "as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n",
        "we hit the token `\"[end]\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRuYL-UdQew_"
      },
      "outputs": [],
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            {\n",
        "                \"encoder_inputs\": tokenized_input_sentence,\n",
        "                \"decoder_inputs\": tokenized_target_sentence,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        sampled_token_index = ops.convert_to_numpy(\n",
        "            ops.argmax(predictions[0, i, :])\n",
        "        ).item(0)\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-B2NpyXMWVj"
      },
      "source": [
        "## Prediction Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs0HiTwb9Prv"
      },
      "outputs": [],
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for i in random.sample(range(1, 1000), 10):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence, \" ==> \", translated)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
