{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyM8AzaFF0/hflEkVco8vAh+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0p7JavHfFCJq"},"source":["#Importing dependencies\n","import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","import tensorflow.keras as keras\n","#!pip install gensim\n","#!pip install google.cloud\n","#import gzip\n","import gensim\n","import logging"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bag of Words"],"metadata":{"id":"ce7z4Lbw7Ocg"}},{"cell_type":"markdown","source":["## Sample Data"],"metadata":{"id":"pF5BZQzg9dpG"}},{"cell_type":"code","source":["corpus = ['king is a strong man','queen is a wise woman','boy is a young man',\n","          'girl is a young woman','prince is a young','prince will be strong',\n","          'princess is young','man is strong','woman is pretty', 'prince is a boy',\n","          'prince will be king', 'princess is a girl', 'princess will be queen']\n","print(corpus)"],"metadata":{"id":"5Pat2uIg7QIB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer()\n","DTM = cv.fit_transform(corpus)\n","DTM = pd.DataFrame(DTM.toarray(), columns=cv.get_feature_names_out())\n","DTM"],"metadata":{"id":"vnd1lAMu9igB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DTM on Review Data"],"metadata":{"id":"ritVM-BeAO-_"}},{"cell_type":"code","source":["data = {'review': ['I loved this movie!', 'It was okay.', 'I hated it.', 'It was amazing!', 'I was disappointed.',\n","                   'It was a great experience.', 'I fell asleep during the movie.', 'It was a total waste of time.',\n","                   'I highly recommend this movie.', 'I would not recommend this movie.'],\n","       'sentiment': ['positive', 'neutral', 'negative', 'positive', 'negative',\n","                      'positive', 'negative', 'negative', 'positive', 'negative']}\n","df = pd.DataFrame(data)\n","df"],"metadata":{"id":"qZNdByGJB8_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the input data into a DTM\n","cv = CountVectorizer()\n","dtm = cv.fit_transform(df['review'])\n","dtm = pd.DataFrame(dtm.toarray(), columns=cv.get_feature_names_out())\n","dtm[\"y_value\"]=df[\"sentiment\"]\n","# Print the DTM\n","dtm"],"metadata":{"id":"rFHZbyJP_Mrf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iILSKkuFecfB"},"source":["# Word Embeddings"]},{"cell_type":"code","source":["statements = [\n","\"Trees tall\",\n","\"Trees green\",\n","\"Trees majestic\",\n","\"Trees essential\",\n","\"Trees diverse\",\n","\"Trees oxygen-giving\",\n","\"computers fast\",\n","\"computers smart\",\n","\"computers useful\",\n","\"computers powerful\",\n","\"computers everywhere\",\n","\"computers changing\"\n","]"],"metadata":{"id":"vIOeAKNk8Hkb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["statements_list = []\n","for statement in statements:\n","  statements_list.append(statement.split())\n","print(statements_list)\n","from gensim.parsing.preprocessing import STOPWORDS\n","documents = [[word for word in document if word not in STOPWORDS] for document in statements_list]"],"metadata":{"id":"moeqXpWAIFQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pbjx851Kecg2"},"source":["from gensim.models import Word2Vec\n","model = Word2Vec(documents, min_count=1, vector_size=3, window = 3)\n","#size： size of word vector, hidden layer\n","#min-count：discard words that appear less than # times\n","#window：Context Window size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JoakJPqjecg8"},"source":["## Hyperparameters\n","\n","### size\n","The hidden nodes size. The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes.\n","\n","### window\n","Context window size. The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n","\n","### min_count\n","Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model."]},{"cell_type":"markdown","source":["## Checking the word2vec output"],"metadata":{"id":"-xS2-TXjOrK8"}},{"cell_type":"code","metadata":{"id":"Uhcoh2J4ecg-"},"source":["for word, vector in zip(model.wv.index_to_key, model.wv.vectors):\n","  print(word, vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","# Visualize the word vectors in 3D space using PCA\n","vectors = model.wv.vectors\n","\n","\n","fig = plt.figure(figsize=(15,10))\n","ax = plt.axes(projection='3d')\n","ax = plt.axes(projection='3d')\n","\n","xdata = vectors[:, 0]\n","ydata = vectors[:, 1]\n","zdata = vectors[:, 2]\n","names=model.wv.index_to_key\n","\n","ax.scatter3D(xdata, ydata, zdata, s=200 , c=xdata)\n","for names, x, y, z in zip(names, xdata, ydata, zdata):\n","    label = names\n","    ax.text(x, y, z, label )\n","plt.show()\n"],"metadata":{"id":"7s68kKcDLPfp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MN-IsdtbjAzC"},"source":["# Word2Vec Example-2"]},{"cell_type":"code","metadata":{"id":"RjCJIU_yi-37"},"source":["import urllib.request\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Amazon_Yelp_Reviews/Review_Data.csv\", \"Review_Data.csv\")\n","data_file=\"Review_Data.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_input(input_file):\n","    with open (input_file, 'rb') as f:\n","        for i, line in enumerate (f):\n","            # do some pre-processing and return a list of words for each review text\n","            yield gensim.utils.simple_preprocess (line)\n","            # read the tokenized reviews into a list\n","            # each review item becomes a series of words\n","            # so this becomes a list of lists\n","    print(\"File reading done !!\")\n","documents = list (read_input (data_file))"],"metadata":{"id":"fGedvZFPL29k"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQmLY_nRi-38"},"source":["print(documents)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vb39Ce9qi-39"},"source":["from gensim.models import Word2Vec\n","model = Word2Vec(documents, min_count=1, vector_size=10)\n","#size： size of word vector, hidden layer\n","#min-count：discard words that appear less than # times\n","#window：Context Window size"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the vectors for a couple of words\n","\n","for word, vector in zip(model.wv.index_to_key, model.wv.vectors):\n","  if word in [\"good\", \"bad\", \"money\"]:\n","    print(word, vector)"],"metadata":{"id":"loHMX_m_JrZC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Word Embeddings on TensorFlow - Sentiment Analysis Project"],"metadata":{"id":"t9bR1Yo4LCKT"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import os\n","import re\n","import shutil\n","import string\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import losses\n","from tensorflow.keras import preprocessing\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","from tensorflow.keras.layers import GlobalAveragePooling1D, Dropout, Embedding, Dense"],"metadata":{"id":"kOmCw76QEied"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Downloading"],"metadata":{"id":"fjPwWdJxxRZR"}},{"cell_type":"code","source":["!gdown https://drive.google.com/u/0/uc?id=1DWm8nOX2nIXU4-1trQE--AcqkmcM8J_K -O aclImdb_v1.tar.gz\n","!tar -zxvf 'aclImdb_v1.tar.gz' # Untar the dataset\n","!ls ./aclImdb/ # Display the contents of the folder"],"metadata":{"id":"mdpyVxldJwgW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pre-processing"],"metadata":{"id":"9wLIp0UUxNQ-"}},{"cell_type":"code","source":["train_dir=\"/content/aclImdb/train\"\n","shutil.rmtree(os.path.join(train_dir, 'unsup')) # Removing 'unsup' folder. Not required here\n","\n","train_datagen = tf.keras.preprocessing.text_dataset_from_directory(\n","    'aclImdb/train', # Train data Folder\n","    validation_split=0.2,  #80% Train and 20% Test\n","    subset='training',\n","    seed=55 #Seed is used to to make sure that evertime we get the same train and test data\n","    )\n","\n","test_datagen = tf.keras.preprocessing.text_dataset_from_directory(\n","    'aclImdb/train',\n","    validation_split=0.2,\n","    subset='validation',\n","    seed=55)\n","\n","#Data Pre-processing - Also known as \"standardizing\"\n","def pre_process(input_data):\n","  lowercase = tf.strings.lower(input_data)\n","  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')  #Remove HTML tags like <br />\n","  return tf.strings.regex_replace(stripped_html,\n","                                  '[%s]' % re.escape(string.punctuation),  #Remove HTML links\n","                                  '')\n","\n","#Mapping words to numbers - Also known as TextVectorization\n","vocab_size = 20000    #Limiting the maximum vocab size in the overall data. take only top frequent words\n","max_sequence_length = 250 #Limiting the maximum length of input sequence of words in a single review\n","\n","vectorize_layer = TextVectorization(\n","    standardize=pre_process,\n","    max_tokens=vocab_size,\n","    output_sequence_length=max_sequence_length # Truncate large sequences or pad with '0' if the sequence is short\n","    )\n","\n","# Define \"vectorize_text\" function to create the sequnce of integers\n","train_text = train_datagen.map(lambda x, y: x) # Take only x data, reviews only ; ignore y data - lables\n","vectorize_layer.adapt(train_text) #Adapt function used to convert the index of strings to integers.\n","def vectorize_text(text, label):\n","  text = tf.expand_dims(text, -1) #One extra dimention will be added at the end. Which will be later used to fill the output col\n","  return vectorize_layer(text), label #Output is the number sequence and label\n","\n","# Final Mapping on Train and Test data\n","train_df = train_datagen.map(vectorize_text)\n","test_df = test_datagen.map(vectorize_text)"],"metadata":{"id":"4-pxWLd5XnPM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model building"],"metadata":{"id":"PkVF6kLmxL4g"}},{"cell_type":"code","metadata":{"id":"In84uxCKhzkI"},"source":["embedding_dim = 16  # Lenghth of Embeddings\n","model = tf.keras.Sequential()\n","model.add(Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim))\n","# input_dim =(1 + maximum integer index occurring in the input data)\n","# output_dim= Lenghth of Embeddings\n","model.add(Dropout(0.3))\n","model.add(GlobalAveragePooling1D())\n","model.add(Dropout(0.3))\n","model.add(Dense(1))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss=losses.BinaryCrossentropy(), optimizer='adam', metrics = ['accuracy'])\n","model.fit(train_df,validation_data=test_df,epochs=10)"],"metadata":{"id":"Kk1SGCOHx0BX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DXYWL7TsRwis"},"source":["## Saving and loading the model"]},{"cell_type":"code","metadata":{"id":"6Vs35CwgRta7"},"source":["model.save_weights('Senti_model_word2Vec_10epochs.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2F3pahyjZmmY"},"source":["## Loading a pre-trained model"]},{"cell_type":"code","metadata":{"id":"eyKC1aSFRvbL"},"source":["!wget raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Saved_models/Senti_model_word2Vec_10epochs.h5\n","model.load_weights('Senti_model_word2Vec_10epochs.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p81azal-K7iG"},"source":["## The model for final prediction"]},{"cell_type":"code","metadata":{"id":"DRzvNaOJK657"},"source":["#Final model for prediction is more than simple prediction from ANN, it we need to include pre-processing also\n","final_model = tf.keras.Sequential([\n","  vectorize_layer, # Vectorization layer\n","  model, # ANN Model\n","  layers.Activation('sigmoid') # Result\n","])\n","\n","final_model.compile(loss=losses.BinaryCrossentropy(), optimizer=\"adam\", metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OX18ktkqJB-K"},"source":["## Prediction using the model"]},{"cell_type":"code","metadata":{"id":"61M1X7_HIbk-"},"source":["examples = [\n","  \"Best indian movie ever.an amazing directer SS Rajamouli made a movie in 2015 with a sequel in 2017  that managed to hit theatres all over the world and also   non indians watched it and gave good reviews.all genres are well mixed -action,drama,romance a little bit of comedy makes bahubali the best.Amazing CGI  says IN YOUR FACE to other indian movies.amazing cinematography,story,makeup and BGM.\",\n","  \"Might be an expensive movie, but acting was horrible and no plot whatsoever. Very predictable throughout. And on top of that bad direction, acting was extremely bad. Wasted three hours of my life.\",\n","  \"What a fantastic performance from all the actors especially Prabhas , putting all his effort and skill in making this fantasy come alive and yet so captivating, I love the wardrobe functions on all the actors , the elegance and pure magic put together just brings this movie to another level. The producers, directors & choreographers and all extra stunt mans have done such an amazing job , HATS OFF TO ALL OF YOU .In a nut shell Baahubali ranks top on all the  fantasy movies listed.\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vnx7hhV_J1D1"},"source":["predictions=final_model.predict(examples)\n","print([\"pos\" if i >0.5 else \"neg\" for i in predictions])"],"execution_count":null,"outputs":[]}]}