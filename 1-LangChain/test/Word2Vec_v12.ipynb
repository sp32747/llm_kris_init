{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyO0+BnhnFm8Ar/q8huccMtY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0p7JavHfFCJq"},"source":["#Importing dependencies\n","import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","import tensorflow.keras as keras\n","#!pip install gensim\n","#!pip install google.cloud\n","#import gzip\n","import gensim\n","import logging"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bag of Words"],"metadata":{"id":"ce7z4Lbw7Ocg"}},{"cell_type":"markdown","source":["## Sample Data"],"metadata":{"id":"pF5BZQzg9dpG"}},{"cell_type":"code","source":["corpus = ['king is a strong man','queen is a wise woman','boy is a young man',\n","          'girl is a young woman','prince is a young','prince will be strong',\n","          'princess is young','man is strong','woman is pretty', 'prince is a boy',\n","          'prince will be king', 'princess is a girl', 'princess will be queen']\n","print(corpus)"],"metadata":{"id":"5Pat2uIg7QIB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer()\n","DTM = cv.fit_transform(corpus)\n","DTM = pd.DataFrame(DTM.toarray(), columns=cv.get_feature_names_out())\n","DTM"],"metadata":{"id":"vnd1lAMu9igB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DTM on Review Data"],"metadata":{"id":"ritVM-BeAO-_"}},{"cell_type":"code","source":["data = {'review': ['I loved this movie!', 'It was okay.', 'I hated it.', 'It was amazing!', 'I was disappointed.',\n","                   'It was a great experience.', 'I fell asleep during the movie.', 'It was a total waste of time.',\n","                   'I highly recommend this movie.', 'I would not recommend this movie.'],\n","       'sentiment': ['positive', 'neutral', 'negative', 'positive', 'negative',\n","                      'positive', 'negative', 'negative', 'positive', 'negative']}\n","df = pd.DataFrame(data)\n","df"],"metadata":{"id":"qZNdByGJB8_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the input data into a DTM\n","cv = CountVectorizer()\n","dtm = cv.fit_transform(df['review'])\n","dtm = pd.DataFrame(dtm.toarray(), columns=cv.get_feature_names_out())\n","dtm[\"y_value\"]=df[\"sentiment\"]\n","# Print the DTM\n","dtm"],"metadata":{"id":"rFHZbyJP_Mrf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iILSKkuFecfB"},"source":["# Word Embeddings"]},{"cell_type":"code","source":["statements = [\n","\"Trees tall\",\n","\"Trees green\",\n","\"Trees majestic\",\n","\"Trees essential\",\n","\"Trees diverse\",\n","\"Trees oxygen-giving\",\n","\"computers fast\",\n","\"computers smart\",\n","\"computers useful\",\n","\"computers powerful\",\n","\"computers everywhere\",\n","\"computers changing\"\n","]"],"metadata":{"id":"vIOeAKNk8Hkb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["statements_list = []\n","for statement in statements:\n","  statements_list.append(statement.split())\n","print(statements_list)\n","from gensim.parsing.preprocessing import STOPWORDS\n","documents = [[word for word in document if word not in STOPWORDS] for document in statements_list]"],"metadata":{"id":"moeqXpWAIFQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pbjx851Kecg2"},"source":["from gensim.models import Word2Vec\n","model = Word2Vec(documents, min_count=1, vector_size=3, window = 3)\n","#size： size of word vector, hidden layer\n","#min-count：discard words that appear less than # times\n","#window：Context Window size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JoakJPqjecg8"},"source":["## Hyperparameters\n","\n","### size\n","The hidden nodes size. The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes.\n","\n","### window\n","Context window size. The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n","\n","### min_count\n","Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model."]},{"cell_type":"markdown","source":["## Checking the word2vec output"],"metadata":{"id":"-xS2-TXjOrK8"}},{"cell_type":"code","metadata":{"id":"Uhcoh2J4ecg-"},"source":["for word, vector in zip(model.wv.index_to_key, model.wv.vectors):\n","  print(word, vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","# Visualize the word vectors in 3D space using PCA\n","vectors = model.wv.vectors\n","\n","\n","fig = plt.figure(figsize=(15,10))\n","ax = plt.axes(projection='3d')\n","ax = plt.axes(projection='3d')\n","\n","xdata = vectors[:, 0]\n","ydata = vectors[:, 1]\n","zdata = vectors[:, 2]\n","names=model.wv.index_to_key\n","\n","ax.scatter3D(xdata, ydata, zdata, s=200 , c=xdata)\n","for names, x, y, z in zip(names, xdata, ydata, zdata):\n","    label = names\n","    ax.text(x, y, z, label )\n","plt.show()\n"],"metadata":{"id":"7s68kKcDLPfp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MN-IsdtbjAzC"},"source":["# Word2Vec Example-2"]},{"cell_type":"code","metadata":{"id":"RjCJIU_yi-37"},"source":["import urllib.request\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Amazon_Yelp_Reviews/Review_Data.csv\", \"Review_Data.csv\")\n","data_file=\"Review_Data.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_input(input_file):\n","    with open (input_file, 'rb') as f:\n","        for i, line in enumerate (f):\n","            # do some pre-processing and return a list of words for each review text\n","            yield gensim.utils.simple_preprocess (line)\n","            # read the tokenized reviews into a list\n","            # each review item becomes a series of words\n","            # so this becomes a list of lists\n","    print(\"File reading done !!\")\n","documents = list (read_input (data_file))"],"metadata":{"id":"fGedvZFPL29k"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQmLY_nRi-38"},"source":["print(documents)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vb39Ce9qi-39"},"source":["from gensim.models import Word2Vec\n","model = Word2Vec(documents, min_count=1, vector_size=10)\n","#size： size of word vector, hidden layer\n","#min-count：discard words that appear less than # times\n","#window：Context Window size"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the vectors for a couple of words\n","\n","for word, vector in zip(model.wv.index_to_key, model.wv.vectors):\n","  if word in [\"good\", \"bad\", \"money\"]:\n","    print(word, vector)"],"metadata":{"id":"loHMX_m_JrZC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Word2Vec Credit Reporting Casestudy\n"],"metadata":{"id":"Ib32wITb_97D"}},{"cell_type":"markdown","source":["Detailed Code explanation on [GitHub](https://github.com/venkatareddykonasani/Codes/blob/main/Word2Vec_Document_Classification.md)"],"metadata":{"id":"BCgO6C5noITr"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import requests\n","import re\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, accuracy_score"],"metadata":{"id":"EnVKURqUAF2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 1: Load the Dataset"],"metadata":{"id":"vx7rD4NDAK-K"}},{"cell_type":"code","source":["!wget https://github.com/venkatareddykonasani/Datasets/raw/master/Bank_Customer_Complaints/complaints_v2.zip\n","!unzip -o complaints_v2.zip\n","complaints_data = pd.read_csv(\"/content/complaints_v2.csv\")\n","complaints_data.head()"],"metadata":{"id":"BZUfTDM1AHIT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step2: Pre-processing"],"metadata":{"id":"SvSJecMQAu0z"}},{"cell_type":"code","source":["#lets take a sample data for building the model quickly\n","data=complaints_data.sample(frac=0.5, random_state=42)\n","print(\"Shape\", data.shape)\n","print(data['product'].value_counts())\n","#Convert all values into text\n","data['processed_text'] = data['text'].astype(str)"],"metadata":{"id":"-Ih0JnhvAJ4p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: Prepare the Data for TensorFlow"],"metadata":{"id":"NlndKvlWAnLS"}},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(data['processed_text'])\n","sequences = tokenizer.texts_to_sequences(data['processed_text'])\n","\n","max_length = 100  # Maximum length of a complaint narrative\n","X = pad_sequences(sequences, maxlen=max_length)\n","y = data['product']\n","\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)"],"metadata":{"id":"CemkKPSEAHLs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: Configure the model"],"metadata":{"id":"Qg4dYKaSBE76"}},{"cell_type":"code","source":["vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n","model = Sequential()\n","model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length))\n","model.add(GlobalAveragePooling1D())\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(len(label_encoder.classes_), activation='softmax'))"],"metadata":{"id":"yk4MDWSXAHPb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 5: Train the Model"],"metadata":{"id":"_CjnHfaZBWKb"}},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))"],"metadata":{"id":"QpJURu-KBXM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the model\n","model.save_weights('complaints_model.h5')\n","\n","#Load the saved model\n","model.load_weights('complaints_model.h5')"],"metadata":{"id":"Qh09RLpiB0FC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 6: Evaluate the Model"],"metadata":{"id":"-ZOk3KvnBhmj"}},{"cell_type":"code","source":["y_pred = np.argmax(model.predict(X_test), axis=1)\n","\n","#Confusion Matrix\n","cm= tf.math.confusion_matrix(y_test, y_pred)\n","print(cm)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy}\")\n","\n","print(\"Classification Report:\")\n","report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n","print(report)"],"metadata":{"id":"18ejxOwhBiNK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Making a prediction on new narration\n","\n","new_complaints=[\n","    \"payment history missing credit report made mistake put account forbearance without authorization \",\n","    ]\n","\n","new_sequences = tokenizer.texts_to_sequences(new_complaints)\n","new_X = pad_sequences(new_sequences, maxlen=max_length)\n","new_predictions = model.predict(new_X)\n","pred_class=np.argmax(new_predictions, axis=1)\n","print(pred_class)\n","print(\"1- Credit Reporting; 2- Credit Card; 3- Debt Collection\")"],"metadata":{"id":"iqyEZhzMCJQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wBVLM85tDGmj"},"execution_count":null,"outputs":[]}]}