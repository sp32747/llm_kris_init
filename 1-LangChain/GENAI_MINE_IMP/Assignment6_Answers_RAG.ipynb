{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ8jfpS6uCh_"
      },
      "source": [
        "#Background and Objective\n",
        "**Talk to Your Code: Interact with Your GitHub Repository Using RAG**üîçüìÇ\n",
        "\n",
        "This project involves interacting with GitHub repository files. We will take a GitHub repository as input, read all the code files, and enable users to ask questions about the repository. Using the RAG (Retrieval-Augmented Generation) technique, we will access and retrieve the relevant code based on the user's queries.\n",
        "\n",
        "\n",
        "*   Input Data- GitHub Repo\n",
        "*   User Query - Realted to code\n",
        "*   Output - Retrieved Code based on the user query\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF1-2kD-wG-g"
      },
      "source": [
        "## Example Input and Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q44B8_4cwKVN"
      },
      "source": [
        "**Input Data** - GitHub Repo Example Link - https://github.com/scikit-learn/scikit-learn\n",
        "\n",
        "**User Query**- \"Code for decision trees\"\n",
        "\n",
        "**RAG Tool Answer** -[ Screenshot](https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Sample_images/Output_Screen.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtVES_9CyXaW"
      },
      "source": [
        "##Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoV_-GxizISI"
      },
      "source": [
        "#Step-1: Importing(Clone) the GitHub Repo to Local Folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joEXiMjJKGmP"
      },
      "source": [
        "### Instructions to Write the Python Code for Cloning a GitHub Repository\n",
        "\n",
        "1. **Install the GitPython Library:**\n",
        "   - Ensure you have the `GitPython` library installed. This library allows you to interact with Git repositories from Python code. Use the following command to install it:\n",
        "\n",
        "2. **Import Required Modules:**\n",
        "   - Import the necessary modules `os` and `Repo` from the `git` library.\n",
        "\n",
        "3. **Set Up the Local Directory:**\n",
        "   - Remove any existing directory named `local_copy_repo` to avoid conflicts, and then create a new directory with the same name. This directory will be used to clone the GitHub repository.\n",
        "\n",
        "\n",
        "4. **Define Local Repository Path and Repository URL:**\n",
        "   - Specify the path where the repository will be cloned locally. Also, define the URL of the GitHub repository you want to clone. In this example, we will use the scikit-learn GitHub repository.\n",
        "\n",
        "\n",
        "5. **Clone the Repository:**\n",
        "   - Use the `Repo.clone_from` method to clone the specified GitHub repository into the defined local directory.\n",
        "     ```python\n",
        "     repo = Repo.clone_from(repo_url, local_repo_path)\n",
        "     ```\n",
        "\n",
        "6. **Verify the Cloned Repository:**\n",
        "   - List the contents of the cloned repository to ensure that it has been successfully cloned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu2uOkxn0woQ",
        "outputId": "f7cd1c53-9c6d-47d6-fa89-dc40e9ed93a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting GitPython\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, GitPython\n",
            "Successfully installed GitPython-3.1.43 gitdb-4.0.11 smmap-5.0.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from git import Repo\n",
        "\n",
        "!rm -rf local_copy_repo\n",
        "!mkdir local_copy_repo\n",
        "\n",
        "local_repo_path=\"local_copy_repo\"\n",
        "#Use scikit-learn github repo in this example\n",
        "repo_url=\"https://github.com/scikit-learn/scikit-learn\"\n",
        "\n",
        "repo = Repo.clone_from(repo_url, local_repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33kGbMzzzY-X",
        "outputId": "812d9a5d-a7fa-4505-9b9e-c449140ab85b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.mailmap', 'setup.cfg', 'CONTRIBUTING.md', '.git', 'CODE_OF_CONDUCT.md', 'README.rst', 'pyproject.toml', 'asv_benchmarks', 'examples', '.github', 'maint_tools', 'sklearn', 'build_tools', '.git-blame-ignore-revs', '.circleci', '.pre-commit-config.yaml', 'meson.build', 'COPYING', '.codecov.yml', 'azure-pipelines.yml', 'SECURITY.md', '.binder', 'Makefile', '.coveragerc', 'benchmarks', '.gitattributes', '.cirrus.star', '.gitignore', 'doc']\n"
          ]
        }
      ],
      "source": [
        "#Check the folders in the downloaded repo\n",
        "print(os.listdir(local_repo_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7moMLVh04EW"
      },
      "source": [
        "#Step-2: Load the files(Code files only)\n",
        "\n",
        "We will load all the python code files, only .py files in this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H68hQqDvKkjh"
      },
      "source": [
        "### Instructions to Write the Python Code for Loading and Parsing `.py` Files from a Specific Folder\n",
        "\n",
        "1. **Import Necessary Modules:**\n",
        "   - Import the `GenericLoader` from `langchain.document_loaders.generic` to load documents.\n",
        "   - Import the `LanguageParser` from `langchain.document_loaders.parsers` to parse Python files.\n",
        "\n",
        "2. **Specify the Path to the Target Folder:**\n",
        "   - Define the path to the folder containing the `.py` files you want to load. In this example, we will use the `examples` folder from the scikit-learn repository.\n",
        "\n",
        "3. **Initialize the Loader:**\n",
        "   - Create an instance of `GenericLoader` by specifying the path to the folder, the file suffix (in this case, `.py` for Python files), and the glob pattern to match all files recursively. Use the `LanguageParser` to parse the Python files.\n",
        "\n",
        "\n",
        "4. **Load the Documents:**\n",
        "   - Use the `load` method of the `GenericLoader` instance to load the documents from the specified folder.\n",
        "\n",
        "\n",
        "5. **Check the Number of Loaded Documents:**\n",
        "   - Verify the number of loaded documents by printing the length of the `documents` list.\n",
        "\n",
        "\n",
        "By following these steps, you will be able to load and parse `.py` files from a specified folder using the `GenericLoader` and `LanguageParser` classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTJjDn6p3TnQ",
        "outputId": "d9f6a090-3bbd-4f81-fce8-92f7bb031a42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "493"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Generic Loader for loading the .py files\n",
        "from langchain.document_loaders.generic import GenericLoader\n",
        "\n",
        "#To parse the python files\n",
        "from langchain.document_loaders.parsers import LanguageParser\n",
        "\n",
        "#We will take only one folder from the Sk-learn repo. Its huge, we will take the folder named \"local_copy_repo/examples\"\n",
        "local_repo_path_example_folder=\"/content/local_copy_repo/examples\"\n",
        "\n",
        "loader=GenericLoader.from_filesystem(local_repo_path_example_folder,\n",
        "                                      suffixes=[\".py\"],\n",
        "                                      glob=\"**/*\",\n",
        "                                      parser=LanguageParser(language=\"python\"))\n",
        "\n",
        "documents=loader.load()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pYeOc833gId"
      },
      "source": [
        "# Step-3: Split the Documents into Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FaoPAQ9LQQr"
      },
      "source": [
        "### Instructions to Write the Python Code for Splitting Python Code into Chunks\n",
        "\n",
        "1. **Import Necessary Modules:**\n",
        "   - Import the `RecursiveCharacterTextSplitter` and `Language` classes from the `langchain.text_splitter` module.\n",
        "\n",
        "2. **Initialize the Text Splitter:**\n",
        "   - Create an instance of `RecursiveCharacterTextSplitter` for the Python language. Define the `chunk_size` (number of characters per chunk) and `chunk_overlap` (number of overlapping characters between chunks).\n",
        "     ```python\n",
        "     code_text_splitter = RecursiveCharacterTextSplitter.from_language(chunk_size=3000,\n",
        "                                                                       chunk_overlap=300,\n",
        "                                                                       language=Language.PYTHON)\n",
        "     ```\n",
        "\n",
        "3. **Split the Documents into Chunks:**\n",
        "   - Use the `split_documents` method of the `code_text_splitter` instance to split the loaded documents into smaller chunks.\n",
        "\n",
        "4. **Print the Number and Content of Chunks:**\n",
        "   - Print the number of generated chunks to verify the splitting process.\n",
        "\n",
        "   - Print the entire list of chunks to see the split content.\n",
        "   - Print the first chunk to inspect its content.\n",
        "\n",
        "\n",
        "By following these steps, you will be able to split Python code documents into smaller chunks using the `RecursiveCharacterTextSplitter` class, making the content easier to manage and process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKV-G91O44po",
        "outputId": "125b0805-b2f4-406c-c3d0-de8423dfa79c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "code_chunks 898\n",
            "[Document(metadata={'source': '/content/local_copy_repo/examples/datasets/plot_iris_dataset.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================\\nThe Iris Dataset\\n================\\nThis data sets consists of 3 different types of irises\\'\\n(Setosa, Versicolour, and Virginica) petal and sepal\\nlength, stored in a 150x4 numpy.ndarray\\n\\nThe rows being the samples and the columns being:\\nSepal Length, Sepal Width, Petal Length and Petal Width.\\n\\nThe below plot uses the first two features.\\nSee `here <https://en.wikipedia.org/wiki/Iris_flower_data_set>`_ for more\\ninformation on this dataset.\\n\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# Modified for documentation by Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Loading the iris dataset\\n# ------------------------\\nfrom sklearn import datasets\\n\\niris = datasets.load_iris()\\n\\n\\n# %%\\n# Scatter Plot of the Iris dataset\\n# --------------------------------\\nimport matplotlib.pyplot as plt\\n\\n_, ax = plt.subplots()\\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\\n_ = ax.legend(\\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\\n)\\n\\n# %%\\n# Each point in the scatter plot refers to one of the 150 iris flowers\\n# in the dataset, with the color indicating their respective type\\n# (Setosa, Versicolour, and Virginica).\\n# You can already see a pattern regarding the Setosa type, which is\\n# easily identifiable based on its short and wide sepal. Only\\n# considering these 2 dimensions, sepal width and length, there\\'s still\\n# overlap between the Versicolor and Virginica types.\\n\\n# %%\\n# Plot a PCA representation\\n# -------------------------\\n# Let\\'s apply a Principal Component Analysis (PCA) to the iris dataset\\n# and then plot the irises across the first three PCA dimensions.\\n# This will allow us to better differentiate between the three types!\\n\\n# unused but required import for doing 3d projections with matplotlib < 3.2\\nimport mpl_toolkits.mplot3d  # noqa: F401\\n\\nfrom sklearn.decomposition import PCA\\n\\nfig = plt.figure(1, figsize=(8, 6))\\nax = fig.add_subplot(111, projection=\"3d\", elev=-150, azim=110)\\n\\nX_reduced = PCA(n_components=3).fit_transform(iris.data)\\nax.scatter(\\n    X_reduced[:, 0],\\n    X_reduced[:, 1],\\n    X_reduced[:, 2],\\n    c=iris.target,\\n    s=40,\\n)\\n\\nax.set_title(\"First three PCA dimensions\")\\nax.set_xlabel(\"1st Eigenvector\")\\nax.xaxis.set_ticklabels([])\\nax.set_ylabel(\"2nd Eigenvector\")\\nax.yaxis.set_ticklabels([])\\nax.set_zlabel(\"3rd Eigenvector\")\\nax.zaxis.set_ticklabels([])\\n\\nplt.show()\\n\\n# %%\\n# PCA will create 3 new features that are a linear combination of the\\n# 4 original features. In addition, this transform maximizes the variance.\\n# With this transformation, we see that we can identify each species using\\n# only the first feature (i.e. first eigenvalues).'), Document(metadata={'source': '/content/local_copy_repo/examples/datasets/plot_random_dataset.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================\\nPlot randomly generated classification dataset\\n==============================================\\n\\nThis example plots several randomly generated classification datasets.\\nFor easy visualization, all datasets have 2 features, plotted on the x and y\\naxis. The color of each point represents its class label.\\n\\nThe first 4 plots use the :func:`~sklearn.datasets.make_classification` with\\ndifferent numbers of informative features, clusters per class and classes.\\nThe final 2 plots use :func:`~sklearn.datasets.make_blobs` and\\n:func:`~sklearn.datasets.make_gaussian_quantiles`.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles\\n\\nplt.figure(figsize=(8, 8))\\nplt.subplots_adjust(bottom=0.05, top=0.9, left=0.05, right=0.95)\\n\\nplt.subplot(321)\\nplt.title(\"One informative feature, one cluster per class\", fontsize=\"small\")\\nX1, Y1 = make_classification(\\n    n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1\\n)\\nplt.scatter(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\\n\\nplt.subplot(322)\\nplt.title(\"Two informative features, one cluster per class\", fontsize=\"small\")\\nX1, Y1 = make_classification(\\n    n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1\\n)\\nplt.scatter(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\\n\\nplt.subplot(323)\\nplt.title(\"Two informative features, two clusters per class\", fontsize=\"small\")\\nX2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)\\nplt.scatter(X2[:, 0], X2[:, 1], marker=\"o\", c=Y2, s=25, edgecolor=\"k\")\\n\\nplt.subplot(324)\\nplt.title(\"Multi-class, two informative features, one cluster\", fontsize=\"small\")\\nX1, Y1 = make_classification(\\n    n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3\\n)\\nplt.scatter(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\\n\\nplt.subplot(325)\\nplt.title(\"Three blobs\", fontsize=\"small\")\\nX1, Y1 = make_blobs(n_features=2, centers=3)\\nplt.scatter(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\\n\\nplt.subplot(326)\\nplt.title(\"Gaussian divided into three quantiles\", fontsize=\"small\")\\nX1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)\\nplt.scatter(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/datasets/plot_digits_last_image.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nThe Digit Dataset\\n=========================================================\\n\\nThis dataset is made up of 1797 8x8 images. Each image,\\nlike the one shown below, is of a hand-written digit.\\nIn order to utilize an 8x8 figure like this, we\\'d have to\\nfirst transform it into a feature vector with length 64.\\n\\nSee `here\\n<https://archive.ics.uci.edu/dataset/81/pen+based+recognition+of+handwritten+digits>`_\\nfor more information about this dataset.\\n\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# Modified for documentation by Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import datasets\\n\\n# Load the digits dataset\\ndigits = datasets.load_digits()\\n\\n# Display the last digit\\nplt.figure(1, figsize=(3, 3))\\nplt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation=\"nearest\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/datasets/plot_random_multilabel_dataset.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_2d(ax, n_labels=1, n_classes=3, length=50):\\n    X, Y, p_c, p_w_c = make_ml_clf(\\n        n_samples=150,\\n        n_features=2,\\n        n_classes=n_classes,\\n        n_labels=n_labels,\\n        length=length,\\n        allow_unlabeled=False,\\n        return_distributions=True,\\n        random_state=RANDOM_SEED,\\n    )\\n\\n    ax.scatter(\\n        X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]).sum(axis=1)), marker=\".\"\\n    )\\n    ax.scatter(\\n        p_w_c[0] * length,\\n        p_w_c[1] * length,\\n        marker=\"*\",\\n        linewidth=0.5,\\n        edgecolor=\"black\",\\n        s=20 + 1500 * p_c**2,\\n        color=COLORS.take([1, 2, 4]),\\n    )\\n    ax.set_xlabel(\"Feature 0 count\")\\n    return p_c, p_w_c'), Document(metadata={'source': '/content/local_copy_repo/examples/datasets/plot_random_multilabel_dataset.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================\\nPlot randomly generated multilabel dataset\\n==============================================\\n\\nThis illustrates the :func:`~sklearn.datasets.make_multilabel_classification`\\ndataset generator. Each sample consists of counts of two features (up to 50 in\\ntotal), which are differently distributed in each of two classes.\\n\\nPoints are labeled as follows, where Y means the class is present:\\n\\n    =====  =====  =====  ======\\n      1      2      3    Color\\n    =====  =====  =====  ======\\n      Y      N      N    Red\\n      N      Y      N    Blue\\n      N      N      Y    Yellow\\n      Y      Y      N    Purple\\n      Y      N      Y    Orange\\n      Y      Y      N    Green\\n      Y      Y      Y    Brown\\n    =====  =====  =====  ======\\n\\nA star marks the expected sample for each class; its size reflects the\\nprobability of selecting that class label.\\n\\nThe left and right examples highlight the ``n_labels`` parameter:\\nmore of the samples in the right plot have 2 or 3 labels.\\n\\nNote that this two-dimensional example is very degenerate:\\ngenerally the number of features would be much greater than the\\n\"document length\", while here we have much larger documents than vocabulary.\\nSimilarly, with ``n_classes > n_features``, it is much less likely that a\\nfeature distinguishes a particular class.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_multilabel_classification as make_ml_clf\\n\\nCOLORS = np.array(\\n    [\\n        \"!\",\\n        \"#FF3333\",  # red\\n        \"#0198E1\",  # blue\\n        \"#BF5FFF\",  # purple\\n        \"#FCD116\",  # yellow\\n        \"#FF7216\",  # orange\\n        \"#4DBD33\",  # green\\n        \"#87421F\",  # brown\\n    ]\\n)\\n\\n# Use same random seed for multiple calls to make_multilabel_classification to\\n# ensure same distributions\\nRANDOM_SEED = np.random.randint(2**10)\\n\\n\\n# Code for: def plot_2d(ax, n_labels=1, n_classes=3, length=50):\\n\\n\\n_, (ax1, ax2) = plt.subplots(1, 2, sharex=\"row\", sharey=\"row\", figsize=(8, 4))\\nplt.subplots_adjust(bottom=0.15)\\n\\np_c, p_w_c = plot_2d(ax1, n_labels=1)\\nax1.set_title(\"n_labels=1, length=50\")\\nax1.set_ylabel(\"Feature 1 count\")\\n\\nplot_2d(ax2, n_labels=3)\\nax2.set_title(\"n_labels=3, length=50\")\\nax2.set_xlim(left=0, auto=True)\\nax2.set_ylim(bottom=0, auto=True)\\n\\nplt.show()\\n\\nprint(\"The data was generated from (random_state=%d):\" % RANDOM_SEED)\\nprint(\"Class\", \"P(C)\", \"P(w0|C)\", \"P(w1|C)\", sep=\"\\\\t\")\\nfor k, p, p_w in zip([\"red\", \"blue\", \"yellow\"], p_c, p_w_c.T):\\n    print(\"%s\\\\t%0.2f\\\\t%0.2f\\\\t%0.2f\" % (k, p, p_w[0], p_w[1]))'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================\\nProbability calibration of classifiers\\n======================================\\n\\nWhen performing classification you often want to predict not only\\nthe class label, but also the associated probability. This probability\\ngives you some kind of confidence on the prediction. However, not all\\nclassifiers provide well-calibrated probabilities, some being over-confident\\nwhile others being under-confident. Thus, a separate calibration of predicted\\nprobabilities is often desirable as a postprocessing. This example illustrates\\ntwo different methods for this calibration and evaluates the quality of the\\nreturned probabilities using Brier\\'s score\\n(see https://en.wikipedia.org/wiki/Brier_score).\\n\\nCompared are the estimated probability using a Gaussian naive Bayes classifier\\nwithout calibration, with a sigmoid calibration, and with a non-parametric\\nisotonic calibration. One can observe that only the non-parametric model is\\nable to provide a probability calibration that returns probabilities close\\nto the expected 0.5 for most of the samples belonging to the middle\\ncluster with heterogeneous labels. This results in a significantly improved\\nBrier score.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate synthetic dataset\\n# --------------------------\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.model_selection import train_test_split\\n\\nn_samples = 50000\\nn_bins = 3  # use 3 bins for calibration_curve as we have 3 clusters here\\n\\n# Generate 3 blobs with 2 classes where the second blob contains\\n# half positive samples and half negative samples. Probability in this\\n# blob is therefore 0.5.\\ncenters = [(-5, -5), (0, 0), (5, 5)]\\nX, y = make_blobs(n_samples=n_samples, centers=centers, shuffle=False, random_state=42)\\n\\ny[: n_samples // 2] = 0\\ny[n_samples // 2 :] = 1\\nsample_weight = np.random.RandomState(42).rand(y.shape[0])\\n\\n# split train, test for calibration\\nX_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\\n    X, y, sample_weight, test_size=0.9, random_state=42\\n)\\n\\n# %%\\n# Gaussian Naive-Bayes\\n# --------------------\\nfrom sklearn.calibration import CalibratedClassifierCV\\nfrom sklearn.metrics import brier_score_loss\\nfrom sklearn.naive_bayes import GaussianNB\\n\\n# With no calibration\\nclf = GaussianNB()\\nclf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights\\nprob_pos_clf = clf.predict_proba(X_test)[:, 1]\\n\\n# With isotonic calibration\\nclf_isotonic = CalibratedClassifierCV(clf, cv=2, method=\"isotonic\")\\nclf_isotonic.fit(X_train, y_train, sample_weight=sw_train)\\nprob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]\\n\\n# With sigmoid calibration\\nclf_sigmoid = CalibratedClassifierCV(clf, cv=2, method=\"sigmoid\")\\nclf_sigmoid.fit(X_train, y_train, sample_weight=sw_train)\\nprob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]\\n\\nprint(\"Brier score losses: (the smaller the better)\")'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# With sigmoid calibration\\nclf_sigmoid = CalibratedClassifierCV(clf, cv=2, method=\"sigmoid\")\\nclf_sigmoid.fit(X_train, y_train, sample_weight=sw_train)\\nprob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]\\n\\nprint(\"Brier score losses: (the smaller the better)\")\\n\\nclf_score = brier_score_loss(y_test, prob_pos_clf, sample_weight=sw_test)\\nprint(\"No calibration: %1.3f\" % clf_score)\\n\\nclf_isotonic_score = brier_score_loss(y_test, prob_pos_isotonic, sample_weight=sw_test)\\nprint(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\\n\\nclf_sigmoid_score = brier_score_loss(y_test, prob_pos_sigmoid, sample_weight=sw_test)\\nprint(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\\n\\n# %%\\n# Plot data and the predicted probabilities\\n# -----------------------------------------\\nimport matplotlib.pyplot as plt\\nfrom matplotlib import cm\\n\\nplt.figure()\\ny_unique = np.unique(y)\\ncolors = cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\\nfor this_y, color in zip(y_unique, colors):\\n    this_X = X_train[y_train == this_y]\\n    this_sw = sw_train[y_train == this_y]\\n    plt.scatter(\\n        this_X[:, 0],\\n        this_X[:, 1],\\n        s=this_sw * 50,\\n        c=color[np.newaxis, :],\\n        alpha=0.5,\\n        edgecolor=\"k\",\\n        label=\"Class %s\" % this_y,\\n    )\\nplt.legend(loc=\"best\")\\nplt.title(\"Data\")\\n\\nplt.figure()\\n\\norder = np.lexsort((prob_pos_clf,))\\nplt.plot(prob_pos_clf[order], \"r\", label=\"No calibration (%1.3f)\" % clf_score)\\nplt.plot(\\n    prob_pos_isotonic[order],\\n    \"g\",\\n    linewidth=3,\\n    label=\"Isotonic calibration (%1.3f)\" % clf_isotonic_score,\\n)\\nplt.plot(\\n    prob_pos_sigmoid[order],\\n    \"b\",\\n    linewidth=3,\\n    label=\"Sigmoid calibration (%1.3f)\" % clf_sigmoid_score,\\n)\\nplt.plot(\\n    np.linspace(0, y_test.size, 51)[1::2],\\n    y_test[order].reshape(25, -1).mean(1),\\n    \"k\",\\n    linewidth=3,\\n    label=r\"Empirical\",\\n)\\nplt.ylim([-0.05, 1.05])\\nplt.xlabel(\"Instances sorted according to predicted probability (uncalibrated GNB)\")\\nplt.ylabel(\"P(y=1)\")\\nplt.legend(loc=\"upper left\")\\nplt.title(\"Gaussian naive Bayes probabilities\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_compare_calibration.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class NaivelyCalibratedLinearSVC(LinearSVC):\\n    \"\"\"LinearSVC with `predict_proba` method that naively scales\\n    `decision_function` output.\"\"\"\\n\\n    def fit(self, X, y):\\n        super().fit(X, y)\\n        df = self.decision_function(X)\\n        self.df_min_ = df.min()\\n        self.df_max_ = df.max()\\n\\n    def predict_proba(self, X):\\n        \"\"\"Min-max scale output of `decision_function` to [0,1].\"\"\"\\n        df = self.decision_function(X)\\n        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)\\n        proba_pos_class = np.clip(calibrated_df, 0, 1)\\n        proba_neg_class = 1 - proba_pos_class\\n        proba = np.c_[proba_neg_class, proba_pos_class]\\n        return proba'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_compare_calibration.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n========================================\\nComparison of Calibration of Classifiers\\n========================================\\n\\nWell calibrated classifiers are probabilistic classifiers for which the output\\nof :term:`predict_proba` can be directly interpreted as a confidence level.\\nFor instance, a well calibrated (binary) classifier should classify the samples\\nsuch that for the samples to which it gave a :term:`predict_proba` value close\\nto 0.8, approximately 80% actually belong to the positive class.\\n\\nIn this example we will compare the calibration of four different\\nmodels: :ref:`Logistic_regression`, :ref:`gaussian_naive_bayes`,\\n:ref:`Random Forest Classifier <forest>` and :ref:`Linear SVM\\n<svm_classification>`.\\n\\n\"\"\"\\n\\n# %%\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n#\\n# Dataset\\n# -------\\n#\\n# We will use a synthetic binary classification dataset with 100,000 samples\\n# and 20 features. Of the 20 features, only 2 are informative, 2 are\\n# redundant (random combinations of the informative features) and the\\n# remaining 16 are uninformative (random numbers).\\n#\\n# Of the 100,000 samples, 100 will be used for model fitting and the remaining\\n# for testing. Note that this split is quite unusual: the goal is to obtain\\n# stable calibration curve estimates for models that are potentially prone to\\n# overfitting. In practice, one should rather use cross-validation with more\\n# balanced splits but this would make the code of this example more complicated\\n# to follow.\\n\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = make_classification(\\n    n_samples=100_000, n_features=20, n_informative=2, n_redundant=2, random_state=42\\n)\\n\\ntrain_samples = 100  # Samples used for training the models\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X,\\n    y,\\n    shuffle=False,\\n    test_size=100_000 - train_samples,\\n)\\n\\n# %%\\n# Calibration curves\\n# ------------------\\n#\\n# Below, we train each of the four models with the small training dataset, then\\n# plot calibration curves (also known as reliability diagrams) using\\n# predicted probabilities of the test dataset. Calibration curves are created\\n# by binning predicted probabilities, then plotting the mean predicted\\n# probability in each bin against the observed frequency (\\'fraction of\\n# positives\\'). Below the calibration curve, we plot a histogram showing\\n# the distribution of the predicted probabilities or more specifically,\\n# the number of samples in each predicted probability bin.\\n\\nimport numpy as np\\n\\nfrom sklearn.svm import LinearSVC\\n\\n\\n# Code for: class NaivelyCalibratedLinearSVC(LinearSVC):\\n\\n\\n# %%\\n\\nfrom sklearn.calibration import CalibrationDisplay\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.linear_model import LogisticRegressionCV\\nfrom sklearn.naive_bayes import GaussianNB'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_compare_calibration.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: class NaivelyCalibratedLinearSVC(LinearSVC):\\n\\n\\n# %%\\n\\nfrom sklearn.calibration import CalibrationDisplay\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.linear_model import LogisticRegressionCV\\nfrom sklearn.naive_bayes import GaussianNB\\n\\n# Define the classifiers to be compared in the study.\\n#\\n# Note that we use a variant of the logistic regression model that can\\n# automatically tune its regularization parameter.\\n#\\n# For a fair comparison, we should run a hyper-parameter search for all the\\n# classifiers but we don\\'t do it here for the sake of keeping the example code\\n# concise and fast to execute.\\nlr = LogisticRegressionCV(\\n    Cs=np.logspace(-6, 6, 101), cv=10, scoring=\"neg_log_loss\", max_iter=1_000\\n)\\ngnb = GaussianNB()\\nsvc = NaivelyCalibratedLinearSVC(C=1.0)\\nrfc = RandomForestClassifier(random_state=42)\\n\\nclf_list = [\\n    (lr, \"Logistic Regression\"),\\n    (gnb, \"Naive Bayes\"),\\n    (svc, \"SVC\"),\\n    (rfc, \"Random forest\"),\\n]\\n\\n# %%\\n\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.gridspec import GridSpec\\n\\nfig = plt.figure(figsize=(10, 10))\\ngs = GridSpec(4, 2)\\ncolors = plt.get_cmap(\"Dark2\")\\n\\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\\ncalibration_displays = {}\\nmarkers = [\"^\", \"v\", \"s\", \"o\"]\\nfor i, (clf, name) in enumerate(clf_list):\\n    clf.fit(X_train, y_train)\\n    display = CalibrationDisplay.from_estimator(\\n        clf,\\n        X_test,\\n        y_test,\\n        n_bins=10,\\n        name=name,\\n        ax=ax_calibration_curve,\\n        color=colors(i),\\n        marker=markers[i],\\n    )\\n    calibration_displays[name] = display\\n\\nax_calibration_curve.grid()\\nax_calibration_curve.set_title(\"Calibration plots\")\\n\\n# Add histogram\\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\\nfor i, (_, name) in enumerate(clf_list):\\n    row, col = grid_positions[i]\\n    ax = fig.add_subplot(gs[row, col])\\n\\n    ax.hist(\\n        calibration_displays[name].y_prob,\\n        range=(0, 1),\\n        bins=10,\\n        label=name,\\n        color=colors(i),\\n    )\\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_compare_calibration.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"# %%\\n#\\n# Analysis of the results\\n# -----------------------\\n#\\n# :class:`~sklearn.linear_model.LogisticRegressionCV` returns reasonably well\\n# calibrated predictions despite the small training set size: its reliability\\n# curve is the closest to the diagonal among the four models.\\n#\\n# Logistic regression is trained by minimizing the log-loss which is a strictly\\n# proper scoring rule: in the limit of infinite training data, strictly proper\\n# scoring rules are minimized by the model that predicts the true conditional\\n# probabilities. That (hypothetical) model would therefore be perfectly\\n# calibrated. However, using a proper scoring rule as training objective is not\\n# sufficient to guarantee a well-calibrated model by itself: even with a very\\n# large training set, logistic regression could still be poorly calibrated, if\\n# it was too strongly regularized or if the choice and preprocessing of input\\n# features made this model mis-specified (e.g. if the true decision boundary of\\n# the dataset is a highly non-linear function of the input features).\\n#\\n# In this example the training set was intentionally kept very small. In this\\n# setting, optimizing the log-loss can still lead to poorly calibrated models\\n# because of overfitting. To mitigate this, the\\n# :class:`~sklearn.linear_model.LogisticRegressionCV` class was configured to\\n# tune the `C` regularization parameter to also minimize the log-loss via inner\\n# cross-validation so as to find the best compromise for this model in the\\n# small training set setting.\\n#\\n# Because of the finite training set size and the lack of guarantee for\\n# well-specification, we observe that the calibration curve of the logistic\\n# regression model is close but not perfectly on the diagonal. The shape of the\\n# calibration curve of this model can be interpreted as slightly\\n# under-confident: the predicted probabilities are a bit too close to 0.5\\n# compared to the true fraction of positive samples.\\n#\\n# The other methods all output less well calibrated probabilities:\\n#\\n# * :class:`~sklearn.naive_bayes.GaussianNB` tends to push probabilities to 0\\n#   or 1 (see histogram) on this particular dataset (over-confidence). This is\\n#   mainly because the naive Bayes equation only provides correct estimate of\\n#   probabilities when the assumption that features are conditionally\\n#   independent holds [2]_. However, features can be correlated and this is the case\\n#   with this dataset, which contains 2 features generated as random linear\\n#   combinations of the informative features. These correlated features are\\n#   effectively being 'counted twice', resulting in pushing the predicted\\n#   probabilities towards 0 and 1 [3]_. Note, however, that changing the seed\\n#   used to generate the dataset can lead to widely varying results for the\\n#   naive Bayes estimator.\\n#\\n# * :class:`~sklearn.svm.LinearSVC` is not a natural probabilistic classifier.\\n#   In order to interpret its prediction as such, we naively scaled the output\"), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_compare_calibration.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='#   used to generate the dataset can lead to widely varying results for the\\n#   naive Bayes estimator.\\n#\\n# * :class:`~sklearn.svm.LinearSVC` is not a natural probabilistic classifier.\\n#   In order to interpret its prediction as such, we naively scaled the output\\n#   of the :term:`decision_function` into [0, 1] by applying min-max scaling in\\n#   the `NaivelyCalibratedLinearSVC` wrapper class defined above. This\\n#   estimator shows a typical sigmoid-shaped calibration curve on this data:\\n#   predictions larger than 0.5 correspond to samples with an even larger\\n#   effective positive class fraction (above the diagonal), while predictions\\n#   below 0.5 corresponds to even lower positive class fractions (below the\\n#   diagonal). This under-confident predictions are typical for maximum-margin\\n#   methods [1]_.\\n#\\n# * :class:`~sklearn.ensemble.RandomForestClassifier`\\'s prediction histogram\\n#   shows peaks at approx. 0.2 and 0.9 probability, while probabilities close to\\n#   0 or 1 are very rare. An explanation for this is given by [1]_:\\n#   \"Methods such as bagging and random forests that average\\n#   predictions from a base set of models can have difficulty making\\n#   predictions near 0 and 1 because variance in the underlying base models\\n#   will bias predictions that should be near zero or one away from these\\n#   values. Because predictions are restricted to the interval [0, 1], errors\\n#   caused by variance tend to be one-sided near zero and one. For example, if\\n#   a model should predict p = 0 for a case, the only way bagging can achieve\\n#   this is if all bagged trees predict zero. If we add noise to the trees that\\n#   bagging is averaging over, this noise will cause some trees to predict\\n#   values larger than 0 for this case, thus moving the average prediction of\\n#   the bagged ensemble away from 0. We observe this effect most strongly with\\n#   random forests because the base-level trees trained with random forests\\n#   have relatively high variance due to feature subsetting.\" This effect can\\n#   make random forests under-confident. Despite this possible bias, note that\\n#   the trees themselves are fit by minimizing either the Gini or Entropy\\n#   criterion, both of which lead to splits that minimize proper scoring rules:\\n#   the Brier score or the log-loss respectively. See :ref:`the user guide\\n#   <tree_mathematical_formulation>` for more details. This can explain why\\n#   this model shows a good enough calibration curve on this particular example\\n#   dataset. Indeed the Random Forest model is not significantly more\\n#   under-confident than the Logistic Regression model.\\n#\\n# Feel free to re-run this example with different random seeds and other\\n# dataset generation parameters to see how different the calibration plots can\\n# look. In general, Logistic Regression and Random Forest will tend to be the\\n# best calibrated classifiers, while SVC will often display the typical\\n# under-confident miscalibration. The naive Bayes model is also often poorly'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_compare_calibration.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# look. In general, Logistic Regression and Random Forest will tend to be the\\n# best calibrated classifiers, while SVC will often display the typical\\n# under-confident miscalibration. The naive Bayes model is also often poorly\\n# calibrated but the general shape of its calibration curve can vary widely\\n# depending on the dataset.\\n#\\n# Finally, note that for some dataset seeds, all models are poorly calibrated,\\n# even when tuning the regularization parameter as above. This is bound to\\n# happen when the training size is too small or when the model is severely\\n# misspecified.\\n#\\n# References\\n# ----------\\n#\\n# .. [1] `Predicting Good Probabilities with Supervised Learning\\n#        <https://dl.acm.org/doi/pdf/10.1145/1102351.1102430>`_, A.\\n#        Niculescu-Mizil & R. Caruana, ICML 2005\\n#\\n# .. [2] `Beyond independence: Conditions for the optimality of the simple\\n#        bayesian classifier\\n#        <https://www.ics.uci.edu/~pazzani/Publications/mlc96-pedro.pdf>`_\\n#        Domingos, P., & Pazzani, M., Proc. 13th Intl. Conf. Machine Learning.\\n#        1996.\\n#\\n# .. [3] `Obtaining calibrated probability estimates from decision trees and\\n#        naive Bayesian classifiers\\n#        <https://citeseerx.ist.psu.edu/doc_view/pid/4f67a122ec3723f08ad5cbefecad119b432b3304>`_\\n#        Zadrozny, Bianca, and Charles Elkan. Icml. Vol. 1. 2001.'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_curve.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class NaivelyCalibratedLinearSVC(LinearSVC):\\n    \"\"\"LinearSVC with `predict_proba` method that naively scales\\n    `decision_function` output for binary classification.\"\"\"\\n\\n    def fit(self, X, y):\\n        super().fit(X, y)\\n        df = self.decision_function(X)\\n        self.df_min_ = df.min()\\n        self.df_max_ = df.max()\\n\\n    def predict_proba(self, X):\\n        \"\"\"Min-max scale output of `decision_function` to [0, 1].\"\"\"\\n        df = self.decision_function(X)\\n        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)\\n        proba_pos_class = np.clip(calibrated_df, 0, 1)\\n        proba_neg_class = 1 - proba_pos_class\\n        proba = np.c_[proba_neg_class, proba_pos_class]\\n        return proba'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_curve.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================\\nProbability Calibration curves\\n==============================\\n\\nWhen performing classification one often wants to predict not only the class\\nlabel, but also the associated probability. This probability gives some\\nkind of confidence on the prediction. This example demonstrates how to\\nvisualize how well calibrated the predicted probabilities are using calibration\\ncurves, also known as reliability diagrams. Calibration of an uncalibrated\\nclassifier will also be demonstrated.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n# %%\\n# Dataset\\n# -------\\n#\\n# We will use a synthetic binary classification dataset with 100,000 samples\\n# and 20 features. Of the 20 features, only 2 are informative, 10 are\\n# redundant (random combinations of the informative features) and the\\n# remaining 8 are uninformative (random numbers). Of the 100,000 samples, 1,000\\n# will be used for model fitting and the rest for testing.\\n\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = make_classification(\\n    n_samples=100_000, n_features=20, n_informative=2, n_redundant=10, random_state=42\\n)\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.99, random_state=42\\n)\\n\\n# %%\\n# Calibration curves\\n# ------------------\\n#\\n# Gaussian Naive Bayes\\n# ^^^^^^^^^^^^^^^^^^^^\\n#\\n# First, we will compare:\\n#\\n# * :class:`~sklearn.linear_model.LogisticRegression` (used as baseline\\n#   since very often, properly regularized logistic regression is well\\n#   calibrated by default thanks to the use of the log-loss)\\n# * Uncalibrated :class:`~sklearn.naive_bayes.GaussianNB`\\n# * :class:`~sklearn.naive_bayes.GaussianNB` with isotonic and sigmoid\\n#   calibration (see :ref:`User Guide <calibration>`)\\n#\\n# Calibration curves for all 4 conditions are plotted below, with the average\\n# predicted probability for each bin on the x-axis and the fraction of positive\\n# classes in each bin on the y-axis.\\n\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.gridspec import GridSpec\\n\\nfrom sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.naive_bayes import GaussianNB\\n\\nlr = LogisticRegression(C=1.0)\\ngnb = GaussianNB()\\ngnb_isotonic = CalibratedClassifierCV(gnb, cv=2, method=\"isotonic\")\\ngnb_sigmoid = CalibratedClassifierCV(gnb, cv=2, method=\"sigmoid\")\\n\\nclf_list = [\\n    (lr, \"Logistic\"),\\n    (gnb, \"Naive Bayes\"),\\n    (gnb_isotonic, \"Naive Bayes + Isotonic\"),\\n    (gnb_sigmoid, \"Naive Bayes + Sigmoid\"),\\n]\\n\\n# %%\\nfig = plt.figure(figsize=(10, 10))\\ngs = GridSpec(4, 2)\\ncolors = plt.get_cmap(\"Dark2\")'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_curve.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='clf_list = [\\n    (lr, \"Logistic\"),\\n    (gnb, \"Naive Bayes\"),\\n    (gnb_isotonic, \"Naive Bayes + Isotonic\"),\\n    (gnb_sigmoid, \"Naive Bayes + Sigmoid\"),\\n]\\n\\n# %%\\nfig = plt.figure(figsize=(10, 10))\\ngs = GridSpec(4, 2)\\ncolors = plt.get_cmap(\"Dark2\")\\n\\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\\ncalibration_displays = {}\\nfor i, (clf, name) in enumerate(clf_list):\\n    clf.fit(X_train, y_train)\\n    display = CalibrationDisplay.from_estimator(\\n        clf,\\n        X_test,\\n        y_test,\\n        n_bins=10,\\n        name=name,\\n        ax=ax_calibration_curve,\\n        color=colors(i),\\n    )\\n    calibration_displays[name] = display\\n\\nax_calibration_curve.grid()\\nax_calibration_curve.set_title(\"Calibration plots (Naive Bayes)\")\\n\\n# Add histogram\\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\\nfor i, (_, name) in enumerate(clf_list):\\n    row, col = grid_positions[i]\\n    ax = fig.add_subplot(gs[row, col])\\n\\n    ax.hist(\\n        calibration_displays[name].y_prob,\\n        range=(0, 1),\\n        bins=10,\\n        label=name,\\n        color=colors(i),\\n    )\\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# %%\\n# Uncalibrated :class:`~sklearn.naive_bayes.GaussianNB` is poorly calibrated\\n# because of\\n# the redundant features which violate the assumption of feature-independence\\n# and result in an overly confident classifier, which is indicated by the\\n# typical transposed-sigmoid curve. Calibration of the probabilities of\\n# :class:`~sklearn.naive_bayes.GaussianNB` with :ref:`isotonic` can fix\\n# this issue as can be seen from the nearly diagonal calibration curve.\\n# :ref:`Sigmoid regression <sigmoid_regressor>` also improves calibration\\n# slightly,\\n# albeit not as strongly as the non-parametric isotonic regression. This can be\\n# attributed to the fact that we have plenty of calibration data such that the\\n# greater flexibility of the non-parametric model can be exploited.\\n#\\n# Below we will make a quantitative analysis considering several classification\\n# metrics: :ref:`brier_score_loss`, :ref:`log_loss`,\\n# :ref:`precision, recall, F1 score <precision_recall_f_measure_metrics>` and\\n# :ref:`ROC AUC <roc_metrics>`.\\n\\nfrom collections import defaultdict\\n\\nimport pandas as pd\\n\\nfrom sklearn.metrics import (\\n    brier_score_loss,\\n    f1_score,\\n    log_loss,\\n    precision_score,\\n    recall_score,\\n    roc_auc_score,\\n)\\n\\nscores = defaultdict(list)\\nfor i, (clf, name) in enumerate(clf_list):\\n    clf.fit(X_train, y_train)\\n    y_prob = clf.predict_proba(X_test)\\n    y_pred = clf.predict(X_test)\\n    scores[\"Classifier\"].append(name)\\n\\n    for metric in [brier_score_loss, log_loss, roc_auc_score]:\\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_curve.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for metric in [brier_score_loss, log_loss, roc_auc_score]:\\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\\n\\n    for metric in [precision_score, recall_score, f1_score]:\\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\\n        scores[score_name].append(metric(y_test, y_pred))\\n\\n    score_df = pd.DataFrame(scores).set_index(\"Classifier\")\\n    score_df.round(decimals=3)\\n\\nscore_df\\n\\n# %%\\n# Notice that although calibration improves the :ref:`brier_score_loss` (a\\n# metric composed\\n# of calibration term and refinement term) and :ref:`log_loss`, it does not\\n# significantly alter the prediction accuracy measures (precision, recall and\\n# F1 score).\\n# This is because calibration should not significantly change prediction\\n# probabilities at the location of the decision threshold (at x = 0.5 on the\\n# graph). Calibration should however, make the predicted probabilities more\\n# accurate and thus more useful for making allocation decisions under\\n# uncertainty.\\n# Further, ROC AUC, should not change at all because calibration is a\\n# monotonic transformation. Indeed, no rank metrics are affected by\\n# calibration.\\n#\\n# Linear support vector classifier\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n# Next, we will compare:\\n#\\n# * :class:`~sklearn.linear_model.LogisticRegression` (baseline)\\n# * Uncalibrated :class:`~sklearn.svm.LinearSVC`. Since SVC does not output\\n#   probabilities by default, we naively scale the output of the\\n#   :term:`decision_function` into [0, 1] by applying min-max scaling.\\n# * :class:`~sklearn.svm.LinearSVC` with isotonic and sigmoid\\n#   calibration (see :ref:`User Guide <calibration>`)\\n\\nimport numpy as np\\n\\nfrom sklearn.svm import LinearSVC\\n\\n\\n# Code for: class NaivelyCalibratedLinearSVC(LinearSVC):\\n\\n\\n# %%\\n\\nlr = LogisticRegression(C=1.0)\\nsvc = NaivelyCalibratedLinearSVC(max_iter=10_000)\\nsvc_isotonic = CalibratedClassifierCV(svc, cv=2, method=\"isotonic\")\\nsvc_sigmoid = CalibratedClassifierCV(svc, cv=2, method=\"sigmoid\")\\n\\nclf_list = [\\n    (lr, \"Logistic\"),\\n    (svc, \"SVC\"),\\n    (svc_isotonic, \"SVC + Isotonic\"),\\n    (svc_sigmoid, \"SVC + Sigmoid\"),\\n]\\n\\n# %%\\nfig = plt.figure(figsize=(10, 10))\\ngs = GridSpec(4, 2)\\n\\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\\ncalibration_displays = {}\\nfor i, (clf, name) in enumerate(clf_list):\\n    clf.fit(X_train, y_train)\\n    display = CalibrationDisplay.from_estimator(\\n        clf,\\n        X_test,\\n        y_test,\\n        n_bins=10,\\n        name=name,\\n        ax=ax_calibration_curve,\\n        color=colors(i),\\n    )\\n    calibration_displays[name] = display\\n\\nax_calibration_curve.grid()\\nax_calibration_curve.set_title(\"Calibration plots (SVC)\")\\n\\n# Add histogram\\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\\nfor i, (_, name) in enumerate(clf_list):\\n    row, col = grid_positions[i]\\n    ax = fig.add_subplot(gs[row, col])'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_curve.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='ax_calibration_curve.grid()\\nax_calibration_curve.set_title(\"Calibration plots (SVC)\")\\n\\n# Add histogram\\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\\nfor i, (_, name) in enumerate(clf_list):\\n    row, col = grid_positions[i]\\n    ax = fig.add_subplot(gs[row, col])\\n\\n    ax.hist(\\n        calibration_displays[name].y_prob,\\n        range=(0, 1),\\n        bins=10,\\n        label=name,\\n        color=colors(i),\\n    )\\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# %%\\n# :class:`~sklearn.svm.LinearSVC` shows the opposite\\n# behavior to :class:`~sklearn.naive_bayes.GaussianNB`; the calibration\\n# curve has a sigmoid shape, which is typical for an under-confident\\n# classifier. In the case of :class:`~sklearn.svm.LinearSVC`, this is caused\\n# by the margin property of the hinge loss, which focuses on samples that are\\n# close to the decision boundary (support vectors). Samples that are far\\n# away from the decision boundary do not impact the hinge loss. It thus makes\\n# sense that :class:`~sklearn.svm.LinearSVC` does not try to separate samples\\n# in the high confidence region regions. This leads to flatter calibration\\n# curves near 0 and 1 and is empirically shown with a variety of datasets\\n# in Niculescu-Mizil & Caruana [1]_.\\n#\\n# Both kinds of calibration (sigmoid and isotonic) can fix this issue and\\n# yield similar results.\\n#\\n# As before, we show the :ref:`brier_score_loss`, :ref:`log_loss`,\\n# :ref:`precision, recall, F1 score <precision_recall_f_measure_metrics>` and\\n# :ref:`ROC AUC <roc_metrics>`.\\n\\nscores = defaultdict(list)\\nfor i, (clf, name) in enumerate(clf_list):\\n    clf.fit(X_train, y_train)\\n    y_prob = clf.predict_proba(X_test)\\n    y_pred = clf.predict(X_test)\\n    scores[\"Classifier\"].append(name)\\n\\n    for metric in [brier_score_loss, log_loss, roc_auc_score]:\\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\\n\\n    for metric in [precision_score, recall_score, f1_score]:\\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\\n        scores[score_name].append(metric(y_test, y_pred))\\n\\n    score_df = pd.DataFrame(scores).set_index(\"Classifier\")\\n    score_df.round(decimals=3)\\n\\nscore_df'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_curve.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='score_df = pd.DataFrame(scores).set_index(\"Classifier\")\\n    score_df.round(decimals=3)\\n\\nscore_df\\n\\n# %%\\n# As with :class:`~sklearn.naive_bayes.GaussianNB` above, calibration improves\\n# both :ref:`brier_score_loss` and :ref:`log_loss` but does not alter the\\n# prediction accuracy measures (precision, recall and F1 score) much.\\n#\\n# Summary\\n# -------\\n#\\n# Parametric sigmoid calibration can deal with situations where the calibration\\n# curve of the base classifier is sigmoid (e.g., for\\n# :class:`~sklearn.svm.LinearSVC`) but not where it is transposed-sigmoid\\n# (e.g., :class:`~sklearn.naive_bayes.GaussianNB`). Non-parametric\\n# isotonic calibration can deal with both situations but may require more\\n# data to produce good results.\\n#\\n# References\\n# ----------\\n#\\n# .. [1] `Predicting Good Probabilities with Supervised Learning\\n#        <https://dl.acm.org/doi/pdf/10.1145/1102351.1102430>`_,\\n#        A. Niculescu-Mizil & R. Caruana, ICML 2005'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_multiclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================================\\nProbability Calibration for 3-class classification\\n==================================================\\n\\nThis example illustrates how sigmoid :ref:`calibration <calibration>` changes\\npredicted probabilities for a 3-class classification problem. Illustrated is\\nthe standard 2-simplex, where the three corners correspond to the three\\nclasses. Arrows point from the probability vectors predicted by an uncalibrated\\nclassifier to the probability vectors predicted by the same classifier after\\nsigmoid calibration on a hold-out validation set. Colors indicate the true'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_multiclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='class of an instance (red: class 1, green: class 2, blue: class 3).\\n\\n\"\"\"\\n\\n# %%\\n# Data\\n# ----\\n# Below, we generate a classification dataset with 2000 samples, 2 features\\n# and 3 target classes. We then split the data as follows:\\n#\\n# * train: 600 samples (for training the classifier)\\n# * valid: 400 samples (for calibrating predicted probabilities)\\n# * test: 1000 samples\\n#\\n# Note that we also create `X_train_valid` and `y_train_valid`, which consists\\n# of both the train and valid subsets. This is used when we only want to train\\n# the classifier but not calibrate the predicted probabilities.\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_blobs\\n\\nnp.random.seed(0)\\n\\nX, y = make_blobs(\\n    n_samples=2000, n_features=2, centers=3, random_state=42, cluster_std=5.0\\n)\\nX_train, y_train = X[:600], y[:600]\\nX_valid, y_valid = X[600:1000], y[600:1000]\\nX_train_valid, y_train_valid = X[:1000], y[:1000]\\nX_test, y_test = X[1000:], y[1000:]\\n\\n# %%\\n# Fitting and calibration\\n# -----------------------\\n#\\n# First, we will train a :class:`~sklearn.ensemble.RandomForestClassifier`\\n# with 25 base estimators (trees) on the concatenated train and validation\\n# data (1000 samples). This is the uncalibrated classifier.\\n\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nclf = RandomForestClassifier(n_estimators=25)\\nclf.fit(X_train_valid, y_train_valid)\\n\\n# %%\\n# To train the calibrated classifier, we start with the same\\n# :class:`~sklearn.ensemble.RandomForestClassifier` but train it using only\\n# the train data subset (600 samples) then calibrate, with `method=\\'sigmoid\\'`,\\n# using the valid data subset (400 samples) in a 2-stage process.\\n\\nfrom sklearn.calibration import CalibratedClassifierCV\\n\\nclf = RandomForestClassifier(n_estimators=25)\\nclf.fit(X_train, y_train)\\ncal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=\"prefit\")\\ncal_clf.fit(X_valid, y_valid)\\n\\n# %%\\n# Compare probabilities\\n# ---------------------\\n# Below we plot a 2-simplex with arrows showing the change in predicted\\n# probabilities of the test samples.\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.figure(figsize=(10, 10))\\ncolors = [\"r\", \"g\", \"b\"]\\n\\nclf_probs = clf.predict_proba(X_test)\\ncal_clf_probs = cal_clf.predict_proba(X_test)\\n# Plot arrows\\nfor i in range(clf_probs.shape[0]):\\n    plt.arrow(\\n        clf_probs[i, 0],\\n        clf_probs[i, 1],\\n        cal_clf_probs[i, 0] - clf_probs[i, 0],\\n        cal_clf_probs[i, 1] - clf_probs[i, 1],\\n        color=colors[y_test[i]],\\n        head_width=1e-2,\\n    )\\n\\n# Plot perfect predictions, at each vertex\\nplt.plot([1.0], [0.0], \"ro\", ms=20, label=\"Class 1\")\\nplt.plot([0.0], [1.0], \"go\", ms=20, label=\"Class 2\")\\nplt.plot([0.0], [0.0], \"bo\", ms=20, label=\"Class 3\")\\n\\n# Plot boundaries of unit simplex\\nplt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], \"k\", label=\"Simplex\")'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_multiclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Plot boundaries of unit simplex\\nplt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], \"k\", label=\"Simplex\")\\n\\n# Annotate points 6 points around the simplex, and mid point inside simplex\\nplt.annotate(\\n    r\"($\\\\frac{1}{3}$, $\\\\frac{1}{3}$, $\\\\frac{1}{3}$)\",\\n    xy=(1.0 / 3, 1.0 / 3),\\n    xytext=(1.0 / 3, 0.23),\\n    xycoords=\"data\",\\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\\n    horizontalalignment=\"center\",\\n    verticalalignment=\"center\",\\n)\\nplt.plot([1.0 / 3], [1.0 / 3], \"ko\", ms=5)\\nplt.annotate(\\n    r\"($\\\\frac{1}{2}$, $0$, $\\\\frac{1}{2}$)\",\\n    xy=(0.5, 0.0),\\n    xytext=(0.5, 0.1),\\n    xycoords=\"data\",\\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\\n    horizontalalignment=\"center\",\\n    verticalalignment=\"center\",\\n)\\nplt.annotate(\\n    r\"($0$, $\\\\frac{1}{2}$, $\\\\frac{1}{2}$)\",\\n    xy=(0.0, 0.5),\\n    xytext=(0.1, 0.5),\\n    xycoords=\"data\",\\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\\n    horizontalalignment=\"center\",\\n    verticalalignment=\"center\",\\n)\\nplt.annotate(\\n    r\"($\\\\frac{1}{2}$, $\\\\frac{1}{2}$, $0$)\",\\n    xy=(0.5, 0.5),\\n    xytext=(0.6, 0.6),\\n    xycoords=\"data\",\\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\\n    horizontalalignment=\"center\",\\n    verticalalignment=\"center\",\\n)\\nplt.annotate(\\n    r\"($0$, $0$, $1$)\",\\n    xy=(0, 0),\\n    xytext=(0.1, 0.1),\\n    xycoords=\"data\",\\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\\n    horizontalalignment=\"center\",\\n    verticalalignment=\"center\",\\n)\\nplt.annotate(\\n    r\"($1$, $0$, $0$)\",\\n    xy=(1, 0),\\n    xytext=(1, 0.1),\\n    xycoords=\"data\",\\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\\n    horizontalalignment=\"center\",\\n    verticalalignment=\"center\",\\n)\\nplt.annotate(\\n    r\"($0$, $1$, $0$)\",\\n    xy=(0, 1),\\n    xytext=(0.1, 1),\\n    xycoords=\"data\",\\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\\n    horizontalalignment=\"center\",\\n    verticalalignment=\"center\",\\n)\\n# Add grid\\nplt.grid(False)\\nfor x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\\n    plt.plot([0, x], [x, 0], \"k\", alpha=0.2)\\n    plt.plot([0, 0 + (1 - x) / 2], [x, x + (1 - x) / 2], \"k\", alpha=0.2)\\n    plt.plot([x, x + (1 - x) / 2], [0, 0 + (1 - x) / 2], \"k\", alpha=0.2)\\n\\nplt.title(\"Change of predicted probabilities on test samples after sigmoid calibration\")\\nplt.xlabel(\"Probability class 1\")\\nplt.ylabel(\"Probability class 2\")\\nplt.xlim(-0.05, 1.05)\\nplt.ylim(-0.05, 1.05)\\n_ = plt.legend(loc=\"best\")'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_multiclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.title(\"Change of predicted probabilities on test samples after sigmoid calibration\")\\nplt.xlabel(\"Probability class 1\")\\nplt.ylabel(\"Probability class 2\")\\nplt.xlim(-0.05, 1.05)\\nplt.ylim(-0.05, 1.05)\\n_ = plt.legend(loc=\"best\")\\n\\n# %%\\n# In the figure above, each vertex of the simplex represents\\n# a perfectly predicted class (e.g., 1, 0, 0). The mid point\\n# inside the simplex represents predicting the three classes with equal\\n# probability (i.e., 1/3, 1/3, 1/3). Each arrow starts at the\\n# uncalibrated probabilities and end with the arrow head at the calibrated\\n# probability. The color of the arrow represents the true class of that test\\n# sample.\\n#\\n# The uncalibrated classifier is overly confident in its predictions and\\n# incurs a large :ref:`log loss <log_loss>`. The calibrated classifier incurs\\n# a lower :ref:`log loss <log_loss>` due to two factors. First, notice in the\\n# figure above that the arrows generally point away from the edges of the\\n# simplex, where the probability of one class is 0. Second, a large proportion\\n# of the arrows point towards the true class, e.g., green arrows (samples where\\n# the true class is \\'green\\') generally point towards the green vertex. This\\n# results in fewer over-confident, 0 predicted probabilities and at the same\\n# time an increase in the predicted probabilities of the correct class.\\n# Thus, the calibrated classifier produces more accurate predicted probabilities\\n# that incur a lower :ref:`log loss <log_loss>`\\n#\\n# We can show this objectively by comparing the :ref:`log loss <log_loss>` of\\n# the uncalibrated and calibrated classifiers on the predictions of the 1000\\n# test samples. Note that an alternative would have been to increase the number\\n# of base estimators (trees) of the\\n# :class:`~sklearn.ensemble.RandomForestClassifier` which would have resulted\\n# in a similar decrease in :ref:`log loss <log_loss>`.\\n\\nfrom sklearn.metrics import log_loss\\n\\nscore = log_loss(y_test, clf_probs)\\ncal_score = log_loss(y_test, cal_clf_probs)\\n\\nprint(\"Log-loss of\")\\nprint(f\" * uncalibrated classifier: {score:.3f}\")\\nprint(f\" * calibrated classifier: {cal_score:.3f}\")\\n\\n# %%\\n# Finally we generate a grid of possible uncalibrated probabilities over\\n# the 2-simplex, compute the corresponding calibrated probabilities and\\n# plot arrows for each. The arrows are colored according the highest\\n# uncalibrated probability. This illustrates the learned calibration map:\\n\\nplt.figure(figsize=(10, 10))\\n# Generate grid of probability values\\np1d = np.linspace(0, 1, 20)\\np0, p1 = np.meshgrid(p1d, p1d)\\np2 = 1 - p0 - p1\\np = np.c_[p0.ravel(), p1.ravel(), p2.ravel()]\\np = p[p[:, 2] >= 0]\\n\\n# Use the three class-wise calibrators to compute calibrated probabilities\\ncalibrated_classifier = cal_clf.calibrated_classifiers_[0]\\nprediction = np.vstack(\\n    [\\n        calibrator.predict(this_p)\\n        for calibrator, this_p in zip(calibrated_classifier.calibrators, p.T)\\n    ]\\n).T'), Document(metadata={'source': '/content/local_copy_repo/examples/calibration/plot_calibration_multiclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Use the three class-wise calibrators to compute calibrated probabilities\\ncalibrated_classifier = cal_clf.calibrated_classifiers_[0]\\nprediction = np.vstack(\\n    [\\n        calibrator.predict(this_p)\\n        for calibrator, this_p in zip(calibrated_classifier.calibrators, p.T)\\n    ]\\n).T\\n\\n# Re-normalize the calibrated predictions to make sure they stay inside the\\n# simplex. This same renormalization step is performed internally by the\\n# predict method of CalibratedClassifierCV on multiclass problems.\\nprediction /= prediction.sum(axis=1)[:, None]\\n\\n# Plot changes in predicted probabilities induced by the calibrators\\nfor i in range(prediction.shape[0]):\\n    plt.arrow(\\n        p[i, 0],\\n        p[i, 1],\\n        prediction[i, 0] - p[i, 0],\\n        prediction[i, 1] - p[i, 1],\\n        head_width=1e-2,\\n        color=colors[np.argmax(p[i])],\\n    )\\n\\n# Plot the boundaries of the unit simplex\\nplt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], \"k\", label=\"Simplex\")\\n\\nplt.grid(False)\\nfor x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\\n    plt.plot([0, x], [x, 0], \"k\", alpha=0.2)\\n    plt.plot([0, 0 + (1 - x) / 2], [x, x + (1 - x) / 2], \"k\", alpha=0.2)\\n    plt.plot([x, x + (1 - x) / 2], [0, 0 + (1 - x) / 2], \"k\", alpha=0.2)\\n\\nplt.title(\"Learned sigmoid calibration map\")\\nplt.xlabel(\"Probability class 1\")\\nplt.ylabel(\"Probability class 2\")\\nplt.xlim(-0.05, 1.05)\\nplt.ylim(-0.05, 1.05)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_permutation_importance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================================================\\nPermutation Importance vs Random Forest Feature Importance (MDI)\\n================================================================\\n\\nIn this example, we will compare the impurity-based feature importance of\\n:class:`~sklearn.ensemble.RandomForestClassifier` with the\\npermutation importance on the titanic dataset using\\n:func:`~sklearn.inspection.permutation_importance`. We will show that the\\nimpurity-based feature importance can inflate the importance of numerical\\nfeatures.\\n\\nFurthermore, the impurity-based feature importance of random forests suffers\\nfrom being computed on statistics derived from the training dataset: the\\nimportances can be high even for features that are not predictive of the target\\nvariable, as long as the model has the capacity to use them to overfit.\\n\\nThis example shows how to use Permutation Importances as an alternative that\\ncan mitigate those limitations.\\n\\n.. rubric:: References\\n\\n* :doi:`L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\\n  2001. <10.1023/A:1010933404324>`\\n\\n\"\"\"\\n\\n# %%\\n# Data Loading and Feature Engineering\\n# ------------------------------------\\n# Let\\'s use pandas to load a copy of the titanic dataset. The following shows\\n# how to apply separate preprocessing on numerical and categorical features.\\n#\\n# We further include two random variables that are not correlated in any way\\n# with the target variable (``survived``):\\n#\\n# - ``random_num`` is a high cardinality numerical variable (as many unique\\n#   values as records).\\n# - ``random_cat`` is a low cardinality categorical variable (3 possible\\n#   values).\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\\nrng = np.random.RandomState(seed=42)\\nX[\"random_cat\"] = rng.randint(3, size=X.shape[0])\\nX[\"random_num\"] = rng.randn(X.shape[0])\\n\\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\", \"random_cat\"]\\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\", \"random_num\"]\\n\\nX = X[categorical_columns + numerical_columns]\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\\n\\n# %%\\n# We define a predictive model based on a random forest. Therefore, we will make\\n# the following preprocessing steps:\\n#\\n# - use :class:`~sklearn.preprocessing.OrdinalEncoder` to encode the\\n#   categorical features;\\n# - use :class:`~sklearn.impute.SimpleImputer` to fill missing values for\\n#   numerical features using a mean strategy.\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OrdinalEncoder\\n\\ncategorical_encoder = OrdinalEncoder(\\n    handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1\\n)\\nnumerical_pipe = SimpleImputer(strategy=\"mean\")'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_permutation_importance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='categorical_encoder = OrdinalEncoder(\\n    handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1\\n)\\nnumerical_pipe = SimpleImputer(strategy=\"mean\")\\n\\npreprocessing = ColumnTransformer(\\n    [\\n        (\"cat\", categorical_encoder, categorical_columns),\\n        (\"num\", numerical_pipe, numerical_columns),\\n    ],\\n    verbose_feature_names_out=False,\\n)\\n\\nrf = Pipeline(\\n    [\\n        (\"preprocess\", preprocessing),\\n        (\"classifier\", RandomForestClassifier(random_state=42)),\\n    ]\\n)\\nrf.fit(X_train, y_train)\\n\\n# %%\\n# Accuracy of the Model\\n# ---------------------\\n# Prior to inspecting the feature importances, it is important to check that\\n# the model predictive performance is high enough. Indeed there would be little\\n# interest of inspecting the important features of a non-predictive model.\\n#\\n# Here one can observe that the train accuracy is very high (the forest model\\n# has enough capacity to completely memorize the training set) but it can still\\n# generalize well enough to the test set thanks to the built-in bagging of\\n# random forests.\\n#\\n# It might be possible to trade some accuracy on the training set for a\\n# slightly better accuracy on the test set by limiting the capacity of the\\n# trees (for instance by setting ``min_samples_leaf=5`` or\\n# ``min_samples_leaf=10``) so as to limit overfitting while not introducing too\\n# much underfitting.\\n#\\n# However let\\'s keep our high capacity random forest model for now so as to\\n# illustrate some pitfalls with feature importance on variables with many\\n# unique values.\\nprint(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")\\n\\n\\n# %%\\n# Tree\\'s Feature Importance from Mean Decrease in Impurity (MDI)\\n# --------------------------------------------------------------\\n# The impurity-based feature importance ranks the numerical features to be the\\n# most important features. As a result, the non-predictive ``random_num``\\n# variable is ranked as one of the most important features!\\n#\\n# This problem stems from two limitations of impurity-based feature\\n# importances:\\n#\\n# - impurity-based importances are biased towards high cardinality features;\\n# - impurity-based importances are computed on training set statistics and\\n#   therefore do not reflect the ability of feature to be useful to make\\n#   predictions that generalize to the test set (when the model has enough\\n#   capacity).\\n#\\n# The bias towards high cardinality features explains why the `random_num` has\\n# a really large importance in comparison with `random_cat` while we would\\n# expect both random features to have a null importance.\\n#\\n# The fact that we use training set statistics explains why both the\\n# `random_num` and `random_cat` features have a non-null importance.\\nimport pandas as pd\\n\\nfeature_names = rf[:-1].get_feature_names_out()\\n\\nmdi_importances = pd.Series(\\n    rf[-1].feature_importances_, index=feature_names\\n).sort_values(ascending=True)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_permutation_importance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='feature_names = rf[:-1].get_feature_names_out()\\n\\nmdi_importances = pd.Series(\\n    rf[-1].feature_importances_, index=feature_names\\n).sort_values(ascending=True)\\n\\n# %%\\nax = mdi_importances.plot.barh()\\nax.set_title(\"Random Forest Feature Importances (MDI)\")\\nax.figure.tight_layout()\\n\\n# %%\\n# As an alternative, the permutation importances of ``rf`` are computed on a\\n# held out test set. This shows that the low cardinality categorical feature,\\n# `sex` and `pclass` are the most important feature. Indeed, permuting the\\n# values of these features will lead to most decrease in accuracy score of the\\n# model on the test set.\\n#\\n# Also note that both random features have very low importances (close to 0) as\\n# expected.\\nfrom sklearn.inspection import permutation_importance\\n\\nresult = permutation_importance(\\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\\n)\\n\\nsorted_importances_idx = result.importances_mean.argsort()\\nimportances = pd.DataFrame(\\n    result.importances[sorted_importances_idx].T,\\n    columns=X.columns[sorted_importances_idx],\\n)\\nax = importances.plot.box(vert=False, whis=10)\\nax.set_title(\"Permutation Importances (test set)\")\\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\\nax.set_xlabel(\"Decrease in accuracy score\")\\nax.figure.tight_layout()\\n\\n# %%\\n# It is also possible to compute the permutation importances on the training\\n# set. This reveals that `random_num` and `random_cat` get a significantly\\n# higher importance ranking than when computed on the test set. The difference\\n# between those two plots is a confirmation that the RF model has enough\\n# capacity to use that random numerical and categorical features to overfit.\\nresult = permutation_importance(\\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\\n)\\n\\nsorted_importances_idx = result.importances_mean.argsort()\\nimportances = pd.DataFrame(\\n    result.importances[sorted_importances_idx].T,\\n    columns=X.columns[sorted_importances_idx],\\n)\\nax = importances.plot.box(vert=False, whis=10)\\nax.set_title(\"Permutation Importances (train set)\")\\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\\nax.set_xlabel(\"Decrease in accuracy score\")\\nax.figure.tight_layout()\\n\\n# %%\\n# We can further retry the experiment by limiting the capacity of the trees\\n# to overfit by setting `min_samples_leaf` at 20 data points.\\nrf.set_params(classifier__min_samples_leaf=20).fit(X_train, y_train)\\n\\n# %%\\n# Observing the accuracy score on the training and testing set, we observe that\\n# the two metrics are very similar now. Therefore, our model is not overfitting\\n# anymore. We can then check the permutation importances with this new model.\\nprint(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_permutation_importance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\ntrain_result = permutation_importance(\\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\\n)\\ntest_results = permutation_importance(\\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\\n)\\nsorted_importances_idx = train_result.importances_mean.argsort()\\n\\n# %%\\ntrain_importances = pd.DataFrame(\\n    train_result.importances[sorted_importances_idx].T,\\n    columns=X.columns[sorted_importances_idx],\\n)\\ntest_importances = pd.DataFrame(\\n    test_results.importances[sorted_importances_idx].T,\\n    columns=X.columns[sorted_importances_idx],\\n)\\n\\n# %%\\nfor name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\\n    ax = importances.plot.box(vert=False, whis=10)\\n    ax.set_title(f\"Permutation Importances ({name} set)\")\\n    ax.set_xlabel(\"Decrease in accuracy score\")\\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\\n    ax.figure.tight_layout()\\n\\n# %%\\n# Now, we can observe that on both sets, the `random_num` and `random_cat`\\n# features have a lower importance compared to the overfitting random forest.\\n# However, the conclusions regarding the importance of the other features are\\n# still valid.'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_permutation_importance_multicollinear.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_permutation_importance(clf, X, y, ax):\\n    result = permutation_importance(clf, X, y, n_repeats=10, random_state=42, n_jobs=2)\\n    perm_sorted_idx = result.importances_mean.argsort()\\n\\n    # `labels` argument in boxplot is deprecated in matplotlib 3.9 and has been\\n    # renamed to `tick_labels`. The following code handles this, but as a\\n    # scikit-learn user you probably can write simpler code by using `labels=...`\\n    # (matplotlib < 3.9) or `tick_labels=...` (matplotlib >= 3.9).\\n    tick_labels_parameter_name = (\\n        \"tick_labels\"\\n        if parse_version(matplotlib.__version__) >= parse_version(\"3.9\")\\n        else \"labels\"\\n    )\\n    tick_labels_dict = {tick_labels_parameter_name: X.columns[perm_sorted_idx]}\\n    ax.boxplot(result.importances[perm_sorted_idx].T, vert=False, **tick_labels_dict)\\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\\n    return ax'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_permutation_importance_multicollinear.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================================\\nPermutation Importance with Multicollinear or Correlated Features\\n=================================================================\\n\\nIn this example, we compute the\\n:func:`~sklearn.inspection.permutation_importance` of the features to a trained\\n:class:`~sklearn.ensemble.RandomForestClassifier` using the\\n:ref:`breast_cancer_dataset`. The model can easily get about 97% accuracy on a\\ntest dataset. Because this dataset contains multicollinear features, the\\npermutation importance shows that none of the features are important, in\\ncontradiction with the high test accuracy.\\n\\nWe demo a possible approach to handling multicollinearity, which consists of\\nhierarchical clustering on the features\\' Spearman rank-order correlations,\\npicking a threshold, and keeping a single feature from each cluster.\\n\\n.. note::\\n    See also\\n    :ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance.py`\\n\\n\"\"\"\\n\\n# %%\\n# Random Forest Feature Importance on Breast Cancer Data\\n# ------------------------------------------------------\\n#\\n# First, we define a function to ease the plotting:\\nimport matplotlib\\n\\nfrom sklearn.inspection import permutation_importance\\nfrom sklearn.utils.fixes import parse_version\\n\\n\\n# Code for: def plot_permutation_importance(clf, X, y, ax):\\n\\n\\n# %%\\n# We then train a :class:`~sklearn.ensemble.RandomForestClassifier` on the\\n# :ref:`breast_cancer_dataset` and evaluate its accuracy on a test set:\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n\\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\\nclf.fit(X_train, y_train)\\nprint(f\"Baseline accuracy on test data: {clf.score(X_test, y_test):.2}\")\\n\\n# %%\\n# Next, we plot the tree based feature importance and the permutation\\n# importance. The permutation importance is calculated on the training set to\\n# show how much the model relies on each feature during training.\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\n\\nmdi_importances = pd.Series(clf.feature_importances_, index=X_train.columns)\\ntree_importance_sorted_idx = np.argsort(clf.feature_importances_)\\n\\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\\nmdi_importances.sort_values().plot.barh(ax=ax1)\\nax1.set_xlabel(\"Gini importance\")\\nplot_permutation_importance(clf, X_train, y_train, ax2)\\nax2.set_xlabel(\"Decrease in accuracy score\")\\nfig.suptitle(\\n    \"Impurity-based vs. permutation importances on multicollinear features (train set)\"\\n)\\n_ = fig.tight_layout()'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_permutation_importance_multicollinear.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The plot on the left shows the Gini importance of the model. As the\\n# scikit-learn implementation of\\n# :class:`~sklearn.ensemble.RandomForestClassifier` uses a random subsets of\\n# :math:`\\\\sqrt{n_\\\\text{features}}` features at each split, it is able to dilute\\n# the dominance of any single correlated feature. As a result, the individual\\n# feature importance may be distributed more evenly among the correlated\\n# features. Since the features have large cardinality and the classifier is\\n# non-overfitted, we can relatively trust those values.\\n#\\n# The permutation importance on the right plot shows that permuting a feature\\n# drops the accuracy by at most `0.012`, which would suggest that none of the\\n# features are important. This is in contradiction with the high test accuracy\\n# computed as baseline: some feature must be important.\\n#\\n# Similarly, the change in accuracy score computed on the test set appears to be\\n# driven by chance:\\n\\nfig, ax = plt.subplots(figsize=(7, 6))\\nplot_permutation_importance(clf, X_test, y_test, ax)\\nax.set_title(\"Permutation Importances on multicollinear features\\\\n(test set)\")\\nax.set_xlabel(\"Decrease in accuracy score\")\\n_ = ax.figure.tight_layout()\\n\\n# %%\\n# Nevertheless, one can still compute a meaningful permutation importance in the\\n# presence of correlated features, as demonstrated in the following section.\\n#\\n# Handling Multicollinear Features\\n# --------------------------------\\n# When features are collinear, permuting one feature has little effect on the\\n# models performance because it can get the same information from a correlated\\n# feature. Note that this is not the case for all predictive models and depends\\n# on their underlying implementation.\\n#\\n# One way to handle multicollinear features is by performing hierarchical\\n# clustering on the Spearman rank-order correlations, picking a threshold, and\\n# keeping a single feature from each cluster. First, we plot a heatmap of the\\n# correlated features:\\nfrom scipy.cluster import hierarchy\\nfrom scipy.spatial.distance import squareform\\nfrom scipy.stats import spearmanr\\n\\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\\ncorr = spearmanr(X).correlation\\n\\n# Ensure the correlation matrix is symmetric\\ncorr = (corr + corr.T) / 2\\nnp.fill_diagonal(corr, 1)\\n\\n# We convert the correlation matrix to a distance matrix before performing\\n# hierarchical clustering using Ward\\'s linkage.\\ndistance_matrix = 1 - np.abs(corr)\\ndist_linkage = hierarchy.ward(squareform(distance_matrix))\\ndendro = hierarchy.dendrogram(\\n    dist_linkage, labels=X.columns.to_list(), ax=ax1, leaf_rotation=90\\n)\\ndendro_idx = np.arange(0, len(dendro[\"ivl\"]))\\n\\nax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\\nax2.set_xticks(dendro_idx)\\nax2.set_yticks(dendro_idx)\\nax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\\nax2.set_yticklabels(dendro[\"ivl\"])\\n_ = fig.tight_layout()'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_permutation_importance_multicollinear.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\\nax2.set_xticks(dendro_idx)\\nax2.set_yticks(dendro_idx)\\nax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\\nax2.set_yticklabels(dendro[\"ivl\"])\\n_ = fig.tight_layout()\\n\\n# %%\\n# Next, we manually pick a threshold by visual inspection of the dendrogram to\\n# group our features into clusters and choose a feature from each cluster to\\n# keep, select those features from our dataset, and train a new random forest.\\n# The test accuracy of the new random forest did not change much compared to the\\n# random forest trained on the complete dataset.\\nfrom collections import defaultdict\\n\\ncluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\\ncluster_id_to_feature_ids = defaultdict(list)\\nfor idx, cluster_id in enumerate(cluster_ids):\\n    cluster_id_to_feature_ids[cluster_id].append(idx)\\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\\nselected_features_names = X.columns[selected_features]\\n\\nX_train_sel = X_train[selected_features_names]\\nX_test_sel = X_test[selected_features_names]\\n\\nclf_sel = RandomForestClassifier(n_estimators=100, random_state=42)\\nclf_sel.fit(X_train_sel, y_train)\\nprint(\\n    \"Baseline accuracy on test data with features removed:\"\\n    f\" {clf_sel.score(X_test_sel, y_test):.2}\"\\n)\\n\\n# %%\\n# We can finally explore the permutation importance of the selected subset of\\n# features:\\n\\nfig, ax = plt.subplots(figsize=(7, 6))\\nplot_permutation_importance(clf_sel, X_test_sel, y_test, ax)\\nax.set_title(\"Permutation Importances on selected subset of features\\\\n(test set)\")\\nax.set_xlabel(\"Decrease in accuracy score\")\\nax.figure.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_causal_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================================\\nFailure of Machine Learning to infer causal effects\\n===================================================\\n\\nMachine Learning models are great for measuring statistical associations.\\nUnfortunately, unless we\\'re willing to make strong assumptions about the data,\\nthose models are unable to infer causal effects.\\n\\nTo illustrate this, we will simulate a situation in which we try to answer one\\nof the most important questions in economics of education: **what is the causal\\neffect of earning a college degree on hourly wages?** Although the answer to\\nthis question is crucial to policy makers, `Omitted-Variable Biases\\n<https://en.wikipedia.org/wiki/Omitted-variable_bias>`_ (OVB) prevent us from\\nidentifying that causal effect.\\n\"\"\"\\n\\n# %%\\n# The dataset: simulated hourly wages\\n# -----------------------------------\\n#\\n# The data generating process is laid out in the code below. Work experience in\\n# years and a measure of ability are drawn from Normal distributions; the\\n# hourly wage of one of the parents is drawn from Beta distribution. We then\\n# create an indicator of college degree which is positively impacted by ability\\n# and parental hourly wage. Finally, we model hourly wages as a linear function\\n# of all the previous variables and a random component. Note that all variables\\n# have a positive effect on hourly wages.\\nimport numpy as np\\nimport pandas as pd\\n\\nn_samples = 10_000\\nrng = np.random.RandomState(32)\\n\\nexperiences = rng.normal(20, 10, size=n_samples).astype(int)\\nexperiences[experiences < 0] = 0\\nabilities = rng.normal(0, 0.15, size=n_samples)\\nparent_hourly_wages = 50 * rng.beta(2, 8, size=n_samples)\\nparent_hourly_wages[parent_hourly_wages < 0] = 0\\ncollege_degrees = (\\n    9 * abilities + 0.02 * parent_hourly_wages + rng.randn(n_samples) > 0.7\\n).astype(int)\\n\\ntrue_coef = pd.Series(\\n    {\\n        \"college degree\": 2.0,\\n        \"ability\": 5.0,\\n        \"experience\": 0.2,\\n        \"parent hourly wage\": 1.0,\\n    }\\n)\\nhourly_wages = (\\n    true_coef[\"experience\"] * experiences\\n    + true_coef[\"parent hourly wage\"] * parent_hourly_wages\\n    + true_coef[\"college degree\"] * college_degrees\\n    + true_coef[\"ability\"] * abilities\\n    + rng.normal(0, 1, size=n_samples)\\n)\\n\\nhourly_wages[hourly_wages < 0] = 0\\n\\n# %%\\n# Description of the simulated data\\n# ---------------------------------\\n#\\n# The following plot shows the distribution of each variable, and pairwise\\n# scatter plots. Key to our OVB story is the positive relationship between\\n# ability and college degree.\\nimport seaborn as sns\\n\\ndf = pd.DataFrame(\\n    {\\n        \"college degree\": college_degrees,\\n        \"ability\": abilities,\\n        \"hourly wage\": hourly_wages,\\n        \"experience\": experiences,\\n        \"parent hourly wage\": parent_hourly_wages,\\n    }\\n)\\n\\ngrid = sns.pairplot(df, diag_kind=\"kde\", corner=True)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_causal_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='df = pd.DataFrame(\\n    {\\n        \"college degree\": college_degrees,\\n        \"ability\": abilities,\\n        \"hourly wage\": hourly_wages,\\n        \"experience\": experiences,\\n        \"parent hourly wage\": parent_hourly_wages,\\n    }\\n)\\n\\ngrid = sns.pairplot(df, diag_kind=\"kde\", corner=True)\\n\\n# %%\\n# In the next section, we train predictive models and we therefore split the\\n# target column from over features and we split the data into a training and a\\n# testing set.\\nfrom sklearn.model_selection import train_test_split\\n\\ntarget_name = \"hourly wage\"\\nX, y = df.drop(columns=target_name), df[target_name]\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\n\\n# %%\\n# Income prediction with fully observed variables\\n# -----------------------------------------------\\n#\\n# First, we train a predictive model, a\\n# :class:`~sklearn.linear_model.LinearRegression` model. In this experiment,\\n# we assume that all variables used by the true generative model are available.\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import r2_score\\n\\nfeatures_names = [\"experience\", \"parent hourly wage\", \"college degree\", \"ability\"]\\n\\nregressor_with_ability = LinearRegression()\\nregressor_with_ability.fit(X_train[features_names], y_train)\\ny_pred_with_ability = regressor_with_ability.predict(X_test[features_names])\\nR2_with_ability = r2_score(y_test, y_pred_with_ability)\\n\\nprint(f\"R2 score with ability: {R2_with_ability:.3f}\")\\n\\n# %%\\n# This model predicts well the hourly wages as shown by the high R2 score. We\\n# plot the model coefficients to show that we exactly recover the values of\\n# the true generative model.\\nimport matplotlib.pyplot as plt\\n\\nmodel_coef = pd.Series(regressor_with_ability.coef_, index=features_names)\\ncoef = pd.concat(\\n    [true_coef[features_names], model_coef],\\n    keys=[\"Coefficients of true generative model\", \"Model coefficients\"],\\n    axis=1,\\n)\\nax = coef.plot.barh()\\nax.set_xlabel(\"Coefficient values\")\\nax.set_title(\"Coefficients of the linear regression including the ability features\")\\n_ = plt.tight_layout()\\n\\n# %%\\n# Income prediction with partial observations\\n# -------------------------------------------\\n#\\n# In practice, intellectual abilities are not observed or are only estimated\\n# from proxies that inadvertently measure education as well (e.g. by IQ tests).\\n# But omitting the \"ability\" feature from a linear model inflates the estimate\\n# via a positive OVB.\\nfeatures_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\\n\\nregressor_without_ability = LinearRegression()\\nregressor_without_ability.fit(X_train[features_names], y_train)\\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\\nR2_without_ability = r2_score(y_test, y_pred_without_ability)\\n\\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_causal_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(f\"R2 score without ability: {R2_without_ability:.3f}\")\\n\\n# %%\\n# The predictive power of our model is similar when we omit the ability feature\\n# in terms of R2 score. We now check if the coefficient of the model are\\n# different from the true generative model.\\n\\nmodel_coef = pd.Series(regressor_without_ability.coef_, index=features_names)\\ncoef = pd.concat(\\n    [true_coef[features_names], model_coef],\\n    keys=[\"Coefficients of true generative model\", \"Model coefficients\"],\\n    axis=1,\\n)\\nax = coef.plot.barh()\\nax.set_xlabel(\"Coefficient values\")\\n_ = ax.set_title(\"Coefficients of the linear regression excluding the ability feature\")\\nplt.tight_layout()\\nplt.show()\\n\\n# %%\\n# To compensate for the omitted variable, the model inflates the coefficient of\\n# the college degree feature. Therefore, interpreting this coefficient value\\n# as a causal effect of the true generative model is incorrect.\\n#\\n# Lessons learned\\n# ---------------\\n#\\n# Machine learning models are not designed for the estimation of causal\\n# effects. While we showed this with a linear model, OVB can affect any type of\\n# model.\\n#\\n# Whenever interpreting a coefficient or a change in predictions brought about\\n# by a change in one of the features, it is important to keep in mind\\n# potentially unobserved variables that could be correlated with both the\\n# feature in question and the target variable. Such variables are called\\n# `Confounding Variables <https://en.wikipedia.org/wiki/Confounding>`_. In\\n# order to still estimate causal effect in the presence of confounding,\\n# researchers usually conduct experiments in which the treatment variable (e.g.\\n# college degree) is randomized. When an experiment is prohibitively expensive\\n# or unethical, researchers can sometimes use other causal inference techniques\\n# such as `Instrumental Variables\\n# <https://en.wikipedia.org/wiki/Instrumental_variables_estimation>`_ (IV)\\n# estimations.'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================================================\\nCommon pitfalls in the interpretation of coefficients of linear models\\n======================================================================\\n\\nIn linear models, the target value is modeled as a linear combination of the\\nfeatures (see the :ref:`linear_model` User Guide section for a description of a\\nset of linear models available in scikit-learn). Coefficients in multiple linear\\nmodels represent the relationship between the given feature, :math:`X_i` and the\\ntarget, :math:`y`, assuming that all the other features remain constant\\n(`conditional dependence\\n<https://en.wikipedia.org/wiki/Conditional_dependence>`_). This is different\\nfrom plotting :math:`X_i` versus :math:`y` and fitting a linear relationship: in\\nthat case all possible values of the other features are taken into account in\\nthe estimation (marginal dependence).\\n\\nThis example will provide some hints in interpreting coefficient in linear\\nmodels, pointing at problems that arise when either the linear model is not\\nappropriate to describe the dataset, or when features are correlated.\\n\\n.. note::\\n\\n    Keep in mind that the features :math:`X` and the outcome :math:`y` are in\\n    general the result of a data generating process that is unknown to us.\\n    Machine learning models are trained to approximate the unobserved\\n    mathematical function that links :math:`X` to :math:`y` from sample data. As\\n    a result, any interpretation made about a model may not necessarily\\n    generalize to the true data generating process. This is especially true when\\n    the model is of bad quality or when the sample data is not representative of\\n    the population.\\n\\nWe will use data from the `\"Current Population Survey\"\\n<https://www.openml.org/d/534>`_ from 1985 to predict wage as a function of\\nvarious features such as experience, age, or education.\\n\\n.. contents::\\n   :local:\\n   :depth: 1\\n\\n\"\"\"\\n\\n# %%\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport scipy as sp\\nimport seaborn as sns\\n\\n# %%\\n# The dataset: wages\\n# ------------------\\n#\\n# We fetch the data from `OpenML <http://openml.org/>`_.\\n# Note that setting the parameter `as_frame` to True will retrieve the data\\n# as a pandas dataframe.\\nfrom sklearn.datasets import fetch_openml\\n\\nsurvey = fetch_openml(data_id=534, as_frame=True)\\n\\n# %%\\n# Then, we identify features `X` and targets `y`: the column WAGE is our\\n# target variable (i.e., the variable which we want to predict).\\n\\nX = survey.data[survey.feature_names]\\nX.describe(include=\"all\")\\n\\n# %%\\n# Note that the dataset contains categorical and numerical variables.\\n# We will need to take this into account when preprocessing the dataset\\n# thereafter.\\n\\nX.head()\\n\\n# %%\\n# Our target for prediction: the wage.\\n# Wages are described as floating-point number in dollars per hour.\\n\\n# %%\\ny = survey.target.values.ravel()\\nsurvey.target.head()'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X.head()\\n\\n# %%\\n# Our target for prediction: the wage.\\n# Wages are described as floating-point number in dollars per hour.\\n\\n# %%\\ny = survey.target.values.ravel()\\nsurvey.target.head()\\n\\n# %%\\n# We split the sample into a train and a test dataset.\\n# Only the train dataset will be used in the following exploratory analysis.\\n# This is a way to emulate a real situation where predictions are performed on\\n# an unknown target, and we don\\'t want our analysis and decisions to be biased\\n# by our knowledge of the test data.\\n\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n\\n# %%\\n# First, let\\'s get some insights by looking at the variable distributions and\\n# at the pairwise relationships between them. Only numerical\\n# variables will be used. In the following plot, each dot represents a sample.\\n#\\n#   .. _marginal_dependencies:\\n\\ntrain_dataset = X_train.copy()\\ntrain_dataset.insert(0, \"WAGE\", y_train)\\n_ = sns.pairplot(train_dataset, kind=\"reg\", diag_kind=\"kde\")\\n\\n# %%\\n# Looking closely at the WAGE distribution reveals that it has a\\n# long tail. For this reason, we should take its logarithm\\n# to turn it approximately into a normal distribution (linear models such\\n# as ridge or lasso work best for a normal distribution of error).\\n#\\n# The WAGE is increasing when EDUCATION is increasing.\\n# Note that the dependence between WAGE and EDUCATION\\n# represented here is a marginal dependence, i.e., it describes the behavior\\n# of a specific variable without keeping the others fixed.\\n#\\n# Also, the EXPERIENCE and AGE are strongly linearly correlated.\\n#\\n# .. _the-pipeline:\\n#\\n# The machine-learning pipeline\\n# -----------------------------\\n#\\n# To design our machine-learning pipeline, we first manually\\n# check the type of data that we are dealing with:\\n\\nsurvey.data.info()\\n\\n# %%\\n# As seen previously, the dataset contains columns with different data types\\n# and we need to apply a specific preprocessing for each data types.\\n# In particular categorical variables cannot be included in linear model if not\\n# coded as integers first. In addition, to avoid categorical features to be\\n# treated as ordered values, we need to one-hot-encode them.\\n# Our pre-processor will\\n#\\n# - one-hot encode (i.e., generate a column by category) the categorical\\n#   columns, only for non-binary categorical variables;\\n# - as a first approach (we will see after how the normalisation of numerical\\n#   values will affect our discussion), keep numerical values as they are.\\n\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\ncategorical_columns = [\"RACE\", \"OCCUPATION\", \"SECTOR\", \"MARR\", \"UNION\", \"SEX\", \"SOUTH\"]\\nnumerical_columns = [\"EDUCATION\", \"EXPERIENCE\", \"AGE\"]\\n\\npreprocessor = make_column_transformer(\\n    (OneHotEncoder(drop=\"if_binary\"), categorical_columns),\\n    remainder=\"passthrough\",\\n    verbose_feature_names_out=False,  # avoid to prepend the preprocessor names\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='preprocessor = make_column_transformer(\\n    (OneHotEncoder(drop=\"if_binary\"), categorical_columns),\\n    remainder=\"passthrough\",\\n    verbose_feature_names_out=False,  # avoid to prepend the preprocessor names\\n)\\n\\n# %%\\n# To describe the dataset as a linear model we use a ridge regressor\\n# with a very small regularization and to model the logarithm of the WAGE.\\n\\nfrom sklearn.compose import TransformedTargetRegressor\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.pipeline import make_pipeline\\n\\nmodel = make_pipeline(\\n    preprocessor,\\n    TransformedTargetRegressor(\\n        regressor=Ridge(alpha=1e-10), func=np.log10, inverse_func=sp.special.exp10\\n    ),\\n)\\n\\n# %%\\n# Processing the dataset\\n# ----------------------\\n#\\n# First, we fit the model.\\n\\nmodel.fit(X_train, y_train)\\n\\n# %%\\n# Then we check the performance of the computed model plotting its predictions\\n# on the test set and computing,\\n# for example, the median absolute error of the model.\\n\\nfrom sklearn.metrics import PredictionErrorDisplay, median_absolute_error\\n\\nmae_train = median_absolute_error(y_train, model.predict(X_train))\\ny_pred = model.predict(X_test)\\nmae_test = median_absolute_error(y_test, y_pred)\\nscores = {\\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\\n}\\n\\n# %%\\n_, ax = plt.subplots(figsize=(5, 5))\\ndisplay = PredictionErrorDisplay.from_predictions(\\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\\n)\\nax.set_title(\"Ridge model, small regularization\")\\nfor name, score in scores.items():\\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\\nax.legend(loc=\"upper left\")\\nplt.tight_layout()\\n\\n# %%\\n# The model learnt is far from being a good model making accurate predictions:\\n# this is obvious when looking at the plot above, where good predictions\\n# should lie on the black dashed line.\\n#\\n# In the following section, we will interpret the coefficients of the model.\\n# While we do so, we should keep in mind that any conclusion we draw is\\n# about the model that we build, rather than about the true (real-world)\\n# generative process of the data.\\n#\\n# Interpreting coefficients: scale matters\\n# ----------------------------------------\\n#\\n# First of all, we can take a look to the values of the coefficients of the\\n# regressor we have fitted.\\nfeature_names = model[:-1].get_feature_names_out()\\n\\ncoefs = pd.DataFrame(\\n    model[-1].regressor_.coef_,\\n    columns=[\"Coefficients\"],\\n    index=feature_names,\\n)\\n\\ncoefs'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='coefs = pd.DataFrame(\\n    model[-1].regressor_.coef_,\\n    columns=[\"Coefficients\"],\\n    index=feature_names,\\n)\\n\\ncoefs\\n\\n# %%\\n# The AGE coefficient is expressed in \"dollars/hour per living years\" while the\\n# EDUCATION one is expressed in \"dollars/hour per years of education\". This\\n# representation of the coefficients has the benefit of making clear the\\n# practical predictions of the model: an increase of :math:`1` year in AGE\\n# means a decrease of :math:`0.030867` dollars/hour, while an increase of\\n# :math:`1` year in EDUCATION means an increase of :math:`0.054699`\\n# dollars/hour. On the other hand, categorical variables (as UNION or SEX) are\\n# adimensional numbers taking either the value 0 or 1. Their coefficients\\n# are expressed in dollars/hour. Then, we cannot compare the magnitude of\\n# different coefficients since the features have different natural scales, and\\n# hence value ranges, because of their different unit of measure. This is more\\n# visible if we plot the coefficients.\\n\\ncoefs.plot.barh(figsize=(9, 7))\\nplt.title(\"Ridge model, small regularization\")\\nplt.axvline(x=0, color=\".5\")\\nplt.xlabel(\"Raw coefficient values\")\\nplt.subplots_adjust(left=0.3)\\n\\n# %%\\n# Indeed, from the plot above the most important factor in determining WAGE\\n# appears to be the\\n# variable UNION, even if our intuition might tell us that variables\\n# like EXPERIENCE should have more impact.\\n#\\n# Looking at the coefficient plot to gauge feature importance can be\\n# misleading as some of them vary on a small scale, while others, like AGE,\\n# varies a lot more, several decades.\\n#\\n# This is visible if we compare the standard deviations of different\\n# features.\\n\\nX_train_preprocessed = pd.DataFrame(\\n    model[:-1].transform(X_train), columns=feature_names\\n)\\n\\nX_train_preprocessed.std(axis=0).plot.barh(figsize=(9, 7))\\nplt.title(\"Feature ranges\")\\nplt.xlabel(\"Std. dev. of feature values\")\\nplt.subplots_adjust(left=0.3)\\n\\n# %%\\n# Multiplying the coefficients by the standard deviation of the related\\n# feature would reduce all the coefficients to the same unit of measure.\\n# As we will see :ref:`after<scaling_num>` this is equivalent to normalize\\n# numerical variables to their standard deviation,\\n# as :math:`y = \\\\sum{coef_i \\\\times X_i} =\\n# \\\\sum{(coef_i \\\\times std_i) \\\\times (X_i / std_i)}`.\\n#\\n# In that way, we emphasize that the\\n# greater the variance of a feature, the larger the weight of the corresponding\\n# coefficient on the output, all else being equal.\\n\\ncoefs = pd.DataFrame(\\n    model[-1].regressor_.coef_ * X_train_preprocessed.std(axis=0),\\n    columns=[\"Coefficient importance\"],\\n    index=feature_names,\\n)\\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\\nplt.xlabel(\"Coefficient values corrected by the feature\\'s std. dev.\")\\nplt.title(\"Ridge model, small regularization\")\\nplt.axvline(x=0, color=\".5\")\\nplt.subplots_adjust(left=0.3)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"# %%\\n# Now that the coefficients have been scaled, we can safely compare them.\\n#\\n# .. warning::\\n#\\n#   Why does the plot above suggest that an increase in age leads to a\\n#   decrease in wage? Why the :ref:`initial pairplot\\n#   <marginal_dependencies>` is telling the opposite?\\n#\\n# The plot above tells us about dependencies between a specific feature and\\n# the target when all other features remain constant, i.e., **conditional\\n# dependencies**. An increase of the AGE will induce a decrease\\n# of the WAGE when all other features remain constant. On the contrary, an\\n# increase of the EXPERIENCE will induce an increase of the WAGE when all\\n# other features remain constant.\\n# Also, AGE, EXPERIENCE and EDUCATION are the three variables that most\\n# influence the model.\\n#\\n# Interpreting coefficients: being cautious about causality\\n# ---------------------------------------------------------\\n#\\n# Linear models are a great tool for measuring statistical association, but we\\n# should be cautious when making statements about causality, after all\\n# correlation doesn't always imply causation. This is particularly difficult in\\n# the social sciences because the variables we observe only function as proxies\\n# for the underlying causal process.\\n#\\n# In our particular case we can think of the EDUCATION of an individual as a\\n# proxy for their professional aptitude, the real variable we're interested in\\n# but can't observe. We'd certainly like to think that staying in school for\\n# longer would increase technical competency, but it's also quite possible that\\n# causality goes the other way too. That is, those who are technically\\n# competent tend to stay in school for longer.\\n#\\n# An employer is unlikely to care which case it is (or if it's a mix of both),\\n# as long as they remain convinced that a person with more EDUCATION is better\\n# suited for the job, they will be happy to pay out a higher WAGE.\\n#\\n# This confounding of effects becomes problematic when thinking about some\\n# form of intervention e.g. government subsidies of university degrees or\\n# promotional material encouraging individuals to take up higher education.\\n# The usefulness of these measures could end up being overstated, especially if\\n# the degree of confounding is strong. Our model predicts a :math:`0.054699`\\n# increase in hourly wage for each year of education. The actual causal effect\\n# might be lower because of this confounding.\\n#\\n# Checking the variability of the coefficients\\n# --------------------------------------------\\n#\\n# We can check the coefficient variability through cross-validation:\\n# it is a form of data perturbation (related to\\n# `resampling <https://en.wikipedia.org/wiki/Resampling_(statistics)>`_).\\n#\\n# If coefficients vary significantly when changing the input dataset\\n# their robustness is not guaranteed, and they should probably be interpreted\\n# with caution.\\n\\nfrom sklearn.model_selection import RepeatedKFold, cross_validate\"), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.model_selection import RepeatedKFold, cross_validate\\n\\ncv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=0)\\ncv_model = cross_validate(\\n    model,\\n    X,\\n    y,\\n    cv=cv,\\n    return_estimator=True,\\n    n_jobs=2,\\n)\\n\\ncoefs = pd.DataFrame(\\n    [\\n        est[-1].regressor_.coef_ * est[:-1].transform(X.iloc[train_idx]).std(axis=0)\\n        for est, (train_idx, _) in zip(cv_model[\"estimator\"], cv.split(X, y))\\n    ],\\n    columns=feature_names,\\n)\\n\\n# %%\\nplt.figure(figsize=(9, 7))\\nsns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\\nsns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=10)\\nplt.axvline(x=0, color=\".5\")\\nplt.xlabel(\"Coefficient importance\")\\nplt.title(\"Coefficient importance and its variability\")\\nplt.suptitle(\"Ridge model, small regularization\")\\nplt.subplots_adjust(left=0.3)\\n\\n# %%\\n# The problem of correlated variables\\n# -----------------------------------\\n#\\n# The AGE and EXPERIENCE coefficients are affected by strong variability which\\n# might be due to the collinearity between the 2 features: as AGE and\\n# EXPERIENCE vary together in the data, their effect is difficult to tease\\n# apart.\\n#\\n# To verify this interpretation we plot the variability of the AGE and\\n# EXPERIENCE coefficient.\\n#\\n# .. _covariation:\\n\\nplt.ylabel(\"Age coefficient\")\\nplt.xlabel(\"Experience coefficient\")\\nplt.grid(True)\\nplt.xlim(-0.4, 0.5)\\nplt.ylim(-0.4, 0.5)\\nplt.scatter(coefs[\"AGE\"], coefs[\"EXPERIENCE\"])\\n_ = plt.title(\"Co-variations of coefficients for AGE and EXPERIENCE across folds\")\\n\\n# %%\\n# Two regions are populated: when the EXPERIENCE coefficient is\\n# positive the AGE one is negative and vice-versa.\\n#\\n# To go further we remove one of the 2 features and check what is the impact\\n# on the model stability.\\n\\ncolumn_to_drop = [\"AGE\"]\\n\\ncv_model = cross_validate(\\n    model,\\n    X.drop(columns=column_to_drop),\\n    y,\\n    cv=cv,\\n    return_estimator=True,\\n    n_jobs=2,\\n)\\n\\ncoefs = pd.DataFrame(\\n    [\\n        est[-1].regressor_.coef_\\n        * est[:-1].transform(X.drop(columns=column_to_drop).iloc[train_idx]).std(axis=0)\\n        for est, (train_idx, _) in zip(cv_model[\"estimator\"], cv.split(X, y))\\n    ],\\n    columns=feature_names[:-1],\\n)\\n\\n# %%\\nplt.figure(figsize=(9, 7))\\nsns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\\nsns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5)\\nplt.axvline(x=0, color=\".5\")\\nplt.title(\"Coefficient importance and its variability\")\\nplt.xlabel(\"Coefficient importance\")\\nplt.suptitle(\"Ridge model, small regularization, AGE dropped\")\\nplt.subplots_adjust(left=0.3)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The estimation of the EXPERIENCE coefficient now shows a much reduced\\n# variability. EXPERIENCE remains important for all models trained during\\n# cross-validation.\\n#\\n# .. _scaling_num:\\n#\\n# Preprocessing numerical variables\\n# ---------------------------------\\n#\\n# As said above (see \":ref:`the-pipeline`\"), we could also choose to scale\\n# numerical values before training the model.\\n# This can be useful when we apply a similar amount of regularization to all of them\\n# in the ridge.\\n# The preprocessor is redefined in order to subtract the mean and scale\\n# variables to unit variance.\\n\\nfrom sklearn.preprocessing import StandardScaler\\n\\npreprocessor = make_column_transformer(\\n    (OneHotEncoder(drop=\"if_binary\"), categorical_columns),\\n    (StandardScaler(), numerical_columns),\\n)\\n\\n# %%\\n# The model will stay unchanged.\\n\\nmodel = make_pipeline(\\n    preprocessor,\\n    TransformedTargetRegressor(\\n        regressor=Ridge(alpha=1e-10), func=np.log10, inverse_func=sp.special.exp10\\n    ),\\n)\\nmodel.fit(X_train, y_train)\\n\\n# %%\\n# Again, we check the performance of the computed\\n# model using, for example, the median absolute error of the model and the R\\n# squared coefficient.\\n\\nmae_train = median_absolute_error(y_train, model.predict(X_train))\\ny_pred = model.predict(X_test)\\nmae_test = median_absolute_error(y_test, y_pred)\\nscores = {\\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\\n}\\n\\n_, ax = plt.subplots(figsize=(5, 5))\\ndisplay = PredictionErrorDisplay.from_predictions(\\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\\n)\\nax.set_title(\"Ridge model, small regularization\")\\nfor name, score in scores.items():\\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\\nax.legend(loc=\"upper left\")\\nplt.tight_layout()\\n\\n# %%\\n# For the coefficient analysis, scaling is not needed this time because it\\n# was performed during the preprocessing step.\\n\\ncoefs = pd.DataFrame(\\n    model[-1].regressor_.coef_,\\n    columns=[\"Coefficients importance\"],\\n    index=feature_names,\\n)\\ncoefs.plot.barh(figsize=(9, 7))\\nplt.title(\"Ridge model, small regularization, normalized variables\")\\nplt.xlabel(\"Raw coefficient values\")\\nplt.axvline(x=0, color=\".5\")\\nplt.subplots_adjust(left=0.3)\\n\\n# %%\\n# We now inspect the coefficients across several cross-validation folds. As in\\n# the above example, we do not need to scale the coefficients by the std. dev.\\n# of the feature values since this scaling was already\\n# done in the preprocessing step of the pipeline.\\n\\ncv_model = cross_validate(\\n    model,\\n    X,\\n    y,\\n    cv=cv,\\n    return_estimator=True,\\n    n_jobs=2,\\n)\\ncoefs = pd.DataFrame(\\n    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='cv_model = cross_validate(\\n    model,\\n    X,\\n    y,\\n    cv=cv,\\n    return_estimator=True,\\n    n_jobs=2,\\n)\\ncoefs = pd.DataFrame(\\n    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\\n)\\n\\n# %%\\nplt.figure(figsize=(9, 7))\\nsns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\\nsns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=10)\\nplt.axvline(x=0, color=\".5\")\\nplt.title(\"Coefficient variability\")\\nplt.subplots_adjust(left=0.3)\\n\\n# %%\\n# The result is quite similar to the non-normalized case.\\n#\\n# Linear models with regularization\\n# ---------------------------------\\n#\\n# In machine-learning practice, ridge regression is more often used with\\n# non-negligible regularization.\\n#\\n# Above, we limited this regularization to a very little amount. Regularization\\n# improves the conditioning of the problem and reduces the variance of the\\n# estimates. :class:`~sklearn.linear_model.RidgeCV` applies cross validation\\n# in order to determine which value of the regularization parameter (`alpha`)\\n# is best suited for prediction.\\n\\nfrom sklearn.linear_model import RidgeCV\\n\\nalphas = np.logspace(-10, 10, 21)  # alpha values to be chosen from by cross-validation\\nmodel = make_pipeline(\\n    preprocessor,\\n    TransformedTargetRegressor(\\n        regressor=RidgeCV(alphas=alphas),\\n        func=np.log10,\\n        inverse_func=sp.special.exp10,\\n    ),\\n)\\nmodel.fit(X_train, y_train)\\n\\n# %%\\n# First we check which value of :math:`\\\\alpha` has been selected.\\n\\nmodel[-1].regressor_.alpha_\\n\\n# %%\\n# Then we check the quality of the predictions.\\nmae_train = median_absolute_error(y_train, model.predict(X_train))\\ny_pred = model.predict(X_test)\\nmae_test = median_absolute_error(y_test, y_pred)\\nscores = {\\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\\n}\\n\\n_, ax = plt.subplots(figsize=(5, 5))\\ndisplay = PredictionErrorDisplay.from_predictions(\\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\\n)\\nax.set_title(\"Ridge model, optimum regularization\")\\nfor name, score in scores.items():\\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\\nax.legend(loc=\"upper left\")\\nplt.tight_layout()\\n\\n# %%\\n# The ability to reproduce the data of the regularized model is similar to\\n# the one of the non-regularized model.\\n\\ncoefs = pd.DataFrame(\\n    model[-1].regressor_.coef_,\\n    columns=[\"Coefficients importance\"],\\n    index=feature_names,\\n)\\ncoefs.plot.barh(figsize=(9, 7))\\nplt.title(\"Ridge model, with regularization, normalized variables\")\\nplt.xlabel(\"Raw coefficient values\")\\nplt.axvline(x=0, color=\".5\")\\nplt.subplots_adjust(left=0.3)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The coefficients are significantly different.\\n# AGE and EXPERIENCE coefficients are both positive but they now have less\\n# influence on the prediction.\\n#\\n# The regularization reduces the influence of correlated\\n# variables on the model because the weight is shared between the two\\n# predictive variables, so neither alone would have strong weights.\\n#\\n# On the other hand, the weights obtained with regularization are more\\n# stable (see the :ref:`ridge_regression` User Guide section). This\\n# increased stability is visible from the plot, obtained from data\\n# perturbations, in a cross-validation. This plot can be compared with\\n# the :ref:`previous one<covariation>`.\\n\\ncv_model = cross_validate(\\n    model,\\n    X,\\n    y,\\n    cv=cv,\\n    return_estimator=True,\\n    n_jobs=2,\\n)\\ncoefs = pd.DataFrame(\\n    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\\n)\\n\\n# %%\\nplt.ylabel(\"Age coefficient\")\\nplt.xlabel(\"Experience coefficient\")\\nplt.grid(True)\\nplt.xlim(-0.4, 0.5)\\nplt.ylim(-0.4, 0.5)\\nplt.scatter(coefs[\"AGE\"], coefs[\"EXPERIENCE\"])\\n_ = plt.title(\"Co-variations of coefficients for AGE and EXPERIENCE across folds\")\\n\\n# %%\\n# Linear models with sparse coefficients\\n# --------------------------------------\\n#\\n# Another possibility to take into account correlated variables in the dataset,\\n# is to estimate sparse coefficients. In some way we already did it manually\\n# when we dropped the AGE column in a previous ridge estimation.\\n#\\n# Lasso models (see the :ref:`lasso` User Guide section) estimates sparse\\n# coefficients. :class:`~sklearn.linear_model.LassoCV` applies cross\\n# validation in order to determine which value of the regularization parameter\\n# (`alpha`) is best suited for the model estimation.\\n\\nfrom sklearn.linear_model import LassoCV\\n\\nalphas = np.logspace(-10, 10, 21)  # alpha values to be chosen from by cross-validation\\nmodel = make_pipeline(\\n    preprocessor,\\n    TransformedTargetRegressor(\\n        regressor=LassoCV(alphas=alphas, max_iter=100_000),\\n        func=np.log10,\\n        inverse_func=sp.special.exp10,\\n    ),\\n)\\n\\n_ = model.fit(X_train, y_train)\\n\\n# %%\\n# First we verify which value of :math:`\\\\alpha` has been selected.\\n\\nmodel[-1].regressor_.alpha_\\n\\n# %%\\n# Then we check the quality of the predictions.\\n\\nmae_train = median_absolute_error(y_train, model.predict(X_train))\\ny_pred = model.predict(X_test)\\nmae_test = median_absolute_error(y_test, y_pred)\\nscores = {\\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\\n}\\n\\n_, ax = plt.subplots(figsize=(6, 6))\\ndisplay = PredictionErrorDisplay.from_predictions(\\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\\n)\\nax.set_title(\"Lasso model, optimum regularization\")\\nfor name, score in scores.items():\\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\\nax.legend(loc=\"upper left\")\\nplt.tight_layout()\\n\\n# %%\\n# For our dataset, again the model is not very predictive.'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# For our dataset, again the model is not very predictive.\\n\\ncoefs = pd.DataFrame(\\n    model[-1].regressor_.coef_,\\n    columns=[\"Coefficients importance\"],\\n    index=feature_names,\\n)\\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\\nplt.title(\"Lasso model, optimum regularization, normalized variables\")\\nplt.axvline(x=0, color=\".5\")\\nplt.subplots_adjust(left=0.3)\\n\\n# %%\\n# A Lasso model identifies the correlation between\\n# AGE and EXPERIENCE and suppresses one of them for the sake of the prediction.\\n#\\n# It is important to keep in mind that the coefficients that have been\\n# dropped may still be related to the outcome by themselves: the model\\n# chose to suppress them because they bring little or no additional\\n# information on top of the other features. Additionally, this selection\\n# is unstable for correlated features, and should be interpreted with\\n# caution.\\n#\\n# Indeed, we can check the variability of the coefficients across folds.\\ncv_model = cross_validate(\\n    model,\\n    X,\\n    y,\\n    cv=cv,\\n    return_estimator=True,\\n    n_jobs=2,\\n)\\ncoefs = pd.DataFrame(\\n    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\\n)\\n\\n# %%\\nplt.figure(figsize=(9, 7))\\nsns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\\nsns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=100)\\nplt.axvline(x=0, color=\".5\")\\nplt.title(\"Coefficient variability\")\\nplt.subplots_adjust(left=0.3)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_linear_model_coefficient_interpretation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\nplt.figure(figsize=(9, 7))\\nsns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\\nsns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=100)\\nplt.axvline(x=0, color=\".5\")\\nplt.title(\"Coefficient variability\")\\nplt.subplots_adjust(left=0.3)\\n\\n# %%\\n# We observe that the AGE and EXPERIENCE coefficients are varying a lot\\n# depending of the fold.\\n#\\n# Wrong causal interpretation\\n# ---------------------------\\n#\\n# Policy makers might want to know the effect of education on wage to assess\\n# whether or not a certain policy designed to entice people to pursue more\\n# education would make economic sense. While Machine Learning models are great\\n# for measuring statistical associations, they are generally unable to infer\\n# causal effects.\\n#\\n# It might be tempting to look at the coefficient of education on wage from our\\n# last model (or any model for that matter) and conclude that it captures the\\n# true effect of a change in the standardized education variable on wages.\\n#\\n# Unfortunately there are likely unobserved confounding variables that either\\n# inflate or deflate that coefficient. A confounding variable is a variable that\\n# causes both EDUCATION and WAGE. One example of such variable is ability.\\n# Presumably, more able people are more likely to pursue education while at the\\n# same time being more likely to earn a higher hourly wage at any level of\\n# education. In this case, ability induces a positive `Omitted Variable Bias\\n# <https://en.wikipedia.org/wiki/Omitted-variable_bias>`_ (OVB) on the EDUCATION\\n# coefficient, thereby exaggerating the effect of education on wages.\\n#\\n# See the :ref:`sphx_glr_auto_examples_inspection_plot_causal_interpretation.py`\\n# for a simulated case of ability OVB.\\n#\\n# Lessons learned\\n# ---------------\\n#\\n# * Coefficients must be scaled to the same unit of measure to retrieve\\n#   feature importance. Scaling them with the standard-deviation of the\\n#   feature is a useful proxy.\\n# * Interpreting causality is difficult when there are confounding effects. If\\n#   the relationship between two variables is also affected by something\\n#   unobserved, we should be careful when making conclusions about causality.\\n# * Coefficients in multivariate linear models represent the dependency\\n#   between a given feature and the target, **conditional** on the other\\n#   features.\\n# * Correlated features induce instabilities in the coefficients of linear\\n#   models and their effects cannot be well teased apart.\\n# * Different linear models respond differently to feature correlation and\\n#   coefficients could significantly vary from one another.\\n# * Inspecting coefficients across the folds of a cross-validation loop\\n#   gives an idea of their stability.\\n# * Coefficients are unlikely to have any causal meaning. They tend\\n#   to be biased by unobserved confounders.\\n# * Inspection tools may not necessarily provide insights on the true\\n#   data generating process.'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_partial_dependence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================================================\\nPartial Dependence and Individual Conditional Expectation Plots\\n===============================================================\\n\\nPartial dependence plots show the dependence between the target function [2]_\\nand a set of features of interest, marginalizing over the values of all other\\nfeatures (the complement features). Due to the limits of human perception, the\\nsize of the set of features of interest must be small (usually, one or two)\\nthus they are usually chosen among the most important features.\\n\\nSimilarly, an individual conditional expectation (ICE) plot [3]_\\nshows the dependence between the target function and a feature of interest.\\nHowever, unlike partial dependence plots, which show the average effect of the\\nfeatures of interest, ICE plots visualize the dependence of the prediction on a\\nfeature for each :term:`sample` separately, with one line per sample.\\nOnly one feature of interest is supported for ICE plots.\\n\\nThis example shows how to obtain partial dependence and ICE plots from a\\n:class:`~sklearn.neural_network.MLPRegressor` and a\\n:class:`~sklearn.ensemble.HistGradientBoostingRegressor` trained on the\\nbike sharing dataset. The example is inspired by [1]_.\\n\\n.. [1] `Molnar, Christoph. \"Interpretable machine learning.\\n       A Guide for Making Black Box Models Explainable\",\\n       2019. <https://christophm.github.io/interpretable-ml-book/>`_\\n\\n.. [2] For classification you can think of it as the regression score before\\n       the link function.\\n\\n.. [3] :arxiv:`Goldstein, A., Kapelner, A., Bleich, J., and Pitkin, E. (2015).\\n       \"Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of\\n       Individual Conditional Expectation\". Journal of Computational and\\n       Graphical Statistics, 24(1): 44-65 <1309.6392>`\\n\"\"\"\\n\\n# %%\\n# Bike sharing dataset preprocessing\\n# ----------------------------------\\n#\\n# We will use the bike sharing dataset. The goal is to predict the number of bike\\n# rentals using weather and season data as well as the datetime information.\\nfrom sklearn.datasets import fetch_openml\\n\\nbikes = fetch_openml(\"Bike_Sharing_Demand\", version=2, as_frame=True)\\n# Make an explicit copy to avoid \"SettingWithCopyWarning\" from pandas\\nX, y = bikes.data.copy(), bikes.target\\n\\n# We use only a subset of the data to speed up the example.\\nX = X.iloc[::5, :]\\ny = y[::5]\\n\\n# %%\\n# The feature `\"weather\"` has a particularity: the category `\"heavy_rain\"` is a rare\\n# category.\\nX[\"weather\"].value_counts()\\n\\n# %%\\n# Because of this rare category, we collapse it into `\"rain\"`.\\nX[\"weather\"] = (\\n    X[\"weather\"]\\n    .astype(object)\\n    .replace(to_replace=\"heavy_rain\", value=\"rain\")\\n    .astype(\"category\")\\n)\\n\\n# %%\\n# We now have a closer look at the `\"year\"` feature:\\nX[\"year\"].value_counts()'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_partial_dependence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Because of this rare category, we collapse it into `\"rain\"`.\\nX[\"weather\"] = (\\n    X[\"weather\"]\\n    .astype(object)\\n    .replace(to_replace=\"heavy_rain\", value=\"rain\")\\n    .astype(\"category\")\\n)\\n\\n# %%\\n# We now have a closer look at the `\"year\"` feature:\\nX[\"year\"].value_counts()\\n\\n# %%\\n# We see that we have data from two years. We use the first year to train the\\n# model and the second year to test the model.\\nmask_training = X[\"year\"] == 0.0\\nX = X.drop(columns=[\"year\"])\\nX_train, y_train = X[mask_training], y[mask_training]\\nX_test, y_test = X[~mask_training], y[~mask_training]\\n\\n# %%\\n# We can check the dataset information to see that we have heterogeneous data types. We\\n# have to preprocess the different columns accordingly.\\nX_train.info()\\n\\n# %%\\n# From the previous information, we will consider the `category` columns as nominal\\n# categorical features. In addition, we will consider the date and time information as\\n# categorical features as well.\\n#\\n# We manually define the columns containing numerical and categorical\\n# features.\\nnumerical_features = [\\n    \"temp\",\\n    \"feel_temp\",\\n    \"humidity\",\\n    \"windspeed\",\\n]\\ncategorical_features = X_train.columns.drop(numerical_features)\\n\\n# %%\\n# Before we go into the details regarding the preprocessing of the different machine\\n# learning pipelines, we will try to get some additional intuition regarding the dataset\\n# that will be helpful to understand the model\\'s statistical performance and results of\\n# the partial dependence analysis.\\n#\\n# We plot the average number of bike rentals by grouping the data by season and\\n# by year.\\nfrom itertools import product\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndays = (\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")\\nhours = tuple(range(24))\\nxticklabels = [f\"{day}\\\\n{hour}:00\" for day, hour in product(days, hours)]\\nxtick_start, xtick_period = 6, 12\\n\\nfig, axs = plt.subplots(nrows=2, figsize=(8, 6), sharey=True, sharex=True)\\naverage_bike_rentals = bikes.frame.groupby(\\n    [\"year\", \"season\", \"weekday\", \"hour\"], observed=True\\n).mean(numeric_only=True)[\"count\"]\\nfor ax, (idx, df) in zip(axs, average_bike_rentals.groupby(\"year\")):\\n    df.groupby(\"season\", observed=True).plot(ax=ax, legend=True)\\n\\n    # decorate the plot\\n    ax.set_xticks(\\n        np.linspace(\\n            start=xtick_start,\\n            stop=len(xticklabels),\\n            num=len(xticklabels) // xtick_period,\\n        )\\n    )\\n    ax.set_xticklabels(xticklabels[xtick_start::xtick_period])\\n    ax.set_xlabel(\"\")\\n    ax.set_ylabel(\"Average number of bike rentals\")\\n    ax.set_title(\\n        f\"Bike rental for {\\'2010 (train set)\\' if idx == 0.0 else \\'2011 (test set)\\'}\"\\n    )\\n    ax.set_ylim(0, 1_000)\\n    ax.set_xlim(0, len(xticklabels))\\n    ax.legend(loc=2)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_partial_dependence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The first striking difference between the train and test set is that the number of\\n# bike rentals is higher in the test set. For this reason, it will not be surprising to\\n# get a machine learning model that underestimates the number of bike rentals. We\\n# also observe that the number of bike rentals is lower during the spring season. In\\n# addition, we see that during working days, there is a specific pattern around 6-7\\n# am and 5-6 pm with some peaks of bike rentals. We can keep in mind these different\\n# insights and use them to understand the partial dependence plot.\\n#\\n# Preprocessor for machine-learning models\\n# ----------------------------------------\\n#\\n# Since we later use two different models, a\\n# :class:`~sklearn.neural_network.MLPRegressor` and a\\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor`, we create two different\\n# preprocessors, specific for each model.\\n#\\n# Preprocessor for the neural network model\\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n#\\n# We will use a :class:`~sklearn.preprocessing.QuantileTransformer` to scale the\\n# numerical features and encode the categorical features with a\\n# :class:`~sklearn.preprocessing.OneHotEncoder`.\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer\\n\\nmlp_preprocessor = ColumnTransformer(\\n    transformers=[\\n        (\"num\", QuantileTransformer(n_quantiles=100), numerical_features),\\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\\n    ]\\n)\\nmlp_preprocessor\\n\\n# %%\\n# Preprocessor for the gradient boosting model\\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n#\\n# For the gradient boosting model, we leave the numerical features as-is and only\\n# encode the categorical features using a\\n# :class:`~sklearn.preprocessing.OrdinalEncoder`.\\nfrom sklearn.preprocessing import OrdinalEncoder\\n\\nhgbdt_preprocessor = ColumnTransformer(\\n    transformers=[\\n        (\"cat\", OrdinalEncoder(), categorical_features),\\n        (\"num\", \"passthrough\", numerical_features),\\n    ],\\n    sparse_threshold=1,\\n    verbose_feature_names_out=False,\\n).set_output(transform=\"pandas\")\\nhgbdt_preprocessor\\n\\n# %%\\n# 1-way partial dependence with different models\\n# ----------------------------------------------\\n#\\n# In this section, we will compute 1-way partial dependence with two different\\n# machine-learning models: (i) a multi-layer perceptron and (ii) a\\n# gradient-boosting model. With these two models, we illustrate how to compute and\\n# interpret both partial dependence plot (PDP) for both numerical and categorical\\n# features and individual conditional expectation (ICE).\\n#\\n# Multi-layer perceptron\\n# ~~~~~~~~~~~~~~~~~~~~~~\\n#\\n# Let\\'s fit a :class:`~sklearn.neural_network.MLPRegressor` and compute\\n# single-variable partial dependence plots.\\nfrom time import time\\n\\nfrom sklearn.neural_network import MLPRegressor\\nfrom sklearn.pipeline import make_pipeline'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_partial_dependence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.neural_network import MLPRegressor\\nfrom sklearn.pipeline import make_pipeline\\n\\nprint(\"Training MLPRegressor...\")\\ntic = time()\\nmlp_model = make_pipeline(\\n    mlp_preprocessor,\\n    MLPRegressor(\\n        hidden_layer_sizes=(30, 15),\\n        learning_rate_init=0.01,\\n        early_stopping=True,\\n        random_state=0,\\n    ),\\n)\\nmlp_model.fit(X_train, y_train)\\nprint(f\"done in {time() - tic:.3f}s\")\\nprint(f\"Test R2 score: {mlp_model.score(X_test, y_test):.2f}\")\\n\\n# %%\\n# We configured a pipeline using the preprocessor that we created specifically for the\\n# neural network and tuned the neural network size and learning rate to get a reasonable\\n# compromise between training time and predictive performance on a test set.\\n#\\n# Importantly, this tabular dataset has very different dynamic ranges for its\\n# features. Neural networks tend to be very sensitive to features with varying\\n# scales and forgetting to preprocess the numeric feature would lead to a very\\n# poor model.\\n#\\n# It would be possible to get even higher predictive performance with a larger\\n# neural network but the training would also be significantly more expensive.\\n#\\n# Note that it is important to check that the model is accurate enough on a\\n# test set before plotting the partial dependence since there would be little\\n# use in explaining the impact of a given feature on the prediction function of\\n# a model with poor predictive performance. In this regard, our MLP model works\\n# reasonably well.\\n#\\n# We will plot the averaged partial dependence.\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.inspection import PartialDependenceDisplay\\n\\ncommon_params = {\\n    \"subsample\": 50,\\n    \"n_jobs\": 2,\\n    \"grid_resolution\": 20,\\n    \"random_state\": 0,\\n}\\n\\nprint(\"Computing partial dependence plots...\")\\nfeatures_info = {\\n    # features of interest\\n    \"features\": [\"temp\", \"humidity\", \"windspeed\", \"season\", \"weather\", \"hour\"],\\n    # type of partial dependence plot\\n    \"kind\": \"average\",\\n    # information regarding categorical features\\n    \"categorical_features\": categorical_features,\\n}\\ntic = time()\\n_, ax = plt.subplots(ncols=3, nrows=2, figsize=(9, 8), constrained_layout=True)\\ndisplay = PartialDependenceDisplay.from_estimator(\\n    mlp_model,\\n    X_train,\\n    **features_info,\\n    ax=ax,\\n    **common_params,\\n)\\nprint(f\"done in {time() - tic:.3f}s\")\\n_ = display.figure_.suptitle(\\n    (\\n        \"Partial dependence of the number of bike rentals\\\\n\"\\n        \"for the bike rental dataset with an MLPRegressor\"\\n    ),\\n    fontsize=16,\\n)\\n\\n# %%\\n# Gradient boosting\\n# ~~~~~~~~~~~~~~~~~\\n#\\n# Let\\'s now fit a :class:`~sklearn.ensemble.HistGradientBoostingRegressor` and\\n# compute the partial dependence on the same features. We also use the\\n# specific preprocessor we created for this model.\\nfrom sklearn.ensemble import HistGradientBoostingRegressor'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_partial_dependence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(\"Training HistGradientBoostingRegressor...\")\\ntic = time()\\nhgbdt_model = make_pipeline(\\n    hgbdt_preprocessor,\\n    HistGradientBoostingRegressor(\\n        categorical_features=categorical_features,\\n        random_state=0,\\n        max_iter=50,\\n    ),\\n)\\nhgbdt_model.fit(X_train, y_train)\\nprint(f\"done in {time() - tic:.3f}s\")\\nprint(f\"Test R2 score: {hgbdt_model.score(X_test, y_test):.2f}\")\\n\\n# %%\\n# Here, we used the default hyperparameters for the gradient boosting model\\n# without any preprocessing as tree-based models are naturally robust to\\n# monotonic transformations of numerical features.\\n#\\n# Note that on this tabular dataset, Gradient Boosting Machines are both\\n# significantly faster to train and more accurate than neural networks. It is\\n# also significantly cheaper to tune their hyperparameters (the defaults tend\\n# to work well while this is not often the case for neural networks).\\n#\\n# We will plot the partial dependence for some of the numerical and categorical\\n# features.\\nprint(\"Computing partial dependence plots...\")\\ntic = time()\\n_, ax = plt.subplots(ncols=3, nrows=2, figsize=(9, 8), constrained_layout=True)\\ndisplay = PartialDependenceDisplay.from_estimator(\\n    hgbdt_model,\\n    X_train,\\n    **features_info,\\n    ax=ax,\\n    **common_params,\\n)\\nprint(f\"done in {time() - tic:.3f}s\")\\n_ = display.figure_.suptitle(\\n    (\\n        \"Partial dependence of the number of bike rentals\\\\n\"\\n        \"for the bike rental dataset with a gradient boosting\"\\n    ),\\n    fontsize=16,\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_partial_dependence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Analysis of the plots\\n# ~~~~~~~~~~~~~~~~~~~~~\\n#\\n# We will first look at the PDPs for the numerical features. For both models, the\\n# general trend of the PDP of the temperature is that the number of bike rentals is\\n# increasing with temperature. We can make a similar analysis but with the opposite\\n# trend for the humidity features. The number of bike rentals is decreasing when the\\n# humidity increases. Finally, we see the same trend for the wind speed feature. The\\n# number of bike rentals is decreasing when the wind speed is increasing for both\\n# models. We also observe that :class:`~sklearn.neural_network.MLPRegressor` has much\\n# smoother predictions than :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.\\n#\\n# Now, we will look at the partial dependence plots for the categorical features.\\n#\\n# We observe that the spring season is the lowest bar for the season feature. With the\\n# weather feature, the rain category is the lowest bar. Regarding the hour feature,\\n# we see two peaks around the 7 am and 6 pm. These findings are in line with the\\n# the observations we made earlier on the dataset.\\n#\\n# However, it is worth noting that we are creating potential meaningless\\n# synthetic samples if features are correlated.\\n#\\n# ICE vs. PDP\\n# ~~~~~~~~~~~\\n# PDP is an average of the marginal effects of the features. We are averaging the\\n# response of all samples of the provided set. Thus, some effects could be hidden. In\\n# this regard, it is possible to plot each individual response. This representation is\\n# called the Individual Effect Plot (ICE). In the plot below, we plot 50 randomly\\n# selected ICEs for the temperature and humidity features.\\nprint(\"Computing partial dependence plots and individual conditional expectation...\")\\ntic = time()\\n_, ax = plt.subplots(ncols=2, figsize=(6, 4), sharey=True, constrained_layout=True)\\n\\nfeatures_info = {\\n    \"features\": [\"temp\", \"humidity\"],\\n    \"kind\": \"both\",\\n    \"centered\": True,\\n}\\n\\ndisplay = PartialDependenceDisplay.from_estimator(\\n    hgbdt_model,\\n    X_train,\\n    **features_info,\\n    ax=ax,\\n    **common_params,\\n)\\nprint(f\"done in {time() - tic:.3f}s\")\\n_ = display.figure_.suptitle(\"ICE and PDP representations\", fontsize=16)\\n\\n# %%\\n# We see that the ICE for the temperature feature gives us some additional information:\\n# Some of the ICE lines are flat while some others show a decrease of the dependence\\n# for temperature above 35 degrees Celsius. We observe a similar pattern for the\\n# humidity feature: some of the ICEs lines show a sharp decrease when the humidity is\\n# above 80%.\\n#\\n# Not all ICE lines are parallel, this indicates that the model finds\\n# interactions between features. We can repeat the experiment by constraining the\\n# gradient boosting model to not use any interactions between features using the\\n# parameter `interaction_cst`:\\nfrom sklearn.base import clone'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_partial_dependence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='interaction_cst = [[i] for i in range(X_train.shape[1])]\\nhgbdt_model_without_interactions = (\\n    clone(hgbdt_model)\\n    .set_params(histgradientboostingregressor__interaction_cst=interaction_cst)\\n    .fit(X_train, y_train)\\n)\\nprint(f\"Test R2 score: {hgbdt_model_without_interactions.score(X_test, y_test):.2f}\")\\n\\n# %%\\n_, ax = plt.subplots(ncols=2, figsize=(6, 4), sharey=True, constrained_layout=True)\\n\\nfeatures_info[\"centered\"] = False\\ndisplay = PartialDependenceDisplay.from_estimator(\\n    hgbdt_model_without_interactions,\\n    X_train,\\n    **features_info,\\n    ax=ax,\\n    **common_params,\\n)\\n_ = display.figure_.suptitle(\"ICE and PDP representations\", fontsize=16)\\n\\n# %%\\n# 2D interaction plots\\n# --------------------\\n#\\n# PDPs with two features of interest enable us to visualize interactions among them.\\n# However, ICEs cannot be plotted in an easy manner and thus interpreted. We will show\\n# the representation of available in\\n# :meth:`~sklearn.inspection.PartialDependenceDisplay.from_estimator` that is a 2D\\n# heatmap.\\nprint(\"Computing partial dependence plots...\")\\nfeatures_info = {\\n    \"features\": [\"temp\", \"humidity\", (\"temp\", \"humidity\")],\\n    \"kind\": \"average\",\\n}\\n_, ax = plt.subplots(ncols=3, figsize=(10, 4), constrained_layout=True)\\ntic = time()\\ndisplay = PartialDependenceDisplay.from_estimator(\\n    hgbdt_model,\\n    X_train,\\n    **features_info,\\n    ax=ax,\\n    **common_params,\\n)\\nprint(f\"done in {time() - tic:.3f}s\")\\n_ = display.figure_.suptitle(\\n    \"1-way vs 2-way of numerical PDP using gradient boosting\", fontsize=16\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_partial_dependence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The two-way partial dependence plot shows the dependence of the number of bike rentals\\n# on joint values of temperature and humidity.\\n# We clearly see an interaction between the two features. For a temperature higher than\\n# 20 degrees Celsius, the humidity has a impact on the number of bike rentals\\n# that seems independent on the temperature.\\n#\\n# On the other hand, for temperatures lower than 20 degrees Celsius, both the\\n# temperature and humidity continuously impact the number of bike rentals.\\n#\\n# Furthermore, the slope of the of the impact ridge of the 20 degrees Celsius\\n# threshold is very dependent on the humidity level: the ridge is steep under\\n# dry conditions but much smoother under wetter conditions above 70% of humidity.\\n#\\n# We now contrast those results with the same plots computed for the model\\n# constrained to learn a prediction function that does not depend on such\\n# non-linear feature interactions.\\nprint(\"Computing partial dependence plots...\")\\nfeatures_info = {\\n    \"features\": [\"temp\", \"humidity\", (\"temp\", \"humidity\")],\\n    \"kind\": \"average\",\\n}\\n_, ax = plt.subplots(ncols=3, figsize=(10, 4), constrained_layout=True)\\ntic = time()\\ndisplay = PartialDependenceDisplay.from_estimator(\\n    hgbdt_model_without_interactions,\\n    X_train,\\n    **features_info,\\n    ax=ax,\\n    **common_params,\\n)\\nprint(f\"done in {time() - tic:.3f}s\")\\n_ = display.figure_.suptitle(\\n    \"1-way vs 2-way of numerical PDP using gradient boosting\", fontsize=16\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/inspection/plot_partial_dependence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The 1D partial dependence plots for the model constrained to not model feature\\n# interactions show local spikes for each features individually, in particular for\\n# for the \"humidity\" feature. Those spikes might be reflecting a degraded behavior\\n# of the model that attempts to somehow compensate for the forbidden interactions\\n# by overfitting particular training points. Note that the predictive performance\\n# of this model as measured on the test set is significantly worse than that of\\n# the original, unconstrained model.\\n#\\n# Also note that the number of local spikes visible on those plots is depends on\\n# the grid resolution parameter of the PD plot itself.\\n#\\n# Those local spikes result in a noisily gridded 2D PD plot. It is quite\\n# challenging to tell whether or not there are no interaction between those\\n# features because of the high frequency oscillations in the humidity feature.\\n# However it can clearly be seen that the simple interaction effect observed when\\n# the temperature crosses the 20 degrees boundary is no longer visible for this\\n# model.\\n#\\n# The partial dependence between categorical features will provide a discrete\\n# representation that can be shown as a heatmap. For instance the interaction between\\n# the season, the weather, and the target would be as follow:\\nprint(\"Computing partial dependence plots...\")\\nfeatures_info = {\\n    \"features\": [\"season\", \"weather\", (\"season\", \"weather\")],\\n    \"kind\": \"average\",\\n    \"categorical_features\": categorical_features,\\n}\\n_, ax = plt.subplots(ncols=3, figsize=(14, 6), constrained_layout=True)\\ntic = time()\\ndisplay = PartialDependenceDisplay.from_estimator(\\n    hgbdt_model,\\n    X_train,\\n    **features_info,\\n    ax=ax,\\n    **common_params,\\n)\\n\\nprint(f\"done in {time() - tic:.3f}s\")\\n_ = display.figure_.suptitle(\\n    \"1-way vs 2-way PDP of categorical features using gradient boosting\", fontsize=16\\n)\\n\\n# %%\\n# 3D representation\\n# ~~~~~~~~~~~~~~~~~\\n#\\n# Let\\'s make the same partial dependence plot for the 2 features interaction,\\n# this time in 3 dimensions.\\n# unused but required import for doing 3d projections with matplotlib < 3.2\\nimport mpl_toolkits.mplot3d  # noqa: F401\\nimport numpy as np\\n\\nfrom sklearn.inspection import partial_dependence\\n\\nfig = plt.figure(figsize=(5.5, 5))\\n\\nfeatures = (\"temp\", \"humidity\")\\npdp = partial_dependence(\\n    hgbdt_model, X_train, features=features, kind=\"average\", grid_resolution=10\\n)\\nXX, YY = np.meshgrid(pdp[\"grid_values\"][0], pdp[\"grid_values\"][1])\\nZ = pdp.average[0].T\\nax = fig.add_subplot(projection=\"3d\")\\nfig.add_axes(ax)\\n\\nsurf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu, edgecolor=\"k\")\\nax.set_xlabel(features[0])\\nax.set_ylabel(features[1])\\nfig.suptitle(\\n    \"PD of number of bike rentals on\\\\nthe temperature and humidity GBDT model\",\\n    fontsize=16,\\n)\\n# pretty init view\\nax.view_init(elev=22, azim=122)\\nclb = plt.colorbar(surf, pad=0.08, shrink=0.6, aspect=10)\\nclb.ax.set_title(\"Partial\\\\ndependence\")\\nplt.show()\\n\\n# %%'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ols_ridge_variance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nOrdinary Least Squares and Ridge Regression Variance\\n=========================================================\\nDue to the few points in each dimension and the straight\\nline that linear regression uses to follow these points\\nas well as it can, noise on the observations will cause\\ngreat variance as shown in the first plot. Every line\\'s slope\\ncan vary quite a bit for each prediction due to the noise\\ninduced in the observations.\\n\\nRidge regression is basically minimizing a penalised version\\nof the least-squared function. The penalising `shrinks` the\\nvalue of the regression coefficients.\\nDespite the few data points in each dimension, the slope\\nof the prediction is much more stable and the variance\\nin the line itself is greatly reduced, in comparison to that\\nof the standard linear regression\\n\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# Modified for documentation by Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import linear_model\\n\\nX_train = np.c_[0.5, 1].T\\ny_train = [0.5, 1]\\nX_test = np.c_[0, 2].T\\n\\nnp.random.seed(0)\\n\\nclassifiers = dict(\\n    ols=linear_model.LinearRegression(), ridge=linear_model.Ridge(alpha=0.1)\\n)\\n\\nfor name, clf in classifiers.items():\\n    fig, ax = plt.subplots(figsize=(4, 3))\\n\\n    for _ in range(6):\\n        this_X = 0.1 * np.random.normal(size=(2, 1)) + X_train\\n        clf.fit(this_X, y_train)\\n\\n        ax.plot(X_test, clf.predict(X_test), color=\"gray\")\\n        ax.scatter(this_X, y_train, s=3, c=\"gray\", marker=\"o\", zorder=10)\\n\\n    clf.fit(X_train, y_train)\\n    ax.plot(X_test, clf.predict(X_test), linewidth=2, color=\"blue\")\\n    ax.scatter(X_train, y_train, s=30, c=\"red\", marker=\"+\", zorder=10)\\n\\n    ax.set_title(name)\\n    ax.set_xlim(0, 2)\\n    ax.set_ylim((0, 1.6))\\n    ax.set_xlabel(\"X\")\\n    ax.set_ylabel(\"y\")\\n\\n    fig.tight_layout()\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_iris.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_hyperplane(c, color):\\n    def line(x0):\\n        return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\\n\\n    plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls=\"--\", color=color)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_iris.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n========================================\\nPlot multi-class SGD on the iris dataset\\n========================================\\n\\nPlot decision surface of multi-class SGD on iris dataset.\\nThe hyperplanes corresponding to the three one-versus-all (OVA) classifiers\\nare represented by the dashed lines.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.linear_model import SGDClassifier\\n\\n# import some data to play with\\niris = datasets.load_iris()\\n\\n# we only take the first two features. We could\\n# avoid this ugly slicing by using a two-dim dataset\\nX = iris.data[:, :2]\\ny = iris.target\\ncolors = \"bry\"\\n\\n# shuffle\\nidx = np.arange(X.shape[0])\\nnp.random.seed(13)\\nnp.random.shuffle(idx)\\nX = X[idx]\\ny = y[idx]\\n\\n# standardize\\nmean = X.mean(axis=0)\\nstd = X.std(axis=0)\\nX = (X - mean) / std\\n\\nclf = SGDClassifier(alpha=0.001, max_iter=100).fit(X, y)\\nax = plt.gca()\\nDecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    cmap=plt.cm.Paired,\\n    ax=ax,\\n    response_method=\"predict\",\\n    xlabel=iris.feature_names[0],\\n    ylabel=iris.feature_names[1],\\n)\\nplt.axis(\"tight\")\\n\\n# Plot also the training points\\nfor i, color in zip(clf.classes_, colors):\\n    idx = np.where(y == i)\\n    plt.scatter(\\n        X[idx, 0],\\n        X[idx, 1],\\n        c=color,\\n        label=iris.target_names[i],\\n        edgecolor=\"black\",\\n        s=20,\\n    )\\nplt.title(\"Decision surface of multi-class SGD\")\\nplt.axis(\"tight\")\\n\\n# Plot the three one-against-all classifiers\\nxmin, xmax = plt.xlim()\\nymin, ymax = plt.ylim()\\ncoef = clf.coef_\\nintercept = clf.intercept_\\n\\n\\n# Code for: def plot_hyperplane(c, color):\\n\\n\\nfor i, color in zip(clf.classes_, colors):\\n    plot_hyperplane(i, color)\\nplt.legend()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ard.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================\\nComparing Linear Bayesian Regressors\\n====================================\\n\\nThis example compares two different bayesian regressors:\\n\\n - a :ref:`automatic_relevance_determination`\\n - a :ref:`bayesian_ridge_regression`\\n\\nIn the first part, we use an :ref:`ordinary_least_squares` (OLS) model as a\\nbaseline for comparing the models\\' coefficients with respect to the true\\ncoefficients. Thereafter, we show that the estimation of such models is done by\\niteratively maximizing the marginal log-likelihood of the observations.\\n\\nIn the last section we plot predictions and uncertainties for the ARD and the\\nBayesian Ridge regressions using a polynomial feature expansion to fit a\\nnon-linear relationship between `X` and `y`.\\n\\n\"\"\"\\n\\n# Author: Arturo Amor <david-arturo.amor-quiroz@inria.fr>\\n\\n# %%\\n# Models robustness to recover the ground truth weights\\n# =====================================================\\n#\\n# Generate synthetic dataset\\n# --------------------------\\n#\\n# We generate a dataset where `X` and `y` are linearly linked: 10 of the\\n# features of `X` will be used to generate `y`. The other features are not\\n# useful at predicting `y`. In addition, we generate a dataset where `n_samples\\n# == n_features`. Such a setting is challenging for an OLS model and leads\\n# potentially to arbitrary large weights. Having a prior on the weights and a\\n# penalty alleviates the problem. Finally, gaussian noise is added.\\n\\nfrom sklearn.datasets import make_regression\\n\\nX, y, true_weights = make_regression(\\n    n_samples=100,\\n    n_features=100,\\n    n_informative=10,\\n    noise=8,\\n    coef=True,\\n    random_state=42,\\n)\\n\\n# %%\\n# Fit the regressors\\n# ------------------\\n#\\n# We now fit both Bayesian models and the OLS to later compare the models\\'\\n# coefficients.\\n\\nimport pandas as pd\\n\\nfrom sklearn.linear_model import ARDRegression, BayesianRidge, LinearRegression\\n\\nolr = LinearRegression().fit(X, y)\\nbrr = BayesianRidge(compute_score=True, max_iter=30).fit(X, y)\\nard = ARDRegression(compute_score=True, max_iter=30).fit(X, y)\\ndf = pd.DataFrame(\\n    {\\n        \"Weights of true generative process\": true_weights,\\n        \"ARDRegression\": ard.coef_,\\n        \"BayesianRidge\": brr.coef_,\\n        \"LinearRegression\": olr.coef_,\\n    }\\n)\\n\\n# %%\\n# Plot the true and estimated coefficients\\n# ----------------------------------------\\n#\\n# Now we compare the coefficients of each model with the weights of\\n# the true generative model.\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom matplotlib.colors import SymLogNorm\\n\\nplt.figure(figsize=(10, 6))\\nax = sns.heatmap(\\n    df.T,\\n    norm=SymLogNorm(linthresh=10e-4, vmin=-80, vmax=80),\\n    cbar_kws={\"label\": \"coefficients\\' values\"},\\n    cmap=\"seismic_r\",\\n)\\nplt.ylabel(\"linear model\")\\nplt.xlabel(\"coefficients\")\\nplt.tight_layout(rect=(0, 0, 1, 0.95))\\n_ = plt.title(\"Models\\' coefficients\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ard.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Due to the added noise, none of the models recover the true weights. Indeed,\\n# all models always have more than 10 non-zero coefficients. Compared to the OLS\\n# estimator, the coefficients using a Bayesian Ridge regression are slightly\\n# shifted toward zero, which stabilises them. The ARD regression provides a\\n# sparser solution: some of the non-informative coefficients are set exactly to\\n# zero, while shifting others closer to zero. Some non-informative coefficients\\n# are still present and retain large values.\\n\\n# %%\\n# Plot the marginal log-likelihood\\n# --------------------------------\\nimport numpy as np\\n\\nard_scores = -np.array(ard.scores_)\\nbrr_scores = -np.array(brr.scores_)\\nplt.plot(ard_scores, color=\"navy\", label=\"ARD\")\\nplt.plot(brr_scores, color=\"red\", label=\"BayesianRidge\")\\nplt.ylabel(\"Log-likelihood\")\\nplt.xlabel(\"Iterations\")\\nplt.xlim(1, 30)\\nplt.legend()\\n_ = plt.title(\"Models log-likelihood\")\\n\\n# %%\\n# Indeed, both models minimize the log-likelihood up to an arbitrary cutoff\\n# defined by the `max_iter` parameter.\\n#\\n# Bayesian regressions with polynomial feature expansion\\n# ======================================================\\n# Generate synthetic dataset\\n# --------------------------\\n# We create a target that is a non-linear function of the input feature.\\n# Noise following a standard uniform distribution is added.\\n\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\\n\\nrng = np.random.RandomState(0)\\nn_samples = 110\\n\\n# sort the data to make plotting easier later\\nX = np.sort(-10 * rng.rand(n_samples) + 10)\\nnoise = rng.normal(0, 1, n_samples) * 1.35\\ny = np.sqrt(X) * np.sin(X) + noise\\nfull_data = pd.DataFrame({\"input_feature\": X, \"target\": y})\\nX = X.reshape((-1, 1))\\n\\n# extrapolation\\nX_plot = np.linspace(10, 10.4, 10)\\ny_plot = np.sqrt(X_plot) * np.sin(X_plot)\\nX_plot = np.concatenate((X, X_plot.reshape((-1, 1))))\\ny_plot = np.concatenate((y - noise, y_plot))\\n\\n# %%\\n# Fit the regressors\\n# ------------------\\n#\\n# Here we try a degree 10 polynomial to potentially overfit, though the bayesian\\n# linear models regularize the size of the polynomial coefficients. As\\n# `fit_intercept=True` by default for\\n# :class:`~sklearn.linear_model.ARDRegression` and\\n# :class:`~sklearn.linear_model.BayesianRidge`, then\\n# :class:`~sklearn.preprocessing.PolynomialFeatures` should not introduce an\\n# additional bias feature. By setting `return_std=True`, the bayesian regressors\\n# return the standard deviation of the posterior distribution for the model\\n# parameters.\\n\\nard_poly = make_pipeline(\\n    PolynomialFeatures(degree=10, include_bias=False),\\n    StandardScaler(),\\n    ARDRegression(),\\n).fit(X, y)\\nbrr_poly = make_pipeline(\\n    PolynomialFeatures(degree=10, include_bias=False),\\n    StandardScaler(),\\n    BayesianRidge(),\\n).fit(X, y)\\n\\ny_ard, y_ard_std = ard_poly.predict(X_plot, return_std=True)\\ny_brr, y_brr_std = brr_poly.predict(X_plot, return_std=True)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ard.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='y_ard, y_ard_std = ard_poly.predict(X_plot, return_std=True)\\ny_brr, y_brr_std = brr_poly.predict(X_plot, return_std=True)\\n\\n# %%\\n# Plotting polynomial regressions with std errors of the scores\\n# -------------------------------------------------------------\\n\\nax = sns.scatterplot(\\n    data=full_data, x=\"input_feature\", y=\"target\", color=\"black\", alpha=0.75\\n)\\nax.plot(X_plot, y_plot, color=\"black\", label=\"Ground Truth\")\\nax.plot(X_plot, y_brr, color=\"red\", label=\"BayesianRidge with polynomial features\")\\nax.plot(X_plot, y_ard, color=\"navy\", label=\"ARD with polynomial features\")\\nax.fill_between(\\n    X_plot.ravel(),\\n    y_ard - y_ard_std,\\n    y_ard + y_ard_std,\\n    color=\"navy\",\\n    alpha=0.3,\\n)\\nax.fill_between(\\n    X_plot.ravel(),\\n    y_brr - y_brr_std,\\n    y_brr + y_brr_std,\\n    color=\"red\",\\n    alpha=0.3,\\n)\\nax.legend()\\n_ = ax.set_title(\"Polynomial fit of a non-linear feature\")\\n\\n# %%\\n# The error bars represent one standard deviation of the predicted gaussian\\n# distribution of the query points. Notice that the ARD regression captures the\\n# ground truth the best when using the default parameters in both models, but\\n# further reducing the `lambda_init` hyperparameter of the Bayesian Ridge can\\n# reduce its bias (see example\\n# :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`).\\n# Finally, due to the intrinsic limitations of a polynomial regression, both\\n# models fail when extrapolating.'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_nnls.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================\\nNon-negative least squares\\n==========================\\n\\nIn this example, we fit a linear model with positive constraints on the\\nregression coefficients and compare the estimated coefficients to a classic\\nlinear regression.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.metrics import r2_score\\n\\n# %%\\n# Generate some random data\\nnp.random.seed(42)\\n\\nn_samples, n_features = 200, 50\\nX = np.random.randn(n_samples, n_features)\\ntrue_coef = 3 * np.random.randn(n_features)\\n# Threshold coefficients to render them non-negative\\ntrue_coef[true_coef < 0] = 0\\ny = np.dot(X, true_coef)\\n\\n# Add some noise\\ny += 5 * np.random.normal(size=(n_samples,))\\n\\n# %%\\n# Split the data in train set and test set\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\\n\\n# %%\\n# Fit the Non-Negative least squares.\\nfrom sklearn.linear_model import LinearRegression\\n\\nreg_nnls = LinearRegression(positive=True)\\ny_pred_nnls = reg_nnls.fit(X_train, y_train).predict(X_test)\\nr2_score_nnls = r2_score(y_test, y_pred_nnls)\\nprint(\"NNLS R2 score\", r2_score_nnls)\\n\\n# %%\\n# Fit an OLS.\\nreg_ols = LinearRegression()\\ny_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)\\nr2_score_ols = r2_score(y_test, y_pred_ols)\\nprint(\"OLS R2 score\", r2_score_ols)\\n\\n\\n# %%\\n# Comparing the regression coefficients between OLS and NNLS, we can observe\\n# they are highly correlated (the dashed line is the identity relation),\\n# but the non-negative constraint shrinks some to 0.\\n# The Non-Negative Least squares inherently yield sparse results.\\n\\nfig, ax = plt.subplots()\\nax.plot(reg_ols.coef_, reg_nnls.coef_, linewidth=0, marker=\".\")\\n\\nlow_x, high_x = ax.get_xlim()\\nlow_y, high_y = ax.get_ylim()\\nlow = max(low_x, low_y)\\nhigh = min(high_x, high_y)\\nax.plot([low, high], [low, high], ls=\"--\", c=\".3\", alpha=0.5)\\nax.set_xlabel(\"OLS regression coefficients\", fontweight=\"bold\")\\nax.set_ylabel(\"NNLS regression coefficients\", fontweight=\"bold\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_and_elasticnet.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================\\nL1-based models for Sparse Signals\\n==================================\\n\\nThe present example compares three l1-based regression models on a synthetic\\nsignal obtained from sparse and correlated features that are further corrupted\\nwith additive gaussian noise:\\n\\n - a :ref:`lasso`;\\n - an :ref:`automatic_relevance_determination`;\\n - an :ref:`elastic_net`.\\n\\nIt is known that the Lasso estimates turn to be close to the model selection\\nestimates when the data dimensions grow, given that the irrelevant variables are\\nnot too correlated with the relevant ones. In the presence of correlated\\nfeatures, Lasso itself cannot select the correct sparsity pattern [1]_.\\n\\nHere we compare the performance of the three models in terms of the :math:`R^2`\\nscore, the fitting time and the sparsity of the estimated coefficients when\\ncompared with the ground-truth.\\n\"\"\"\\n\\n# Author: Arturo Amor <david-arturo.amor-quiroz@inria.fr>\\n\\n# %%\\n# Generate synthetic dataset\\n# --------------------------\\n#\\n# We generate a dataset where the number of samples is lower than the total\\n# number of features. This leads to an underdetermined system, i.e. the solution\\n# is not unique, and thus we cannot apply an :ref:`ordinary_least_squares` by\\n# itself. Regularization introduces a penalty term to the objective function,\\n# which modifies the optimization problem and can help alleviate the\\n# underdetermined nature of the system.\\n#\\n# The target `y` is a linear combination with alternating signs of sinusoidal\\n# signals. Only the 10 lowest out of the 100 frequencies in `X` are used to\\n# generate `y`, while the rest of the features are not informative. This results\\n# in a high dimensional sparse feature space, where some degree of\\n# l1-penalization is necessary.\\n\\nimport numpy as np\\n\\nrng = np.random.RandomState(0)\\nn_samples, n_features, n_informative = 50, 100, 10\\ntime_step = np.linspace(-2, 2, n_samples)\\nfreqs = 2 * np.pi * np.sort(rng.rand(n_features)) / 0.01\\nX = np.zeros((n_samples, n_features))\\n\\nfor i in range(n_features):\\n    X[:, i] = np.sin(freqs[i] * time_step)\\n\\nidx = np.arange(n_features)\\ntrue_coef = (-1) ** idx * np.exp(-idx / 10)\\ntrue_coef[n_informative:] = 0  # sparsify coef\\ny = np.dot(X, true_coef)\\n\\n# %%\\n# Some of the informative features have close frequencies to induce\\n# (anti-)correlations.\\n\\nfreqs[:n_informative]\\n\\n# %%\\n# A random phase is introduced using :func:`numpy.random.random_sample`\\n# and some gaussian noise (implemented by :func:`numpy.random.normal`)\\n# is added to both the features and the target.\\n\\nfor i in range(n_features):\\n    X[:, i] = np.sin(freqs[i] * time_step + 2 * (rng.random_sample() - 0.5))\\n    X[:, i] += 0.2 * rng.normal(0, 1, n_samples)\\n\\ny += 0.2 * rng.normal(0, 1, n_samples)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_and_elasticnet.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for i in range(n_features):\\n    X[:, i] = np.sin(freqs[i] * time_step + 2 * (rng.random_sample() - 0.5))\\n    X[:, i] += 0.2 * rng.normal(0, 1, n_samples)\\n\\ny += 0.2 * rng.normal(0, 1, n_samples)\\n\\n# %%\\n# Such sparse, noisy and correlated features can be obtained, for instance, from\\n# sensor nodes monitoring some environmental variables, as they typically register\\n# similar values depending on their positions (spatial correlations).\\n# We can visualize the target.\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(time_step, y)\\nplt.ylabel(\"target signal\")\\nplt.xlabel(\"time\")\\n_ = plt.title(\"Superposition of sinusoidal signals\")\\n\\n# %%\\n# We split the data into train and test sets for simplicity. In practice one\\n# should use a :class:`~sklearn.model_selection.TimeSeriesSplit`\\n# cross-validation to estimate the variance of the test score. Here we set\\n# `shuffle=\"False\"` as we must not use training data that succeed the testing\\n# data when dealing with data that have a temporal relationship.\\n\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\\n\\n# %%\\n# In the following, we compute the performance of three l1-based models in terms\\n# of the goodness of fit :math:`R^2` score and the fitting time. Then we make a\\n# plot to compare the sparsity of the estimated coefficients with respect to the\\n# ground-truth coefficients and finally we analyze the previous results.\\n#\\n# Lasso\\n# -----\\n#\\n# In this example, we demo a :class:`~sklearn.linear_model.Lasso` with a fixed\\n# value of the regularization parameter `alpha`. In practice, the optimal\\n# parameter `alpha` should be selected by passing a\\n# :class:`~sklearn.model_selection.TimeSeriesSplit` cross-validation strategy to a\\n# :class:`~sklearn.linear_model.LassoCV`. To keep the example simple and fast to\\n# execute, we directly set the optimal value for alpha here.\\nfrom time import time\\n\\nfrom sklearn.linear_model import Lasso\\nfrom sklearn.metrics import r2_score\\n\\nt0 = time()\\nlasso = Lasso(alpha=0.14).fit(X_train, y_train)\\nprint(f\"Lasso fit done in {(time() - t0):.3f}s\")\\n\\ny_pred_lasso = lasso.predict(X_test)\\nr2_score_lasso = r2_score(y_test, y_pred_lasso)\\nprint(f\"Lasso r^2 on test data : {r2_score_lasso:.3f}\")\\n\\n# %%\\n# Automatic Relevance Determination (ARD)\\n# ---------------------------------------\\n#\\n# An ARD regression is the bayesian version of the Lasso. It can produce\\n# interval estimates for all of the parameters, including the error variance, if\\n# required. It is a suitable option when the signals have gaussian noise. See\\n# the example :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py` for a\\n# comparison of :class:`~sklearn.linear_model.ARDRegression` and\\n# :class:`~sklearn.linear_model.BayesianRidge` regressors.\\n\\nfrom sklearn.linear_model import ARDRegression\\n\\nt0 = time()\\nard = ARDRegression().fit(X_train, y_train)\\nprint(f\"ARD fit done in {(time() - t0):.3f}s\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_and_elasticnet.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.linear_model import ARDRegression\\n\\nt0 = time()\\nard = ARDRegression().fit(X_train, y_train)\\nprint(f\"ARD fit done in {(time() - t0):.3f}s\")\\n\\ny_pred_ard = ard.predict(X_test)\\nr2_score_ard = r2_score(y_test, y_pred_ard)\\nprint(f\"ARD r^2 on test data : {r2_score_ard:.3f}\")\\n\\n# %%\\n# ElasticNet\\n# ----------\\n#\\n# :class:`~sklearn.linear_model.ElasticNet` is a middle ground between\\n# :class:`~sklearn.linear_model.Lasso` and :class:`~sklearn.linear_model.Ridge`,\\n# as it combines a L1 and a L2-penalty. The amount of regularization is\\n# controlled by the two hyperparameters `l1_ratio` and `alpha`. For `l1_ratio =\\n# 0` the penalty is pure L2 and the model is equivalent to a\\n# :class:`~sklearn.linear_model.Ridge`. Similarly, `l1_ratio = 1` is a pure L1\\n# penalty and the model is equivalent to a :class:`~sklearn.linear_model.Lasso`.\\n# For `0 < l1_ratio < 1`, the penalty is a combination of L1 and L2.\\n#\\n# As done before, we train the model with fix values for `alpha` and `l1_ratio`.\\n# To select their optimal value we used an\\n# :class:`~sklearn.linear_model.ElasticNetCV`, not shown here to keep the\\n# example simple.\\n\\nfrom sklearn.linear_model import ElasticNet\\n\\nt0 = time()\\nenet = ElasticNet(alpha=0.08, l1_ratio=0.5).fit(X_train, y_train)\\nprint(f\"ElasticNet fit done in {(time() - t0):.3f}s\")\\n\\ny_pred_enet = enet.predict(X_test)\\nr2_score_enet = r2_score(y_test, y_pred_enet)\\nprint(f\"ElasticNet r^2 on test data : {r2_score_enet:.3f}\")\\n\\n# %%\\n# Plot and analysis of the results\\n# --------------------------------\\n#\\n# In this section, we use a heatmap to visualize the sparsity of the true\\n# and estimated coefficients of the respective linear models.\\n\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport seaborn as sns\\nfrom matplotlib.colors import SymLogNorm\\n\\ndf = pd.DataFrame(\\n    {\\n        \"True coefficients\": true_coef,\\n        \"Lasso\": lasso.coef_,\\n        \"ARDRegression\": ard.coef_,\\n        \"ElasticNet\": enet.coef_,\\n    }\\n)\\n\\nplt.figure(figsize=(10, 6))\\nax = sns.heatmap(\\n    df.T,\\n    norm=SymLogNorm(linthresh=10e-4, vmin=-1, vmax=1),\\n    cbar_kws={\"label\": \"coefficients\\' values\"},\\n    cmap=\"seismic_r\",\\n)\\nplt.ylabel(\"linear model\")\\nplt.xlabel(\"coefficients\")\\nplt.title(\\n    f\"Models\\' coefficients\\\\nLasso $R^2$: {r2_score_lasso:.3f}, \"\\n    f\"ARD $R^2$: {r2_score_ard:.3f}, \"\\n    f\"ElasticNet $R^2$: {r2_score_enet:.3f}\"\\n)\\nplt.tight_layout()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_and_elasticnet.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# In the present example :class:`~sklearn.linear_model.ElasticNet` yields the\\n# best score and captures the most of the predictive features, yet still fails\\n# at finding all the true components. Notice that both\\n# :class:`~sklearn.linear_model.ElasticNet` and\\n# :class:`~sklearn.linear_model.ARDRegression` result in a less sparse model\\n# than a :class:`~sklearn.linear_model.Lasso`.\\n#\\n# Conclusions\\n# -----------\\n#\\n# :class:`~sklearn.linear_model.Lasso` is known to recover sparse data\\n# effectively but does not perform well with highly correlated features. Indeed,\\n# if several correlated features contribute to the target,\\n# :class:`~sklearn.linear_model.Lasso` would end up selecting a single one of\\n# them. In the case of sparse yet non-correlated features, a\\n# :class:`~sklearn.linear_model.Lasso` model would be more suitable.\\n#\\n# :class:`~sklearn.linear_model.ElasticNet` introduces some sparsity on the\\n# coefficients and shrinks their values to zero. Thus, in the presence of\\n# correlated features that contribute to the target, the model is still able to\\n# reduce their weights without setting them exactly to zero. This results in a\\n# less sparse model than a pure :class:`~sklearn.linear_model.Lasso` and may\\n# capture non-predictive features as well.\\n#\\n# :class:`~sklearn.linear_model.ARDRegression` is better when handling gaussian\\n# noise, but is still unable to handle correlated features and requires a larger\\n# amount of time due to fitting a prior.\\n#\\n# References\\n# ----------\\n#\\n#   .. [1] :doi:`\"Lasso-type recovery of sparse representations for\\n#    high-dimensional data\" N. Meinshausen, B. Yu - The Annals of Statistics\\n#    2009, Vol. 37, No. 1, 246-270 <10.1214/07-AOS582>`'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_loss_functions.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def modified_huber_loss(y_true, y_pred):\\n    z = y_pred * y_true\\n    loss = -4 * z\\n    loss[z >= -1] = (1 - z[z >= -1]) ** 2\\n    loss[z >= 1.0] = 0\\n    return loss'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_loss_functions.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================\\nSGD: convex loss functions\\n==========================\\n\\nA plot that compares the various convex loss functions supported by\\n:class:`~sklearn.linear_model.SGDClassifier` .\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n\\n# Code for: def modified_huber_loss(y_true, y_pred):\\n\\n\\nxmin, xmax = -4, 4\\nxx = np.linspace(xmin, xmax, 100)\\nlw = 2\\nplt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color=\"gold\", lw=lw, label=\"Zero-one loss\")\\nplt.plot(xx, np.where(xx < 1, 1 - xx, 0), color=\"teal\", lw=lw, label=\"Hinge loss\")\\nplt.plot(xx, -np.minimum(xx, 0), color=\"yellowgreen\", lw=lw, label=\"Perceptron loss\")\\nplt.plot(xx, np.log2(1 + np.exp(-xx)), color=\"cornflowerblue\", lw=lw, label=\"Log loss\")\\nplt.plot(\\n    xx,\\n    np.where(xx < 1, 1 - xx, 0) ** 2,\\n    color=\"orange\",\\n    lw=lw,\\n    label=\"Squared hinge loss\",\\n)\\nplt.plot(\\n    xx,\\n    modified_huber_loss(xx, 1),\\n    color=\"darkorchid\",\\n    lw=lw,\\n    linestyle=\"--\",\\n    label=\"Modified Huber loss\",\\n)\\nplt.ylim((0, 8))\\nplt.legend(loc=\"upper right\")\\nplt.xlabel(r\"Decision function $f(x)$\")\\nplt.ylabel(\"$L(y=1, f(x))$\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================================================================\\nFitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples\\n==========================================================================\\n\\nThe following example shows how to precompute the gram matrix\\nwhile using weighted samples with an :class:`~sklearn.linear_model.ElasticNet`.\\n\\nIf weighted samples are used, the design matrix must be centered and then\\nrescaled by the square root of the weight vector before the gram matrix\\nis computed.\\n\\n.. note::\\n  `sample_weight` vector is also rescaled to sum to `n_samples`, see the\\n   documentation for the `sample_weight` parameter to\\n   :meth:`~sklearn.linear_model.ElasticNet.fit`.\\n\\n\"\"\"\\n\\n# %%\\n# Let\\'s start by loading the dataset and creating some sample weights.\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_regression\\n\\nrng = np.random.RandomState(0)\\n\\nn_samples = int(1e5)\\nX, y = make_regression(n_samples=n_samples, noise=0.5, random_state=rng)\\n\\nsample_weight = rng.lognormal(size=n_samples)\\n# normalize the sample weights\\nnormalized_weights = sample_weight * (n_samples / (sample_weight.sum()))\\n\\n# %%\\n# To fit the elastic net using the `precompute` option together with the sample\\n# weights, we must first center the design matrix,  and rescale it by the\\n# normalized weights prior to computing the gram matrix.\\nX_offset = np.average(X, axis=0, weights=normalized_weights)\\nX_centered = X - np.average(X, axis=0, weights=normalized_weights)\\nX_scaled = X_centered * np.sqrt(normalized_weights)[:, np.newaxis]\\ngram = np.dot(X_scaled.T, X_scaled)\\n\\n# %%\\n# We can now proceed with fitting. We must passed the centered design matrix to\\n# `fit` otherwise the elastic net estimator will detect that it is uncentered\\n# and discard the gram matrix we passed. However, if we pass the scaled design\\n# matrix, the preprocessing code will incorrectly rescale it a second time.\\nfrom sklearn.linear_model import ElasticNet\\n\\nlm = ElasticNet(alpha=0.01, precompute=gram)\\nlm.fit(X_centered, y, sample_weight=normalized_weights)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_coordinate_descent_path.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================\\nLasso and Elastic Net\\n=====================\\n\\nLasso and elastic net (L1 and L2 penalisation) implemented using a\\ncoordinate descent.\\n\\nThe coefficients can be forced to be positive.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom itertools import cycle\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import datasets\\nfrom sklearn.linear_model import enet_path, lasso_path\\n\\nX, y = datasets.load_diabetes(return_X_y=True)\\n\\n\\nX /= X.std(axis=0)  # Standardize data (easier to set the l1_ratio parameter)\\n\\n# Compute paths\\n\\neps = 5e-3  # the smaller it is the longer is the path\\n\\nprint(\"Computing regularization path using the lasso...\")\\nalphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps=eps)\\n\\nprint(\"Computing regularization path using the positive lasso...\")\\nalphas_positive_lasso, coefs_positive_lasso, _ = lasso_path(\\n    X, y, eps=eps, positive=True\\n)\\nprint(\"Computing regularization path using the elastic net...\")\\nalphas_enet, coefs_enet, _ = enet_path(X, y, eps=eps, l1_ratio=0.8)\\n\\nprint(\"Computing regularization path using the positive elastic net...\")\\nalphas_positive_enet, coefs_positive_enet, _ = enet_path(\\n    X, y, eps=eps, l1_ratio=0.8, positive=True\\n)\\n\\n# Display results\\n\\nplt.figure(1)\\ncolors = cycle([\"b\", \"r\", \"g\", \"c\", \"k\"])\\nfor coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):\\n    l1 = plt.semilogx(alphas_lasso, coef_l, c=c)\\n    l2 = plt.semilogx(alphas_enet, coef_e, linestyle=\"--\", c=c)\\n\\nplt.xlabel(\"alpha\")\\nplt.ylabel(\"coefficients\")\\nplt.title(\"Lasso and Elastic-Net Paths\")\\nplt.legend((l1[-1], l2[-1]), (\"Lasso\", \"Elastic-Net\"), loc=\"lower right\")\\nplt.axis(\"tight\")\\n\\n\\nplt.figure(2)\\nfor coef_l, coef_pl, c in zip(coefs_lasso, coefs_positive_lasso, colors):\\n    l1 = plt.semilogy(alphas_lasso, coef_l, c=c)\\n    l2 = plt.semilogy(alphas_positive_lasso, coef_pl, linestyle=\"--\", c=c)\\n\\nplt.xlabel(\"alpha\")\\nplt.ylabel(\"coefficients\")\\nplt.title(\"Lasso and positive Lasso\")\\nplt.legend((l1[-1], l2[-1]), (\"Lasso\", \"positive Lasso\"), loc=\"lower right\")\\nplt.axis(\"tight\")\\n\\n\\nplt.figure(3)\\nfor coef_e, coef_pe, c in zip(coefs_enet, coefs_positive_enet, colors):\\n    l1 = plt.semilogx(alphas_enet, coef_e, c=c)\\n    l2 = plt.semilogx(alphas_positive_enet, coef_pe, linestyle=\"--\", c=c)\\n\\nplt.xlabel(\"alpha\")\\nplt.ylabel(\"coefficients\")\\nplt.title(\"Elastic-Net and positive Elastic-Net\")\\nplt.legend((l1[-1], l2[-1]), (\"Elastic-Net\", \"positive Elastic-Net\"), loc=\"lower right\")\\nplt.axis(\"tight\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_logistic_path.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================\\nRegularization path of L1- Logistic Regression\\n==============================================\\n\\n\\nTrain l1-penalized logistic regression models on a binary classification\\nproblem derived from the Iris dataset.\\n\\nThe models are ordered from strongest regularized to least regularized. The 4\\ncoefficients of the models are collected and plotted as a \"regularization\\npath\": on the left-hand side of the figure (strong regularizers), all the\\ncoefficients are exactly 0. When regularization gets progressively looser,\\ncoefficients can get non-zero values one after the other.\\n\\nHere we choose the liblinear solver because it can efficiently optimize for the\\nLogistic Regression loss with a non-smooth, sparsity inducing l1 penalty.\\n\\nAlso note that we set a low value for the tolerance to make sure that the model\\nhas converged before collecting the coefficients.\\n\\nWe also use warm_start=True which means that the coefficients of the models are\\nreused to initialize the next model fit to speed-up the computation of the\\nfull-path.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Load data\\n# ---------\\n\\nfrom sklearn import datasets\\n\\niris = datasets.load_iris()\\nX = iris.data\\ny = iris.target\\n\\nX = X[y != 2]\\ny = y[y != 2]\\n\\nX /= X.max()  # Normalize X to speed-up convergence\\n\\n# %%\\n# Compute regularization path\\n# ---------------------------\\n\\nimport numpy as np\\n\\nfrom sklearn import linear_model\\nfrom sklearn.svm import l1_min_c\\n\\ncs = l1_min_c(X, y, loss=\"log\") * np.logspace(0, 10, 16)\\n\\nclf = linear_model.LogisticRegression(\\n    penalty=\"l1\",\\n    solver=\"liblinear\",\\n    tol=1e-6,\\n    max_iter=int(1e6),\\n    warm_start=True,\\n    intercept_scaling=10000.0,\\n)\\ncoefs_ = []\\nfor c in cs:\\n    clf.set_params(C=c)\\n    clf.fit(X, y)\\n    coefs_.append(clf.coef_.ravel().copy())\\n\\ncoefs_ = np.array(coefs_)\\n\\n# %%\\n# Plot regularization path\\n# ------------------------\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(np.log10(cs), coefs_, marker=\"o\")\\nymin, ymax = plt.ylim()\\nplt.xlabel(\"log(C)\")\\nplt.ylabel(\"Coefficients\")\\nplt.title(\"Logistic Regression Path\")\\nplt.axis(\"tight\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ridge_coeffs.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nRidge coefficients as a function of the L2 Regularization\\n=========================================================\\n\\nA model that overfits learns the training data too well, capturing both the\\nunderlying patterns and the noise in the data. However, when applied to unseen\\ndata, the learned associations may not hold. We normally detect this when we\\napply our trained predictions to the test data and see the statistical\\nperformance drop significantly compared to the training data.\\n\\nOne way to overcome overfitting is through regularization, which can be done by\\npenalizing large weights (coefficients) in linear models, forcing the model to\\nshrink all coefficients. Regularization reduces a model\\'s reliance on specific\\ninformation obtained from the training samples.\\n\\nThis example illustrates how L2 regularization in a\\n:class:`~sklearn.linear_model.Ridge` regression affects a model\\'s performance by\\nadding a penalty term to the loss that increases with the coefficients\\n:math:`\\\\\\\\beta`.\\n\\nThe regularized loss function is given by: :math:`\\\\\\\\mathcal{L}(X, y, \\\\\\\\beta) =\\n\\\\\\\\| y - X \\\\\\\\beta \\\\\\\\|^{2}_{2} + \\\\\\\\alpha \\\\\\\\| \\\\\\\\beta \\\\\\\\|^{2}_{2}`\\n\\nwhere :math:`X` is the input data, :math:`y` is the target variable,\\n:math:`\\\\\\\\beta` is the vector of coefficients associated with the features, and\\n:math:`\\\\\\\\alpha` is the regularization strength.\\n\\nThe regularized loss function aims to balance the trade-off between accurately\\npredicting the training set and to prevent overfitting.\\n\\nIn this regularized loss, the left-hand side (e.g. :math:`\\\\\\\\|y -\\nX\\\\\\\\beta\\\\\\\\|^{2}_{2}`) measures the squared difference between the actual target\\nvariable, :math:`y`, and the predicted values. Minimizing this term alone could\\nlead to overfitting, as the model may become too complex and sensitive to noise\\nin the training data.\\n\\nTo address overfitting, Ridge regularization adds a constraint, called a penalty\\nterm, (:math:`\\\\\\\\alpha \\\\\\\\| \\\\\\\\beta\\\\\\\\|^{2}_{2}`) to the loss function. This penalty\\nterm is the sum of the squares of the model\\'s coefficients, multiplied by the\\nregularization strength :math:`\\\\\\\\alpha`. By introducing this constraint, Ridge\\nregularization discourages any single coefficient :math:`\\\\\\\\beta_{i}` from taking\\nan excessively large value and encourages smaller and more evenly distributed\\ncoefficients. Higher values of :math:`\\\\\\\\alpha` force the coefficients towards\\nzero. However, an excessively high :math:`\\\\\\\\alpha` can result in an underfit\\nmodel that fails to capture important patterns in the data.\\n\\nTherefore, the regularized loss function combines the prediction accuracy term\\nand the penalty term. By adjusting the regularization strength, practitioners\\ncan fine-tune the degree of constraint imposed on the weights, training a model\\ncapable of generalizing well to unseen data while avoiding overfitting.\\n\"\"\"\\n\\n# Author: Kornel Kielczewski -- <kornel.k@plusnet.pl>'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ridge_coeffs.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Author: Kornel Kielczewski -- <kornel.k@plusnet.pl>\\n\\n# %%\\n# Purpose of this example\\n# -----------------------\\n# For the purpose of showing how Ridge regularization works, we will create a\\n# non-noisy data set. Then we will train a regularized model on a range of\\n# regularization strengths (:math:`\\\\alpha`) and plot how the trained\\n# coefficients and the mean squared error between those and the original values\\n# behave as functions of the regularization strength.\\n#\\n# Creating a non-noisy data set\\n# *****************************\\n# We make a toy data set with 100 samples and 10 features, that\\'s suitable to\\n# detect regression. Out of the 10 features, 8 are informative and contribute to\\n# the regression, while the remaining 2 features do not have any effect on the\\n# target variable (their true coefficients are 0). Please note that in this\\n# example the data is non-noisy, hence we can expect our regression model to\\n# recover exactly the true coefficients w.\\nfrom sklearn.datasets import make_regression\\n\\nX, y, w = make_regression(\\n    n_samples=100, n_features=10, n_informative=8, coef=True, random_state=1\\n)\\n\\n# Obtain the true coefficients\\nprint(f\"The true coefficient of this regression problem are:\\\\n{w}\")\\n\\n# %%\\n# Training the Ridge Regressor\\n# ****************************\\n# We use :class:`~sklearn.linear_model.Ridge`, a linear model with L2\\n# regularization. We train several models, each with a different value for the\\n# model parameter `alpha`, which is a positive constant that multiplies the\\n# penalty term, controlling the regularization strength. For each trained model\\n# we then compute the error between the true coefficients `w` and the\\n# coefficients found by the model `clf`. We store the identified coefficients\\n# and the calculated errors for the corresponding coefficients in lists, which\\n# makes it convenient for us to plot them.\\nimport numpy as np\\n\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.metrics import mean_squared_error\\n\\nclf = Ridge()\\n\\n# Generate values for `alpha` that are evenly distributed on a logarithmic scale\\nalphas = np.logspace(-3, 4, 200)\\ncoefs = []\\nerrors_coefs = []\\n\\n# Train the model with different regularisation strengths\\nfor a in alphas:\\n    clf.set_params(alpha=a).fit(X, y)\\n    coefs.append(clf.coef_)\\n    errors_coefs.append(mean_squared_error(clf.coef_, w))\\n\\n# %%\\n# Plotting trained Coefficients and Mean Squared Errors\\n# *****************************************************\\n# We now plot the 10 different regularized coefficients as a function of the\\n# regularization parameter `alpha` where each color represents a different\\n# coefficient.\\n#\\n# On the right-hand-side, we plot how the errors of the coefficients from the\\n# estimator change as a function of regularization.\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\nalphas = pd.Index(alphas, name=\"alpha\")\\ncoefs = pd.DataFrame(coefs, index=alphas, columns=[f\"Feature {i}\" for i in range(10)])\\nerrors = pd.Series(errors_coefs, index=alphas, name=\"Mean squared error\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ridge_coeffs.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='alphas = pd.Index(alphas, name=\"alpha\")\\ncoefs = pd.DataFrame(coefs, index=alphas, columns=[f\"Feature {i}\" for i in range(10)])\\nerrors = pd.Series(errors_coefs, index=alphas, name=\"Mean squared error\")\\n\\nfig, axs = plt.subplots(1, 2, figsize=(20, 6))'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ridge_coeffs.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig, axs = plt.subplots(1, 2, figsize=(20, 6))\\n\\ncoefs.plot(\\n    ax=axs[0],\\n    logx=True,\\n    title=\"Ridge coefficients as a function of the regularization strength\",\\n)\\naxs[0].set_ylabel(\"Ridge coefficient values\")\\nerrors.plot(\\n    ax=axs[1],\\n    logx=True,\\n    title=\"Coefficient error as a function of the regularization strength\",\\n)\\n_ = axs[1].set_ylabel(\"Mean squared error\")\\n# %%\\n# Interpreting the plots\\n# **********************\\n# The plot on the left-hand side shows how the regularization strength (`alpha`)\\n# affects the Ridge regression coefficients. Smaller values of `alpha` (weak\\n# regularization), allow the coefficients to closely resemble the true\\n# coefficients (`w`) used to generate the data set. This is because no\\n# additional noise was added to our artificial data set. As `alpha` increases,\\n# the coefficients shrink towards zero, gradually reducing the impact of the\\n# features that were formerly more significant.\\n#\\n# The right-hand side plot shows the mean squared error (MSE) between the\\n# coefficients found by the model and the true coefficients (`w`). It provides a\\n# measure that relates to how exact our ridge model is in comparison to the true\\n# generative model. A low error means that it found coefficients closer to the\\n# ones of the true generative model. In this case, since our toy data set was\\n# non-noisy, we can see that the least regularized model retrieves coefficients\\n# closest to the true coefficients (`w`) (error is close to 0).\\n#\\n# When `alpha` is small, the model captures the intricate details of the\\n# training data, whether those were caused by noise or by actual information. As\\n# `alpha` increases, the highest coefficients shrink more rapidly, rendering\\n# their corresponding features less influential in the training process. This\\n# can enhance a model\\'s ability to generalize to unseen data (if there was a lot\\n# of noise to capture), but it also poses the risk of losing performance if the\\n# regularization becomes too strong compared to the amount of noise the data\\n# contained (as in this example).\\n#\\n# In real-world scenarios where data typically includes noise, selecting an\\n# appropriate `alpha` value becomes crucial in striking a balance between an\\n# overfitting and an underfitting model.\\n#\\n# Here, we saw that :class:`~sklearn.linear_model.Ridge` adds a penalty to the\\n# coefficients to fight overfitting. Another problem that occurs is linked to\\n# the presence of outliers in the training dataset. An outlier is a data point\\n# that differs significantly from other observations. Concretely, these outliers\\n# impact the left-hand side term of the loss function that we showed earlier.\\n# Some other linear models are formulated to be robust to outliers such as the\\n# :class:`~sklearn.linear_model.HuberRegressor`. You can learn more about it in\\n# the :ref:`sphx_glr_auto_examples_linear_model_plot_huber_vs_ridge.py` example.'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_huber_vs_ridge.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=======================================================\\nHuberRegressor vs Ridge on dataset with strong outliers\\n=======================================================\\n\\nFit Ridge and HuberRegressor on a dataset with outliers.\\n\\nThe example shows that the predictions in ridge are strongly influenced\\nby the outliers present in the dataset. The Huber regressor is less\\ninfluenced by the outliers since the model uses the linear loss for these.\\nAs the parameter epsilon is increased for the Huber regressor, the decision\\nfunction approaches that of the ridge.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.linear_model import HuberRegressor, Ridge\\n\\n# Generate toy data.\\nrng = np.random.RandomState(0)\\nX, y = make_regression(\\n    n_samples=20, n_features=1, random_state=0, noise=4.0, bias=100.0\\n)\\n\\n# Add four strong outliers to the dataset.\\nX_outliers = rng.normal(0, 0.5, size=(4, 1))\\ny_outliers = rng.normal(0, 2.0, size=4)\\nX_outliers[:2, :] += X.max() + X.mean() / 4.0\\nX_outliers[2:, :] += X.min() - X.mean() / 4.0\\ny_outliers[:2] += y.min() - y.mean() / 4.0\\ny_outliers[2:] += y.max() + y.mean() / 4.0\\nX = np.vstack((X, X_outliers))\\ny = np.concatenate((y, y_outliers))\\nplt.plot(X, y, \"b.\")\\n\\n# Fit the huber regressor over a series of epsilon values.\\ncolors = [\"r-\", \"b-\", \"y-\", \"m-\"]\\n\\nx = np.linspace(X.min(), X.max(), 7)\\nepsilon_values = [1, 1.5, 1.75, 1.9]\\nfor k, epsilon in enumerate(epsilon_values):\\n    huber = HuberRegressor(alpha=0.0, epsilon=epsilon)\\n    huber.fit(X, y)\\n    coef_ = huber.coef_ * x + huber.intercept_\\n    plt.plot(x, coef_, colors[k], label=\"huber loss, %s\" % epsilon)\\n\\n# Fit a ridge regressor to compare it to huber regressor.\\nridge = Ridge(alpha=0.0, random_state=0)\\nridge.fit(X, y)\\ncoef_ridge = ridge.coef_\\ncoef_ = ridge.coef_ * x + ridge.intercept_\\nplt.plot(x, coef_, \"g-\", label=\"ridge regression\")\\n\\nplt.title(\"Comparison of HuberRegressor vs Ridge\")\\nplt.xlabel(\"X\")\\nplt.ylabel(\"y\")\\nplt.legend(loc=0)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgdocsvm_vs_ocsvm.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================================================\\nOne-Class SVM versus One-Class SVM using Stochastic Gradient Descent\\n====================================================================\\n\\nThis example shows how to approximate the solution of\\n:class:`sklearn.svm.OneClassSVM` in the case of an RBF kernel with\\n:class:`sklearn.linear_model.SGDOneClassSVM`, a Stochastic Gradient Descent\\n(SGD) version of the One-Class SVM. A kernel approximation is first used in\\norder to apply :class:`sklearn.linear_model.SGDOneClassSVM` which implements a\\nlinear One-Class SVM using SGD.\\n\\nNote that :class:`sklearn.linear_model.SGDOneClassSVM` scales linearly with\\nthe number of samples whereas the complexity of a kernelized\\n:class:`sklearn.svm.OneClassSVM` is at best quadratic with respect to the\\nnumber of samples. It is not the purpose of this example to illustrate the\\nbenefits of such an approximation in terms of computation time but rather to\\nshow that we obtain similar results on a toy dataset.\\n\\n\"\"\"  # noqa: E501\\n\\n# %%\\nimport matplotlib\\nimport matplotlib.lines as mlines\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.kernel_approximation import Nystroem\\nfrom sklearn.linear_model import SGDOneClassSVM\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.svm import OneClassSVM\\n\\nfont = {\"weight\": \"normal\", \"size\": 15}\\n\\nmatplotlib.rc(\"font\", **font)\\n\\nrandom_state = 42\\nrng = np.random.RandomState(random_state)\\n\\n# Generate train data\\nX = 0.3 * rng.randn(500, 2)\\nX_train = np.r_[X + 2, X - 2]\\n# Generate some regular novel observations\\nX = 0.3 * rng.randn(20, 2)\\nX_test = np.r_[X + 2, X - 2]\\n# Generate some abnormal novel observations\\nX_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\\n\\n# OCSVM hyperparameters\\nnu = 0.05\\ngamma = 2.0\\n\\n# Fit the One-Class SVM\\nclf = OneClassSVM(gamma=gamma, kernel=\"rbf\", nu=nu)\\nclf.fit(X_train)\\ny_pred_train = clf.predict(X_train)\\ny_pred_test = clf.predict(X_test)\\ny_pred_outliers = clf.predict(X_outliers)\\nn_error_train = y_pred_train[y_pred_train == -1].size\\nn_error_test = y_pred_test[y_pred_test == -1].size\\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\\n\\n# Fit the One-Class SVM using a kernel approximation and SGD\\ntransform = Nystroem(gamma=gamma, random_state=random_state)\\nclf_sgd = SGDOneClassSVM(\\n    nu=nu, shuffle=True, fit_intercept=True, random_state=random_state, tol=1e-4\\n)\\npipe_sgd = make_pipeline(transform, clf_sgd)\\npipe_sgd.fit(X_train)\\ny_pred_train_sgd = pipe_sgd.predict(X_train)\\ny_pred_test_sgd = pipe_sgd.predict(X_test)\\ny_pred_outliers_sgd = pipe_sgd.predict(X_outliers)\\nn_error_train_sgd = y_pred_train_sgd[y_pred_train_sgd == -1].size\\nn_error_test_sgd = y_pred_test_sgd[y_pred_test_sgd == -1].size\\nn_error_outliers_sgd = y_pred_outliers_sgd[y_pred_outliers_sgd == 1].size\\n\\n\\n# %%\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\n_, ax = plt.subplots(figsize=(9, 6))'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgdocsvm_vs_ocsvm.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\n_, ax = plt.subplots(figsize=(9, 6))\\n\\nxx, yy = np.meshgrid(np.linspace(-4.5, 4.5, 50), np.linspace(-4.5, 4.5, 50))\\nX = np.concatenate([xx.ravel().reshape(-1, 1), yy.ravel().reshape(-1, 1)], axis=1)\\nDecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    response_method=\"decision_function\",\\n    plot_method=\"contourf\",\\n    ax=ax,\\n    cmap=\"PuBu\",\\n)\\nDecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    response_method=\"decision_function\",\\n    plot_method=\"contour\",\\n    ax=ax,\\n    linewidths=2,\\n    colors=\"darkred\",\\n    levels=[0],\\n)\\nDecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    response_method=\"decision_function\",\\n    plot_method=\"contourf\",\\n    ax=ax,\\n    colors=\"palevioletred\",\\n    levels=[0, clf.decision_function(X).max()],\\n)\\n\\ns = 20\\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\\n\\nax.set(\\n    title=\"One-Class SVM\",\\n    xlim=(-4.5, 4.5),\\n    ylim=(-4.5, 4.5),\\n    xlabel=(\\n        f\"error train: {n_error_train}/{X_train.shape[0]}; \"\\n        f\"errors novel regular: {n_error_test}/{X_test.shape[0]}; \"\\n        f\"errors novel abnormal: {n_error_outliers}/{X_outliers.shape[0]}\"\\n    ),\\n)\\n_ = ax.legend(\\n    [mlines.Line2D([], [], color=\"darkred\", label=\"learned frontier\"), b1, b2, c],\\n    [\\n        \"learned frontier\",\\n        \"training observations\",\\n        \"new regular observations\",\\n        \"new abnormal observations\",\\n    ],\\n    loc=\"upper left\",\\n)\\n\\n# %%\\n_, ax = plt.subplots(figsize=(9, 6))\\n\\nxx, yy = np.meshgrid(np.linspace(-4.5, 4.5, 50), np.linspace(-4.5, 4.5, 50))\\nX = np.concatenate([xx.ravel().reshape(-1, 1), yy.ravel().reshape(-1, 1)], axis=1)\\nDecisionBoundaryDisplay.from_estimator(\\n    pipe_sgd,\\n    X,\\n    response_method=\"decision_function\",\\n    plot_method=\"contourf\",\\n    ax=ax,\\n    cmap=\"PuBu\",\\n)\\nDecisionBoundaryDisplay.from_estimator(\\n    pipe_sgd,\\n    X,\\n    response_method=\"decision_function\",\\n    plot_method=\"contour\",\\n    ax=ax,\\n    linewidths=2,\\n    colors=\"darkred\",\\n    levels=[0],\\n)\\nDecisionBoundaryDisplay.from_estimator(\\n    pipe_sgd,\\n    X,\\n    response_method=\"decision_function\",\\n    plot_method=\"contourf\",\\n    ax=ax,\\n    colors=\"palevioletred\",\\n    levels=[0, pipe_sgd.decision_function(X).max()],\\n)\\n\\ns = 20\\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgdocsvm_vs_ocsvm.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='s = 20\\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\\n\\nax.set(\\n    title=\"Online One-Class SVM\",\\n    xlim=(-4.5, 4.5),\\n    ylim=(-4.5, 4.5),\\n    xlabel=(\\n        f\"error train: {n_error_train_sgd}/{X_train.shape[0]}; \"\\n        f\"errors novel regular: {n_error_test_sgd}/{X_test.shape[0]}; \"\\n        f\"errors novel abnormal: {n_error_outliers_sgd}/{X_outliers.shape[0]}\"\\n    ),\\n)\\nax.legend(\\n    [mlines.Line2D([], [], color=\"darkred\", label=\"learned frontier\"), b1, b2, c],\\n    [\\n        \"learned frontier\",\\n        \"training observations\",\\n        \"new regular observations\",\\n        \"new abnormal observations\",\\n    ],\\n    loc=\"upper left\",\\n)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_logistic.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nLogistic function\\n=========================================================\\n\\nShown in the plot is how the logistic regression would, in this\\nsynthetic dataset, classify values as either 0 or 1,\\ni.e. class one or two, using the logistic curve.\\n\\n\"\"\"\\n\\n# Code source: Gael Varoquaux\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom scipy.special import expit\\n\\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\\n\\n# Generate a toy dataset, it\\'s just a straight line with some Gaussian noise:\\nxmin, xmax = -5, 5\\nn_samples = 100\\nnp.random.seed(0)\\nX = np.random.normal(size=n_samples)\\ny = (X > 0).astype(float)\\nX[X > 0] *= 4\\nX += 0.3 * np.random.normal(size=n_samples)\\n\\nX = X[:, np.newaxis]\\n\\n# Fit the classifier\\nclf = LogisticRegression(C=1e5)\\nclf.fit(X, y)\\n\\n# and plot the result\\nplt.figure(1, figsize=(4, 3))\\nplt.clf()\\nplt.scatter(X.ravel(), y, label=\"example data\", color=\"black\", zorder=20)\\nX_test = np.linspace(-5, 10, 300)\\n\\nloss = expit(X_test * clf.coef_ + clf.intercept_).ravel()\\nplt.plot(X_test, loss, label=\"Logistic Regression Model\", color=\"red\", linewidth=3)\\n\\nols = LinearRegression()\\nols.fit(X, y)\\nplt.plot(\\n    X_test,\\n    ols.coef_ * X_test + ols.intercept_,\\n    label=\"Linear Regression Model\",\\n    linewidth=1,\\n)\\nplt.axhline(0.5, color=\".5\")\\n\\nplt.ylabel(\"y\")\\nplt.xlabel(\"X\")\\nplt.xticks(range(-5, 10))\\nplt.yticks([0, 0.5, 1])\\nplt.ylim(-0.25, 1.25)\\nplt.xlim(-4, 10)\\nplt.legend(\\n    loc=\"lower right\",\\n    fontsize=\"small\",\\n)\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sparse_logistic_regression_mnist.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================================================\\nMNIST classification using multinomial logistic + L1\\n=====================================================\\n\\nHere we fit a multinomial logistic regression with L1 penalty on a subset of\\nthe MNIST digits classification task. We use the SAGA algorithm for this\\npurpose: this a solver that is fast when the number of samples is significantly\\nlarger than the number of features and is able to finely optimize non-smooth\\nobjective functions which is the case with the l1-penalty. Test accuracy\\nreaches > 0.8, while weight vectors remains *sparse* and therefore more easily\\n*interpretable*.\\n\\nNote that this accuracy of this l1-penalized linear model is significantly\\nbelow what can be reached by an l2-penalized linear model or a non-linear\\nmulti-layer perceptron model on this dataset.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.utils import check_random_state\\n\\n# Turn down for faster convergence\\nt0 = time.time()\\ntrain_samples = 5000\\n\\n# Load data from https://www.openml.org/d/554\\nX, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\\n\\nrandom_state = check_random_state(0)\\npermutation = random_state.permutation(X.shape[0])\\nX = X[permutation]\\ny = y[permutation]\\nX = X.reshape((X.shape[0], -1))\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, train_size=train_samples, test_size=10000\\n)\\n\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\n# Turn up tolerance for faster convergence\\nclf = LogisticRegression(C=50.0 / train_samples, penalty=\"l1\", solver=\"saga\", tol=0.1)\\nclf.fit(X_train, y_train)\\nsparsity = np.mean(clf.coef_ == 0) * 100\\nscore = clf.score(X_test, y_test)\\n# print(\\'Best C % .4f\\' % clf.C_)\\nprint(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\\nprint(\"Test score with L1 penalty: %.4f\" % score)\\n\\ncoef = clf.coef_.copy()\\nplt.figure(figsize=(10, 5))\\nscale = np.abs(coef).max()\\nfor i in range(10):\\n    l1_plot = plt.subplot(2, 5, i + 1)\\n    l1_plot.imshow(\\n        coef[i].reshape(28, 28),\\n        interpolation=\"nearest\",\\n        cmap=plt.cm.RdBu,\\n        vmin=-scale,\\n        vmax=scale,\\n    )\\n    l1_plot.set_xticks(())\\n    l1_plot.set_yticks(())\\n    l1_plot.set_xlabel(\"Class %i\" % i)\\nplt.suptitle(\"Classification vector for...\")\\n\\nrun_time = time.time() - t0\\nprint(\"Example run in %.3f s\" % run_time)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_theilsen.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================\\nTheil-Sen Regression\\n====================\\n\\nComputes a Theil-Sen Regression on a synthetic dataset.\\n\\nSee :ref:`theil_sen_regression` for more information on the regressor.\\n\\nCompared to the OLS (ordinary least squares) estimator, the Theil-Sen\\nestimator is robust against outliers. It has a breakdown point of about 29.3%\\nin case of a simple linear regression which means that it can tolerate\\narbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional\\ncase.\\n\\nThe estimation of the model is done by calculating the slopes and intercepts\\nof a subpopulation of all possible combinations of p subsample points. If an\\nintercept is fitted, p must be greater than or equal to n_features + 1. The\\nfinal slope and intercept is then defined as the spatial median of these\\nslopes and intercepts.\\n\\nIn certain cases Theil-Sen performs better than :ref:`RANSAC\\n<ransac_regression>` which is also a robust method. This is illustrated in the\\nsecond example below where outliers with respect to the x-axis perturb RANSAC.\\nTuning the ``residual_threshold`` parameter of RANSAC remedies this but in\\ngeneral a priori knowledge about the data and the nature of the outliers is\\nneeded.\\nDue to the computational complexity of Theil-Sen it is recommended to use it\\nonly for small problems in terms of number of samples and features. For larger\\nproblems the ``max_subpopulation`` parameter restricts the magnitude of all\\npossible combinations of p subsample points to a randomly chosen subset and\\ntherefore also limits the runtime. Therefore, Theil-Sen is applicable to larger\\nproblems with the drawback of losing some of its mathematical properties since\\nit then works on a random subset.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.linear_model import LinearRegression, RANSACRegressor, TheilSenRegressor\\n\\nestimators = [\\n    (\"OLS\", LinearRegression()),\\n    (\"Theil-Sen\", TheilSenRegressor(random_state=42)),\\n    (\"RANSAC\", RANSACRegressor(random_state=42)),\\n]\\ncolors = {\"OLS\": \"turquoise\", \"Theil-Sen\": \"gold\", \"RANSAC\": \"lightgreen\"}\\nlw = 2\\n\\n# %%\\n# Outliers only in the y direction\\n# --------------------------------\\n\\nnp.random.seed(0)\\nn_samples = 200\\n# Linear model y = 3*x + N(2, 0.1**2)\\nx = np.random.randn(n_samples)\\nw = 3.0\\nc = 2.0\\nnoise = 0.1 * np.random.randn(n_samples)\\ny = w * x + c + noise\\n# 10% outliers\\ny[-20:] += -20 * x[-20:]\\nX = x[:, np.newaxis]\\n\\nplt.scatter(x, y, color=\"indigo\", marker=\"x\", s=40)\\nline_x = np.array([-3, 3])\\nfor name, estimator in estimators:\\n    t0 = time.time()\\n    estimator.fit(X, y)\\n    elapsed_time = time.time() - t0\\n    y_pred = estimator.predict(line_x.reshape(2, 1))\\n    plt.plot(\\n        line_x,\\n        y_pred,\\n        color=colors[name],\\n        linewidth=lw,\\n        label=\"%s (fit time: %.2fs)\" % (name, elapsed_time),\\n    )\\n\\nplt.axis(\"tight\")\\nplt.legend(loc=\"upper left\")\\n_ = plt.title(\"Corrupt y\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_theilsen.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.axis(\"tight\")\\nplt.legend(loc=\"upper left\")\\n_ = plt.title(\"Corrupt y\")\\n\\n# %%\\n# Outliers in the X direction\\n# ---------------------------\\n\\nnp.random.seed(0)\\n# Linear model y = 3*x + N(2, 0.1**2)\\nx = np.random.randn(n_samples)\\nnoise = 0.1 * np.random.randn(n_samples)\\ny = 3 * x + 2 + noise\\n# 10% outliers\\nx[-20:] = 9.9\\ny[-20:] += 22\\nX = x[:, np.newaxis]\\n\\nplt.figure()\\nplt.scatter(x, y, color=\"indigo\", marker=\"x\", s=40)\\n\\nline_x = np.array([-3, 10])\\nfor name, estimator in estimators:\\n    t0 = time.time()\\n    estimator.fit(X, y)\\n    elapsed_time = time.time() - t0\\n    y_pred = estimator.predict(line_x.reshape(2, 1))\\n    plt.plot(\\n        line_x,\\n        y_pred,\\n        color=colors[name],\\n        linewidth=lw,\\n        label=\"%s (fit time: %.2fs)\" % (name, elapsed_time),\\n    )\\n\\nplt.axis(\"tight\")\\nplt.legend(loc=\"upper left\")\\nplt.title(\"Corrupt x\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ols.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nLinear Regression Example\\n=========================================================\\nThe example below uses only the first feature of the `diabetes` dataset,\\nin order to illustrate the data points within the two-dimensional plot.\\nThe straight line can be seen in the plot, showing how linear regression\\nattempts to draw a straight line that will best minimize the\\nresidual sum of squares between the observed responses in the dataset,\\nand the responses predicted by the linear approximation.\\n\\nThe coefficients, residual sum of squares and the coefficient of\\ndetermination are also calculated.\\n\\n\"\"\"\\n\\n# Code source: Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets, linear_model\\nfrom sklearn.metrics import mean_squared_error, r2_score\\n\\n# Load the diabetes dataset\\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\\n\\n# Use only one feature\\ndiabetes_X = diabetes_X[:, np.newaxis, 2]\\n\\n# Split the data into training/testing sets\\ndiabetes_X_train = diabetes_X[:-20]\\ndiabetes_X_test = diabetes_X[-20:]\\n\\n# Split the targets into training/testing sets\\ndiabetes_y_train = diabetes_y[:-20]\\ndiabetes_y_test = diabetes_y[-20:]\\n\\n# Create linear regression object\\nregr = linear_model.LinearRegression()\\n\\n# Train the model using the training sets\\nregr.fit(diabetes_X_train, diabetes_y_train)\\n\\n# Make predictions using the testing set\\ndiabetes_y_pred = regr.predict(diabetes_X_test)\\n\\n# The coefficients\\nprint(\"Coefficients: \\\\n\", regr.coef_)\\n# The mean squared error\\nprint(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\\n# The coefficient of determination: 1 is perfect prediction\\nprint(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\\n\\n# Plot outputs\\nplt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\\nplt.plot(diabetes_X_test, diabetes_y_pred, color=\"blue\", linewidth=3)\\n\\nplt.xticks(())\\nplt.yticks(())\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_dense_vs_sparse_data.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================\\nLasso on dense and sparse data\\n==============================\\n\\nWe show that linear_model.Lasso provides the same results for dense and sparse\\ndata and that in the case of sparse data the speed is improved.\\n\\n\"\"\"\\n\\nfrom time import time\\n\\nfrom scipy import linalg, sparse\\n\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.linear_model import Lasso\\n\\n# %%\\n# Comparing the two Lasso implementations on Dense data\\n# -----------------------------------------------------\\n#\\n# We create a linear regression problem that is suitable for the Lasso,\\n# that is to say, with more features than samples. We then store the data\\n# matrix in both dense (the usual) and sparse format, and train a Lasso on\\n# each. We compute the runtime of both and check that they learned the\\n# same model by computing the Euclidean norm of the difference between the\\n# coefficients they learned. Because the data is dense, we expect better\\n# runtime with a dense data format.\\n\\nX, y = make_regression(n_samples=200, n_features=5000, random_state=0)\\n# create a copy of X in sparse format\\nX_sp = sparse.coo_matrix(X)\\n\\nalpha = 1\\nsparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\\ndense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\\n\\nt0 = time()\\nsparse_lasso.fit(X_sp, y)\\nprint(f\"Sparse Lasso done in {(time() - t0):.3f}s\")\\n\\nt0 = time()\\ndense_lasso.fit(X, y)\\nprint(f\"Dense Lasso done in {(time() - t0):.3f}s\")\\n\\n# compare the regression coefficients\\ncoeff_diff = linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_)\\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")\\n\\n#\\n# %%\\n# Comparing the two Lasso implementations on Sparse data\\n# ------------------------------------------------------\\n#\\n# We make the previous problem sparse by replacing all small values with 0\\n# and run the same comparisons as above. Because the data is now sparse, we\\n# expect the implementation that uses the sparse data format to be faster.\\n\\n# make a copy of the previous data\\nXs = X.copy()\\n# make Xs sparse by replacing the values lower than 2.5 with 0s\\nXs[Xs < 2.5] = 0.0\\n# create a copy of Xs in sparse format\\nXs_sp = sparse.coo_matrix(Xs)\\nXs_sp = Xs_sp.tocsc()\\n\\n# compute the proportion of non-zero coefficient in the data matrix\\nprint(f\"Matrix density : {(Xs_sp.nnz / float(X.size) * 100):.3f}%\")\\n\\nalpha = 0.1\\nsparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\\ndense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\\n\\nt0 = time()\\nsparse_lasso.fit(Xs_sp, y)\\nprint(f\"Sparse Lasso done in {(time() - t0):.3f}s\")\\n\\nt0 = time()\\ndense_lasso.fit(Xs, y)\\nprint(f\"Dense Lasso done in  {(time() - t0):.3f}s\")\\n\\n# compare the regression coefficients\\ncoeff_diff = linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_)\\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")\\n\\n# %%'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_penalties.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============\\nSGD: Penalties\\n==============\\n\\nContours of where the penalty is equal to 1\\nfor the three penalties L1, L2 and elastic-net.\\n\\nAll of the above are supported by :class:`~sklearn.linear_model.SGDClassifier`\\nand :class:`~sklearn.linear_model.SGDRegressor`.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nl1_color = \"navy\"\\nl2_color = \"c\"\\nelastic_net_color = \"darkorange\"\\n\\nline = np.linspace(-1.5, 1.5, 1001)\\nxx, yy = np.meshgrid(line, line)\\n\\nl2 = xx**2 + yy**2\\nl1 = np.abs(xx) + np.abs(yy)\\nrho = 0.5\\nelastic_net = rho * l1 + (1 - rho) * l2\\n\\nplt.figure(figsize=(10, 10), dpi=100)\\nax = plt.gca()\\n\\nelastic_net_contour = plt.contour(\\n    xx, yy, elastic_net, levels=[1], colors=elastic_net_color\\n)\\nl2_contour = plt.contour(xx, yy, l2, levels=[1], colors=l2_color)\\nl1_contour = plt.contour(xx, yy, l1, levels=[1], colors=l1_color)\\nax.set_aspect(\"equal\")\\nax.spines[\"left\"].set_position(\"center\")\\nax.spines[\"right\"].set_color(\"none\")\\nax.spines[\"bottom\"].set_position(\"center\")\\nax.spines[\"top\"].set_color(\"none\")\\n\\nplt.clabel(\\n    elastic_net_contour,\\n    inline=1,\\n    fontsize=18,\\n    fmt={1.0: \"elastic-net\"},\\n    manual=[(-1, -1)],\\n)\\nplt.clabel(l2_contour, inline=1, fontsize=18, fmt={1.0: \"L2\"}, manual=[(-1, -1)])\\nplt.clabel(l1_contour, inline=1, fontsize=18, fmt={1.0: \"L1\"}, manual=[(-1, -1)])\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_model_selection.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def highlight_min(x):\\n    x_min = x.min()\\n    return [\"font-weight: bold\" if v == x_min else \"\" for v in x]'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_model_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================\\nLasso model selection: AIC-BIC / cross-validation\\n=================================================\\n\\nThis example focuses on model selection for Lasso models that are\\nlinear models with an L1 penalty for regression problems.\\n\\nIndeed, several strategies can be used to select the value of the\\nregularization parameter: via cross-validation or using an information\\ncriterion, namely AIC or BIC.\\n\\nIn what follows, we will discuss in details the different strategies.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Dataset\\n# -------\\n# In this example, we will use the diabetes dataset.\\nfrom sklearn.datasets import load_diabetes\\n\\nX, y = load_diabetes(return_X_y=True, as_frame=True)\\nX.head()\\n\\n# %%\\n# In addition, we add some random features to the original data to\\n# better illustrate the feature selection performed by the Lasso model.\\nimport numpy as np\\nimport pandas as pd\\n\\nrng = np.random.RandomState(42)\\nn_random_features = 14\\nX_random = pd.DataFrame(\\n    rng.randn(X.shape[0], n_random_features),\\n    columns=[f\"random_{i:02d}\" for i in range(n_random_features)],\\n)\\nX = pd.concat([X, X_random], axis=1)\\n# Show only a subset of the columns\\nX[X.columns[::3]].head()\\n\\n# %%\\n# Selecting Lasso via an information criterion\\n# --------------------------------------------\\n# :class:`~sklearn.linear_model.LassoLarsIC` provides a Lasso estimator that\\n# uses the Akaike information criterion (AIC) or the Bayes information\\n# criterion (BIC) to select the optimal value of the regularization\\n# parameter alpha.\\n#\\n# Before fitting the model, we will standardize the data with a\\n# :class:`~sklearn.preprocessing.StandardScaler`. In addition, we will\\n# measure the time to fit and tune the hyperparameter alpha in order to\\n# compare with the cross-validation strategy.\\n#\\n# We will first fit a Lasso model with the AIC criterion.\\nimport time\\n\\nfrom sklearn.linear_model import LassoLarsIC\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nstart_time = time.time()\\nlasso_lars_ic = make_pipeline(StandardScaler(), LassoLarsIC(criterion=\"aic\")).fit(X, y)\\nfit_time = time.time() - start_time\\n\\n# %%\\n# We store the AIC metric for each value of alpha used during `fit`.\\nresults = pd.DataFrame(\\n    {\\n        \"alphas\": lasso_lars_ic[-1].alphas_,\\n        \"AIC criterion\": lasso_lars_ic[-1].criterion_,\\n    }\\n).set_index(\"alphas\")\\nalpha_aic = lasso_lars_ic[-1].alpha_\\n\\n# %%\\n# Now, we perform the same analysis using the BIC criterion.\\nlasso_lars_ic.set_params(lassolarsic__criterion=\"bic\").fit(X, y)\\nresults[\"BIC criterion\"] = lasso_lars_ic[-1].criterion_\\nalpha_bic = lasso_lars_ic[-1].alpha_\\n\\n\\n# %%\\n# We can check which value of `alpha` leads to the minimum AIC and BIC.\\n# Code for: def highlight_min(x):\\n\\n\\nresults.style.apply(highlight_min)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_model_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We can check which value of `alpha` leads to the minimum AIC and BIC.\\n# Code for: def highlight_min(x):\\n\\n\\nresults.style.apply(highlight_min)\\n\\n# %%\\n# Finally, we can plot the AIC and BIC values for the different alpha values.\\n# The vertical lines in the plot correspond to the alpha chosen for each\\n# criterion. The selected alpha corresponds to the minimum of the AIC or BIC\\n# criterion.\\nax = results.plot()\\nax.vlines(\\n    alpha_aic,\\n    results[\"AIC criterion\"].min(),\\n    results[\"AIC criterion\"].max(),\\n    label=\"alpha: AIC estimate\",\\n    linestyles=\"--\",\\n    color=\"tab:blue\",\\n)\\nax.vlines(\\n    alpha_bic,\\n    results[\"BIC criterion\"].min(),\\n    results[\"BIC criterion\"].max(),\\n    label=\"alpha: BIC estimate\",\\n    linestyle=\"--\",\\n    color=\"tab:orange\",\\n)\\nax.set_xlabel(r\"$\\\\alpha$\")\\nax.set_ylabel(\"criterion\")\\nax.set_xscale(\"log\")\\nax.legend()\\n_ = ax.set_title(\\n    f\"Information-criterion for model selection (training time {fit_time:.2f}s)\"\\n)\\n\\n# %%\\n# Model selection with an information-criterion is very fast. It relies on\\n# computing the criterion on the in-sample set provided to `fit`. Both criteria\\n# estimate the model generalization error based on the training set error and\\n# penalize this overly optimistic error. However, this penalty relies on a\\n# proper estimation of the degrees of freedom and the noise variance. Both are\\n# derived for large samples (asymptotic results) and assume the model is\\n# correct, i.e. that the data are actually generated by this model.\\n#\\n# These models also tend to break when the problem is badly conditioned (more\\n# features than samples). It is then required to provide an estimate of the\\n# noise variance.\\n#\\n# Selecting Lasso via cross-validation\\n# ------------------------------------\\n# The Lasso estimator can be implemented with different solvers: coordinate\\n# descent and least angle regression. They differ with regards to their\\n# execution speed and sources of numerical errors.\\n#\\n# In scikit-learn, two different estimators are available with integrated\\n# cross-validation: :class:`~sklearn.linear_model.LassoCV` and\\n# :class:`~sklearn.linear_model.LassoLarsCV` that respectively solve the\\n# problem with coordinate descent and least angle regression.\\n#\\n# In the remainder of this section, we will present both approaches. For both\\n# algorithms, we will use a 20-fold cross-validation strategy.\\n#\\n# Lasso via coordinate descent\\n# ............................\\n# Let\\'s start by making the hyperparameter tuning using\\n# :class:`~sklearn.linear_model.LassoCV`.\\nfrom sklearn.linear_model import LassoCV\\n\\nstart_time = time.time()\\nmodel = make_pipeline(StandardScaler(), LassoCV(cv=20)).fit(X, y)\\nfit_time = time.time() - start_time\\n\\n# %%\\nimport matplotlib.pyplot as plt'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_model_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='start_time = time.time()\\nmodel = make_pipeline(StandardScaler(), LassoCV(cv=20)).fit(X, y)\\nfit_time = time.time() - start_time\\n\\n# %%\\nimport matplotlib.pyplot as plt\\n\\nymin, ymax = 2300, 3800\\nlasso = model[-1]\\nplt.semilogx(lasso.alphas_, lasso.mse_path_, linestyle=\":\")\\nplt.plot(\\n    lasso.alphas_,\\n    lasso.mse_path_.mean(axis=-1),\\n    color=\"black\",\\n    label=\"Average across the folds\",\\n    linewidth=2,\\n)\\nplt.axvline(lasso.alpha_, linestyle=\"--\", color=\"black\", label=\"alpha: CV estimate\")\\n\\nplt.ylim(ymin, ymax)\\nplt.xlabel(r\"$\\\\alpha$\")\\nplt.ylabel(\"Mean square error\")\\nplt.legend()\\n_ = plt.title(\\n    f\"Mean square error on each fold: coordinate descent (train time: {fit_time:.2f}s)\"\\n)\\n\\n# %%\\n# Lasso via least angle regression\\n# ................................\\n# Let\\'s start by making the hyperparameter tuning using\\n# :class:`~sklearn.linear_model.LassoLarsCV`.\\nfrom sklearn.linear_model import LassoLarsCV\\n\\nstart_time = time.time()\\nmodel = make_pipeline(StandardScaler(), LassoLarsCV(cv=20)).fit(X, y)\\nfit_time = time.time() - start_time\\n\\n# %%\\nlasso = model[-1]\\nplt.semilogx(lasso.cv_alphas_, lasso.mse_path_, \":\")\\nplt.semilogx(\\n    lasso.cv_alphas_,\\n    lasso.mse_path_.mean(axis=-1),\\n    color=\"black\",\\n    label=\"Average across the folds\",\\n    linewidth=2,\\n)\\nplt.axvline(lasso.alpha_, linestyle=\"--\", color=\"black\", label=\"alpha CV\")\\n\\nplt.ylim(ymin, ymax)\\nplt.xlabel(r\"$\\\\alpha$\")\\nplt.ylabel(\"Mean square error\")\\nplt.legend()\\n_ = plt.title(f\"Mean square error on each fold: Lars (train time: {fit_time:.2f}s)\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_model_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.ylim(ymin, ymax)\\nplt.xlabel(r\"$\\\\alpha$\")\\nplt.ylabel(\"Mean square error\")\\nplt.legend()\\n_ = plt.title(f\"Mean square error on each fold: Lars (train time: {fit_time:.2f}s)\")\\n\\n# %%\\n# Summary of cross-validation approach\\n# ....................................\\n# Both algorithms give roughly the same results.\\n#\\n# Lars computes a solution path only for each kink in the path. As a result, it\\n# is very efficient when there are only of few kinks, which is the case if\\n# there are few features or samples. Also, it is able to compute the full path\\n# without setting any hyperparameter. On the opposite, coordinate descent\\n# computes the path points on a pre-specified grid (here we use the default).\\n# Thus it is more efficient if the number of grid points is smaller than the\\n# number of kinks in the path. Such a strategy can be interesting if the number\\n# of features is really large and there are enough samples to be selected in\\n# each of the cross-validation fold. In terms of numerical errors, for heavily\\n# correlated variables, Lars will accumulate more errors, while the coordinate\\n# descent algorithm will only sample the path on a grid.\\n#\\n# Note how the optimal value of alpha varies for each fold. This illustrates\\n# why nested-cross validation is a good strategy when trying to evaluate the\\n# performance of a method for which a parameter is chosen by cross-validation:\\n# this choice of parameter may not be optimal for a final evaluation on\\n# unseen test set only.\\n#\\n# Conclusion\\n# ----------\\n# In this tutorial, we presented two approaches for selecting the best\\n# hyperparameter `alpha`: one strategy finds the optimal value of `alpha`\\n# by only using the training set and some information criterion, and another\\n# strategy is based on cross-validation.\\n#\\n# In this example, both approaches are working similarly. The in-sample\\n# hyperparameter selection even shows its efficacy in terms of computational\\n# performance. However, it can only be used when the number of samples is large\\n# enough compared to the number of features.\\n#\\n# That\\'s why hyperparameter optimization via cross-validation is a safe\\n# strategy: it works in different settings.'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_logistic_l1_l2_sparsity.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================\\nL1 Penalty and Sparsity in Logistic Regression\\n==============================================\\n\\nComparison of the sparsity (percentage of zero coefficients) of solutions when\\nL1, L2 and Elastic-Net penalty are used for different values of C. We can see\\nthat large values of C give more freedom to the model.  Conversely, smaller\\nvalues of C constrain the model more. In the L1 penalty case, this leads to\\nsparser solutions. As expected, the Elastic-Net penalty sparsity is between\\nthat of L1 and L2.\\n\\nWe classify 8x8 images of digits into two classes: 0-4 against 5-9.\\nThe visualization shows coefficients of the models for varying C.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\n\\nX, y = datasets.load_digits(return_X_y=True)\\n\\nX = StandardScaler().fit_transform(X)\\n\\n# classify small against large digits\\ny = (y > 4).astype(int)\\n\\nl1_ratio = 0.5  # L1 weight in the Elastic-Net regularization\\n\\nfig, axes = plt.subplots(3, 3)\\n\\n# Set regularization parameter\\nfor i, (C, axes_row) in enumerate(zip((1, 0.1, 0.01), axes)):\\n    # Increase tolerance for short training time\\n    clf_l1_LR = LogisticRegression(C=C, penalty=\"l1\", tol=0.01, solver=\"saga\")\\n    clf_l2_LR = LogisticRegression(C=C, penalty=\"l2\", tol=0.01, solver=\"saga\")\\n    clf_en_LR = LogisticRegression(\\n        C=C, penalty=\"elasticnet\", solver=\"saga\", l1_ratio=l1_ratio, tol=0.01\\n    )\\n    clf_l1_LR.fit(X, y)\\n    clf_l2_LR.fit(X, y)\\n    clf_en_LR.fit(X, y)\\n\\n    coef_l1_LR = clf_l1_LR.coef_.ravel()\\n    coef_l2_LR = clf_l2_LR.coef_.ravel()\\n    coef_en_LR = clf_en_LR.coef_.ravel()\\n\\n    # coef_l1_LR contains zeros due to the\\n    # L1 sparsity inducing norm\\n\\n    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\\n    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\\n    sparsity_en_LR = np.mean(coef_en_LR == 0) * 100\\n\\n    print(f\"C={C:.2f}\")\\n    print(f\"{\\'Sparsity with L1 penalty:\\':<40} {sparsity_l1_LR:.2f}%\")\\n    print(f\"{\\'Sparsity with Elastic-Net penalty:\\':<40} {sparsity_en_LR:.2f}%\")\\n    print(f\"{\\'Sparsity with L2 penalty:\\':<40} {sparsity_l2_LR:.2f}%\")\\n    print(f\"{\\'Score with L1 penalty:\\':<40} {clf_l1_LR.score(X, y):.2f}\")\\n    print(f\"{\\'Score with Elastic-Net penalty:\\':<40} {clf_en_LR.score(X, y):.2f}\")\\n    print(f\"{\\'Score with L2 penalty:\\':<40} {clf_l2_LR.score(X, y):.2f}\")\\n\\n    if i == 0:\\n        axes_row[0].set_title(\"L1 penalty\")\\n        axes_row[1].set_title(\"Elastic-Net\\\\nl1_ratio = %s\" % l1_ratio)\\n        axes_row[2].set_title(\"L2 penalty\")\\n\\n    for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):\\n        ax.imshow(\\n            np.abs(coefs.reshape(8, 8)),\\n            interpolation=\"nearest\",\\n            cmap=\"binary\",\\n            vmax=1,\\n            vmin=0,\\n        )\\n        ax.set_xticks(())\\n        ax.set_yticks(())'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_logistic_l1_l2_sparsity.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='axes_row[0].set_ylabel(f\"C = {C}\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_omp.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================\\nOrthogonal Matching Pursuit\\n===========================\\n\\nUsing orthogonal matching pursuit for recovering a sparse signal from a noisy\\nmeasurement encoded with a dictionary\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_sparse_coded_signal\\nfrom sklearn.linear_model import OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV\\n\\nn_components, n_features = 512, 100\\nn_nonzero_coefs = 17\\n\\n# generate the data\\n\\n# y = Xw\\n# |x|_0 = n_nonzero_coefs\\n\\ny, X, w = make_sparse_coded_signal(\\n    n_samples=1,\\n    n_components=n_components,\\n    n_features=n_features,\\n    n_nonzero_coefs=n_nonzero_coefs,\\n    random_state=0,\\n)\\nX = X.T\\n\\n(idx,) = w.nonzero()\\n\\n# distort the clean signal\\ny_noisy = y + 0.05 * np.random.randn(len(y))\\n\\n# plot the sparse signal\\nplt.figure(figsize=(7, 7))\\nplt.subplot(4, 1, 1)\\nplt.xlim(0, 512)\\nplt.title(\"Sparse signal\")\\nplt.stem(idx, w[idx])\\n\\n# plot the noise-free reconstruction\\nomp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\\nomp.fit(X, y)\\ncoef = omp.coef_\\n(idx_r,) = coef.nonzero()\\nplt.subplot(4, 1, 2)\\nplt.xlim(0, 512)\\nplt.title(\"Recovered signal from noise-free measurements\")\\nplt.stem(idx_r, coef[idx_r])\\n\\n# plot the noisy reconstruction\\nomp.fit(X, y_noisy)\\ncoef = omp.coef_\\n(idx_r,) = coef.nonzero()\\nplt.subplot(4, 1, 3)\\nplt.xlim(0, 512)\\nplt.title(\"Recovered signal from noisy measurements\")\\nplt.stem(idx_r, coef[idx_r])\\n\\n# plot the noisy reconstruction with number of non-zeros set by CV\\nomp_cv = OrthogonalMatchingPursuitCV()\\nomp_cv.fit(X, y_noisy)\\ncoef = omp_cv.coef_\\n(idx_r,) = coef.nonzero()\\nplt.subplot(4, 1, 4)\\nplt.xlim(0, 512)\\nplt.title(\"Recovered signal from noisy measurements with CV\")\\nplt.stem(idx_r, coef[idx_r])\\n\\nplt.subplots_adjust(0.06, 0.04, 0.94, 0.90, 0.20, 0.38)\\nplt.suptitle(\"Sparse signal recovery with Orthogonal Matching Pursuit\", fontsize=16)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def score_estimator(estimator, df_test):\\n    \"\"\"Score an estimator on the test set.\"\"\"\\n    y_pred = estimator.predict(df_test)\\n\\n    print(\\n        \"MSE: %.3f\"\\n        % mean_squared_error(\\n            df_test[\"Frequency\"], y_pred, sample_weight=df_test[\"Exposure\"]\\n        )\\n    )\\n    print(\\n        \"MAE: %.3f\"\\n        % mean_absolute_error(\\n            df_test[\"Frequency\"], y_pred, sample_weight=df_test[\"Exposure\"]\\n        )\\n    )\\n\\n    # Ignore non-positive predictions, as they are invalid for\\n    # the Poisson deviance.\\n    mask = y_pred > 0\\n    if (~mask).any():\\n        n_masked, n_samples = (~mask).sum(), mask.shape[0]\\n        print(\\n            \"WARNING: Estimator yields invalid, non-positive predictions \"\\n            f\" for {n_masked} samples out of {n_samples}. These predictions \"\\n            \"are ignored when computing the Poisson deviance.\"\\n        )\\n\\n    print(\\n        \"mean Poisson deviance: %.3f\"\\n        % mean_poisson_deviance(\\n            df_test[\"Frequency\"][mask],\\n            y_pred[mask],\\n            sample_weight=df_test[\"Exposure\"][mask],\\n        )\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def _mean_frequency_by_risk_group(y_true, y_pred, sample_weight=None, n_bins=100):\\n    \"\"\"Compare predictions and observations for bins ordered by y_pred.\\n\\n    We order the samples by ``y_pred`` and split it in bins.\\n    In each bin the observed mean is compared with the predicted mean.\\n\\n    Parameters\\n    ----------\\n    y_true: array-like of shape (n_samples,)\\n        Ground truth (correct) target values.\\n    y_pred: array-like of shape (n_samples,)\\n        Estimated target values.\\n    sample_weight : array-like of shape (n_samples,)\\n        Sample weights.\\n    n_bins: int\\n        Number of bins to use.\\n\\n    Returns\\n    -------\\n    bin_centers: ndarray of shape (n_bins,)\\n        bin centers\\n    y_true_bin: ndarray of shape (n_bins,)\\n        average y_pred for each bin\\n    y_pred_bin: ndarray of shape (n_bins,)\\n        average y_pred for each bin\\n    \"\"\"\\n    idx_sort = np.argsort(y_pred)\\n    bin_centers = np.arange(0, 1, 1 / n_bins) + 0.5 / n_bins\\n    y_pred_bin = np.zeros(n_bins)\\n    y_true_bin = np.zeros(n_bins)\\n\\n    for n, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\\n        weights = sample_weight[idx_sort][sl]\\n        y_pred_bin[n] = np.average(y_pred[idx_sort][sl], weights=weights)\\n        y_true_bin[n] = np.average(y_true[idx_sort][sl], weights=weights)\\n    return bin_centers, y_true_bin, y_pred_bin'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def lorenz_curve(y_true, y_pred, exposure):\\n    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\\n    exposure = np.asarray(exposure)\\n\\n    # order samples by increasing predicted risk:\\n    ranking = np.argsort(y_pred)\\n    ranked_frequencies = y_true[ranking]\\n    ranked_exposure = exposure[ranking]\\n    cumulated_claims = np.cumsum(ranked_frequencies * ranked_exposure)\\n    cumulated_claims /= cumulated_claims[-1]\\n    cumulated_exposure = np.cumsum(ranked_exposure)\\n    cumulated_exposure /= cumulated_exposure[-1]\\n    return cumulated_exposure, cumulated_claims'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\"\"\"\\n======================================\\nPoisson regression and non-normal loss\\n======================================\\n\\nThis example illustrates the use of log-linear Poisson regression on the\\n`French Motor Third-Party Liability Claims dataset\\n<https://www.openml.org/d/41214>`_ from [1]_ and compares it with a linear\\nmodel fitted with the usual least squared error and a non-linear GBRT model\\nfitted with the Poisson loss (and a log-link).\\n\\nA few definitions:\\n\\n- A **policy** is a contract between an insurance company and an individual:\\n  the **policyholder**, that is, the vehicle driver in this case.\\n\\n- A **claim** is the request made by a policyholder to the insurer to\\n  compensate for a loss covered by the insurance.\\n\\n- The **exposure** is the duration of the insurance coverage of a given policy,\\n  in years.\\n\\n- The claim **frequency** is the number of claims divided by the exposure,\\n  typically measured in number of claims per year.\\n\\nIn this dataset, each sample corresponds to an insurance policy. Available\\nfeatures include driver age, vehicle age, vehicle power, etc.\\n\\nOur goal is to predict the expected frequency of claims following car accidents\\nfor a new policyholder given the historical data over a population of\\npolicyholders.\\n\\n.. [1]  A. Noll, R. Salzmann and M.V. Wuthrich, Case Study: French Motor\\n    Third-Party Liability Claims (November 8, 2018). `doi:10.2139/ssrn.3164764\\n    <https://doi.org/10.2139/ssrn.3164764>`_\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\n\\n##############################################################################\\n# The French Motor Third-Party Liability Claims dataset\\n# -----------------------------------------------------\\n#\\n# Let\\'s load the motor claim dataset from OpenML:\\n# https://www.openml.org/d/41214\\nfrom sklearn.datasets import fetch_openml\\n\\ndf = fetch_openml(data_id=41214, as_frame=True).frame\\ndf\\n\\n# %%\\n# The number of claims (``ClaimNb``) is a positive integer that can be modeled\\n# as a Poisson distribution. It is then assumed to be the number of discrete\\n# events occurring with a constant rate in a given time interval (``Exposure``,\\n# in units of years).\\n#\\n# Here we want to model the frequency ``y = ClaimNb / Exposure`` conditionally\\n# on ``X`` via a (scaled) Poisson distribution, and use ``Exposure`` as\\n# ``sample_weight``.\\n\\ndf[\"Frequency\"] = df[\"ClaimNb\"] / df[\"Exposure\"]\\n\\nprint(\\n    \"Average Frequency = {}\".format(np.average(df[\"Frequency\"], weights=df[\"Exposure\"]))\\n)\\n\\nprint(\\n    \"Fraction of exposure with zero claims = {0:.1%}\".format(\\n        df.loc[df[\"ClaimNb\"] == 0, \"Exposure\"].sum() / df[\"Exposure\"].sum()\\n    )\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(\\n    \"Average Frequency = {}\".format(np.average(df[\"Frequency\"], weights=df[\"Exposure\"]))\\n)\\n\\nprint(\\n    \"Fraction of exposure with zero claims = {0:.1%}\".format(\\n        df.loc[df[\"ClaimNb\"] == 0, \"Exposure\"].sum() / df[\"Exposure\"].sum()\\n    )\\n)\\n\\nfig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(16, 4))\\nax0.set_title(\"Number of claims\")\\n_ = df[\"ClaimNb\"].hist(bins=30, log=True, ax=ax0)\\nax1.set_title(\"Exposure in years\")\\n_ = df[\"Exposure\"].hist(bins=30, log=True, ax=ax1)\\nax2.set_title(\"Frequency (number of claims per year)\")\\n_ = df[\"Frequency\"].hist(bins=30, log=True, ax=ax2)\\n\\n# %%\\n# The remaining columns can be used to predict the frequency of claim events.\\n# Those columns are very heterogeneous with a mix of categorical and numeric\\n# variables with different scales, possibly very unevenly distributed.\\n#\\n# In order to fit linear models with those predictors it is therefore\\n# necessary to perform standard feature transformations as follows:\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import (\\n    FunctionTransformer,\\n    KBinsDiscretizer,\\n    OneHotEncoder,\\n    StandardScaler,\\n)\\n\\nlog_scale_transformer = make_pipeline(\\n    FunctionTransformer(np.log, validate=False), StandardScaler()\\n)\\n\\nlinear_model_preprocessor = ColumnTransformer(\\n    [\\n        (\"passthrough_numeric\", \"passthrough\", [\"BonusMalus\"]),\\n        (\\n            \"binned_numeric\",\\n            KBinsDiscretizer(n_bins=10, random_state=0),\\n            [\"VehAge\", \"DrivAge\"],\\n        ),\\n        (\"log_scaled_numeric\", log_scale_transformer, [\"Density\"]),\\n        (\\n            \"onehot_categorical\",\\n            OneHotEncoder(),\\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"],\\n        ),\\n    ],\\n    remainder=\"drop\",\\n)\\n\\n# %%\\n# A constant prediction baseline\\n# ------------------------------\\n#\\n# It is worth noting that more than 93% of policyholders have zero claims. If\\n# we were to convert this problem into a binary classification task, it would\\n# be significantly imbalanced, and even a simplistic model that would only\\n# predict mean can achieve an accuracy of 93%.\\n#\\n# To evaluate the pertinence of the used metrics, we will consider as a\\n# baseline a \"dummy\" estimator that constantly predicts the mean frequency of\\n# the training sample.\\n\\nfrom sklearn.dummy import DummyRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.pipeline import Pipeline\\n\\ndf_train, df_test = train_test_split(df, test_size=0.33, random_state=0)\\n\\ndummy = Pipeline(\\n    [\\n        (\"preprocessor\", linear_model_preprocessor),\\n        (\"regressor\", DummyRegressor(strategy=\"mean\")),\\n    ]\\n).fit(df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"])\\n\\n\\n##############################################################################\\n# Let\\'s compute the performance of this constant prediction baseline with 3\\n# different regression metrics:'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='##############################################################################\\n# Let\\'s compute the performance of this constant prediction baseline with 3\\n# different regression metrics:\\n\\nfrom sklearn.metrics import (\\n    mean_absolute_error,\\n    mean_poisson_deviance,\\n    mean_squared_error,\\n)\\n\\n\\n# Code for: def score_estimator(estimator, df_test):\\n\\n\\nprint(\"Constant mean frequency evaluation:\")\\nscore_estimator(dummy, df_test)\\n\\n# %%\\n# (Generalized) linear models\\n# ---------------------------\\n#\\n# We start by modeling the target variable with the (l2 penalized) least\\n# squares linear regression model, more commonly known as Ridge regression. We\\n# use a low penalization `alpha`, as we expect such a linear model to under-fit\\n# on such a large dataset.\\n\\nfrom sklearn.linear_model import Ridge\\n\\nridge_glm = Pipeline(\\n    [\\n        (\"preprocessor\", linear_model_preprocessor),\\n        (\"regressor\", Ridge(alpha=1e-6)),\\n    ]\\n).fit(df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"])\\n\\n# %%\\n# The Poisson deviance cannot be computed on non-positive values predicted by\\n# the model. For models that do return a few non-positive predictions (e.g.\\n# :class:`~sklearn.linear_model.Ridge`) we ignore the corresponding samples,\\n# meaning that the obtained Poisson deviance is approximate. An alternative\\n# approach could be to use :class:`~sklearn.compose.TransformedTargetRegressor`\\n# meta-estimator to map ``y_pred`` to a strictly positive domain.\\n\\nprint(\"Ridge evaluation:\")\\nscore_estimator(ridge_glm, df_test)\\n\\n# %%\\n# Next we fit the Poisson regressor on the target variable. We set the\\n# regularization strength ``alpha`` to approximately 1e-6 over number of\\n# samples (i.e. `1e-12`) in order to mimic the Ridge regressor whose L2 penalty\\n# term scales differently with the number of samples.\\n#\\n# Since the Poisson regressor internally models the log of the expected target\\n# value instead of the expected value directly (log vs identity link function),\\n# the relationship between X and y is not exactly linear anymore. Therefore the\\n# Poisson regressor is called a Generalized Linear Model (GLM) rather than a\\n# vanilla linear model as is the case for Ridge regression.\\n\\nfrom sklearn.linear_model import PoissonRegressor\\n\\nn_samples = df_train.shape[0]\\n\\npoisson_glm = Pipeline(\\n    [\\n        (\"preprocessor\", linear_model_preprocessor),\\n        (\"regressor\", PoissonRegressor(alpha=1e-12, solver=\"newton-cholesky\")),\\n    ]\\n)\\npoisson_glm.fit(\\n    df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"]\\n)\\n\\nprint(\"PoissonRegressor evaluation:\")\\nscore_estimator(poisson_glm, df_test)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(\"PoissonRegressor evaluation:\")\\nscore_estimator(poisson_glm, df_test)\\n\\n# %%\\n# Gradient Boosting Regression Trees for Poisson regression\\n# ---------------------------------------------------------\\n#\\n# Finally, we will consider a non-linear model, namely Gradient Boosting\\n# Regression Trees. Tree-based models do not require the categorical data to be\\n# one-hot encoded: instead, we can encode each category label with an arbitrary\\n# integer using :class:`~sklearn.preprocessing.OrdinalEncoder`. With this\\n# encoding, the trees will treat the categorical features as ordered features,\\n# which might not be always a desired behavior. However this effect is limited\\n# for deep enough trees which are able to recover the categorical nature of the\\n# features. The main advantage of the\\n# :class:`~sklearn.preprocessing.OrdinalEncoder` over the\\n# :class:`~sklearn.preprocessing.OneHotEncoder` is that it will make training\\n# faster.\\n#\\n# Gradient Boosting also gives the possibility to fit the trees with a Poisson\\n# loss (with an implicit log-link function) instead of the default\\n# least-squares loss. Here we only fit trees with the Poisson loss to keep this\\n# example concise.\\n\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\nfrom sklearn.preprocessing import OrdinalEncoder\\n\\ntree_preprocessor = ColumnTransformer(\\n    [\\n        (\\n            \"categorical\",\\n            OrdinalEncoder(),\\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"],\\n        ),\\n        (\"numeric\", \"passthrough\", [\"VehAge\", \"DrivAge\", \"BonusMalus\", \"Density\"]),\\n    ],\\n    remainder=\"drop\",\\n)\\npoisson_gbrt = Pipeline(\\n    [\\n        (\"preprocessor\", tree_preprocessor),\\n        (\\n            \"regressor\",\\n            HistGradientBoostingRegressor(loss=\"poisson\", max_leaf_nodes=128),\\n        ),\\n    ]\\n)\\npoisson_gbrt.fit(\\n    df_train, df_train[\"Frequency\"], regressor__sample_weight=df_train[\"Exposure\"]\\n)\\n\\nprint(\"Poisson Gradient Boosted Trees evaluation:\")\\nscore_estimator(poisson_gbrt, df_test)\\n\\n# %%\\n# Like the Poisson GLM above, the gradient boosted trees model minimizes\\n# the Poisson deviance. However, because of a higher predictive power,\\n# it reaches lower values of Poisson deviance.\\n#\\n# Evaluating models with a single train / test split is prone to random\\n# fluctuations. If computing resources allow, it should be verified that\\n# cross-validated performance metrics would lead to similar conclusions.\\n#\\n# The qualitative difference between these models can also be visualized by\\n# comparing the histogram of observed target values with that of predicted\\n# values:\\n\\nfig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 6), sharey=True)\\nfig.subplots_adjust(bottom=0.2)\\nn_bins = 20\\nfor row_idx, label, df in zip(range(2), [\"train\", \"test\"], [df_train, df_test]):\\n    df[\"Frequency\"].hist(bins=np.linspace(-1, 30, n_bins), ax=axes[row_idx, 0])'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 6), sharey=True)\\nfig.subplots_adjust(bottom=0.2)\\nn_bins = 20\\nfor row_idx, label, df in zip(range(2), [\"train\", \"test\"], [df_train, df_test]):\\n    df[\"Frequency\"].hist(bins=np.linspace(-1, 30, n_bins), ax=axes[row_idx, 0])\\n\\n    axes[row_idx, 0].set_title(\"Data\")\\n    axes[row_idx, 0].set_yscale(\"log\")\\n    axes[row_idx, 0].set_xlabel(\"y (observed Frequency)\")\\n    axes[row_idx, 0].set_ylim([1e1, 5e5])\\n    axes[row_idx, 0].set_ylabel(label + \" samples\")\\n\\n    for idx, model in enumerate([ridge_glm, poisson_glm, poisson_gbrt]):\\n        y_pred = model.predict(df)\\n\\n        pd.Series(y_pred).hist(\\n            bins=np.linspace(-1, 4, n_bins), ax=axes[row_idx, idx + 1]\\n        )\\n        axes[row_idx, idx + 1].set(\\n            title=model[-1].__class__.__name__,\\n            yscale=\"log\",\\n            xlabel=\"y_pred (predicted expected Frequency)\",\\n        )\\nplt.tight_layout()\\n\\n# %%\\n# The experimental data presents a long tail distribution for ``y``. In all\\n# models, we predict the expected frequency of a random variable, so we will\\n# have necessarily fewer extreme values than for the observed realizations of\\n# that random variable. This explains that the mode of the histograms of model\\n# predictions doesn\\'t necessarily correspond to the smallest value.\\n# Additionally, the normal distribution used in ``Ridge`` has a constant\\n# variance, while for the Poisson distribution used in ``PoissonRegressor`` and\\n# ``HistGradientBoostingRegressor``, the variance is proportional to the\\n# predicted expected value.\\n#\\n# Thus, among the considered estimators, ``PoissonRegressor`` and\\n# ``HistGradientBoostingRegressor`` are a-priori better suited for modeling the\\n# long tail distribution of the non-negative data as compared to the ``Ridge``\\n# model which makes a wrong assumption on the distribution of the target\\n# variable.\\n#\\n# The ``HistGradientBoostingRegressor`` estimator has the most flexibility and\\n# is able to predict higher expected values.\\n#\\n# Note that we could have used the least squares loss for the\\n# ``HistGradientBoostingRegressor`` model. This would wrongly assume a normal\\n# distributed response variable as does the `Ridge` model, and possibly\\n# also lead to slightly negative predictions. However the gradient boosted\\n# trees would still perform relatively well and in particular better than\\n# ``PoissonRegressor`` thanks to the flexibility of the trees combined with the\\n# large number of training samples.\\n#\\n# Evaluation of the calibration of predictions\\n# --------------------------------------------\\n#\\n# To ensure that estimators yield reasonable predictions for different\\n# policyholder types, we can bin test samples according to ``y_pred`` returned\\n# by each model. Then for each bin, we compare the mean predicted ``y_pred``,\\n# with the mean observed target:\\n\\nfrom sklearn.utils import gen_even_slices\\n\\n\\n# Code for: def _mean_frequency_by_risk_group(y_true, y_pred, sample_weight=None, n_bins=100):'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.utils import gen_even_slices\\n\\n\\n# Code for: def _mean_frequency_by_risk_group(y_true, y_pred, sample_weight=None, n_bins=100):\\n\\n\\nprint(f\"Actual number of claims: {df_test[\\'ClaimNb\\'].sum()}\")\\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\\nplt.subplots_adjust(wspace=0.3)\\n\\nfor axi, model in zip(ax.ravel(), [ridge_glm, poisson_glm, poisson_gbrt, dummy]):\\n    y_pred = model.predict(df_test)\\n    y_true = df_test[\"Frequency\"].values\\n    exposure = df_test[\"Exposure\"].values\\n    q, y_true_seg, y_pred_seg = _mean_frequency_by_risk_group(\\n        y_true, y_pred, sample_weight=exposure, n_bins=10\\n    )\\n\\n    # Name of the model after the estimator used in the last step of the\\n    # pipeline.\\n    print(f\"Predicted number of claims by {model[-1]}: {np.sum(y_pred * exposure):.1f}\")\\n\\n    axi.plot(q, y_pred_seg, marker=\"x\", linestyle=\"--\", label=\"predictions\")\\n    axi.plot(q, y_true_seg, marker=\"o\", linestyle=\"--\", label=\"observations\")\\n    axi.set_xlim(0, 1.0)\\n    axi.set_ylim(0, 0.5)\\n    axi.set(\\n        title=model[-1],\\n        xlabel=\"Fraction of samples sorted by y_pred\",\\n        ylabel=\"Mean Frequency (y_pred)\",\\n    )\\n    axi.legend()\\nplt.tight_layout()\\n\\n# %%\\n# The dummy regression model predicts a constant frequency. This model does not\\n# attribute the same tied rank to all samples but is none-the-less globally\\n# well calibrated (to estimate the mean frequency of the entire population).\\n#\\n# The ``Ridge`` regression model can predict very low expected frequencies that\\n# do not match the data. It can therefore severely under-estimate the risk for\\n# some policyholders.\\n#\\n# ``PoissonRegressor`` and ``HistGradientBoostingRegressor`` show better\\n# consistency between predicted and observed targets, especially for low\\n# predicted target values.\\n#\\n# The sum of all predictions also confirms the calibration issue of the\\n# ``Ridge`` model: it under-estimates by more than 3% the total number of\\n# claims in the test set while the other three models can approximately recover\\n# the total number of claims of the test portfolio.\\n#\\n# Evaluation of the ranking power\\n# -------------------------------\\n#\\n# For some business applications, we are interested in the ability of the model\\n# to rank the riskiest from the safest policyholders, irrespective of the\\n# absolute value of the prediction. In this case, the model evaluation would\\n# cast the problem as a ranking problem rather than a regression problem.\\n#\\n# To compare the 3 models from this perspective, one can plot the cumulative\\n# proportion of claims vs the cumulative proportion of exposure for the test\\n# samples order by the model predictions, from safest to riskiest according to\\n# each model.\\n#\\n# This plot is called a Lorenz curve and can be summarized by the Gini index:\\n\\nfrom sklearn.metrics import auc\\n\\n\\n# Code for: def lorenz_curve(y_true, y_pred, exposure):\\n\\n\\nfig, ax = plt.subplots(figsize=(8, 8))'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.metrics import auc\\n\\n\\n# Code for: def lorenz_curve(y_true, y_pred, exposure):\\n\\n\\nfig, ax = plt.subplots(figsize=(8, 8))\\n\\nfor model in [dummy, ridge_glm, poisson_glm, poisson_gbrt]:\\n    y_pred = model.predict(df_test)\\n    cum_exposure, cum_claims = lorenz_curve(\\n        df_test[\"Frequency\"], y_pred, df_test[\"Exposure\"]\\n    )\\n    gini = 1 - 2 * auc(cum_exposure, cum_claims)\\n    label = \"{} (Gini: {:.2f})\".format(model[-1], gini)\\n    ax.plot(cum_exposure, cum_claims, linestyle=\"-\", label=label)\\n\\n# Oracle model: y_pred == y_test\\ncum_exposure, cum_claims = lorenz_curve(\\n    df_test[\"Frequency\"], df_test[\"Frequency\"], df_test[\"Exposure\"]\\n)\\ngini = 1 - 2 * auc(cum_exposure, cum_claims)\\nlabel = \"Oracle (Gini: {:.2f})\".format(gini)\\nax.plot(cum_exposure, cum_claims, linestyle=\"-.\", color=\"gray\", label=label)\\n\\n# Random Baseline\\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Random baseline\")\\nax.set(\\n    title=\"Lorenz curves by model\",\\n    xlabel=\"Cumulative proportion of exposure (from safest to riskiest)\",\\n    ylabel=\"Cumulative proportion of claims\",\\n)\\nax.legend(loc=\"upper left\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_poisson_regression_non_normal_loss.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Random Baseline\\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Random baseline\")\\nax.set(\\n    title=\"Lorenz curves by model\",\\n    xlabel=\"Cumulative proportion of exposure (from safest to riskiest)\",\\n    ylabel=\"Cumulative proportion of claims\",\\n)\\nax.legend(loc=\"upper left\")\\n\\n# %%\\n# As expected, the dummy regressor is unable to correctly rank the samples and\\n# therefore performs the worst on this plot.\\n#\\n# The tree-based model is significantly better at ranking policyholders by risk\\n# while the two linear models perform similarly.\\n#\\n# All three models are significantly better than chance but also very far from\\n# making perfect predictions.\\n#\\n# This last point is expected due to the nature of the problem: the occurrence\\n# of accidents is mostly dominated by circumstantial causes that are not\\n# captured in the columns of the dataset and can indeed be considered as purely\\n# random.\\n#\\n# The linear models assume no interactions between the input variables which\\n# likely causes under-fitting. Inserting a polynomial feature extractor\\n# (:func:`~sklearn.preprocessing.PolynomialFeatures`) indeed increases their\\n# discrimative power by 2 points of Gini index. In particular it improves the\\n# ability of the models to identify the top 5% riskiest profiles.\\n#\\n# Main takeaways\\n# --------------\\n#\\n# - The performance of the models can be evaluated by their ability to yield\\n#   well-calibrated predictions and a good ranking.\\n#\\n# - The calibration of the model can be assessed by plotting the mean observed\\n#   value vs the mean predicted value on groups of test samples binned by\\n#   predicted risk.\\n#\\n# - The least squares loss (along with the implicit use of the identity link\\n#   function) of the Ridge regression model seems to cause this model to be\\n#   badly calibrated. In particular, it tends to underestimate the risk and can\\n#   even predict invalid negative frequencies.\\n#\\n# - Using the Poisson loss with a log-link can correct these problems and lead\\n#   to a well-calibrated linear model.\\n#\\n# - The Gini index reflects the ability of a model to rank predictions\\n#   irrespective of their absolute values, and therefore only assess their\\n#   ranking power.\\n#\\n# - Despite the improvement in calibration, the ranking power of both linear\\n#   models are comparable and well below the ranking power of the Gradient\\n#   Boosting Regression Trees.\\n#\\n# - The Poisson deviance computed as an evaluation metric reflects both the\\n#   calibration and the ranking power of the model. It also makes a linear\\n#   assumption on the ideal relationship between the expected value and the\\n#   variance of the response variable. For the sake of conciseness we did not\\n#   check whether this assumption holds.\\n#\\n# - Traditional regression metrics such as Mean Squared Error and Mean Absolute\\n#   Error are hard to meaningfully interpret on count values with many zeros.\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_robust_fit.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\nRobust linear estimator fitting\\n===============================\\n\\nHere a sine function is fit with a polynomial of order 3, for values\\nclose to zero.\\n\\nRobust fitting is demoed in different situations:\\n\\n- No measurement errors, only modelling errors (fitting a sine with a\\n  polynomial)\\n\\n- Measurement errors in X\\n\\n- Measurement errors in y\\n\\nThe median absolute deviation to non corrupt new data is used to judge\\nthe quality of the prediction.\\n\\nWhat we can see that:\\n\\n- RANSAC is good for strong outliers in the y direction\\n\\n- TheilSen is good for small outliers, both in direction X and y, but has\\n  a break point above which it performs worse than OLS.\\n\\n- The scores of HuberRegressor may not be compared directly to both TheilSen\\n  and RANSAC because it does not attempt to completely filter the outliers\\n  but lessen their effect.\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn.linear_model import (\\n    HuberRegressor,\\n    LinearRegression,\\n    RANSACRegressor,\\n    TheilSenRegressor,\\n)\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import PolynomialFeatures\\n\\nnp.random.seed(42)\\n\\nX = np.random.normal(size=400)\\ny = np.sin(X)\\n# Make sure that it X is 2D\\nX = X[:, np.newaxis]\\n\\nX_test = np.random.normal(size=200)\\ny_test = np.sin(X_test)\\nX_test = X_test[:, np.newaxis]\\n\\ny_errors = y.copy()\\ny_errors[::3] = 3\\n\\nX_errors = X.copy()\\nX_errors[::3] = 3\\n\\ny_errors_large = y.copy()\\ny_errors_large[::3] = 10\\n\\nX_errors_large = X.copy()\\nX_errors_large[::3] = 10\\n\\nestimators = [\\n    (\"OLS\", LinearRegression()),\\n    (\"Theil-Sen\", TheilSenRegressor(random_state=42)),\\n    (\"RANSAC\", RANSACRegressor(random_state=42)),\\n    (\"HuberRegressor\", HuberRegressor()),\\n]\\ncolors = {\\n    \"OLS\": \"turquoise\",\\n    \"Theil-Sen\": \"gold\",\\n    \"RANSAC\": \"lightgreen\",\\n    \"HuberRegressor\": \"black\",\\n}\\nlinestyle = {\"OLS\": \"-\", \"Theil-Sen\": \"-.\", \"RANSAC\": \"--\", \"HuberRegressor\": \"--\"}\\nlw = 3\\n\\nx_plot = np.linspace(X.min(), X.max())\\nfor title, this_X, this_y in [\\n    (\"Modeling Errors Only\", X, y),\\n    (\"Corrupt X, Small Deviants\", X_errors, y),\\n    (\"Corrupt y, Small Deviants\", X, y_errors),\\n    (\"Corrupt X, Large Deviants\", X_errors_large, y),\\n    (\"Corrupt y, Large Deviants\", X, y_errors_large),\\n]:\\n    plt.figure(figsize=(5, 4))\\n    plt.plot(this_X[:, 0], this_y, \"b+\")\\n\\n    for name, estimator in estimators:\\n        model = make_pipeline(PolynomialFeatures(3), estimator)\\n        model.fit(this_X, this_y)\\n        mse = mean_squared_error(model.predict(X_test), y_test)\\n        y_plot = model.predict(x_plot[:, np.newaxis])\\n        plt.plot(\\n            x_plot,\\n            y_plot,\\n            color=colors[name],\\n            linestyle=linestyle[name],\\n            linewidth=lw,\\n            label=\"%s: error = %.3f\" % (name, mse),\\n        )'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_robust_fit.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='legend_title = \"Error of Mean\\\\nAbsolute Deviation\\\\nto Non-corrupt Data\"\\n    legend = plt.legend(\\n        loc=\"upper right\", frameon=False, title=legend_title, prop=dict(size=\"x-small\")\\n    )\\n    plt.xlim(-4, 10.2)\\n    plt.ylim(-2, 10.2)\\n    plt.title(title)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_multi_task_lasso_support.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================\\nJoint feature selection with multi-task Lasso\\n=============================================\\n\\nThe multi-task lasso allows to fit multiple regression problems\\njointly enforcing the selected features to be the same across\\ntasks. This example simulates sequential measurements, each task\\nis a time instant, and the relevant features vary in amplitude\\nover time while being the same. The multi-task lasso imposes that\\nfeatures that are selected at one time point are select for all time\\npoint. This makes feature selection by the Lasso more stable.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate data\\n# -------------\\n\\nimport numpy as np\\n\\nrng = np.random.RandomState(42)\\n\\n# Generate some 2D coefficients with sine waves with random frequency and phase\\nn_samples, n_features, n_tasks = 100, 30, 40\\nn_relevant_features = 5\\ncoef = np.zeros((n_tasks, n_features))\\ntimes = np.linspace(0, 2 * np.pi, n_tasks)\\nfor k in range(n_relevant_features):\\n    coef[:, k] = np.sin((1.0 + rng.randn(1)) * times + 3 * rng.randn(1))\\n\\nX = rng.randn(n_samples, n_features)\\nY = np.dot(X, coef.T) + rng.randn(n_samples, n_tasks)\\n\\n# %%\\n# Fit models\\n# ----------\\n\\nfrom sklearn.linear_model import Lasso, MultiTaskLasso\\n\\ncoef_lasso_ = np.array([Lasso(alpha=0.5).fit(X, y).coef_ for y in Y.T])\\ncoef_multi_task_lasso_ = MultiTaskLasso(alpha=1.0).fit(X, Y).coef_\\n\\n# %%\\n# Plot support and time series\\n# ----------------------------\\n\\nimport matplotlib.pyplot as plt\\n\\nfig = plt.figure(figsize=(8, 5))\\nplt.subplot(1, 2, 1)\\nplt.spy(coef_lasso_)\\nplt.xlabel(\"Feature\")\\nplt.ylabel(\"Time (or Task)\")\\nplt.text(10, 5, \"Lasso\")\\nplt.subplot(1, 2, 2)\\nplt.spy(coef_multi_task_lasso_)\\nplt.xlabel(\"Feature\")\\nplt.ylabel(\"Time (or Task)\")\\nplt.text(10, 5, \"MultiTaskLasso\")\\nfig.suptitle(\"Coefficient non-zero location\")\\n\\nfeature_to_plot = 0\\nplt.figure()\\nlw = 2\\nplt.plot(coef[:, feature_to_plot], color=\"seagreen\", linewidth=lw, label=\"Ground truth\")\\nplt.plot(\\n    coef_lasso_[:, feature_to_plot], color=\"cornflowerblue\", linewidth=lw, label=\"Lasso\"\\n)\\nplt.plot(\\n    coef_multi_task_lasso_[:, feature_to_plot],\\n    color=\"gold\",\\n    linewidth=lw,\\n    label=\"MultiTaskLasso\",\\n)\\nplt.legend(loc=\"upper center\")\\nplt.axis(\"tight\")\\nplt.ylim([-1.1, 1.1])\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_lars.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================\\nLasso path using LARS\\n=====================\\n\\nComputes Lasso Path along the regularization parameter using the LARS\\nalgorithm on the diabetes dataset. Each color represents a different\\nfeature of the coefficient vector, and this is displayed as a function\\nof the regularization parameter.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets, linear_model\\n\\nX, y = datasets.load_diabetes(return_X_y=True)\\n\\nprint(\"Computing regularization path using the LARS ...\")\\n_, _, coefs = linear_model.lars_path(X, y, method=\"lasso\", verbose=True)\\n\\nxx = np.sum(np.abs(coefs.T), axis=1)\\nxx /= xx[-1]\\n\\nplt.plot(xx, coefs.T)\\nymin, ymax = plt.ylim()\\nplt.vlines(xx, ymin, ymax, linestyle=\"dashed\")\\nplt.xlabel(\"|coef| / max|coef|\")\\nplt.ylabel(\"Coefficients\")\\nplt.title(\"LASSO Path\")\\nplt.axis(\"tight\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_weighted_samples.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================\\nSGD: Weighted samples\\n=====================\\n\\nPlot decision function of a weighted dataset, where the size of points\\nis proportional to its weight.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import linear_model\\n\\n# we create 20 points\\nnp.random.seed(0)\\nX = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\\ny = [1] * 10 + [-1] * 10\\nsample_weight = 100 * np.abs(np.random.randn(20))\\n# and assign a bigger weight to the last 10 samples\\nsample_weight[:10] *= 10\\n\\n# plot the weighted data points\\nxx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\\nfig, ax = plt.subplots()\\nax.scatter(\\n    X[:, 0],\\n    X[:, 1],\\n    c=y,\\n    s=sample_weight,\\n    alpha=0.9,\\n    cmap=plt.cm.bone,\\n    edgecolor=\"black\",\\n)\\n\\n# fit the unweighted model\\nclf = linear_model.SGDClassifier(alpha=0.01, max_iter=100)\\nclf.fit(X, y)\\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\\nZ = Z.reshape(xx.shape)\\nno_weights = ax.contour(xx, yy, Z, levels=[0], linestyles=[\"solid\"])\\n\\n# fit the weighted model\\nclf = linear_model.SGDClassifier(alpha=0.01, max_iter=100)\\nclf.fit(X, y, sample_weight=sample_weight)\\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\\nZ = Z.reshape(xx.shape)\\nsamples_weights = ax.contour(xx, yy, Z, levels=[0], linestyles=[\"dashed\"])\\n\\nno_weights_handles, _ = no_weights.legend_elements()\\nweights_handles, _ = samples_weights.legend_elements()\\nax.legend(\\n    [no_weights_handles[0], weights_handles[0]],\\n    [\"no weights\", \"with weights\"],\\n    loc=\"lower left\",\\n)\\n\\nax.set(xticks=(), yticks=())\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================\\nComparing various online solvers\\n==================================\\nAn example showing how different online solvers perform\\non the hand-written digits dataset.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.linear_model import (\\n    LogisticRegression,\\n    PassiveAggressiveClassifier,\\n    Perceptron,\\n    SGDClassifier,\\n)\\nfrom sklearn.model_selection import train_test_split\\n\\nheldout = [0.95, 0.90, 0.75, 0.50, 0.01]\\n# Number of rounds to fit and evaluate an estimator.\\nrounds = 10\\nX, y = datasets.load_digits(return_X_y=True)\\n\\nclassifiers = [\\n    (\"SGD\", SGDClassifier(max_iter=110)),\\n    (\"ASGD\", SGDClassifier(max_iter=110, average=True)),\\n    (\"Perceptron\", Perceptron(max_iter=110)),\\n    (\\n        \"Passive-Aggressive I\",\\n        PassiveAggressiveClassifier(max_iter=110, loss=\"hinge\", C=1.0, tol=1e-4),\\n    ),\\n    (\\n        \"Passive-Aggressive II\",\\n        PassiveAggressiveClassifier(\\n            max_iter=110, loss=\"squared_hinge\", C=1.0, tol=1e-4\\n        ),\\n    ),\\n    (\\n        \"SAG\",\\n        LogisticRegression(max_iter=110, solver=\"sag\", tol=1e-1, C=1.0e4 / X.shape[0]),\\n    ),\\n]\\n\\nxx = 1.0 - np.array(heldout)\\n\\nfor name, clf in classifiers:\\n    print(\"training %s\" % name)\\n    rng = np.random.RandomState(42)\\n    yy = []\\n    for i in heldout:\\n        yy_ = []\\n        for r in range(rounds):\\n            X_train, X_test, y_train, y_test = train_test_split(\\n                X, y, test_size=i, random_state=rng\\n            )\\n            clf.fit(X_train, y_train)\\n            y_pred = clf.predict(X_test)\\n            yy_.append(1 - np.mean(y_pred == y_test))\\n        yy.append(np.mean(yy_))\\n    plt.plot(xx, yy, label=name)\\n\\nplt.legend(loc=\"upper right\")\\nplt.xlabel(\"Proportion train\")\\nplt.ylabel(\"Test Error Rate\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_quantile_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================\\nQuantile regression\\n===================\\n\\nThis example illustrates how quantile regression can predict non-trivial\\nconditional quantiles.\\n\\nThe left figure shows the case when the error distribution is normal,\\nbut has non-constant variance, i.e. with heteroscedasticity.\\n\\nThe right figure shows an example of an asymmetric error distribution,\\nnamely the Pareto distribution.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Dataset generation\\n# ------------------\\n#\\n# To illustrate the behaviour of quantile regression, we will generate two\\n# synthetic datasets. The true generative random processes for both datasets\\n# will be composed by the same expected value with a linear relationship with a\\n# single feature `x`.\\nimport numpy as np\\n\\nrng = np.random.RandomState(42)\\nx = np.linspace(start=0, stop=10, num=100)\\nX = x[:, np.newaxis]\\ny_true_mean = 10 + 0.5 * x\\n\\n# %%\\n# We will create two subsequent problems by changing the distribution of the\\n# target `y` while keeping the same expected value:\\n#\\n# - in the first case, a heteroscedastic Normal noise is added;\\n# - in the second case, an asymmetric Pareto noise is added.\\ny_normal = y_true_mean + rng.normal(loc=0, scale=0.5 + 0.5 * x, size=x.shape[0])\\na = 5\\ny_pareto = y_true_mean + 10 * (rng.pareto(a, size=x.shape[0]) - 1 / (a - 1))\\n\\n# %%\\n# Let\\'s first visualize the datasets as well as the distribution of the\\n# residuals `y - mean(y)`.\\nimport matplotlib.pyplot as plt\\n\\n_, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 11), sharex=\"row\", sharey=\"row\")\\n\\naxs[0, 0].plot(x, y_true_mean, label=\"True mean\")\\naxs[0, 0].scatter(x, y_normal, color=\"black\", alpha=0.5, label=\"Observations\")\\naxs[1, 0].hist(y_true_mean - y_normal, edgecolor=\"black\")\\n\\n\\naxs[0, 1].plot(x, y_true_mean, label=\"True mean\")\\naxs[0, 1].scatter(x, y_pareto, color=\"black\", alpha=0.5, label=\"Observations\")\\naxs[1, 1].hist(y_true_mean - y_pareto, edgecolor=\"black\")\\n\\naxs[0, 0].set_title(\"Dataset with heteroscedastic Normal distributed targets\")\\naxs[0, 1].set_title(\"Dataset with asymmetric Pareto distributed target\")\\naxs[1, 0].set_title(\\n    \"Residuals distribution for heteroscedastic Normal distributed targets\"\\n)\\naxs[1, 1].set_title(\"Residuals distribution for asymmetric Pareto distributed target\")\\naxs[0, 0].legend()\\naxs[0, 1].legend()\\naxs[0, 0].set_ylabel(\"y\")\\naxs[1, 0].set_ylabel(\"Counts\")\\naxs[0, 1].set_xlabel(\"x\")\\naxs[0, 0].set_xlabel(\"x\")\\naxs[1, 0].set_xlabel(\"Residuals\")\\n_ = axs[1, 1].set_xlabel(\"Residuals\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_quantile_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# With the heteroscedastic Normal distributed target, we observe that the\\n# variance of the noise is increasing when the value of the feature `x` is\\n# increasing.\\n#\\n# With the asymmetric Pareto distributed target, we observe that the positive\\n# residuals are bounded.\\n#\\n# These types of noisy targets make the estimation via\\n# :class:`~sklearn.linear_model.LinearRegression` less efficient, i.e. we need\\n# more data to get stable results and, in addition, large outliers can have a\\n# huge impact on the fitted coefficients. (Stated otherwise: in a setting with\\n# constant variance, ordinary least squares estimators converge much faster to\\n# the *true* coefficients with increasing sample size.)\\n#\\n# In this asymmetric setting, the median or different quantiles give additional\\n# insights. On top of that, median estimation is much more robust to outliers\\n# and heavy tailed distributions. But note that extreme quantiles are estimated\\n# by very few data points. 95% quantile are more or less estimated by the 5%\\n# largest values and thus also a bit sensitive outliers.\\n#\\n# In the remainder of this tutorial, we will show how\\n# :class:`~sklearn.linear_model.QuantileRegressor` can be used in practice and\\n# give the intuition into the properties of the fitted models. Finally,\\n# we will compare the both :class:`~sklearn.linear_model.QuantileRegressor`\\n# and :class:`~sklearn.linear_model.LinearRegression`.\\n#\\n# Fitting a `QuantileRegressor`\\n# -----------------------------\\n#\\n# In this section, we want to estimate the conditional median as well as\\n# a low and high quantile fixed at 5% and 95%, respectively. Thus, we will get\\n# three linear models, one for each quantile.\\n#\\n# We will use the quantiles at 5% and 95% to find the outliers in the training\\n# sample beyond the central 90% interval.\\n\\n# %%\\nfrom sklearn.linear_model import QuantileRegressor\\n\\nquantiles = [0.05, 0.5, 0.95]\\npredictions = {}\\nout_bounds_predictions = np.zeros_like(y_true_mean, dtype=np.bool_)\\nfor quantile in quantiles:\\n    qr = QuantileRegressor(quantile=quantile, alpha=0)\\n    y_pred = qr.fit(X, y_normal).predict(X)\\n    predictions[quantile] = y_pred\\n\\n    if quantile == min(quantiles):\\n        out_bounds_predictions = np.logical_or(\\n            out_bounds_predictions, y_pred >= y_normal\\n        )\\n    elif quantile == max(quantiles):\\n        out_bounds_predictions = np.logical_or(\\n            out_bounds_predictions, y_pred <= y_normal\\n        )\\n\\n# %%\\n# Now, we can plot the three linear models and the distinguished samples that\\n# are within the central 90% interval from samples that are outside this\\n# interval.\\nplt.plot(X, y_true_mean, color=\"black\", linestyle=\"dashed\", label=\"True mean\")\\n\\nfor quantile, y_pred in predictions.items():\\n    plt.plot(X, y_pred, label=f\"Quantile: {quantile}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_quantile_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for quantile, y_pred in predictions.items():\\n    plt.plot(X, y_pred, label=f\"Quantile: {quantile}\")\\n\\nplt.scatter(\\n    x[out_bounds_predictions],\\n    y_normal[out_bounds_predictions],\\n    color=\"black\",\\n    marker=\"+\",\\n    alpha=0.5,\\n    label=\"Outside interval\",\\n)\\nplt.scatter(\\n    x[~out_bounds_predictions],\\n    y_normal[~out_bounds_predictions],\\n    color=\"black\",\\n    alpha=0.5,\\n    label=\"Inside interval\",\\n)\\n\\nplt.legend()\\nplt.xlabel(\"x\")\\nplt.ylabel(\"y\")\\n_ = plt.title(\"Quantiles of heteroscedastic Normal distributed target\")\\n\\n# %%\\n# Since the noise is still Normally distributed, in particular is symmetric,\\n# the true conditional mean and the true conditional median coincide. Indeed,\\n# we see that the estimated median almost hits the true mean. We observe the\\n# effect of having an increasing noise variance on the 5% and 95% quantiles:\\n# the slopes of those quantiles are very different and the interval between\\n# them becomes wider with increasing `x`.\\n#\\n# To get an additional intuition regarding the meaning of the 5% and 95%\\n# quantiles estimators, one can count the number of samples above and below the\\n# predicted quantiles (represented by a cross on the above plot), considering\\n# that we have a total of 100 samples.\\n#\\n# We can repeat the same experiment using the asymmetric Pareto distributed\\n# target.\\nquantiles = [0.05, 0.5, 0.95]\\npredictions = {}\\nout_bounds_predictions = np.zeros_like(y_true_mean, dtype=np.bool_)\\nfor quantile in quantiles:\\n    qr = QuantileRegressor(quantile=quantile, alpha=0)\\n    y_pred = qr.fit(X, y_pareto).predict(X)\\n    predictions[quantile] = y_pred\\n\\n    if quantile == min(quantiles):\\n        out_bounds_predictions = np.logical_or(\\n            out_bounds_predictions, y_pred >= y_pareto\\n        )\\n    elif quantile == max(quantiles):\\n        out_bounds_predictions = np.logical_or(\\n            out_bounds_predictions, y_pred <= y_pareto\\n        )\\n\\n# %%\\nplt.plot(X, y_true_mean, color=\"black\", linestyle=\"dashed\", label=\"True mean\")\\n\\nfor quantile, y_pred in predictions.items():\\n    plt.plot(X, y_pred, label=f\"Quantile: {quantile}\")\\n\\nplt.scatter(\\n    x[out_bounds_predictions],\\n    y_pareto[out_bounds_predictions],\\n    color=\"black\",\\n    marker=\"+\",\\n    alpha=0.5,\\n    label=\"Outside interval\",\\n)\\nplt.scatter(\\n    x[~out_bounds_predictions],\\n    y_pareto[~out_bounds_predictions],\\n    color=\"black\",\\n    alpha=0.5,\\n    label=\"Inside interval\",\\n)\\n\\nplt.legend()\\nplt.xlabel(\"x\")\\nplt.ylabel(\"y\")\\n_ = plt.title(\"Quantiles of asymmetric Pareto distributed target\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_quantile_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.legend()\\nplt.xlabel(\"x\")\\nplt.ylabel(\"y\")\\n_ = plt.title(\"Quantiles of asymmetric Pareto distributed target\")\\n\\n\\n# %%\\n# Due to the asymmetry of the distribution of the noise, we observe that the\\n# true mean and estimated conditional median are different. We also observe\\n# that each quantile model has different parameters to better fit the desired\\n# quantile. Note that ideally, all quantiles would be parallel in this case,\\n# which would become more visible with more data points or less extreme\\n# quantiles, e.g. 10% and 90%.\\n#\\n# Comparing `QuantileRegressor` and `LinearRegression`\\n# ----------------------------------------------------\\n#\\n# In this section, we will linger on the difference regarding the error that\\n# :class:`~sklearn.linear_model.QuantileRegressor` and\\n# :class:`~sklearn.linear_model.LinearRegression` are minimizing.\\n#\\n# Indeed, :class:`~sklearn.linear_model.LinearRegression` is a least squares\\n# approach minimizing the mean squared error (MSE) between the training and\\n# predicted targets. In contrast,\\n# :class:`~sklearn.linear_model.QuantileRegressor` with `quantile=0.5`\\n# minimizes the mean absolute error (MAE) instead.\\n#\\n# Let\\'s first compute the training errors of such models in terms of mean\\n# squared error and mean absolute error. We will use the asymmetric Pareto\\n# distributed target to make it more interesting as mean and median are not\\n# equal.\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\\n\\nlinear_regression = LinearRegression()\\nquantile_regression = QuantileRegressor(quantile=0.5, alpha=0)\\n\\ny_pred_lr = linear_regression.fit(X, y_pareto).predict(X)\\ny_pred_qr = quantile_regression.fit(X, y_pareto).predict(X)\\n\\nprint(\\n    f\"\"\"Training error (in-sample performance)\\n    {linear_regression.__class__.__name__}:\\n    MAE = {mean_absolute_error(y_pareto, y_pred_lr):.3f}\\n    MSE = {mean_squared_error(y_pareto, y_pred_lr):.3f}\\n    {quantile_regression.__class__.__name__}:\\n    MAE = {mean_absolute_error(y_pareto, y_pred_qr):.3f}\\n    MSE = {mean_squared_error(y_pareto, y_pred_qr):.3f}\\n    \"\"\"\\n)\\n\\n# %%\\n# On the training set, we see that MAE is lower for\\n# :class:`~sklearn.linear_model.QuantileRegressor` than\\n# :class:`~sklearn.linear_model.LinearRegression`. In contrast to that, MSE is\\n# lower for :class:`~sklearn.linear_model.LinearRegression` than\\n# :class:`~sklearn.linear_model.QuantileRegressor`. These results confirms that\\n# MAE is the loss minimized by :class:`~sklearn.linear_model.QuantileRegressor`\\n# while MSE is the loss minimized\\n# :class:`~sklearn.linear_model.LinearRegression`.\\n#\\n# We can make a similar evaluation by looking at the test error obtained by\\n# cross-validation.\\nfrom sklearn.model_selection import cross_validate'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_quantile_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='cv_results_lr = cross_validate(\\n    linear_regression,\\n    X,\\n    y_pareto,\\n    cv=3,\\n    scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"],\\n)\\ncv_results_qr = cross_validate(\\n    quantile_regression,\\n    X,\\n    y_pareto,\\n    cv=3,\\n    scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"],\\n)\\nprint(\\n    f\"\"\"Test error (cross-validated performance)\\n    {linear_regression.__class__.__name__}:\\n    MAE = {-cv_results_lr[\"test_neg_mean_absolute_error\"].mean():.3f}\\n    MSE = {-cv_results_lr[\"test_neg_mean_squared_error\"].mean():.3f}\\n    {quantile_regression.__class__.__name__}:\\n    MAE = {-cv_results_qr[\"test_neg_mean_absolute_error\"].mean():.3f}\\n    MSE = {-cv_results_qr[\"test_neg_mean_squared_error\"].mean():.3f}\\n    \"\"\"\\n)\\n\\n# %%\\n# We reach similar conclusions on the out-of-sample evaluation.'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def load_mtpl2(n_samples=None):\\n    \"\"\"Fetch the French Motor Third-Party Liability Claims dataset.\\n\\n    Parameters\\n    ----------\\n    n_samples: int, default=None\\n      number of samples to select (for faster run time). Full dataset has\\n      678013 samples.\\n    \"\"\"\\n    # freMTPL2freq dataset from https://www.openml.org/d/41214\\n    df_freq = fetch_openml(data_id=41214, as_frame=True).data\\n    df_freq[\"IDpol\"] = df_freq[\"IDpol\"].astype(int)\\n    df_freq.set_index(\"IDpol\", inplace=True)\\n\\n    # freMTPL2sev dataset from https://www.openml.org/d/41215\\n    df_sev = fetch_openml(data_id=41215, as_frame=True).data\\n\\n    # sum ClaimAmount over identical IDs\\n    df_sev = df_sev.groupby(\"IDpol\").sum()\\n\\n    df = df_freq.join(df_sev, how=\"left\")\\n    df[\"ClaimAmount\"] = df[\"ClaimAmount\"].fillna(0)\\n\\n    # unquote string fields\\n    for column_name in df.columns[df.dtypes.values == object]:\\n        df[column_name] = df[column_name].str.strip(\"\\'\")\\n    return df.iloc[:n_samples]'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_obs_pred(\\n    df,\\n    feature,\\n    weight,\\n    observed,\\n    predicted,\\n    y_label=None,\\n    title=None,\\n    ax=None,\\n    fill_legend=False,\\n):\\n    \"\"\"Plot observed and predicted - aggregated per feature level.\\n\\n    Parameters\\n    ----------\\n    df : DataFrame\\n        input data\\n    feature: str\\n        a column name of df for the feature to be plotted\\n    weight : str\\n        column name of df with the values of weights or exposure\\n    observed : str\\n        a column name of df with the observed target\\n    predicted : DataFrame\\n        a dataframe, with the same index as df, with the predicted target\\n    fill_legend : bool, default=False\\n        whether to show fill_between legend\\n    \"\"\"\\n    # aggregate observed and predicted variables by feature level\\n    df_ = df.loc[:, [feature, weight]].copy()\\n    df_[\"observed\"] = df[observed] * df[weight]\\n    df_[\"predicted\"] = predicted * df[weight]\\n    df_ = (\\n        df_.groupby([feature])[[weight, \"observed\", \"predicted\"]]\\n        .sum()\\n        .assign(observed=lambda x: x[\"observed\"] / x[weight])\\n        .assign(predicted=lambda x: x[\"predicted\"] / x[weight])\\n    )\\n\\n    ax = df_.loc[:, [\"observed\", \"predicted\"]].plot(style=\".\", ax=ax)\\n    y_max = df_.loc[:, [\"observed\", \"predicted\"]].values.max() * 0.8\\n    p2 = ax.fill_between(\\n        df_.index,\\n        0,\\n        y_max * df_[weight] / df_[weight].values.max(),\\n        color=\"g\",\\n        alpha=0.1,\\n    )\\n    if fill_legend:\\n        ax.legend([p2], [\"{} distribution\".format(feature)])\\n    ax.set(\\n        ylabel=y_label if y_label is not None else None,\\n        title=title if title is not None else \"Train: Observed vs Predicted\",\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def score_estimator(\\n    estimator,\\n    X_train,\\n    X_test,\\n    df_train,\\n    df_test,\\n    target,\\n    weights,\\n    tweedie_powers=None,\\n):\\n    \"\"\"Evaluate an estimator on train and test sets with different metrics\"\"\"\\n\\n    metrics = [\\n        (\"D¬≤ explained\", None),  # Use default scorer if it exists\\n        (\"mean abs. error\", mean_absolute_error),\\n        (\"mean squared error\", mean_squared_error),\\n    ]\\n    if tweedie_powers:\\n        metrics += [\\n            (\\n                \"mean Tweedie dev p={:.4f}\".format(power),\\n                partial(mean_tweedie_deviance, power=power),\\n            )\\n            for power in tweedie_powers\\n        ]\\n\\n    res = []\\n    for subset_label, X, df in [\\n        (\"train\", X_train, df_train),\\n        (\"test\", X_test, df_test),\\n    ]:\\n        y, _weights = df[target], df[weights]\\n        for score_label, metric in metrics:\\n            if isinstance(estimator, tuple) and len(estimator) == 2:\\n                # Score the model consisting of the product of frequency and\\n                # severity models.\\n                est_freq, est_sev = estimator\\n                y_pred = est_freq.predict(X) * est_sev.predict(X)\\n            else:\\n                y_pred = estimator.predict(X)\\n\\n            if metric is None:\\n                if not hasattr(estimator, \"score\"):\\n                    continue\\n                score = estimator.score(X, y, sample_weight=_weights)\\n            else:\\n                score = metric(y, y_pred, sample_weight=_weights)\\n\\n            res.append({\"subset\": subset_label, \"metric\": score_label, \"score\": score})\\n\\n    res = (\\n        pd.DataFrame(res)\\n        .set_index([\"metric\", \"subset\"])\\n        .score.unstack(-1)\\n        .round(4)\\n        .loc[:, [\"train\", \"test\"]]\\n    )\\n    return res'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def lorenz_curve(y_true, y_pred, exposure):\\n    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\\n    exposure = np.asarray(exposure)\\n\\n    # order samples by increasing predicted risk:\\n    ranking = np.argsort(y_pred)\\n    ranked_exposure = exposure[ranking]\\n    ranked_pure_premium = y_true[ranking]\\n    cumulated_claim_amount = np.cumsum(ranked_pure_premium * ranked_exposure)\\n    cumulated_claim_amount /= cumulated_claim_amount[-1]\\n    cumulated_samples = np.linspace(0, 1, len(cumulated_claim_amount))\\n    return cumulated_samples, cumulated_claim_amount'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\"\"\"\\n======================================\\nTweedie regression on insurance claims\\n======================================\\n\\nThis example illustrates the use of Poisson, Gamma and Tweedie regression on\\nthe `French Motor Third-Party Liability Claims dataset\\n<https://www.openml.org/d/41214>`_, and is inspired by an R tutorial [1]_.\\n\\nIn this dataset, each sample corresponds to an insurance policy, i.e. a\\ncontract within an insurance company and an individual (policyholder).\\nAvailable features include driver age, vehicle age, vehicle power, etc.\\n\\nA few definitions: a *claim* is the request made by a policyholder to the\\ninsurer to compensate for a loss covered by the insurance. The *claim amount*\\nis the amount of money that the insurer must pay. The *exposure* is the\\nduration of the insurance coverage of a given policy, in years.\\n\\nHere our goal is to predict the expected\\nvalue, i.e. the mean, of the total claim amount per exposure unit also\\nreferred to as the pure premium.\\n\\nThere are several possibilities to do that, two of which are:\\n\\n1. Model the number of claims with a Poisson distribution, and the average\\n   claim amount per claim, also known as severity, as a Gamma distribution\\n   and multiply the predictions of both in order to get the total claim\\n   amount.\\n2. Model the total claim amount per exposure directly, typically with a Tweedie\\n   distribution of Tweedie power :math:`p \\\\\\\\in (1, 2)`.\\n\\nIn this example we will illustrate both approaches. We start by defining a few\\nhelper functions for loading the data and visualizing results.\\n\\n.. [1]  A. Noll, R. Salzmann and M.V. Wuthrich, Case Study: French Motor\\n    Third-Party Liability Claims (November 8, 2018). `doi:10.2139/ssrn.3164764\\n    <https://doi.org/10.2139/ssrn.3164764>`_\\n\"\"\"\\n\\n# %%\\n\\nfrom functools import partial\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.metrics import (\\n    mean_absolute_error,\\n    mean_squared_error,\\n    mean_tweedie_deviance,\\n)\\n\\n\\n# Code for: def load_mtpl2(n_samples=None):\\n\\n\\n# Code for: def plot_obs_pred(\\n\\n\\n# Code for: def score_estimator(\\n\\n\\n# %%\\n# Loading datasets, basic feature extraction and target definitions\\n# -----------------------------------------------------------------\\n#\\n# We construct the freMTPL2 dataset by joining the freMTPL2freq table,\\n# containing the number of claims (``ClaimNb``), with the freMTPL2sev table,\\n# containing the claim amount (``ClaimAmount``) for the same policy ids\\n# (``IDpol``).\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import (\\n    FunctionTransformer,\\n    KBinsDiscretizer,\\n    OneHotEncoder,\\n    StandardScaler,\\n)\\n\\ndf = load_mtpl2()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='df = load_mtpl2()\\n\\n\\n# Correct for unreasonable observations (that might be data error)\\n# and a few exceptionally large claim amounts\\ndf[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\\ndf[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\\ndf[\"ClaimAmount\"] = df[\"ClaimAmount\"].clip(upper=200000)\\n# If the claim amount is 0, then we do not count it as a claim. The loss function\\n# used by the severity model needs strictly positive claim amounts. This way\\n# frequency and severity are more consistent with each other.\\ndf.loc[(df[\"ClaimAmount\"] == 0) & (df[\"ClaimNb\"] >= 1), \"ClaimNb\"] = 0\\n\\nlog_scale_transformer = make_pipeline(\\n    FunctionTransformer(func=np.log), StandardScaler()\\n)\\n\\ncolumn_trans = ColumnTransformer(\\n    [\\n        (\\n            \"binned_numeric\",\\n            KBinsDiscretizer(n_bins=10, random_state=0),\\n            [\"VehAge\", \"DrivAge\"],\\n        ),\\n        (\\n            \"onehot_categorical\",\\n            OneHotEncoder(),\\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"],\\n        ),\\n        (\"passthrough_numeric\", \"passthrough\", [\"BonusMalus\"]),\\n        (\"log_scaled_numeric\", log_scale_transformer, [\"Density\"]),\\n    ],\\n    remainder=\"drop\",\\n)\\nX = column_trans.fit_transform(df)\\n\\n# Insurances companies are interested in modeling the Pure Premium, that is\\n# the expected total claim amount per unit of exposure for each policyholder\\n# in their portfolio:\\ndf[\"PurePremium\"] = df[\"ClaimAmount\"] / df[\"Exposure\"]\\n\\n# This can be indirectly approximated by a 2-step modeling: the product of the\\n# Frequency times the average claim amount per claim:\\ndf[\"Frequency\"] = df[\"ClaimNb\"] / df[\"Exposure\"]\\ndf[\"AvgClaimAmount\"] = df[\"ClaimAmount\"] / np.fmax(df[\"ClaimNb\"], 1)\\n\\nwith pd.option_context(\"display.max_columns\", 15):\\n    print(df[df.ClaimAmount > 0].head())\\n\\n# %%\\n#\\n# Frequency model -- Poisson distribution\\n# ---------------------------------------\\n#\\n# The number of claims (``ClaimNb``) is a positive integer (0 included).\\n# Thus, this target can be modelled by a Poisson distribution.\\n# It is then assumed to be the number of discrete events occurring with a\\n# constant rate in a given time interval (``Exposure``, in units of years).\\n# Here we model the frequency ``y = ClaimNb / Exposure``, which is still a\\n# (scaled) Poisson distribution, and use ``Exposure`` as `sample_weight`.\\nfrom sklearn.linear_model import PoissonRegressor\\nfrom sklearn.model_selection import train_test_split\\n\\ndf_train, df_test, X_train, X_test = train_test_split(df, X, random_state=0)\\n\\n# %%\\n#\\n# Let us keep in mind that despite the seemingly large number of data points in\\n# this dataset, the number of evaluation points where the claim amount is\\n# non-zero is quite small:\\nlen(df_test)\\n\\n# %%\\nlen(df_test[df_test[\"ClaimAmount\"] > 0])'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n#\\n# Let us keep in mind that despite the seemingly large number of data points in\\n# this dataset, the number of evaluation points where the claim amount is\\n# non-zero is quite small:\\nlen(df_test)\\n\\n# %%\\nlen(df_test[df_test[\"ClaimAmount\"] > 0])\\n\\n# %%\\n#\\n# As a consequence, we expect a significant variability in our\\n# evaluation upon random resampling of the train test split.\\n#\\n# The parameters of the model are estimated by minimizing the Poisson deviance\\n# on the training set via a Newton solver. Some of the features are collinear\\n# (e.g. because we did not drop any categorical level in the `OneHotEncoder`),\\n# we use a weak L2 penalization to avoid numerical issues.\\nglm_freq = PoissonRegressor(alpha=1e-4, solver=\"newton-cholesky\")\\nglm_freq.fit(X_train, df_train[\"Frequency\"], sample_weight=df_train[\"Exposure\"])\\n\\nscores = score_estimator(\\n    glm_freq,\\n    X_train,\\n    X_test,\\n    df_train,\\n    df_test,\\n    target=\"Frequency\",\\n    weights=\"Exposure\",\\n)\\nprint(\"Evaluation of PoissonRegressor on target Frequency\")\\nprint(scores)\\n\\n# %%\\n#\\n# Note that the score measured on the test set is surprisingly better than on\\n# the training set. This might be specific to this random train-test split.\\n# Proper cross-validation could help us to assess the sampling variability of\\n# these results.\\n#\\n# We can visually compare observed and predicted values, aggregated by the\\n# drivers age (``DrivAge``), vehicle age (``VehAge``) and the insurance\\n# bonus/malus (``BonusMalus``).\\n\\nfig, ax = plt.subplots(ncols=2, nrows=2, figsize=(16, 8))\\nfig.subplots_adjust(hspace=0.3, wspace=0.2)\\n\\nplot_obs_pred(\\n    df=df_train,\\n    feature=\"DrivAge\",\\n    weight=\"Exposure\",\\n    observed=\"Frequency\",\\n    predicted=glm_freq.predict(X_train),\\n    y_label=\"Claim Frequency\",\\n    title=\"train data\",\\n    ax=ax[0, 0],\\n)\\n\\nplot_obs_pred(\\n    df=df_test,\\n    feature=\"DrivAge\",\\n    weight=\"Exposure\",\\n    observed=\"Frequency\",\\n    predicted=glm_freq.predict(X_test),\\n    y_label=\"Claim Frequency\",\\n    title=\"test data\",\\n    ax=ax[0, 1],\\n    fill_legend=True,\\n)\\n\\nplot_obs_pred(\\n    df=df_test,\\n    feature=\"VehAge\",\\n    weight=\"Exposure\",\\n    observed=\"Frequency\",\\n    predicted=glm_freq.predict(X_test),\\n    y_label=\"Claim Frequency\",\\n    title=\"test data\",\\n    ax=ax[1, 0],\\n    fill_legend=True,\\n)\\n\\nplot_obs_pred(\\n    df=df_test,\\n    feature=\"BonusMalus\",\\n    weight=\"Exposure\",\\n    observed=\"Frequency\",\\n    predicted=glm_freq.predict(X_test),\\n    y_label=\"Claim Frequency\",\\n    title=\"test data\",\\n    ax=ax[1, 1],\\n    fill_legend=True,\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plot_obs_pred(\\n    df=df_test,\\n    feature=\"BonusMalus\",\\n    weight=\"Exposure\",\\n    observed=\"Frequency\",\\n    predicted=glm_freq.predict(X_test),\\n    y_label=\"Claim Frequency\",\\n    title=\"test data\",\\n    ax=ax[1, 1],\\n    fill_legend=True,\\n)\\n\\n\\n# %%\\n# According to the observed data, the frequency of accidents is higher for\\n# drivers younger than 30 years old, and is positively correlated with the\\n# `BonusMalus` variable. Our model is able to mostly correctly model this\\n# behaviour.\\n#\\n# Severity Model -  Gamma distribution\\n# ------------------------------------\\n# The mean claim amount or severity (`AvgClaimAmount`) can be empirically\\n# shown to follow approximately a Gamma distribution. We fit a GLM model for\\n# the severity with the same features as the frequency model.\\n#\\n# Note:\\n#\\n# - We filter out ``ClaimAmount == 0`` as the Gamma distribution has support\\n#   on :math:`(0, \\\\infty)`, not :math:`[0, \\\\infty)`.\\n# - We use ``ClaimNb`` as `sample_weight` to account for policies that contain\\n#   more than one claim.\\nfrom sklearn.linear_model import GammaRegressor\\n\\nmask_train = df_train[\"ClaimAmount\"] > 0\\nmask_test = df_test[\"ClaimAmount\"] > 0\\n\\nglm_sev = GammaRegressor(alpha=10.0, solver=\"newton-cholesky\")\\n\\nglm_sev.fit(\\n    X_train[mask_train.values],\\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\\n)\\n\\nscores = score_estimator(\\n    glm_sev,\\n    X_train[mask_train.values],\\n    X_test[mask_test.values],\\n    df_train[mask_train],\\n    df_test[mask_test],\\n    target=\"AvgClaimAmount\",\\n    weights=\"ClaimNb\",\\n)\\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\\nprint(scores)\\n\\n# %%\\n#\\n# Those values of the metrics are not necessarily easy to interpret. It can be\\n# insightful to compare them with a model that does not use any input\\n# features and always predicts a constant value, i.e. the average claim\\n# amount, in the same setting:\\n\\nfrom sklearn.dummy import DummyRegressor\\n\\ndummy_sev = DummyRegressor(strategy=\"mean\")\\ndummy_sev.fit(\\n    X_train[mask_train.values],\\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\\n)\\n\\nscores = score_estimator(\\n    dummy_sev,\\n    X_train[mask_train.values],\\n    X_test[mask_test.values],\\n    df_train[mask_train],\\n    df_test[mask_test],\\n    target=\"AvgClaimAmount\",\\n    weights=\"ClaimNb\",\\n)\\nprint(\"Evaluation of a mean predictor on target AvgClaimAmount\")\\nprint(scores)\\n\\n# %%\\n#\\n# We conclude that the claim amount is very challenging to predict. Still, the\\n# :class:`~sklearn.linear_model.GammaRegressor` is able to leverage some\\n# information from the input features to slightly improve upon the mean\\n# baseline in terms of D¬≤.\\n#\\n# Note that the resulting model is the average claim amount per claim. As such,\\n# it is conditional on having at least one claim, and cannot be used to predict\\n# the average claim amount per policy. For this, it needs to be combined with\\n# a claims frequency model.'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(\\n    \"Mean AvgClaim Amount per policy:              %.2f \"\\n    % df_train[\"AvgClaimAmount\"].mean()\\n)\\nprint(\\n    \"Mean AvgClaim Amount | NbClaim > 0:           %.2f\"\\n    % df_train[\"AvgClaimAmount\"][df_train[\"AvgClaimAmount\"] > 0].mean()\\n)\\nprint(\\n    \"Predicted Mean AvgClaim Amount | NbClaim > 0: %.2f\"\\n    % glm_sev.predict(X_train).mean()\\n)\\nprint(\\n    \"Predicted Mean AvgClaim Amount (dummy) | NbClaim > 0: %.2f\"\\n    % dummy_sev.predict(X_train).mean()\\n)\\n\\n# %%\\n# We can visually compare observed and predicted values, aggregated for\\n# the drivers age (``DrivAge``).\\n\\nfig, ax = plt.subplots(ncols=1, nrows=2, figsize=(16, 6))\\n\\nplot_obs_pred(\\n    df=df_train.loc[mask_train],\\n    feature=\"DrivAge\",\\n    weight=\"Exposure\",\\n    observed=\"AvgClaimAmount\",\\n    predicted=glm_sev.predict(X_train[mask_train.values]),\\n    y_label=\"Average Claim Severity\",\\n    title=\"train data\",\\n    ax=ax[0],\\n)\\n\\nplot_obs_pred(\\n    df=df_test.loc[mask_test],\\n    feature=\"DrivAge\",\\n    weight=\"Exposure\",\\n    observed=\"AvgClaimAmount\",\\n    predicted=glm_sev.predict(X_test[mask_test.values]),\\n    y_label=\"Average Claim Severity\",\\n    title=\"test data\",\\n    ax=ax[1],\\n    fill_legend=True,\\n)\\nplt.tight_layout()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plot_obs_pred(\\n    df=df_test.loc[mask_test],\\n    feature=\"DrivAge\",\\n    weight=\"Exposure\",\\n    observed=\"AvgClaimAmount\",\\n    predicted=glm_sev.predict(X_test[mask_test.values]),\\n    y_label=\"Average Claim Severity\",\\n    title=\"test data\",\\n    ax=ax[1],\\n    fill_legend=True,\\n)\\nplt.tight_layout()\\n\\n# %%\\n# Overall, the drivers age (``DrivAge``) has a weak impact on the claim\\n# severity, both in observed and predicted data.\\n#\\n# Pure Premium Modeling via a Product Model vs single TweedieRegressor\\n# --------------------------------------------------------------------\\n# As mentioned in the introduction, the total claim amount per unit of\\n# exposure can be modeled as the product of the prediction of the\\n# frequency model by the prediction of the severity model.\\n#\\n# Alternatively, one can directly model the total loss with a unique\\n# Compound Poisson Gamma generalized linear model (with a log link function).\\n# This model is a special case of the Tweedie GLM with a \"power\" parameter\\n# :math:`p \\\\in (1, 2)`. Here, we fix apriori the `power` parameter of the\\n# Tweedie model to some arbitrary value (1.9) in the valid range. Ideally one\\n# would select this value via grid-search by minimizing the negative\\n# log-likelihood of the Tweedie model, but unfortunately the current\\n# implementation does not allow for this (yet).\\n#\\n# We will compare the performance of both approaches.\\n# To quantify the performance of both models, one can compute\\n# the mean deviance of the train and test data assuming a Compound\\n# Poisson-Gamma distribution of the total claim amount. This is equivalent to\\n# a Tweedie distribution with a `power` parameter between 1 and 2.\\n#\\n# The :func:`sklearn.metrics.mean_tweedie_deviance` depends on a `power`\\n# parameter. As we do not know the true value of the `power` parameter, we here\\n# compute the mean deviances for a grid of possible values, and compare the\\n# models side by side, i.e. we compare them at identical values of `power`.\\n# Ideally, we hope that one model will be consistently better than the other,\\n# regardless of `power`.\\nfrom sklearn.linear_model import TweedieRegressor\\n\\nglm_pure_premium = TweedieRegressor(power=1.9, alpha=0.1, solver=\"newton-cholesky\")\\nglm_pure_premium.fit(\\n    X_train, df_train[\"PurePremium\"], sample_weight=df_train[\"Exposure\"]\\n)\\n\\ntweedie_powers = [1.5, 1.7, 1.8, 1.9, 1.99, 1.999, 1.9999]\\n\\nscores_product_model = score_estimator(\\n    (glm_freq, glm_sev),\\n    X_train,\\n    X_test,\\n    df_train,\\n    df_test,\\n    target=\"PurePremium\",\\n    weights=\"Exposure\",\\n    tweedie_powers=tweedie_powers,\\n)\\n\\nscores_glm_pure_premium = score_estimator(\\n    glm_pure_premium,\\n    X_train,\\n    X_test,\\n    df_train,\\n    df_test,\\n    target=\"PurePremium\",\\n    weights=\"Exposure\",\\n    tweedie_powers=tweedie_powers,\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='scores_glm_pure_premium = score_estimator(\\n    glm_pure_premium,\\n    X_train,\\n    X_test,\\n    df_train,\\n    df_test,\\n    target=\"PurePremium\",\\n    weights=\"Exposure\",\\n    tweedie_powers=tweedie_powers,\\n)\\n\\nscores = pd.concat(\\n    [scores_product_model, scores_glm_pure_premium],\\n    axis=1,\\n    sort=True,\\n    keys=(\"Product Model\", \"TweedieRegressor\"),\\n)\\nprint(\"Evaluation of the Product Model and the Tweedie Regressor on target PurePremium\")\\nwith pd.option_context(\"display.expand_frame_repr\", False):\\n    print(scores)\\n\\n# %%\\n# In this example, both modeling approaches yield comparable performance\\n# metrics. For implementation reasons, the percentage of explained variance\\n# :math:`D^2` is not available for the product model.\\n#\\n# We can additionally validate these models by comparing observed and\\n# predicted total claim amount over the test and train subsets. We see that,\\n# on average, both model tend to underestimate the total claim (but this\\n# behavior depends on the amount of regularization).\\n\\nres = []\\nfor subset_label, X, df in [\\n    (\"train\", X_train, df_train),\\n    (\"test\", X_test, df_test),\\n]:\\n    exposure = df[\"Exposure\"].values\\n    res.append(\\n        {\\n            \"subset\": subset_label,\\n            \"observed\": df[\"ClaimAmount\"].values.sum(),\\n            \"predicted, frequency*severity model\": np.sum(\\n                exposure * glm_freq.predict(X) * glm_sev.predict(X)\\n            ),\\n            \"predicted, tweedie, power=%.2f\"\\n            % glm_pure_premium.power: np.sum(exposure * glm_pure_premium.predict(X)),\\n        }\\n    )\\n\\nprint(pd.DataFrame(res).set_index(\"subset\").T)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(pd.DataFrame(res).set_index(\"subset\").T)\\n\\n# %%\\n#\\n# Finally, we can compare the two models using a plot of cumulated claims: for\\n# each model, the policyholders are ranked from safest to riskiest based on the\\n# model predictions and the fraction of observed total cumulated claims is\\n# plotted on the y axis. This plot is often called the ordered Lorenz curve of\\n# the model.\\n#\\n# The Gini coefficient (based on the area between the curve and the diagonal)\\n# can be used as a model selection metric to quantify the ability of the model\\n# to rank policyholders. Note that this metric does not reflect the ability of\\n# the models to make accurate predictions in terms of absolute value of total\\n# claim amounts but only in terms of relative amounts as a ranking metric. The\\n# Gini coefficient is upper bounded by 1.0 but even an oracle model that ranks\\n# the policyholders by the observed claim amounts cannot reach a score of 1.0.\\n#\\n# We observe that both models are able to rank policyholders by risky-ness\\n# significantly better than chance although they are also both far from the\\n# oracle model due to the natural difficulty of the prediction problem from a\\n# few features: most accidents are not predictable and can be caused by\\n# environmental circumstances that are not described at all by the input\\n# features of the models.\\n#\\n# Note that the Gini index only characterizes the ranking performance of the\\n# model but not its calibration: any monotonic transformation of the predictions\\n# leaves the Gini index of the model unchanged.\\n#\\n# Finally one should highlight that the Compound Poisson Gamma model that is\\n# directly fit on the pure premium is operationally simpler to develop and\\n# maintain as it consists of a single scikit-learn estimator instead of a pair\\n# of models, each with its own set of hyperparameters.\\nfrom sklearn.metrics import auc\\n\\n\\n# Code for: def lorenz_curve(y_true, y_pred, exposure):\\n\\n\\nfig, ax = plt.subplots(figsize=(8, 8))\\n\\ny_pred_product = glm_freq.predict(X_test) * glm_sev.predict(X_test)\\ny_pred_total = glm_pure_premium.predict(X_test)\\n\\nfor label, y_pred in [\\n    (\"Frequency * Severity model\", y_pred_product),\\n    (\"Compound Poisson Gamma\", y_pred_total),\\n]:\\n    ordered_samples, cum_claims = lorenz_curve(\\n        df_test[\"PurePremium\"], y_pred, df_test[\"Exposure\"]\\n    )\\n    gini = 1 - 2 * auc(ordered_samples, cum_claims)\\n    label += \" (Gini index: {:.3f})\".format(gini)\\n    ax.plot(ordered_samples, cum_claims, linestyle=\"-\", label=label)\\n\\n# Oracle model: y_pred == y_test\\nordered_samples, cum_claims = lorenz_curve(\\n    df_test[\"PurePremium\"], df_test[\"PurePremium\"], df_test[\"Exposure\"]\\n)\\ngini = 1 - 2 * auc(ordered_samples, cum_claims)\\nlabel = \"Oracle (Gini index: {:.3f})\".format(gini)\\nax.plot(ordered_samples, cum_claims, linestyle=\"-.\", color=\"gray\", label=label)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_tweedie_regression_insurance_claims.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Random baseline\\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Random baseline\")\\nax.set(\\n    title=\"Lorenz Curves\",\\n    xlabel=\"Fraction of policyholders\\\\n(ordered by model from safest to riskiest)\",\\n    ylabel=\"Fraction of total claim amount\",\\n)\\nax.legend(loc=\"upper left\")\\nplt.plot()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================================\\nMulticlass sparse logistic regression on 20newgroups\\n====================================================\\n\\nComparison of multinomial logistic L1 vs one-versus-rest L1 logistic regression\\nto classify documents from the newgroups20 dataset. Multinomial logistic\\nregression yields more accurate results and is faster to train on the larger\\nscale dataset.\\n\\nHere we use the l1 sparsity that trims the weights of not informative\\nfeatures to zero. This is good if the goal is to extract the strongly\\ndiscriminative vocabulary of each class. If the goal is to get the best\\npredictive accuracy, it is better to use the non sparsity-inducing l2 penalty\\ninstead.\\n\\nA more traditional (and possibly better) way to predict on a sparse subset of\\ninput features would be to use univariate feature selection followed by a\\ntraditional (l2-penalised) logistic regression model.\\n\\n\"\"\"\\n\\n# Author: Arthur Mensch\\n\\nimport timeit\\nimport warnings\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_20newsgroups_vectorized\\nfrom sklearn.exceptions import ConvergenceWarning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.multiclass import OneVsRestClassifier\\n\\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\\nt0 = timeit.default_timer()\\n\\n# We use SAGA solver\\nsolver = \"saga\"\\n\\n# Turn down for faster run time\\nn_samples = 5000\\n\\nX, y = fetch_20newsgroups_vectorized(subset=\"all\", return_X_y=True)\\nX = X[:n_samples]\\ny = y[:n_samples]\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, random_state=42, stratify=y, test_size=0.1\\n)\\ntrain_samples, n_features = X_train.shape\\nn_classes = np.unique(y).shape[0]\\n\\nprint(\\n    \"Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i\"\\n    % (train_samples, n_features, n_classes)\\n)\\n\\nmodels = {\\n    \"ovr\": {\"name\": \"One versus Rest\", \"iters\": [1, 2, 3]},\\n    \"multinomial\": {\"name\": \"Multinomial\", \"iters\": [1, 2, 5]},\\n}\\n\\nfor model in models:\\n    # Add initial chance-level values for plotting purpose\\n    accuracies = [1 / n_classes]\\n    times = [0]\\n    densities = [1]\\n\\n    model_params = models[model]\\n\\n    # Small number of epochs for fast runtime\\n    for this_max_iter in model_params[\"iters\"]:\\n        print(\\n            \"[model=%s, solver=%s] Number of epochs: %s\"\\n            % (model_params[\"name\"], solver, this_max_iter)\\n        )\\n        clf = LogisticRegression(\\n            solver=solver,\\n            penalty=\"l1\",\\n            max_iter=this_max_iter,\\n            random_state=42,\\n        )\\n        if model == \"ovr\":\\n            clf = OneVsRestClassifier(clf)\\n        t1 = timeit.default_timer()\\n        clf.fit(X_train, y_train)\\n        train_time = timeit.default_timer() - t1'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='y_pred = clf.predict(X_test)\\n        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\\n        if model == \"ovr\":\\n            coef = np.concatenate([est.coef_ for est in clf.estimators_])\\n        else:\\n            coef = clf.coef_\\n        density = np.mean(coef != 0, axis=1) * 100\\n        accuracies.append(accuracy)\\n        densities.append(density)\\n        times.append(train_time)\\n    models[model][\"times\"] = times\\n    models[model][\"densities\"] = densities\\n    models[model][\"accuracies\"] = accuracies\\n    print(\"Test accuracy for model %s: %.4f\" % (model, accuracies[-1]))\\n    print(\\n        \"%% non-zero coefficients for model %s, per class:\\\\n %s\"\\n        % (model, densities[-1])\\n    )\\n    print(\\n        \"Run time (%i epochs) for model %s:%.2f\"\\n        % (model_params[\"iters\"][-1], model, times[-1])\\n    )\\n\\nfig = plt.figure()\\nax = fig.add_subplot(111)\\n\\nfor model in models:\\n    name = models[model][\"name\"]\\n    times = models[model][\"times\"]\\n    accuracies = models[model][\"accuracies\"]\\n    ax.plot(times, accuracies, marker=\"o\", label=\"Model: %s\" % name)\\n    ax.set_xlabel(\"Train time (s)\")\\n    ax.set_ylabel(\"Test accuracy\")\\nax.legend()\\nfig.suptitle(\"Multinomial vs One-vs-Rest Logistic L1\\\\nDataset %s\" % \"20newsgroups\")\\nfig.tight_layout()\\nfig.subplots_adjust(top=0.85)\\nrun_time = timeit.default_timer() - t0\\nprint(\"Example run in %.3f s\" % run_time)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_polynomial_interpolation.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def f(x):\\n    \"\"\"Function to be approximated by polynomial interpolation.\"\"\"\\n    return x * np.sin(x)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_polynomial_interpolation.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def g(x):\\n    \"\"\"Function to be approximated by periodic spline interpolation.\"\"\"\\n    return np.sin(x) - 0.7 * np.cos(x * 3)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_polynomial_interpolation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================\\nPolynomial and Spline interpolation\\n===================================\\n\\nThis example demonstrates how to approximate a function with polynomials up to\\ndegree ``degree`` by using ridge regression. We show two different ways given\\n``n_samples`` of 1d points ``x_i``:\\n\\n- :class:`~sklearn.preprocessing.PolynomialFeatures` generates all monomials\\n  up to ``degree``. This gives us the so called Vandermonde matrix with\\n  ``n_samples`` rows and ``degree + 1`` columns::\\n\\n    [[1, x_0, x_0 ** 2, x_0 ** 3, ..., x_0 ** degree],\\n     [1, x_1, x_1 ** 2, x_1 ** 3, ..., x_1 ** degree],\\n     ...]\\n\\n  Intuitively, this matrix can be interpreted as a matrix of pseudo features\\n  (the points raised to some power). The matrix is akin to (but different from)\\n  the matrix induced by a polynomial kernel.\\n\\n- :class:`~sklearn.preprocessing.SplineTransformer` generates B-spline basis\\n  functions. A basis function of a B-spline is a piece-wise polynomial function\\n  of degree ``degree`` that is non-zero only between ``degree+1`` consecutive\\n  knots. Given ``n_knots`` number of knots, this results in matrix of\\n  ``n_samples`` rows and ``n_knots + degree - 1`` columns::\\n\\n    [[basis_1(x_0), basis_2(x_0), ...],\\n     [basis_1(x_1), basis_2(x_1), ...],\\n     ...]\\n\\nThis example shows that these two transformers are well suited to model\\nnon-linear effects with a linear model, using a pipeline to add non-linear\\nfeatures. Kernel methods extend this idea and can induce very high (even\\ninfinite) dimensional feature spaces.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import PolynomialFeatures, SplineTransformer\\n\\n# %%\\n# We start by defining a function that we intend to approximate and prepare\\n# plotting it.\\n\\n\\n# Code for: def f(x):\\n\\n\\n# whole range we want to plot\\nx_plot = np.linspace(-1, 11, 100)\\n\\n# %%\\n# To make it interesting, we only give a small subset of points to train on.\\n\\nx_train = np.linspace(0, 10, 100)\\nrng = np.random.RandomState(0)\\nx_train = np.sort(rng.choice(x_train, size=20, replace=False))\\ny_train = f(x_train)\\n\\n# create 2D-array versions of these arrays to feed to transformers\\nX_train = x_train[:, np.newaxis]\\nX_plot = x_plot[:, np.newaxis]\\n\\n# %%\\n# Now we are ready to create polynomial features and splines, fit on the\\n# training points and show how well they interpolate.\\n\\n# plot function\\nlw = 2\\nfig, ax = plt.subplots()\\nax.set_prop_cycle(\\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\\n)\\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\\n\\n# plot training points\\nax.scatter(x_train, y_train, label=\"training points\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_polynomial_interpolation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# plot function\\nlw = 2\\nfig, ax = plt.subplots()\\nax.set_prop_cycle(\\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\\n)\\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\\n\\n# plot training points\\nax.scatter(x_train, y_train, label=\"training points\")\\n\\n# polynomial features\\nfor degree in [3, 4, 5]:\\n    model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=1e-3))\\n    model.fit(X_train, y_train)\\n    y_plot = model.predict(X_plot)\\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\\n\\n# B-spline with 4 + 3 - 1 = 6 basis functions\\nmodel = make_pipeline(SplineTransformer(n_knots=4, degree=3), Ridge(alpha=1e-3))\\nmodel.fit(X_train, y_train)\\n\\ny_plot = model.predict(X_plot)\\nax.plot(x_plot, y_plot, label=\"B-spline\")\\nax.legend(loc=\"lower center\")\\nax.set_ylim(-20, 10)\\nplt.show()\\n\\n# %%\\n# This shows nicely that higher degree polynomials can fit the data better. But\\n# at the same time, too high powers can show unwanted oscillatory behaviour\\n# and are particularly dangerous for extrapolation beyond the range of fitted\\n# data. This is an advantage of B-splines. They usually fit the data as well as\\n# polynomials and show very nice and smooth behaviour. They have also good\\n# options to control the extrapolation, which defaults to continue with a\\n# constant. Note that most often, you would rather increase the number of knots\\n# but keep ``degree=3``.\\n#\\n# In order to give more insights into the generated feature bases, we plot all\\n# columns of both transformers separately.\\n\\nfig, axes = plt.subplots(ncols=2, figsize=(16, 5))\\npft = PolynomialFeatures(degree=3).fit(X_train)\\naxes[0].plot(x_plot, pft.transform(X_plot))\\naxes[0].legend(axes[0].lines, [f\"degree {n}\" for n in range(4)])\\naxes[0].set_title(\"PolynomialFeatures\")\\n\\nsplt = SplineTransformer(n_knots=4, degree=3).fit(X_train)\\naxes[1].plot(x_plot, splt.transform(X_plot))\\naxes[1].legend(axes[1].lines, [f\"spline {n}\" for n in range(6)])\\naxes[1].set_title(\"SplineTransformer\")\\n\\n# plot knots of spline\\nknots = splt.bsplines_[0].t\\naxes[1].vlines(knots[3:-3], ymin=0, ymax=0.8, linestyles=\"dashed\")\\nplt.show()\\n\\n# %%\\n# In the left plot, we recognize the lines corresponding to simple monomials\\n# from ``x**0`` to ``x**3``. In the right figure, we see the six B-spline\\n# basis functions of ``degree=3`` and also the four knot positions that were\\n# chosen during ``fit``. Note that there are ``degree`` number of additional\\n# knots each to the left and to the right of the fitted interval. These are\\n# there for technical reasons, so we refrain from showing them. Every basis\\n# function has local support and is continued as a constant beyond the fitted\\n# range. This extrapolating behaviour could be changed by the argument\\n# ``extrapolation``.'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_polynomial_interpolation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Periodic Splines\\n# ----------------\\n# In the previous example we saw the limitations of polynomials and splines for\\n# extrapolation beyond the range of the training observations. In some\\n# settings, e.g. with seasonal effects, we expect a periodic continuation of\\n# the underlying signal. Such effects can be modelled using periodic splines,\\n# which have equal function value and equal derivatives at the first and last\\n# knot. In the following case we show how periodic splines provide a better fit\\n# both within and outside of the range of training data given the additional\\n# information of periodicity. The splines period is the distance between\\n# the first and last knot, which we specify manually.\\n#\\n# Periodic splines can also be useful for naturally periodic features (such as\\n# day of the year), as the smoothness at the boundary knots prevents a jump in\\n# the transformed values (e.g. from Dec 31st to Jan 1st). For such naturally\\n# periodic features or more generally features where the period is known, it is\\n# advised to explicitly pass this information to the `SplineTransformer` by\\n# setting the knots manually.\\n\\n\\n# %%\\n# Code for: def g(x):\\n\\n\\ny_train = g(x_train)\\n\\n# Extend the test data into the future:\\nx_plot_ext = np.linspace(-1, 21, 200)\\nX_plot_ext = x_plot_ext[:, np.newaxis]\\n\\nlw = 2\\nfig, ax = plt.subplots()\\nax.set_prop_cycle(color=[\"black\", \"tomato\", \"teal\"])\\nax.plot(x_plot_ext, g(x_plot_ext), linewidth=lw, label=\"ground truth\")\\nax.scatter(x_train, y_train, label=\"training points\")\\n\\nfor transformer, label in [\\n    (SplineTransformer(degree=3, n_knots=10), \"spline\"),\\n    (\\n        SplineTransformer(\\n            degree=3,\\n            knots=np.linspace(0, 2 * np.pi, 10)[:, None],\\n            extrapolation=\"periodic\",\\n        ),\\n        \"periodic spline\",\\n    ),\\n]:\\n    model = make_pipeline(transformer, Ridge(alpha=1e-3))\\n    model.fit(X_train, y_train)\\n    y_plot_ext = model.predict(X_plot_ext)\\n    ax.plot(x_plot_ext, y_plot_ext, label=label)\\n\\nax.legend()\\nfig.show()\\n\\n# %% We again plot the underlying splines.\\nfig, ax = plt.subplots()\\nknots = np.linspace(0, 2 * np.pi, 4)\\nsplt = SplineTransformer(knots=knots[:, None], degree=3, extrapolation=\"periodic\").fit(\\n    X_train\\n)\\nax.plot(x_plot_ext, splt.transform(X_plot_ext))\\nax.legend(ax.lines, [f\"spline {n}\" for n in range(3)])\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_early_stopping.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def load_mnist(n_samples=None, class_0=\"0\", class_1=\"8\"):\\n    \"\"\"Load MNIST, select two classes, shuffle and return only n_samples.\"\"\"\\n    # Load data from http://openml.org/d/554\\n    mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\\n\\n    # take only two classes for binary classification\\n    mask = np.logical_or(mnist.target == class_0, mnist.target == class_1)\\n\\n    X, y = shuffle(mnist.data[mask], mnist.target[mask], random_state=42)\\n    if n_samples is not None:\\n        X, y = X[:n_samples], y[:n_samples]\\n    return X, y'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_early_stopping.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def fit_and_score(estimator, max_iter, X_train, X_test, y_train, y_test):\\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\\n    estimator.set_params(max_iter=max_iter)\\n    estimator.set_params(random_state=0)\\n\\n    start = time.time()\\n    estimator.fit(X_train, y_train)\\n\\n    fit_time = time.time() - start\\n    n_iter = estimator.n_iter_\\n    train_score = estimator.score(X_train, y_train)\\n    test_score = estimator.score(X_test, y_test)\\n\\n    return fit_time, n_iter, train_score, test_score'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_early_stopping.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================\\nEarly stopping of Stochastic Gradient Descent\\n=============================================\\n\\nStochastic Gradient Descent is an optimization technique which minimizes a loss\\nfunction in a stochastic fashion, performing a gradient descent step sample by\\nsample. In particular, it is a very efficient method to fit linear models.\\n\\nAs a stochastic method, the loss function is not necessarily decreasing at each\\niteration, and convergence is only guaranteed in expectation. For this reason,\\nmonitoring the convergence on the loss function can be difficult.\\n\\nAnother approach is to monitor convergence on a validation score. In this case,\\nthe input data is split into a training set and a validation set. The model is\\nthen fitted on the training set and the stopping criterion is based on the\\nprediction score computed on the validation set. This enables us to find the\\nleast number of iterations which is sufficient to build a model that\\ngeneralizes well to unseen data and reduces the chance of over-fitting the\\ntraining data.\\n\\nThis early stopping strategy is activated if ``early_stopping=True``; otherwise\\nthe stopping criterion only uses the training loss on the entire input data. To\\nbetter control the early stopping strategy, we can specify a parameter\\n``validation_fraction`` which set the fraction of the input dataset that we\\nkeep aside to compute the validation score. The optimization will continue\\nuntil the validation score did not improve by at least ``tol`` during the last\\n``n_iter_no_change`` iterations. The actual number of iterations is available\\nat the attribute ``n_iter_``.\\n\\nThis example illustrates how the early stopping can used in the\\n:class:`~sklearn.linear_model.SGDClassifier` model to achieve almost the same\\naccuracy as compared to a model built without early stopping. This can\\nsignificantly reduce training time. Note that scores differ between the\\nstopping criteria even from early iterations because some of the training data\\nis held out with the validation stopping criterion.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport sys\\nimport time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom sklearn import linear_model\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.exceptions import ConvergenceWarning\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import shuffle\\nfrom sklearn.utils._testing import ignore_warnings\\n\\n\\n# Code for: def load_mnist(n_samples=None, class_0=\"0\", class_1=\"8\"):\\n\\n\\n@ignore_warnings(category=ConvergenceWarning)\\n# Code for: def fit_and_score(estimator, max_iter, X_train, X_test, y_train, y_test):'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_early_stopping.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def load_mnist(n_samples=None, class_0=\"0\", class_1=\"8\"):\\n\\n\\n@ignore_warnings(category=ConvergenceWarning)\\n# Code for: def fit_and_score(estimator, max_iter, X_train, X_test, y_train, y_test):\\n\\n\\n# Define the estimators to compare\\nestimator_dict = {\\n    \"No stopping criterion\": linear_model.SGDClassifier(n_iter_no_change=3),\\n    \"Training loss\": linear_model.SGDClassifier(\\n        early_stopping=False, n_iter_no_change=3, tol=0.1\\n    ),\\n    \"Validation score\": linear_model.SGDClassifier(\\n        early_stopping=True, n_iter_no_change=3, tol=0.0001, validation_fraction=0.2\\n    ),\\n}\\n\\n# Load the dataset\\nX, y = load_mnist(n_samples=10000)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\\n\\nresults = []\\nfor estimator_name, estimator in estimator_dict.items():\\n    print(estimator_name + \": \", end=\"\")\\n    for max_iter in range(1, 50):\\n        print(\".\", end=\"\")\\n        sys.stdout.flush()\\n\\n        fit_time, n_iter, train_score, test_score = fit_and_score(\\n            estimator, max_iter, X_train, X_test, y_train, y_test\\n        )\\n\\n        results.append(\\n            (estimator_name, max_iter, fit_time, n_iter, train_score, test_score)\\n        )\\n    print(\"\")\\n\\n# Transform the results in a pandas dataframe for easy plotting\\ncolumns = [\\n    \"Stopping criterion\",\\n    \"max_iter\",\\n    \"Fit time (sec)\",\\n    \"n_iter_\",\\n    \"Train score\",\\n    \"Test score\",\\n]\\nresults_df = pd.DataFrame(results, columns=columns)\\n\\n# Define what to plot\\nlines = \"Stopping criterion\"\\nx_axis = \"max_iter\"\\nstyles = [\"-.\", \"--\", \"-\"]\\n\\n# First plot: train and test scores\\nfig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(12, 4))\\nfor ax, y_axis in zip(axes, [\"Train score\", \"Test score\"]):\\n    for style, (criterion, group_df) in zip(styles, results_df.groupby(lines)):\\n        group_df.plot(x=x_axis, y=y_axis, label=criterion, ax=ax, style=style)\\n    ax.set_title(y_axis)\\n    ax.legend(title=lines)\\nfig.tight_layout()\\n\\n# Second plot: n_iter and fit time\\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\\nfor ax, y_axis in zip(axes, [\"n_iter_\", \"Fit time (sec)\"]):\\n    for style, (criterion, group_df) in zip(styles, results_df.groupby(lines)):\\n        group_df.plot(x=x_axis, y=y_axis, label=criterion, ax=ax, style=style)\\n    ax.set_title(y_axis)\\n    ax.legend(title=lines)\\nfig.tight_layout()\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_logistic_multinomial.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================================\\nPlot multinomial and One-vs-Rest Logistic Regression\\n====================================================\\n\\nPlot decision surface of multinomial and One-vs-Rest Logistic Regression.\\nThe hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers\\nare represented by the dashed lines.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.multiclass import OneVsRestClassifier\\n\\n# make 3-class dataset for classification\\ncenters = [[-5, 0], [0, 1.5], [5, -1]]\\nX, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\\ntransformation = [[0.4, 0.2], [-0.4, 1.2]]\\nX = np.dot(X, transformation)\\n\\nfor multi_class in (\"multinomial\", \"ovr\"):\\n    clf = LogisticRegression(solver=\"sag\", max_iter=100, random_state=42)\\n    if multi_class == \"ovr\":\\n        clf = OneVsRestClassifier(clf)\\n    clf.fit(X, y)\\n\\n    # print the training scores\\n    print(\"training score : %.3f (%s)\" % (clf.score(X, y), multi_class))\\n\\n    _, ax = plt.subplots()\\n    DecisionBoundaryDisplay.from_estimator(\\n        clf, X, response_method=\"predict\", cmap=plt.cm.Paired, ax=ax\\n    )\\n    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\\n    plt.axis(\"tight\")\\n\\n    # Plot also the training points\\n    colors = \"bry\"\\n    for i, color in zip(clf.classes_, colors):\\n        idx = np.where(y == i)\\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, edgecolor=\"black\", s=20)\\n\\n    # Plot the three one-against-all classifiers\\n    xmin, xmax = plt.xlim()\\n    ymin, ymax = plt.ylim()\\n    if multi_class == \"ovr\":\\n        coef = np.concatenate([est.coef_ for est in clf.estimators_])\\n        intercept = np.concatenate([est.intercept_ for est in clf.estimators_])\\n    else:\\n        coef = clf.coef_\\n        intercept = clf.intercept_\\n\\n    def plot_hyperplane(c, color):\\n        def line(x0):\\n            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\\n\\n        plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls=\"--\", color=color)\\n\\n    for i, color in zip(clf.classes_, colors):\\n        plot_hyperplane(i, color)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_lars_ic.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def zou_et_al_criterion_rescaling(criterion, n_samples, noise_variance):\\n    \"\"\"Rescale the information criterion to follow the definition of Zou et al.\"\"\"\\n    return criterion - n_samples * np.log(2 * np.pi * noise_variance) - n_samples'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_lars_ic.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================\\nLasso model selection via information criteria\\n==============================================\\n\\nThis example reproduces the example of Fig. 2 of [ZHT2007]_. A\\n:class:`~sklearn.linear_model.LassoLarsIC` estimator is fit on a\\ndiabetes dataset and the AIC and the BIC criteria are used to select\\nthe best model.\\n\\n.. note::\\n    It is important to note that the optimization to find `alpha` with\\n    :class:`~sklearn.linear_model.LassoLarsIC` relies on the AIC or BIC\\n    criteria that are computed in-sample, thus on the training set directly.\\n    This approach differs from the cross-validation procedure. For a comparison\\n    of the two approaches, you can refer to the following example:\\n    :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py`.\\n\\n.. rubric:: References\\n\\n.. [ZHT2007] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\\n    \"On the degrees of freedom of the lasso.\"\\n    The Annals of Statistics 35.5 (2007): 2173-2192.\\n    <0712.0881>`\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# We will use the diabetes dataset.\\nfrom sklearn.datasets import load_diabetes\\n\\nX, y = load_diabetes(return_X_y=True, as_frame=True)\\nn_samples = X.shape[0]\\nX.head()\\n\\n# %%\\n# Scikit-learn provides an estimator called\\n# :class:`~sklearn.linear_model.LassoLarsIC` that uses either Akaike\\'s\\n# information criterion (AIC) or the Bayesian information criterion (BIC) to\\n# select the best model. Before fitting\\n# this model, we will scale the dataset.\\n#\\n# In the following, we are going to fit two models to compare the values\\n# reported by AIC and BIC.\\nfrom sklearn.linear_model import LassoLarsIC\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nlasso_lars_ic = make_pipeline(StandardScaler(), LassoLarsIC(criterion=\"aic\")).fit(X, y)\\n\\n\\n# %%\\n# To be in line with the definition in [ZHT2007]_, we need to rescale the\\n# AIC and the BIC. Indeed, Zou et al. are ignoring some constant terms\\n# compared to the original definition of AIC derived from the maximum\\n# log-likelihood of a linear model. You can refer to\\n# :ref:`mathematical detail section for the User Guide <lasso_lars_ic>`.\\n# Code for: def zou_et_al_criterion_rescaling(criterion, n_samples, noise_variance):\\n\\n\\n# %%\\nimport numpy as np\\n\\naic_criterion = zou_et_al_criterion_rescaling(\\n    lasso_lars_ic[-1].criterion_,\\n    n_samples,\\n    lasso_lars_ic[-1].noise_variance_,\\n)\\n\\nindex_alpha_path_aic = np.flatnonzero(\\n    lasso_lars_ic[-1].alphas_ == lasso_lars_ic[-1].alpha_\\n)[0]\\n\\n# %%\\nlasso_lars_ic.set_params(lassolarsic__criterion=\"bic\").fit(X, y)\\n\\nbic_criterion = zou_et_al_criterion_rescaling(\\n    lasso_lars_ic[-1].criterion_,\\n    n_samples,\\n    lasso_lars_ic[-1].noise_variance_,\\n)\\n\\nindex_alpha_path_bic = np.flatnonzero(\\n    lasso_lars_ic[-1].alphas_ == lasso_lars_ic[-1].alpha_\\n)[0]'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_lasso_lars_ic.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='bic_criterion = zou_et_al_criterion_rescaling(\\n    lasso_lars_ic[-1].criterion_,\\n    n_samples,\\n    lasso_lars_ic[-1].noise_variance_,\\n)\\n\\nindex_alpha_path_bic = np.flatnonzero(\\n    lasso_lars_ic[-1].alphas_ == lasso_lars_ic[-1].alpha_\\n)[0]\\n\\n# %%\\n# Now that we collected the AIC and BIC, we can as well check that the minima\\n# of both criteria happen at the same alpha. Then, we can simplify the\\n# following plot.\\nindex_alpha_path_aic == index_alpha_path_bic\\n\\n# %%\\n# Finally, we can plot the AIC and BIC criterion and the subsequent selected\\n# regularization parameter.\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(aic_criterion, color=\"tab:blue\", marker=\"o\", label=\"AIC criterion\")\\nplt.plot(bic_criterion, color=\"tab:orange\", marker=\"o\", label=\"BIC criterion\")\\nplt.vlines(\\n    index_alpha_path_bic,\\n    aic_criterion.min(),\\n    aic_criterion.max(),\\n    color=\"black\",\\n    linestyle=\"--\",\\n    label=\"Selected alpha\",\\n)\\nplt.legend()\\nplt.ylabel(\"Information criterion\")\\nplt.xlabel(\"Lasso model sequence\")\\n_ = plt.title(\"Lasso model selection via AIC and BIC\")'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ridge_path.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================================\\nPlot Ridge coefficients as a function of the regularization\\n===========================================================\\n\\nShows the effect of collinearity in the coefficients of an estimator.\\n\\n.. currentmodule:: sklearn.linear_model\\n\\n:class:`Ridge` Regression is the estimator used in this example.\\nEach color represents a different feature of the\\ncoefficient vector, and this is displayed as a function of the\\nregularization parameter.\\n\\nThis example also shows the usefulness of applying Ridge regression\\nto highly ill-conditioned matrices. For such matrices, a slight\\nchange in the target variable can cause huge variances in the\\ncalculated weights. In such cases, it is useful to set a certain\\nregularization (alpha) to reduce this variation (noise).\\n\\nWhen alpha is very large, the regularization effect dominates the\\nsquared loss function and the coefficients tend to zero.\\nAt the end of the path, as alpha tends toward zero\\nand the solution tends towards the ordinary least squares, coefficients\\nexhibit big oscillations. In practise it is necessary to tune alpha\\nin such a way that a balance is maintained between both.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import linear_model\\n\\n# X is the 10x10 Hilbert matrix\\nX = 1.0 / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])\\ny = np.ones(10)\\n\\n# %%\\n# Compute paths\\n# -------------\\n\\nn_alphas = 200\\nalphas = np.logspace(-10, -2, n_alphas)\\n\\ncoefs = []\\nfor a in alphas:\\n    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\\n    ridge.fit(X, y)\\n    coefs.append(ridge.coef_)\\n\\n# %%\\n# Display results\\n# ---------------\\n\\nax = plt.gca()\\n\\nax.plot(alphas, coefs)\\nax.set_xscale(\"log\")\\nax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\\nplt.xlabel(\"alpha\")\\nplt.ylabel(\"weights\")\\nplt.title(\"Ridge coefficients as a function of the regularization\")\\nplt.axis(\"tight\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ransac.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================\\nRobust linear model estimation using RANSAC\\n===========================================\\n\\nIn this example, we see how to robustly fit a linear model to faulty data using\\nthe :ref:`RANSAC <ransac_regression>` algorithm.\\n\\nThe ordinary linear regressor is sensitive to outliers, and the fitted line can\\neasily be skewed away from the true underlying relationship of data.\\n\\nThe RANSAC regressor automatically splits the data into inliers and outliers,\\nand the fitted line is determined only by the identified inliers.\\n\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn import datasets, linear_model\\n\\nn_samples = 1000\\nn_outliers = 50\\n\\n\\nX, y, coef = datasets.make_regression(\\n    n_samples=n_samples,\\n    n_features=1,\\n    n_informative=1,\\n    noise=10,\\n    coef=True,\\n    random_state=0,\\n)\\n\\n# Add outlier data\\nnp.random.seed(0)\\nX[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\\ny[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\\n\\n# Fit line using all data\\nlr = linear_model.LinearRegression()\\nlr.fit(X, y)\\n\\n# Robustly fit linear model with RANSAC algorithm\\nransac = linear_model.RANSACRegressor()\\nransac.fit(X, y)\\ninlier_mask = ransac.inlier_mask_\\noutlier_mask = np.logical_not(inlier_mask)\\n\\n# Predict data of estimated models\\nline_X = np.arange(X.min(), X.max())[:, np.newaxis]\\nline_y = lr.predict(line_X)\\nline_y_ransac = ransac.predict(line_X)\\n\\n# Compare estimated coefficients\\nprint(\"Estimated coefficients (true, linear regression, RANSAC):\")\\nprint(coef, lr.coef_, ransac.estimator_.coef_)\\n\\nlw = 2\\nplt.scatter(\\n    X[inlier_mask], y[inlier_mask], color=\"yellowgreen\", marker=\".\", label=\"Inliers\"\\n)\\nplt.scatter(\\n    X[outlier_mask], y[outlier_mask], color=\"gold\", marker=\".\", label=\"Outliers\"\\n)\\nplt.plot(line_X, line_y, color=\"navy\", linewidth=lw, label=\"Linear regressor\")\\nplt.plot(\\n    line_X,\\n    line_y_ransac,\\n    color=\"cornflowerblue\",\\n    linewidth=lw,\\n    label=\"RANSAC regressor\",\\n)\\nplt.legend(loc=\"lower right\")\\nplt.xlabel(\"Input\")\\nplt.ylabel(\"Response\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_iris_logistic.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nLogistic Regression 3-class Classifier\\n=========================================================\\n\\nShow below is a logistic-regression classifiers decision boundaries on the\\nfirst two dimensions (sepal length and width) of the `iris\\n<https://en.wikipedia.org/wiki/Iris_flower_data_set>`_ dataset. The datapoints\\nare colored according to their labels.\\n\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# Modified for documentation by Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import datasets\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.linear_model import LogisticRegression\\n\\n# import some data to play with\\niris = datasets.load_iris()\\nX = iris.data[:, :2]  # we only take the first two features.\\nY = iris.target\\n\\n# Create an instance of Logistic Regression Classifier and fit the data.\\nlogreg = LogisticRegression(C=1e5)\\nlogreg.fit(X, Y)\\n\\n_, ax = plt.subplots(figsize=(4, 3))\\nDecisionBoundaryDisplay.from_estimator(\\n    logreg,\\n    X,\\n    cmap=plt.cm.Paired,\\n    ax=ax,\\n    response_method=\"predict\",\\n    plot_method=\"pcolormesh\",\\n    shading=\"auto\",\\n    xlabel=\"Sepal length\",\\n    ylabel=\"Sepal width\",\\n    eps=0.5,\\n)\\n\\n# Plot also the training points\\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors=\"k\", cmap=plt.cm.Paired)\\n\\n\\nplt.xticks(())\\nplt.yticks(())\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_sgd_separating_hyperplane.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================\\nSGD: Maximum margin separating hyperplane\\n=========================================\\n\\nPlot the maximum margin separating hyperplane within a two-class\\nseparable dataset using a linear Support Vector Machines classifier\\ntrained using SGD.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.linear_model import SGDClassifier\\n\\n# we create 50 separable points\\nX, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\\n\\n# fit the model\\nclf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200)\\n\\nclf.fit(X, Y)\\n\\n# plot the line, the points, and the nearest vectors to the plane\\nxx = np.linspace(-1, 5, 10)\\nyy = np.linspace(-1, 5, 10)\\n\\nX1, X2 = np.meshgrid(xx, yy)\\nZ = np.empty(X1.shape)\\nfor (i, j), val in np.ndenumerate(X1):\\n    x1 = val\\n    x2 = X2[i, j]\\n    p = clf.decision_function([[x1, x2]])\\n    Z[i, j] = p[0]\\nlevels = [-1.0, 0.0, 1.0]\\nlinestyles = [\"dashed\", \"solid\", \"dashed\"]\\ncolors = \"k\"\\nplt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolor=\"black\", s=20)\\n\\nplt.axis(\"tight\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ols_3d.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_figs(fig_num, elev, azim, X_train, clf):\\n    fig = plt.figure(fig_num, figsize=(4, 3))\\n    plt.clf()\\n    ax = fig.add_subplot(111, projection=\"3d\", elev=elev, azim=azim)\\n\\n    ax.scatter(X_train[:, 0], X_train[:, 1], y_train, c=\"k\", marker=\"+\")\\n    ax.plot_surface(\\n        np.array([[-0.1, -0.1], [0.15, 0.15]]),\\n        np.array([[-0.1, 0.15], [-0.1, 0.15]]),\\n        clf.predict(\\n            np.array([[-0.1, -0.1, 0.15, 0.15], [-0.1, 0.15, -0.1, 0.15]]).T\\n        ).reshape((2, 2)),\\n        alpha=0.5,\\n    )\\n    ax.set_xlabel(\"X_1\")\\n    ax.set_ylabel(\"X_2\")\\n    ax.set_zlabel(\"Y\")\\n    ax.xaxis.set_ticklabels([])\\n    ax.yaxis.set_ticklabels([])\\n    ax.zaxis.set_ticklabels([])'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_ols_3d.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nSparsity Example: Fitting only features 1  and 2\\n=========================================================\\n\\nFeatures 1 and 2 of the diabetes-dataset are fitted and\\nplotted below. It illustrates that although feature 2\\nhas a strong coefficient on the full model, it does not\\ngive us much regarding `y` when compared to just feature 1.\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# Modified for documentation by Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# First we load the diabetes dataset.\\n\\nimport numpy as np\\n\\nfrom sklearn import datasets\\n\\nX, y = datasets.load_diabetes(return_X_y=True)\\nindices = (0, 1)\\n\\nX_train = X[:-20, indices]\\nX_test = X[-20:, indices]\\ny_train = y[:-20]\\ny_test = y[-20:]\\n\\n# %%\\n# Next we fit a linear regression model.\\n\\nfrom sklearn import linear_model\\n\\nols = linear_model.LinearRegression()\\n_ = ols.fit(X_train, y_train)\\n\\n\\n# %%\\n# Finally we plot the figure from three different views.\\n\\nimport matplotlib.pyplot as plt\\n\\n# unused but required import for doing 3d projections with matplotlib < 3.2\\nimport mpl_toolkits.mplot3d  # noqa: F401\\n\\n\\n# Code for: def plot_figs(fig_num, elev, azim, X_train, clf):\\n\\n\\n# Generate the three different figures from different views\\nelev = 43.5\\nazim = -110\\nplot_figs(1, elev, azim, X_train, ols)\\n\\nelev = -0.5\\nazim = 0\\nplot_figs(2, elev, azim, X_train, ols)\\n\\nelev = -0.5\\nazim = 90\\nplot_figs(3, elev, azim, X_train, ols)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_bayesian_ridge_curvefit.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def func(x):\\n    return np.sin(2 * np.pi * x)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_bayesian_ridge_curvefit.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================================\\nCurve Fitting with Bayesian Ridge Regression\\n============================================\\n\\nComputes a Bayesian Ridge Regression of Sinusoids.\\n\\nSee :ref:`bayesian_ridge_regression` for more information on the regressor.\\n\\nIn general, when fitting a curve with a polynomial by Bayesian ridge\\nregression, the selection of initial values of\\nthe regularization parameters (alpha, lambda) may be important.\\nThis is because the regularization parameters are determined by an iterative\\nprocedure that depends on initial values.\\n\\nIn this example, the sinusoid is approximated by a polynomial using different\\npairs of initial values.\\n\\nWhen starting from the default values (alpha_init = 1.90, lambda_init = 1.),\\nthe bias of the resulting curve is large, and the variance is small.\\nSo, lambda_init should be relatively small (1.e-3) so as to reduce the bias.\\n\\nAlso, by evaluating log marginal likelihood (L) of\\nthese models, we can determine which one is better.\\nIt can be concluded that the model with larger L is more likely.\\n\\n\"\"\"\\n\\n# Author: Yoshihiro Uchida <nimbus1after2a1sun7shower@gmail.com>\\n\\n# %%\\n# Generate sinusoidal data with noise\\n# -----------------------------------\\nimport numpy as np\\n\\n\\n# Code for: def func(x):\\n\\n\\nsize = 25\\nrng = np.random.RandomState(1234)\\nx_train = rng.uniform(0.0, 1.0, size)\\ny_train = func(x_train) + rng.normal(scale=0.1, size=size)\\nx_test = np.linspace(0.0, 1.0, 100)\\n\\n\\n# %%\\n# Fit by cubic polynomial\\n# -----------------------\\nfrom sklearn.linear_model import BayesianRidge\\n\\nn_order = 3\\nX_train = np.vander(x_train, n_order + 1, increasing=True)\\nX_test = np.vander(x_test, n_order + 1, increasing=True)\\nreg = BayesianRidge(tol=1e-6, fit_intercept=False, compute_score=True)\\n\\n# %%\\n# Plot the true and predicted curves with log marginal likelihood (L)\\n# -------------------------------------------------------------------\\nimport matplotlib.pyplot as plt\\n\\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\\nfor i, ax in enumerate(axes):\\n    # Bayesian ridge regression with different initial value pairs\\n    if i == 0:\\n        init = [1 / np.var(y_train), 1.0]  # Default values\\n    elif i == 1:\\n        init = [1.0, 1e-3]\\n        reg.set_params(alpha_init=init[0], lambda_init=init[1])\\n    reg.fit(X_train, y_train)\\n    ymean, ystd = reg.predict(X_test, return_std=True)'), Document(metadata={'source': '/content/local_copy_repo/examples/linear_model/plot_bayesian_ridge_curvefit.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='ax.plot(x_test, func(x_test), color=\"blue\", label=\"sin($2\\\\\\\\pi x$)\")\\n    ax.scatter(x_train, y_train, s=50, alpha=0.5, label=\"observation\")\\n    ax.plot(x_test, ymean, color=\"red\", label=\"predict mean\")\\n    ax.fill_between(\\n        x_test, ymean - ystd, ymean + ystd, color=\"pink\", alpha=0.5, label=\"predict std\"\\n    )\\n    ax.set_ylim(-1.3, 1.3)\\n    ax.legend()\\n    title = \"$\\\\\\\\alpha$_init$={:.2f},\\\\\\\\ \\\\\\\\lambda$_init$={}$\".format(init[0], init[1])\\n    if i == 0:\\n        title += \" (Default)\"\\n    ax.set_title(title, fontsize=12)\\n    text = \"$\\\\\\\\alpha={:.1f}$\\\\n$\\\\\\\\lambda={:.3f}$\\\\n$L={:.1f}$\".format(\\n        reg.alpha_, reg.lambda_, reg.scores_[-1]\\n    )\\n    ax.text(0.05, -1.0, text, fontsize=12)\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_ica_blind_source_separation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================================\\nBlind source separation using FastICA\\n=====================================\\n\\nAn example of estimating sources from noisy data.\\n\\n:ref:`ICA` is used to estimate sources given noisy measurements.\\nImagine 3 instruments playing simultaneously and 3 microphones\\nrecording the mixed signals. ICA is used to recover the sources\\nie. what is played by each instrument. Importantly, PCA fails\\nat recovering our `instruments` since the related signals reflect\\nnon-Gaussian processes.\\n\\n\"\"\"\\n\\n# %%\\n# Generate sample data\\n# --------------------\\n\\nimport numpy as np\\nfrom scipy import signal\\n\\nnp.random.seed(0)\\nn_samples = 2000\\ntime = np.linspace(0, 8, n_samples)\\n\\ns1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\\ns2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\\ns3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\\n\\nS = np.c_[s1, s2, s3]\\nS += 0.2 * np.random.normal(size=S.shape)  # Add noise\\n\\nS /= S.std(axis=0)  # Standardize data\\n# Mix data\\nA = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\\nX = np.dot(S, A.T)  # Generate observations\\n\\n# %%\\n# Fit ICA and PCA models\\n# ----------------------\\n\\nfrom sklearn.decomposition import PCA, FastICA\\n\\n# Compute ICA\\nica = FastICA(n_components=3, whiten=\"arbitrary-variance\")\\nS_ = ica.fit_transform(X)  # Reconstruct signals\\nA_ = ica.mixing_  # Get estimated mixing matrix\\n\\n# We can `prove` that the ICA model applies by reverting the unmixing.\\nassert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)\\n\\n# For comparison, compute PCA\\npca = PCA(n_components=3)\\nH = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components\\n\\n# %%\\n# Plot results\\n# ------------\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.figure()\\n\\nmodels = [X, S, S_, H]\\nnames = [\\n    \"Observations (mixed signal)\",\\n    \"True Sources\",\\n    \"ICA recovered signals\",\\n    \"PCA recovered signals\",\\n]\\ncolors = [\"red\", \"steelblue\", \"orange\"]\\n\\nfor ii, (model, name) in enumerate(zip(models, names), 1):\\n    plt.subplot(4, 1, ii)\\n    plt.title(name)\\n    for sig, color in zip(model.T, colors):\\n        plt.plot(sig, color=color)\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_pca_vs_lda.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=======================================================\\nComparison of LDA and PCA 2D projection of Iris dataset\\n=======================================================\\n\\nThe Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour\\nand Virginica) with 4 attributes: sepal length, sepal width, petal length\\nand petal width.\\n\\nPrincipal Component Analysis (PCA) applied to this data identifies the\\ncombination of attributes (principal components, or directions in the\\nfeature space) that account for the most variance in the data. Here we\\nplot the different samples on the 2 first principal components.\\n\\nLinear Discriminant Analysis (LDA) tries to identify attributes that\\naccount for the most variance *between classes*. In particular,\\nLDA, in contrast to PCA, is a supervised method, using known class labels.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import datasets\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\n\\niris = datasets.load_iris()\\n\\nX = iris.data\\ny = iris.target\\ntarget_names = iris.target_names\\n\\npca = PCA(n_components=2)\\nX_r = pca.fit(X).transform(X)\\n\\nlda = LinearDiscriminantAnalysis(n_components=2)\\nX_r2 = lda.fit(X, y).transform(X)\\n\\n# Percentage of variance explained for each components\\nprint(\\n    \"explained variance ratio (first two components): %s\"\\n    % str(pca.explained_variance_ratio_)\\n)\\n\\nplt.figure()\\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\\nlw = 2\\n\\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\\n    plt.scatter(\\n        X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\\n    )\\nplt.legend(loc=\"best\", shadow=False, scatterpoints=1)\\nplt.title(\"PCA of IRIS dataset\")\\n\\nplt.figure()\\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\\n    plt.scatter(\\n        X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name\\n    )\\nplt.legend(loc=\"best\", shadow=False, scatterpoints=1)\\nplt.title(\"LDA of IRIS dataset\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_pca_iris.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nPCA example with Iris Data-set\\n=========================================================\\n\\nPrincipal Component Analysis applied to the Iris dataset.\\n\\nSee `here <https://en.wikipedia.org/wiki/Iris_flower_data_set>`_ for more\\ninformation on this dataset.\\n\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\n\\n# unused but required import for doing 3d projections with matplotlib < 3.2\\nimport mpl_toolkits.mplot3d  # noqa: F401\\nimport numpy as np\\n\\nfrom sklearn import datasets, decomposition\\n\\nnp.random.seed(5)\\n\\niris = datasets.load_iris()\\nX = iris.data\\ny = iris.target\\n\\nfig = plt.figure(1, figsize=(4, 3))\\nplt.clf()\\n\\nax = fig.add_subplot(111, projection=\"3d\", elev=48, azim=134)\\nax.set_position([0, 0, 0.95, 1])\\n\\n\\nplt.cla()\\npca = decomposition.PCA(n_components=3)\\npca.fit(X)\\nX = pca.transform(X)\\n\\nfor name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\\n    ax.text3D(\\n        X[y == label, 0].mean(),\\n        X[y == label, 1].mean() + 1.5,\\n        X[y == label, 2].mean(),\\n        name,\\n        horizontalalignment=\"center\",\\n        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\\n    )\\n# Reorder the labels to have colors matching the cluster results\\ny = np.choose(y, [1, 2, 0]).astype(float)\\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor=\"k\")\\n\\nax.xaxis.set_ticklabels([])\\nax.yaxis.set_ticklabels([])\\nax.zaxis.set_ticklabels([])\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_kernel_pca.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========\\nKernel PCA\\n==========\\n\\nThis example shows the difference between the Principal Components Analysis\\n(:class:`~sklearn.decomposition.PCA`) and its kernelized version\\n(:class:`~sklearn.decomposition.KernelPCA`).\\n\\nOn the one hand, we show that :class:`~sklearn.decomposition.KernelPCA` is able\\nto find a projection of the data which linearly separates them while it is not the case\\nwith :class:`~sklearn.decomposition.PCA`.\\n\\nFinally, we show that inverting this projection is an approximation with\\n:class:`~sklearn.decomposition.KernelPCA`, while it is exact with\\n:class:`~sklearn.decomposition.PCA`.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Projecting data: `PCA` vs. `KernelPCA`\\n# --------------------------------------\\n#\\n# In this section, we show the advantages of using a kernel when\\n# projecting data using a Principal Component Analysis (PCA). We create a\\n# dataset made of two nested circles.\\nfrom sklearn.datasets import make_circles\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = make_circles(n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n\\n# %%\\n# Let\\'s have a quick first look at the generated dataset.\\nimport matplotlib.pyplot as plt\\n\\n_, (train_ax, test_ax) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 4))\\n\\ntrain_ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\\ntrain_ax.set_ylabel(\"Feature #1\")\\ntrain_ax.set_xlabel(\"Feature #0\")\\ntrain_ax.set_title(\"Training data\")\\n\\ntest_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\\ntest_ax.set_xlabel(\"Feature #0\")\\n_ = test_ax.set_title(\"Testing data\")\\n\\n# %%\\n# The samples from each class cannot be linearly separated: there is no\\n# straight line that can split the samples of the inner set from the outer\\n# set.\\n#\\n# Now, we will use PCA with and without a kernel to see what is the effect of\\n# using such a kernel. The kernel used here is a radial basis function (RBF)\\n# kernel.\\nfrom sklearn.decomposition import PCA, KernelPCA\\n\\npca = PCA(n_components=2)\\nkernel_pca = KernelPCA(\\n    n_components=None, kernel=\"rbf\", gamma=10, fit_inverse_transform=True, alpha=0.1\\n)\\n\\nX_test_pca = pca.fit(X_train).transform(X_test)\\nX_test_kernel_pca = kernel_pca.fit(X_train).transform(X_test)\\n\\n# %%\\nfig, (orig_data_ax, pca_proj_ax, kernel_pca_proj_ax) = plt.subplots(\\n    ncols=3, figsize=(14, 4)\\n)\\n\\norig_data_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\\norig_data_ax.set_ylabel(\"Feature #1\")\\norig_data_ax.set_xlabel(\"Feature #0\")\\norig_data_ax.set_title(\"Testing data\")\\n\\npca_proj_ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test)\\npca_proj_ax.set_ylabel(\"Principal component #1\")\\npca_proj_ax.set_xlabel(\"Principal component #0\")\\npca_proj_ax.set_title(\"Projection of testing data\\\\n using PCA\")'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_kernel_pca.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='pca_proj_ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test)\\npca_proj_ax.set_ylabel(\"Principal component #1\")\\npca_proj_ax.set_xlabel(\"Principal component #0\")\\npca_proj_ax.set_title(\"Projection of testing data\\\\n using PCA\")\\n\\nkernel_pca_proj_ax.scatter(X_test_kernel_pca[:, 0], X_test_kernel_pca[:, 1], c=y_test)\\nkernel_pca_proj_ax.set_ylabel(\"Principal component #1\")\\nkernel_pca_proj_ax.set_xlabel(\"Principal component #0\")\\n_ = kernel_pca_proj_ax.set_title(\"Projection of testing data\\\\n using KernelPCA\")\\n\\n# %%\\n# We recall that PCA transforms the data linearly. Intuitively, it means that\\n# the coordinate system will be centered, rescaled on each component\\n# with respected to its variance and finally be rotated.\\n# The obtained data from this transformation is isotropic and can now be\\n# projected on its *principal components*.\\n#\\n# Thus, looking at the projection made using PCA (i.e. the middle figure), we\\n# see that there is no change regarding the scaling; indeed the data being two\\n# concentric circles centered in zero, the original data is already isotropic.\\n# However, we can see that the data have been rotated. As a\\n# conclusion, we see that such a projection would not help if define a linear\\n# classifier to distinguish samples from both classes.\\n#\\n# Using a kernel allows to make a non-linear projection. Here, by using an RBF\\n# kernel, we expect that the projection will unfold the dataset while keeping\\n# approximately preserving the relative distances of pairs of data points that\\n# are close to one another in the original space.\\n#\\n# We observe such behaviour in the figure on the right: the samples of a given\\n# class are closer to each other than the samples from the opposite class,\\n# untangling both sample sets. Now, we can use a linear classifier to separate\\n# the samples from the two classes.\\n#\\n# Projecting into the original feature space\\n# ------------------------------------------\\n#\\n# One particularity to have in mind when using\\n# :class:`~sklearn.decomposition.KernelPCA` is related to the reconstruction\\n# (i.e. the back projection in the original feature space). With\\n# :class:`~sklearn.decomposition.PCA`, the reconstruction will be exact if\\n# `n_components` is the same than the number of original features.\\n# This is the case in this example.\\n#\\n# We can investigate if we get the original dataset when back projecting with\\n# :class:`~sklearn.decomposition.KernelPCA`.\\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\\nX_reconstructed_kernel_pca = kernel_pca.inverse_transform(kernel_pca.transform(X_test))\\n\\n# %%\\nfig, (orig_data_ax, pca_back_proj_ax, kernel_pca_back_proj_ax) = plt.subplots(\\n    ncols=3, sharex=True, sharey=True, figsize=(13, 4)\\n)\\n\\norig_data_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\\norig_data_ax.set_ylabel(\"Feature #1\")\\norig_data_ax.set_xlabel(\"Feature #0\")\\norig_data_ax.set_title(\"Original test data\")'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_kernel_pca.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='orig_data_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\\norig_data_ax.set_ylabel(\"Feature #1\")\\norig_data_ax.set_xlabel(\"Feature #0\")\\norig_data_ax.set_title(\"Original test data\")\\n\\npca_back_proj_ax.scatter(X_reconstructed_pca[:, 0], X_reconstructed_pca[:, 1], c=y_test)\\npca_back_proj_ax.set_xlabel(\"Feature #0\")\\npca_back_proj_ax.set_title(\"Reconstruction via PCA\")\\n\\nkernel_pca_back_proj_ax.scatter(\\n    X_reconstructed_kernel_pca[:, 0], X_reconstructed_kernel_pca[:, 1], c=y_test\\n)\\nkernel_pca_back_proj_ax.set_xlabel(\"Feature #0\")\\n_ = kernel_pca_back_proj_ax.set_title(\"Reconstruction via KernelPCA\")\\n\\n# %%\\n# While we see a perfect reconstruction with\\n# :class:`~sklearn.decomposition.PCA` we observe a different result for\\n# :class:`~sklearn.decomposition.KernelPCA`.\\n#\\n# Indeed, :meth:`~sklearn.decomposition.KernelPCA.inverse_transform` cannot\\n# rely on an analytical back-projection and thus an exact reconstruction.\\n# Instead, a :class:`~sklearn.kernel_ridge.KernelRidge` is internally trained\\n# to learn a mapping from the kernalized PCA basis to the original feature\\n# space. This method therefore comes with an approximation introducing small\\n# differences when back projecting in the original feature space.\\n#\\n# To improve the reconstruction using\\n# :meth:`~sklearn.decomposition.KernelPCA.inverse_transform`, one can tune\\n# `alpha` in :class:`~sklearn.decomposition.KernelPCA`, the regularization term\\n# which controls the reliance on the training data during the training of\\n# the mapping.'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_sparse_coding.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def ricker_function(resolution, center, width):\\n    \"\"\"Discrete sub-sampled Ricker (Mexican hat) wavelet\"\"\"\\n    x = np.linspace(0, resolution - 1, resolution)\\n    x = (\\n        (2 / (np.sqrt(3 * width) * np.pi**0.25))\\n        * (1 - (x - center) ** 2 / width**2)\\n        * np.exp(-((x - center) ** 2) / (2 * width**2))\\n    )\\n    return x'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_sparse_coding.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def ricker_matrix(width, resolution, n_components):\\n    \"\"\"Dictionary of Ricker (Mexican hat) wavelets\"\"\"\\n    centers = np.linspace(0, resolution - 1, n_components)\\n    D = np.empty((n_components, resolution))\\n    for i, center in enumerate(centers):\\n        D[i] = ricker_function(resolution, center, width)\\n    D /= np.sqrt(np.sum(D**2, axis=1))[:, np.newaxis]\\n    return D'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_sparse_coding.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================\\nSparse coding with a precomputed dictionary\\n===========================================\\n\\nTransform a signal as a sparse combination of Ricker wavelets. This example\\nvisually compares different sparse coding methods using the\\n:class:`~sklearn.decomposition.SparseCoder` estimator. The Ricker (also known\\nas Mexican hat or the second derivative of a Gaussian) is not a particularly\\ngood kernel to represent piecewise constant signals like this one. It can\\ntherefore be seen how much adding different widths of atoms matters and it\\ntherefore motivates learning the dictionary to best fit your type of signals.\\n\\nThe richer dictionary on the right is not larger in size, heavier subsampling\\nis performed in order to stay on the same order of magnitude.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.decomposition import SparseCoder\\n\\n\\n# Code for: def ricker_function(resolution, center, width):\\n\\n\\n# Code for: def ricker_matrix(width, resolution, n_components):\\n\\n\\nresolution = 1024\\nsubsampling = 3  # subsampling factor\\nwidth = 100\\nn_components = resolution // subsampling\\n\\n# Compute a wavelet dictionary\\nD_fixed = ricker_matrix(width=width, resolution=resolution, n_components=n_components)\\nD_multi = np.r_[\\n    tuple(\\n        ricker_matrix(width=w, resolution=resolution, n_components=n_components // 5)\\n        for w in (10, 50, 100, 500, 1000)\\n    )\\n]\\n\\n# Generate a signal\\ny = np.linspace(0, resolution - 1, resolution)\\nfirst_quarter = y < resolution / 4\\ny[first_quarter] = 3.0\\ny[np.logical_not(first_quarter)] = -1.0\\n\\n# List the different sparse coding methods in the following format:\\n# (title, transform_algorithm, transform_alpha,\\n#  transform_n_nozero_coefs, color)\\nestimators = [\\n    (\"OMP\", \"omp\", None, 15, \"navy\"),\\n    (\"Lasso\", \"lasso_lars\", 2, None, \"turquoise\"),\\n]\\nlw = 2\\n\\nplt.figure(figsize=(13, 6))\\nfor subplot, (D, title) in enumerate(\\n    zip((D_fixed, D_multi), (\"fixed width\", \"multiple widths\"))\\n):\\n    plt.subplot(1, 2, subplot + 1)\\n    plt.title(\"Sparse coding against %s dictionary\" % title)\\n    plt.plot(y, lw=lw, linestyle=\"--\", label=\"Original signal\")\\n    # Do a wavelet approximation\\n    for title, algo, alpha, n_nonzero, color in estimators:\\n        coder = SparseCoder(\\n            dictionary=D,\\n            transform_n_nonzero_coefs=n_nonzero,\\n            transform_alpha=alpha,\\n            transform_algorithm=algo,\\n        )\\n        x = coder.transform(y.reshape(1, -1))\\n        density = len(np.flatnonzero(x))\\n        x = np.ravel(np.dot(x, D))\\n        squared_error = np.sum((y - x) ** 2)\\n        plt.plot(\\n            x,\\n            color=color,\\n            lw=lw,\\n            label=\"%s: %s nonzero coefs,\\\\n%.2f error\" % (title, density, squared_error),\\n        )'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_sparse_coding.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Soft thresholding debiasing\\n    coder = SparseCoder(\\n        dictionary=D, transform_algorithm=\"threshold\", transform_alpha=20\\n    )\\n    x = coder.transform(y.reshape(1, -1))\\n    _, idx = np.where(x != 0)\\n    x[0, idx], _, _, _ = np.linalg.lstsq(D[idx, :].T, y, rcond=None)\\n    x = np.ravel(np.dot(x, D))\\n    squared_error = np.sum((y - x) ** 2)\\n    plt.plot(\\n        x,\\n        color=\"darkorange\",\\n        lw=lw,\\n        label=\"Thresholding w/ debiasing:\\\\n%d nonzero coefs, %.2f error\"\\n        % (len(idx), squared_error),\\n    )\\n    plt.axis(\"tight\")\\n    plt.legend(shadow=False, loc=\"best\")\\nplt.subplots_adjust(0.04, 0.07, 0.97, 0.90, 0.09, 0.2)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_incremental_pca.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n\\n===============\\nIncremental PCA\\n===============\\n\\nIncremental principal component analysis (IPCA) is typically used as a\\nreplacement for principal component analysis (PCA) when the dataset to be\\ndecomposed is too large to fit in memory. IPCA builds a low-rank approximation\\nfor the input data using an amount of memory which is independent of the\\nnumber of input data samples. It is still dependent on the input data features,\\nbut changing the batch size allows for control of memory usage.\\n\\nThis example serves as a visual check that IPCA is able to find a similar\\nprojection of the data to PCA (to a sign flip), while only processing a\\nfew samples at a time. This can be considered a \"toy example\", as IPCA is\\nintended for large datasets which do not fit in main memory, requiring\\nincremental approaches.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.decomposition import PCA, IncrementalPCA\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\n\\nn_components = 2\\nipca = IncrementalPCA(n_components=n_components, batch_size=10)\\nX_ipca = ipca.fit_transform(X)\\n\\npca = PCA(n_components=n_components)\\nX_pca = pca.fit_transform(X)\\n\\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\\n\\nfor X_transformed, title in [(X_ipca, \"Incremental PCA\"), (X_pca, \"PCA\")]:\\n    plt.figure(figsize=(8, 8))\\n    for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\\n        plt.scatter(\\n            X_transformed[y == i, 0],\\n            X_transformed[y == i, 1],\\n            color=color,\\n            lw=2,\\n            label=target_name,\\n        )\\n\\n    if \"Incremental\" in title:\\n        err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()\\n        plt.title(title + \" of iris dataset\\\\nMean absolute unsigned error %.6f\" % err)\\n    else:\\n        plt.title(title + \" of iris dataset\")\\n    plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\\n    plt.axis([-4, 4, -1.5, 1.5])\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_ica_vs_pca.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_samples(S, axis_list=None):\\n    plt.scatter(\\n        S[:, 0], S[:, 1], s=2, marker=\"o\", zorder=10, color=\"steelblue\", alpha=0.5\\n    )\\n    if axis_list is not None:\\n        for axis, color, label in axis_list:\\n            axis /= axis.std()\\n            x_axis, y_axis = axis\\n            plt.quiver(\\n                (0, 0),\\n                (0, 0),\\n                x_axis,\\n                y_axis,\\n                zorder=11,\\n                width=0.01,\\n                scale=6,\\n                color=color,\\n                label=label,\\n            )\\n\\n    plt.hlines(0, -3, 3)\\n    plt.vlines(0, -3, 3)\\n    plt.xlim(-3, 3)\\n    plt.ylim(-3, 3)\\n    plt.xlabel(\"x\")\\n    plt.ylabel(\"y\")'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_ica_vs_pca.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================\\nFastICA on 2D point clouds\\n==========================\\n\\nThis example illustrates visually in the feature space a comparison by\\nresults using two different component analysis techniques.\\n\\n:ref:`ICA` vs :ref:`PCA`.\\n\\nRepresenting ICA in the feature space gives the view of \\'geometric ICA\\':\\nICA is an algorithm that finds directions in the feature space\\ncorresponding to projections with high non-Gaussianity. These directions\\nneed not be orthogonal in the original feature space, but they are\\northogonal in the whitened feature space, in which all directions\\ncorrespond to the same variance.\\n\\nPCA, on the other hand, finds orthogonal directions in the raw feature\\nspace that correspond to directions accounting for maximum variance.\\n\\nHere we simulate independent sources using a highly non-Gaussian\\nprocess, 2 student T with a low number of degrees of freedom (top left\\nfigure). We mix them to create observations (top right figure).\\nIn this raw observation space, directions identified by PCA are\\nrepresented by orange vectors. We represent the signal in the PCA space,\\nafter whitening by the variance corresponding to the PCA vectors (lower\\nleft). Running ICA corresponds to finding a rotation in this space to\\nidentify the directions of largest non-Gaussianity (lower right).\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate sample data\\n# --------------------\\nimport numpy as np\\n\\nfrom sklearn.decomposition import PCA, FastICA\\n\\nrng = np.random.RandomState(42)\\nS = rng.standard_t(1.5, size=(20000, 2))\\nS[:, 0] *= 2.0\\n\\n# Mix data\\nA = np.array([[1, 1], [0, 2]])  # Mixing matrix\\n\\nX = np.dot(S, A.T)  # Generate observations\\n\\npca = PCA()\\nS_pca_ = pca.fit(X).transform(X)\\n\\nica = FastICA(random_state=rng, whiten=\"arbitrary-variance\")\\nS_ica_ = ica.fit(X).transform(X)  # Estimate the sources\\n\\n\\n# %%\\n# Plot results\\n# ------------\\nimport matplotlib.pyplot as plt\\n\\n\\n# Code for: def plot_samples(S, axis_list=None):\\n\\n\\nplt.figure()\\nplt.subplot(2, 2, 1)\\nplot_samples(S / S.std())\\nplt.title(\"True Independent Sources\")\\n\\naxis_list = [(pca.components_.T, \"orange\", \"PCA\"), (ica.mixing_, \"red\", \"ICA\")]\\nplt.subplot(2, 2, 2)\\nplot_samples(X / np.std(X), axis_list=axis_list)\\nlegend = plt.legend(loc=\"lower right\")\\nlegend.set_zorder(100)\\n\\nplt.title(\"Observations\")\\n\\nplt.subplot(2, 2, 3)\\nplot_samples(S_pca_ / np.std(S_pca_, axis=0))\\nplt.title(\"PCA recovered signals\")\\n\\nplt.subplot(2, 2, 4)\\nplot_samples(S_ica_ / np.std(S_ica_))\\nplt.title(\"ICA recovered signals\")\\n\\nplt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_faces_decomposition.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):\\n    fig, axs = plt.subplots(\\n        nrows=n_row,\\n        ncols=n_col,\\n        figsize=(2.0 * n_col, 2.3 * n_row),\\n        facecolor=\"white\",\\n        constrained_layout=True,\\n    )\\n    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)\\n    fig.set_edgecolor(\"black\")\\n    fig.suptitle(title, size=16)\\n    for ax, vec in zip(axs.flat, images):\\n        vmax = max(vec.max(), -vec.min())\\n        im = ax.imshow(\\n            vec.reshape(image_shape),\\n            cmap=cmap,\\n            interpolation=\"nearest\",\\n            vmin=-vmax,\\n            vmax=vmax,\\n        )\\n        ax.axis(\"off\")\\n\\n    fig.colorbar(im, ax=axs, orientation=\"horizontal\", shrink=0.99, aspect=40, pad=0.01)\\n    plt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_faces_decomposition.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================\\nFaces dataset decompositions\\n============================\\n\\nThis example applies to :ref:`olivetti_faces_dataset` different unsupervised\\nmatrix decomposition (dimension reduction) methods from the module\\n:mod:`sklearn.decomposition` (see the documentation chapter\\n:ref:`decompositions`).\\n\\n\\n- Authors: Vlad Niculae, Alexandre Gramfort\\n- License: BSD 3 clause\\n\"\"\"\\n\\n# %%\\n# Dataset preparation\\n# -------------------\\n#\\n# Loading and preprocessing the Olivetti faces dataset.\\n\\nimport logging\\n\\nimport matplotlib.pyplot as plt\\nfrom numpy.random import RandomState\\n\\nfrom sklearn import cluster, decomposition\\nfrom sklearn.datasets import fetch_olivetti_faces\\n\\nrng = RandomState(0)\\n\\n# Display progress logs on stdout\\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\\n\\nfaces, _ = fetch_olivetti_faces(return_X_y=True, shuffle=True, random_state=rng)\\nn_samples, n_features = faces.shape\\n\\n# Global centering (focus on one feature, centering all samples)\\nfaces_centered = faces - faces.mean(axis=0)\\n\\n# Local centering (focus on one sample, centering all features)\\nfaces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\\n\\nprint(\"Dataset consists of %d faces\" % n_samples)\\n\\n# %%\\n# Define a base function to plot the gallery of faces.\\n\\nn_row, n_col = 2, 3\\nn_components = n_row * n_col\\nimage_shape = (64, 64)\\n\\n\\n# Code for: def plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):\\n\\n\\n# %%\\n# Let\\'s take a look at our data. Gray color indicates negative values,\\n# white indicates positive values.\\n\\nplot_gallery(\"Faces from dataset\", faces_centered[:n_components])\\n\\n# %%\\n# Decomposition\\n# -------------\\n#\\n# Initialise different estimators for decomposition and fit each\\n# of them on all images and plot some results. Each estimator extracts\\n# 6 components as vectors :math:`h \\\\in \\\\mathbb{R}^{4096}`.\\n# We just displayed these vectors in human-friendly visualisation as 64x64 pixel images.\\n#\\n# Read more in the :ref:`User Guide <decompositions>`.\\n\\n# %%\\n# Eigenfaces - PCA using randomized SVD\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n# Linear dimensionality reduction using Singular Value Decomposition (SVD) of the data\\n# to project it to a lower dimensional space.\\n#\\n#\\n# .. note::\\n#\\n#     The Eigenfaces estimator, via the :py:mod:`sklearn.decomposition.PCA`,\\n#     also provides a scalar `noise_variance_` (the mean of pixelwise variance)\\n#     that cannot be displayed as an image.\\n\\n# %%\\npca_estimator = decomposition.PCA(\\n    n_components=n_components, svd_solver=\"randomized\", whiten=True\\n)\\npca_estimator.fit(faces_centered)\\nplot_gallery(\\n    \"Eigenfaces - PCA using randomized SVD\", pca_estimator.components_[:n_components]\\n)\\n\\n# %%\\n# Non-negative components - NMF\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# Estimate non-negative original data as production of two non-negative matrices.'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_faces_decomposition.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Non-negative components - NMF\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# Estimate non-negative original data as production of two non-negative matrices.\\n\\n# %%\\nnmf_estimator = decomposition.NMF(n_components=n_components, tol=5e-3)\\nnmf_estimator.fit(faces)  # original non- negative dataset\\nplot_gallery(\"Non-negative components - NMF\", nmf_estimator.components_[:n_components])\\n\\n# %%\\n# Independent components - FastICA\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n# Independent component analysis separates a multivariate vectors into additive\\n# subcomponents that are maximally independent.\\n\\n# %%\\nica_estimator = decomposition.FastICA(\\n    n_components=n_components, max_iter=400, whiten=\"arbitrary-variance\", tol=15e-5\\n)\\nica_estimator.fit(faces_centered)\\nplot_gallery(\\n    \"Independent components - FastICA\", ica_estimator.components_[:n_components]\\n)\\n\\n# %%\\n# Sparse components - MiniBatchSparsePCA\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# Mini-batch sparse PCA (:class:`~sklearn.decomposition.MiniBatchSparsePCA`)\\n# extracts the set of sparse components that best reconstruct the data. This\\n# variant is faster but less accurate than the similar\\n# :class:`~sklearn.decomposition.SparsePCA`.\\n\\n# %%\\nbatch_pca_estimator = decomposition.MiniBatchSparsePCA(\\n    n_components=n_components, alpha=0.1, max_iter=100, batch_size=3, random_state=rng\\n)\\nbatch_pca_estimator.fit(faces_centered)\\nplot_gallery(\\n    \"Sparse components - MiniBatchSparsePCA\",\\n    batch_pca_estimator.components_[:n_components],\\n)\\n\\n# %%\\n# Dictionary learning\\n# ^^^^^^^^^^^^^^^^^^^\\n#\\n# By default, :class:`~sklearn.decomposition.MiniBatchDictionaryLearning`\\n# divides the data into mini-batches and optimizes in an online manner by\\n# cycling over the mini-batches for the specified number of iterations.\\n\\n# %%\\nbatch_dict_estimator = decomposition.MiniBatchDictionaryLearning(\\n    n_components=n_components, alpha=0.1, max_iter=50, batch_size=3, random_state=rng\\n)\\nbatch_dict_estimator.fit(faces_centered)\\nplot_gallery(\"Dictionary learning\", batch_dict_estimator.components_[:n_components])\\n\\n# %%\\n# Cluster centers - MiniBatchKMeans\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# :class:`sklearn.cluster.MiniBatchKMeans` is computationally efficient and\\n# implements on-line learning with a\\n# :meth:`~sklearn.cluster.MiniBatchKMeans.partial_fit` method. That is\\n# why it could be beneficial to enhance some time-consuming algorithms with\\n# :class:`~sklearn.cluster.MiniBatchKMeans`.\\n\\n# %%\\nkmeans_estimator = cluster.MiniBatchKMeans(\\n    n_clusters=n_components,\\n    tol=1e-3,\\n    batch_size=20,\\n    max_iter=50,\\n    random_state=rng,\\n)\\nkmeans_estimator.fit(faces_centered)\\nplot_gallery(\\n    \"Cluster centers - MiniBatchKMeans\",\\n    kmeans_estimator.cluster_centers_[:n_components],\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_faces_decomposition.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Factor Analysis components - FA\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# :class:`~sklearn.decomposition.FactorAnalysis` is similar to\\n# :class:`~sklearn.decomposition.PCA` but has the advantage of modelling the\\n# variance in every direction of the input space independently (heteroscedastic\\n# noise). Read more in the :ref:`User Guide <FA>`.\\n\\n# %%\\nfa_estimator = decomposition.FactorAnalysis(n_components=n_components, max_iter=20)\\nfa_estimator.fit(faces_centered)\\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\\n\\n# --- Pixelwise variance\\nplt.figure(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\\nvec = fa_estimator.noise_variance_\\nvmax = max(vec.max(), -vec.min())\\nplt.imshow(\\n    vec.reshape(image_shape),\\n    cmap=plt.cm.gray,\\n    interpolation=\"nearest\",\\n    vmin=-vmax,\\n    vmax=vmax,\\n)\\nplt.axis(\"off\")\\nplt.title(\"Pixelwise variance from \\\\n Factor Analysis (FA)\", size=16, wrap=True)\\nplt.colorbar(orientation=\"horizontal\", shrink=0.8, pad=0.03)\\nplt.show()\\n\\n# %%\\n# Decomposition: Dictionary learning\\n# ----------------------------------\\n#\\n# In the further section, let\\'s consider :ref:`DictionaryLearning` more precisely.\\n# Dictionary learning is a problem that amounts to finding a sparse representation\\n# of the input data as a combination of simple elements. These simple elements form\\n# a dictionary. It is possible to constrain the dictionary and/or coding coefficients\\n# to be positive to match constraints that may be present in the data.\\n#\\n# :class:`~sklearn.decomposition.MiniBatchDictionaryLearning` implements a\\n# faster, but less accurate version of the dictionary learning algorithm that\\n# is better suited for large datasets. Read more in the :ref:`User Guide\\n# <MiniBatchDictionaryLearning>`.\\n\\n# %%\\n# Plot the same samples from our dataset but with another colormap.\\n# Red indicates negative values, blue indicates positive values,\\n# and white represents zeros.\\n\\nplot_gallery(\"Faces from dataset\", faces_centered[:n_components], cmap=plt.cm.RdBu)\\n\\n# %%\\n# Similar to the previous examples, we change parameters and train\\n# :class:`~sklearn.decomposition.MiniBatchDictionaryLearning` estimator on all\\n# images. Generally, the dictionary learning and sparse encoding decompose\\n# input data into the dictionary and the coding coefficients matrices. :math:`X\\n# \\\\approx UV`, where :math:`X = [x_1, . . . , x_n]`, :math:`X \\\\in\\n# \\\\mathbb{R}^{m√ón}`, dictionary :math:`U \\\\in \\\\mathbb{R}^{m√ók}`, coding\\n# coefficients :math:`V \\\\in \\\\mathbb{R}^{k√ón}`.\\n#\\n# Also below are the results when the dictionary and coding\\n# coefficients are positively constrained.\\n\\n# %%\\n# Dictionary learning - positive dictionary\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# In the following section we enforce positivity when finding the dictionary.'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_faces_decomposition.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Dictionary learning - positive dictionary\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# In the following section we enforce positivity when finding the dictionary.\\n\\n# %%\\ndict_pos_dict_estimator = decomposition.MiniBatchDictionaryLearning(\\n    n_components=n_components,\\n    alpha=0.1,\\n    max_iter=50,\\n    batch_size=3,\\n    random_state=rng,\\n    positive_dict=True,\\n)\\ndict_pos_dict_estimator.fit(faces_centered)\\nplot_gallery(\\n    \"Dictionary learning - positive dictionary\",\\n    dict_pos_dict_estimator.components_[:n_components],\\n    cmap=plt.cm.RdBu,\\n)\\n\\n# %%\\n# Dictionary learning - positive code\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# Below we constrain the coding coefficients as a positive matrix.\\n\\n# %%\\ndict_pos_code_estimator = decomposition.MiniBatchDictionaryLearning(\\n    n_components=n_components,\\n    alpha=0.1,\\n    max_iter=50,\\n    batch_size=3,\\n    fit_algorithm=\"cd\",\\n    random_state=rng,\\n    positive_code=True,\\n)\\ndict_pos_code_estimator.fit(faces_centered)\\nplot_gallery(\\n    \"Dictionary learning - positive code\",\\n    dict_pos_code_estimator.components_[:n_components],\\n    cmap=plt.cm.RdBu,\\n)\\n\\n# %%\\n# Dictionary learning - positive dictionary & code\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# Also below are the results if the dictionary values and coding\\n# coefficients are positively constrained.\\n\\n# %%\\ndict_pos_estimator = decomposition.MiniBatchDictionaryLearning(\\n    n_components=n_components,\\n    alpha=0.1,\\n    max_iter=50,\\n    batch_size=3,\\n    fit_algorithm=\"cd\",\\n    random_state=rng,\\n    positive_dict=True,\\n    positive_code=True,\\n)\\ndict_pos_estimator.fit(faces_centered)\\nplot_gallery(\\n    \"Dictionary learning - positive dictionary & code\",\\n    dict_pos_estimator.components_[:n_components],\\n    cmap=plt.cm.RdBu,\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_image_denoising.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def show_with_diff(image, reference, title):\\n    \"\"\"Helper function to display denoising\"\"\"\\n    plt.figure(figsize=(5, 3.3))\\n    plt.subplot(1, 2, 1)\\n    plt.title(\"Image\")\\n    plt.imshow(image, vmin=0, vmax=1, cmap=plt.cm.gray, interpolation=\"nearest\")\\n    plt.xticks(())\\n    plt.yticks(())\\n    plt.subplot(1, 2, 2)\\n    difference = image - reference\\n\\n    plt.title(\"Difference (norm: %.2f)\" % np.sqrt(np.sum(difference**2)))\\n    plt.imshow(\\n        difference, vmin=-0.5, vmax=0.5, cmap=plt.cm.PuOr, interpolation=\"nearest\"\\n    )\\n    plt.xticks(())\\n    plt.yticks(())\\n    plt.suptitle(title, size=16)\\n    plt.subplots_adjust(0.02, 0.02, 0.98, 0.79, 0.02, 0.2)'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_image_denoising.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================\\nImage denoising using dictionary learning\\n=========================================\\n\\nAn example comparing the effect of reconstructing noisy fragments\\nof a raccoon face image using firstly online :ref:`DictionaryLearning` and\\nvarious transform methods.\\n\\nThe dictionary is fitted on the distorted left half of the image, and\\nsubsequently used to reconstruct the right half. Note that even better\\nperformance could be achieved by fitting to an undistorted (i.e.\\nnoiseless) image, but here we start from the assumption that it is not\\navailable.\\n\\nA common practice for evaluating the results of image denoising is by looking\\nat the difference between the reconstruction and the original image. If the\\nreconstruction is perfect this will look like Gaussian noise.\\n\\nIt can be seen from the plots that the results of :ref:`omp` with two\\nnon-zero coefficients is a bit less biased than when keeping only one\\n(the edges look less prominent). It is in addition closer from the ground\\ntruth in Frobenius norm.\\n\\nThe result of :ref:`least_angle_regression` is much more strongly biased: the\\ndifference is reminiscent of the local intensity value of the original image.\\n\\nThresholding is clearly not useful for denoising, but it is here to show that\\nit can produce a suggestive output with very high speed, and thus be useful\\nfor other tasks such as object classification, where performance is not\\nnecessarily related to visualisation.\\n\\n\"\"\"\\n\\n# %%\\n# Generate distorted image\\n# ------------------------\\nimport numpy as np\\n\\ntry:  # Scipy >= 1.10\\n    from scipy.datasets import face\\nexcept ImportError:\\n    from scipy.misc import face\\n\\nraccoon_face = face(gray=True)\\n\\n# Convert from uint8 representation with values between 0 and 255 to\\n# a floating point representation with values between 0 and 1.\\nraccoon_face = raccoon_face / 255.0\\n\\n# downsample for higher speed\\nraccoon_face = (\\n    raccoon_face[::4, ::4]\\n    + raccoon_face[1::4, ::4]\\n    + raccoon_face[::4, 1::4]\\n    + raccoon_face[1::4, 1::4]\\n)\\nraccoon_face /= 4.0\\nheight, width = raccoon_face.shape\\n\\n# Distort the right half of the image\\nprint(\"Distorting image...\")\\ndistorted = raccoon_face.copy()\\ndistorted[:, width // 2 :] += 0.075 * np.random.randn(height, width // 2)\\n\\n\\n# %%\\n# Display the distorted image\\n# ---------------------------\\nimport matplotlib.pyplot as plt\\n\\n\\n# Code for: def show_with_diff(image, reference, title):\\n\\n\\nshow_with_diff(distorted, raccoon_face, \"Distorted image\")\\n\\n\\n# %%\\n# Extract reference patches\\n# ----------------------------\\nfrom time import time\\n\\nfrom sklearn.feature_extraction.image import extract_patches_2d'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_image_denoising.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def show_with_diff(image, reference, title):\\n\\n\\nshow_with_diff(distorted, raccoon_face, \"Distorted image\")\\n\\n\\n# %%\\n# Extract reference patches\\n# ----------------------------\\nfrom time import time\\n\\nfrom sklearn.feature_extraction.image import extract_patches_2d\\n\\n# Extract all reference patches from the left half of the image\\nprint(\"Extracting reference patches...\")\\nt0 = time()\\npatch_size = (7, 7)\\ndata = extract_patches_2d(distorted[:, : width // 2], patch_size)\\ndata = data.reshape(data.shape[0], -1)\\ndata -= np.mean(data, axis=0)\\ndata /= np.std(data, axis=0)\\nprint(f\"{data.shape[0]} patches extracted in %.2fs.\" % (time() - t0))\\n\\n\\n# %%\\n# Learn the dictionary from reference patches\\n# -------------------------------------------\\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\\n\\nprint(\"Learning the dictionary...\")\\nt0 = time()\\ndico = MiniBatchDictionaryLearning(\\n    # increase to 300 for higher quality results at the cost of slower\\n    # training times.\\n    n_components=50,\\n    batch_size=200,\\n    alpha=1.0,\\n    max_iter=10,\\n)\\nV = dico.fit(data).components_\\ndt = time() - t0\\nprint(f\"{dico.n_iter_} iterations / {dico.n_steps_} steps in {dt:.2f}.\")\\n\\nplt.figure(figsize=(4.2, 4))\\nfor i, comp in enumerate(V[:100]):\\n    plt.subplot(10, 10, i + 1)\\n    plt.imshow(comp.reshape(patch_size), cmap=plt.cm.gray_r, interpolation=\"nearest\")\\n    plt.xticks(())\\n    plt.yticks(())\\nplt.suptitle(\\n    \"Dictionary learned from face patches\\\\n\"\\n    + \"Train time %.1fs on %d patches\" % (dt, len(data)),\\n    fontsize=16,\\n)\\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\\n\\n\\n# %%\\n# Extract noisy patches and reconstruct them using the dictionary\\n# ---------------------------------------------------------------\\nfrom sklearn.feature_extraction.image import reconstruct_from_patches_2d\\n\\nprint(\"Extracting noisy patches... \")\\nt0 = time()\\ndata = extract_patches_2d(distorted[:, width // 2 :], patch_size)\\ndata = data.reshape(data.shape[0], -1)\\nintercept = np.mean(data, axis=0)\\ndata -= intercept\\nprint(\"done in %.2fs.\" % (time() - t0))\\n\\ntransform_algorithms = [\\n    (\"Orthogonal Matching Pursuit\\\\n1 atom\", \"omp\", {\"transform_n_nonzero_coefs\": 1}),\\n    (\"Orthogonal Matching Pursuit\\\\n2 atoms\", \"omp\", {\"transform_n_nonzero_coefs\": 2}),\\n    (\"Least-angle regression\\\\n4 atoms\", \"lars\", {\"transform_n_nonzero_coefs\": 4}),\\n    (\"Thresholding\\\\n alpha=0.1\", \"threshold\", {\"transform_alpha\": 0.1}),\\n]\\n\\nreconstructions = {}\\nfor title, transform_algorithm, kwargs in transform_algorithms:\\n    print(title + \"...\")\\n    reconstructions[title] = raccoon_face.copy()\\n    t0 = time()\\n    dico.set_params(transform_algorithm=transform_algorithm, **kwargs)\\n    code = dico.transform(data)\\n    patches = np.dot(code, V)'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_image_denoising.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='patches += intercept\\n    patches = patches.reshape(len(data), *patch_size)\\n    if transform_algorithm == \"threshold\":\\n        patches -= patches.min()\\n        patches /= patches.max()\\n    reconstructions[title][:, width // 2 :] = reconstruct_from_patches_2d(\\n        patches, (height, width // 2)\\n    )\\n    dt = time() - t0\\n    print(\"done in %.2fs.\" % dt)\\n    show_with_diff(reconstructions[title], raccoon_face, title + \" (time: %.1fs)\" % dt)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_pca_vs_fa_model_selection.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def compute_scores(X):\\n    pca = PCA(svd_solver=\"full\")\\n    fa = FactorAnalysis()\\n\\n    pca_scores, fa_scores = [], []\\n    for n in n_components:\\n        pca.n_components = n\\n        fa.n_components = n\\n        pca_scores.append(np.mean(cross_val_score(pca, X)))\\n        fa_scores.append(np.mean(cross_val_score(fa, X)))\\n\\n    return pca_scores, fa_scores'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_pca_vs_fa_model_selection.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def shrunk_cov_score(X):\\n    shrinkages = np.logspace(-2, 0, 30)\\n    cv = GridSearchCV(ShrunkCovariance(), {\"shrinkage\": shrinkages})\\n    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_pca_vs_fa_model_selection.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def lw_score(X):\\n    return np.mean(cross_val_score(LedoitWolf(), X))'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_pca_vs_fa_model_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================================================\\nModel selection with Probabilistic PCA and Factor Analysis (FA)\\n===============================================================\\n\\nProbabilistic PCA and Factor Analysis are probabilistic models.\\nThe consequence is that the likelihood of new data can be used\\nfor model selection and covariance estimation.\\nHere we compare PCA and FA with cross-validation on low rank data corrupted\\nwith homoscedastic noise (noise variance\\nis the same for each feature) or heteroscedastic noise (noise variance\\nis the different for each feature). In a second step we compare the model\\nlikelihood to the likelihoods obtained from shrinkage covariance estimators.\\n\\nOne can observe that with homoscedastic noise both FA and PCA succeed\\nin recovering the size of the low rank subspace. The likelihood with PCA\\nis higher than FA in this case. However PCA fails and overestimates\\nthe rank when heteroscedastic noise is present. Under appropriate\\ncircumstances (choice of the number of components), the held-out\\ndata is more likely for low rank models than for shrinkage models.\\n\\nThe automatic estimation from\\nAutomatic Choice of Dimensionality for PCA. NIPS 2000: 598-604\\nby Thomas P. Minka is also compared.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Create the data\\n# ---------------\\n\\nimport numpy as np\\nfrom scipy import linalg\\n\\nn_samples, n_features, rank = 500, 25, 5\\nsigma = 1.0\\nrng = np.random.RandomState(42)\\nU, _, _ = linalg.svd(rng.randn(n_features, n_features))\\nX = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)\\n\\n# Adding homoscedastic noise\\nX_homo = X + sigma * rng.randn(n_samples, n_features)\\n\\n# Adding heteroscedastic noise\\nsigmas = sigma * rng.rand(n_features) + sigma / 2.0\\nX_hetero = X + rng.randn(n_samples, n_features) * sigmas\\n\\n# %%\\n# Fit the models\\n# --------------\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.covariance import LedoitWolf, ShrunkCovariance\\nfrom sklearn.decomposition import PCA, FactorAnalysis\\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\\n\\nn_components = np.arange(0, n_features, 5)  # options for n_components\\n\\n\\n# Code for: def compute_scores(X):\\n\\n\\n# Code for: def shrunk_cov_score(X):\\n\\n\\n# Code for: def lw_score(X):\\n\\n\\nfor X, title in [(X_homo, \"Homoscedastic Noise\"), (X_hetero, \"Heteroscedastic Noise\")]:\\n    pca_scores, fa_scores = compute_scores(X)\\n    n_components_pca = n_components[np.argmax(pca_scores)]\\n    n_components_fa = n_components[np.argmax(fa_scores)]\\n\\n    pca = PCA(svd_solver=\"full\", n_components=\"mle\")\\n    pca.fit(X)\\n    n_components_pca_mle = pca.n_components_\\n\\n    print(\"best n_components by PCA CV = %d\" % n_components_pca)\\n    print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\\n    print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_pca_vs_fa_model_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(\"best n_components by PCA CV = %d\" % n_components_pca)\\n    print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\\n    print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)\\n\\n    plt.figure()\\n    plt.plot(n_components, pca_scores, \"b\", label=\"PCA scores\")\\n    plt.plot(n_components, fa_scores, \"r\", label=\"FA scores\")\\n    plt.axvline(rank, color=\"g\", label=\"TRUTH: %d\" % rank, linestyle=\"-\")\\n    plt.axvline(\\n        n_components_pca,\\n        color=\"b\",\\n        label=\"PCA CV: %d\" % n_components_pca,\\n        linestyle=\"--\",\\n    )\\n    plt.axvline(\\n        n_components_fa,\\n        color=\"r\",\\n        label=\"FactorAnalysis CV: %d\" % n_components_fa,\\n        linestyle=\"--\",\\n    )\\n    plt.axvline(\\n        n_components_pca_mle,\\n        color=\"k\",\\n        label=\"PCA MLE: %d\" % n_components_pca_mle,\\n        linestyle=\"--\",\\n    )\\n\\n    # compare with other covariance estimators\\n    plt.axhline(\\n        shrunk_cov_score(X),\\n        color=\"violet\",\\n        label=\"Shrunk Covariance MLE\",\\n        linestyle=\"-.\",\\n    )\\n    plt.axhline(\\n        lw_score(X),\\n        color=\"orange\",\\n        label=\"LedoitWolf MLE\" % n_components_pca_mle,\\n        linestyle=\"-.\",\\n    )\\n\\n    plt.xlabel(\"nb of components\")\\n    plt.ylabel(\"CV scores\")\\n    plt.legend(loc=\"lower right\")\\n    plt.title(title)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/decomposition/plot_varimax_fa.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================================================\\nFactor Analysis (with rotation) to visualize patterns\\n===============================================================\\n\\nInvestigating the Iris dataset, we see that sepal length, petal\\nlength and petal width are highly correlated. Sepal width is\\nless redundant. Matrix decomposition techniques can uncover\\nthese latent patterns. Applying rotations to the resulting\\ncomponents does not inherently improve the predictive value\\nof the derived latent space, but can help visualise their\\nstructure; here, for example, the varimax rotation, which\\nis found by maximizing the squared variances of the weights,\\nfinds a structure where the second component only loads\\npositively on sepal width.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.decomposition import PCA, FactorAnalysis\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# %%\\n# Load Iris data\\ndata = load_iris()\\nX = StandardScaler().fit_transform(data[\"data\"])\\nfeature_names = data[\"feature_names\"]\\n\\n# %%\\n# Plot covariance of Iris features\\nax = plt.axes()\\n\\nim = ax.imshow(np.corrcoef(X.T), cmap=\"RdBu_r\", vmin=-1, vmax=1)\\n\\nax.set_xticks([0, 1, 2, 3])\\nax.set_xticklabels(list(feature_names), rotation=90)\\nax.set_yticks([0, 1, 2, 3])\\nax.set_yticklabels(list(feature_names))\\n\\nplt.colorbar(im).ax.set_ylabel(\"$r$\", rotation=0)\\nax.set_title(\"Iris feature correlation matrix\")\\nplt.tight_layout()\\n\\n# %%\\n# Run factor analysis with Varimax rotation\\nn_comps = 2\\n\\nmethods = [\\n    (\"PCA\", PCA()),\\n    (\"Unrotated FA\", FactorAnalysis()),\\n    (\"Varimax FA\", FactorAnalysis(rotation=\"varimax\")),\\n]\\nfig, axes = plt.subplots(ncols=len(methods), figsize=(10, 8), sharey=True)\\n\\nfor ax, (method, fa) in zip(axes, methods):\\n    fa.set_params(n_components=n_comps)\\n    fa.fit(X)\\n\\n    components = fa.components_.T\\n    print(\"\\\\n\\\\n %s :\\\\n\" % method)\\n    print(components)\\n\\n    vmax = np.abs(components).max()\\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\\n    ax.set_yticks(np.arange(len(feature_names)))\\n    ax.set_yticklabels(feature_names)\\n    ax.set_title(str(method))\\n    ax.set_xticks([0, 1])\\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\\nfig.suptitle(\"Factors\")\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_digits.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def bench_k_means(kmeans, name, data, labels):\\n    \"\"\"Benchmark to evaluate the KMeans initialization methods.\\n\\n    Parameters\\n    ----------\\n    kmeans : KMeans instance\\n        A :class:`~sklearn.cluster.KMeans` instance with the initialization\\n        already set.\\n    name : str\\n        Name given to the strategy. It will be used to show the results in a\\n        table.\\n    data : ndarray of shape (n_samples, n_features)\\n        The data to cluster.\\n    labels : ndarray of shape (n_samples,)\\n        The labels used to compute the clustering metrics which requires some\\n        supervision.\\n    \"\"\"\\n    t0 = time()\\n    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\\n    fit_time = time() - t0\\n    results = [name, fit_time, estimator[-1].inertia_]\\n\\n    # Define the metrics which require only the true labels and estimator\\n    # labels\\n    clustering_metrics = [\\n        metrics.homogeneity_score,\\n        metrics.completeness_score,\\n        metrics.v_measure_score,\\n        metrics.adjusted_rand_score,\\n        metrics.adjusted_mutual_info_score,\\n    ]\\n    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\\n\\n    # The silhouette score requires the full dataset\\n    results += [\\n        metrics.silhouette_score(\\n            data,\\n            estimator[-1].labels_,\\n            metric=\"euclidean\",\\n            sample_size=300,\\n        )\\n    ]\\n\\n    # Show the results\\n    formatter_result = (\\n        \"{:9s}\\\\t{:.3f}s\\\\t{:.0f}\\\\t{:.3f}\\\\t{:.3f}\\\\t{:.3f}\\\\t{:.3f}\\\\t{:.3f}\\\\t{:.3f}\"\\n    )\\n    print(formatter_result.format(*results))'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_digits.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================================\\nA demo of K-Means clustering on the handwritten digits data\\n===========================================================\\n\\nIn this example we compare the various initialization strategies for K-means in\\nterms of runtime and quality of the results.\\n\\nAs the ground truth is known here, we also apply different cluster quality\\nmetrics to judge the goodness of fit of the cluster labels to the ground truth.\\n\\nCluster quality metrics evaluated (see :ref:`clustering_evaluation` for\\ndefinitions and discussions of the metrics):\\n\\n=========== ========================================================\\nShorthand    full name\\n=========== ========================================================\\nhomo         homogeneity score\\ncompl        completeness score\\nv-meas       V measure\\nARI          adjusted Rand index\\nAMI          adjusted mutual information\\nsilhouette   silhouette coefficient\\n=========== ========================================================\\n\\n\"\"\"\\n\\n# %%\\n# Load the dataset\\n# ----------------\\n#\\n# We will start by loading the `digits` dataset. This dataset contains\\n# handwritten digits from 0 to 9. In the context of clustering, one would like\\n# to group images such that the handwritten digits on the image are the same.\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_digits\\n\\ndata, labels = load_digits(return_X_y=True)\\n(n_samples, n_features), n_digits = data.shape, np.unique(labels).size\\n\\nprint(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")\\n\\n# %%\\n# Define our evaluation benchmark\\n# -------------------------------\\n#\\n# We will first our evaluation benchmark. During this benchmark, we intend to\\n# compare different initialization methods for KMeans. Our benchmark will:\\n#\\n# * create a pipeline which will scale the data using a\\n#   :class:`~sklearn.preprocessing.StandardScaler`;\\n# * train and time the pipeline fitting;\\n# * measure the performance of the clustering obtained via different metrics.\\nfrom time import time\\n\\nfrom sklearn import metrics\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\n# Code for: def bench_k_means(kmeans, name, data, labels):\\n\\n\\n# %%\\n# Run the benchmark\\n# -----------------\\n#\\n# We will compare three approaches:\\n#\\n# * an initialization using `k-means++`. This method is stochastic and we will\\n#   run the initialization 4 times;\\n# * a random initialization. This method is stochastic as well and we will run\\n#   the initialization 4 times;\\n# * an initialization based on a :class:`~sklearn.decomposition.PCA`\\n#   projection. Indeed, we will use the components of the\\n#   :class:`~sklearn.decomposition.PCA` to initialize KMeans. This method is\\n#   deterministic and a single initialization suffice.\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.decomposition import PCA\\n\\nprint(82 * \"_\")\\nprint(\"init\\\\t\\\\ttime\\\\tinertia\\\\thomo\\\\tcompl\\\\tv-meas\\\\tARI\\\\tAMI\\\\tsilhouette\")'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_digits.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(82 * \"_\")\\nprint(\"init\\\\t\\\\ttime\\\\tinertia\\\\thomo\\\\tcompl\\\\tv-meas\\\\tARI\\\\tAMI\\\\tsilhouette\")\\n\\nkmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4, random_state=0)\\nbench_k_means(kmeans=kmeans, name=\"k-means++\", data=data, labels=labels)\\n\\nkmeans = KMeans(init=\"random\", n_clusters=n_digits, n_init=4, random_state=0)\\nbench_k_means(kmeans=kmeans, name=\"random\", data=data, labels=labels)\\n\\npca = PCA(n_components=n_digits).fit(data)\\nkmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)\\nbench_k_means(kmeans=kmeans, name=\"PCA-based\", data=data, labels=labels)\\n\\nprint(82 * \"_\")\\n\\n# %%\\n# Visualize the results on PCA-reduced data\\n# -----------------------------------------\\n#\\n# :class:`~sklearn.decomposition.PCA` allows to project the data from the\\n# original 64-dimensional space into a lower dimensional space. Subsequently,\\n# we can use :class:`~sklearn.decomposition.PCA` to project into a\\n# 2-dimensional space and plot the data and the clusters in this new space.\\nimport matplotlib.pyplot as plt\\n\\nreduced_data = PCA(n_components=2).fit_transform(data)\\nkmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4)\\nkmeans.fit(reduced_data)\\n\\n# Step size of the mesh. Decrease to increase the quality of the VQ.\\nh = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\\n\\n# Plot the decision boundary. For that, we will assign a color to each\\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n\\n# Obtain labels for each point in mesh. Use last trained model.\\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\\n\\n# Put the result into a color plot\\nZ = Z.reshape(xx.shape)\\nplt.figure(1)\\nplt.clf()\\nplt.imshow(\\n    Z,\\n    interpolation=\"nearest\",\\n    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\\n    cmap=plt.cm.Paired,\\n    aspect=\"auto\",\\n    origin=\"lower\",\\n)\\n\\nplt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\\n# Plot the centroids as a white X\\ncentroids = kmeans.cluster_centers_\\nplt.scatter(\\n    centroids[:, 0],\\n    centroids[:, 1],\\n    marker=\"x\",\\n    s=169,\\n    linewidths=3,\\n    color=\"w\",\\n    zorder=10,\\n)\\nplt.title(\\n    \"K-means clustering on the digits dataset (PCA-reduced data)\\\\n\"\\n    \"Centroids are marked with white cross\"\\n)\\nplt.xlim(x_min, x_max)\\nplt.ylim(y_min, y_max)\\nplt.xticks(())\\nplt.yticks(())\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_dbscan.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================\\nDemo of DBSCAN clustering algorithm\\n===================================\\n\\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds core\\nsamples in regions of high density and expands clusters from them. This\\nalgorithm is good for data which contains clusters of similar density.\\n\\nSee the :ref:`sphx_glr_auto_examples_cluster_plot_cluster_comparison.py` example\\nfor a demo of different clustering algorithms on 2D datasets.\\n\\n\"\"\"\\n\\n# %%\\n# Data generation\\n# ---------------\\n#\\n# We use :class:`~sklearn.datasets.make_blobs` to create 3 synthetic clusters.\\n\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.preprocessing import StandardScaler\\n\\ncenters = [[1, 1], [-1, -1], [1, -1]]\\nX, labels_true = make_blobs(\\n    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\\n)\\n\\nX = StandardScaler().fit_transform(X)\\n\\n# %%\\n# We can visualize the resulting data:\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.scatter(X[:, 0], X[:, 1])\\nplt.show()\\n\\n# %%\\n# Compute DBSCAN\\n# --------------\\n#\\n# One can access the labels assigned by :class:`~sklearn.cluster.DBSCAN` using\\n# the `labels_` attribute. Noisy samples are given the label math:`-1`.\\n\\nimport numpy as np\\n\\nfrom sklearn import metrics\\nfrom sklearn.cluster import DBSCAN\\n\\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\\nlabels = db.labels_\\n\\n# Number of clusters in labels, ignoring noise if present.\\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\\nn_noise_ = list(labels).count(-1)\\n\\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\\nprint(\"Estimated number of noise points: %d\" % n_noise_)\\n\\n# %%\\n# Clustering algorithms are fundamentally unsupervised learning methods.\\n# However, since :class:`~sklearn.datasets.make_blobs` gives access to the true\\n# labels of the synthetic clusters, it is possible to use evaluation metrics\\n# that leverage this \"supervised\" ground truth information to quantify the\\n# quality of the resulting clusters. Examples of such metrics are the\\n# homogeneity, completeness, V-measure, Rand-Index, Adjusted Rand-Index and\\n# Adjusted Mutual Information (AMI).\\n#\\n# If the ground truth labels are not known, evaluation can only be performed\\n# using the model results itself. In that case, the Silhouette Coefficient comes\\n# in handy.\\n#\\n# For more information, see the\\n# :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`\\n# example or the :ref:`clustering_evaluation` module.\\n\\nprint(f\"Homogeneity: {metrics.homogeneity_score(labels_true, labels):.3f}\")\\nprint(f\"Completeness: {metrics.completeness_score(labels_true, labels):.3f}\")\\nprint(f\"V-measure: {metrics.v_measure_score(labels_true, labels):.3f}\")\\nprint(f\"Adjusted Rand Index: {metrics.adjusted_rand_score(labels_true, labels):.3f}\")\\nprint(\\n    \"Adjusted Mutual Information:\"\\n    f\" {metrics.adjusted_mutual_info_score(labels_true, labels):.3f}\"\\n)\\nprint(f\"Silhouette Coefficient: {metrics.silhouette_score(X, labels):.3f}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_dbscan.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Plot results\\n# ------------\\n#\\n# Core samples (large dots) and non-core samples (small dots) are color-coded\\n# according to the assigned cluster. Samples tagged as noise are represented in\\n# black.\\n\\nunique_labels = set(labels)\\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\\ncore_samples_mask[db.core_sample_indices_] = True\\n\\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\\nfor k, col in zip(unique_labels, colors):\\n    if k == -1:\\n        # Black used for noise.\\n        col = [0, 0, 0, 1]\\n\\n    class_member_mask = labels == k\\n\\n    xy = X[class_member_mask & core_samples_mask]\\n    plt.plot(\\n        xy[:, 0],\\n        xy[:, 1],\\n        \"o\",\\n        markerfacecolor=tuple(col),\\n        markeredgecolor=\"k\",\\n        markersize=14,\\n    )\\n\\n    xy = X[class_member_mask & ~core_samples_mask]\\n    plt.plot(\\n        xy[:, 0],\\n        xy[:, 1],\\n        \"o\",\\n        markerfacecolor=tuple(col),\\n        markeredgecolor=\"k\",\\n        markersize=6,\\n    )\\n\\nplt.title(f\"Estimated number of clusters: {n_clusters_}\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_ward_structured_vs_unstructured.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================================\\nHierarchical clustering: structured vs unstructured ward\\n===========================================================\\n\\nExample builds a swiss roll dataset and runs\\nhierarchical clustering on their position.\\n\\nFor more information, see :ref:`hierarchical_clustering`.\\n\\nIn a first step, the hierarchical clustering is performed without connectivity\\nconstraints on the structure and is solely based on distance, whereas in\\na second step the clustering is restricted to the k-Nearest Neighbors\\ngraph: it\\'s a hierarchical clustering with structure prior.\\n\\nSome of the clusters learned without connectivity constraints do not\\nrespect the structure of the swiss roll and extend across different folds of\\nthe manifolds. On the opposite, when opposing connectivity constraints,\\nthe clusters form a nice parcellation of the swiss roll.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport time as time\\n\\n# The following import is required\\n# for 3D projection to work with matplotlib < 3.2\\nimport mpl_toolkits.mplot3d  # noqa: F401\\nimport numpy as np\\n\\n# %%\\n# Generate data\\n# -------------\\n#\\n# We start by generating the Swiss Roll dataset.\\nfrom sklearn.datasets import make_swiss_roll\\n\\nn_samples = 1500\\nnoise = 0.05\\nX, _ = make_swiss_roll(n_samples, noise=noise)\\n# Make it thinner\\nX[:, 1] *= 0.5\\n\\n# %%\\n# Compute clustering\\n# ------------------\\n#\\n# We perform AgglomerativeClustering which comes under Hierarchical Clustering\\n# without any connectivity constraints.\\n\\nfrom sklearn.cluster import AgglomerativeClustering\\n\\nprint(\"Compute unstructured hierarchical clustering...\")\\nst = time.time()\\nward = AgglomerativeClustering(n_clusters=6, linkage=\"ward\").fit(X)\\nelapsed_time = time.time() - st\\nlabel = ward.labels_\\nprint(f\"Elapsed time: {elapsed_time:.2f}s\")\\nprint(f\"Number of points: {label.size}\")\\n\\n# %%\\n# Plot result\\n# -----------\\n# Plotting the unstructured hierarchical clusters.\\n\\nimport matplotlib.pyplot as plt\\n\\nfig1 = plt.figure()\\nax1 = fig1.add_subplot(111, projection=\"3d\", elev=7, azim=-80)\\nax1.set_position([0, 0, 0.95, 1])\\nfor l in np.unique(label):\\n    ax1.scatter(\\n        X[label == l, 0],\\n        X[label == l, 1],\\n        X[label == l, 2],\\n        color=plt.cm.jet(float(l) / np.max(label + 1)),\\n        s=20,\\n        edgecolor=\"k\",\\n    )\\n_ = fig1.suptitle(f\"Without connectivity constraints (time {elapsed_time:.2f}s)\")\\n\\n# %%\\n# We are defining k-Nearest Neighbors with 10 neighbors\\n# -----------------------------------------------------\\n\\nfrom sklearn.neighbors import kneighbors_graph\\n\\nconnectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)\\n\\n# %%\\n# Compute clustering\\n# ------------------\\n#\\n# We perform AgglomerativeClustering again with connectivity constraints.'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_ward_structured_vs_unstructured.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.neighbors import kneighbors_graph\\n\\nconnectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)\\n\\n# %%\\n# Compute clustering\\n# ------------------\\n#\\n# We perform AgglomerativeClustering again with connectivity constraints.\\n\\nprint(\"Compute structured hierarchical clustering...\")\\nst = time.time()\\nward = AgglomerativeClustering(\\n    n_clusters=6, connectivity=connectivity, linkage=\"ward\"\\n).fit(X)\\nelapsed_time = time.time() - st\\nlabel = ward.labels_\\nprint(f\"Elapsed time: {elapsed_time:.2f}s\")\\nprint(f\"Number of points: {label.size}\")\\n\\n# %%\\n# Plot result\\n# -----------\\n#\\n# Plotting the structured hierarchical clusters.\\n\\nfig2 = plt.figure()\\nax2 = fig2.add_subplot(121, projection=\"3d\", elev=7, azim=-80)\\nax2.set_position([0, 0, 0.95, 1])\\nfor l in np.unique(label):\\n    ax2.scatter(\\n        X[label == l, 0],\\n        X[label == l, 1],\\n        X[label == l, 2],\\n        color=plt.cm.jet(float(l) / np.max(label + 1)),\\n        s=20,\\n        edgecolor=\"k\",\\n    )\\nfig2.suptitle(f\"With connectivity constraints (time {elapsed_time:.2f}s)\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_digits_agglomeration.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nFeature agglomeration\\n=========================================================\\n\\nThese images show how similar features are merged together using\\nfeature agglomeration.\\n\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# Modified for documentation by Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import cluster, datasets\\nfrom sklearn.feature_extraction.image import grid_to_graph\\n\\ndigits = datasets.load_digits()\\nimages = digits.images\\nX = np.reshape(images, (len(images), -1))\\nconnectivity = grid_to_graph(*images[0].shape)\\n\\nagglo = cluster.FeatureAgglomeration(connectivity=connectivity, n_clusters=32)\\n\\nagglo.fit(X)\\nX_reduced = agglo.transform(X)\\n\\nX_restored = agglo.inverse_transform(X_reduced)\\nimages_restored = np.reshape(X_restored, images.shape)\\nplt.figure(1, figsize=(4, 3.5))\\nplt.clf()\\nplt.subplots_adjust(left=0.01, right=0.99, bottom=0.01, top=0.91)\\nfor i in range(4):\\n    plt.subplot(3, 4, i + 1)\\n    plt.imshow(images[i], cmap=plt.cm.gray, vmax=16, interpolation=\"nearest\")\\n    plt.xticks(())\\n    plt.yticks(())\\n    if i == 1:\\n        plt.title(\"Original data\")\\n    plt.subplot(3, 4, 4 + i + 1)\\n    plt.imshow(images_restored[i], cmap=plt.cm.gray, vmax=16, interpolation=\"nearest\")\\n    if i == 1:\\n        plt.title(\"Agglomerated data\")\\n    plt.xticks(())\\n    plt.yticks(())\\n\\nplt.subplot(3, 4, 10)\\nplt.imshow(\\n    np.reshape(agglo.labels_, images[0].shape),\\n    interpolation=\"nearest\",\\n    cmap=plt.cm.nipy_spectral,\\n)\\nplt.xticks(())\\nplt.yticks(())\\nplt.title(\"Labels\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_stability_low_dim_dense.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def make_data(random_state, n_samples_per_center, grid_size, scale):\\n    random_state = check_random_state(random_state)\\n    centers = np.array([[i, j] for i in range(grid_size) for j in range(grid_size)])\\n    n_clusters_true, n_features = centers.shape\\n\\n    noise = random_state.normal(\\n        scale=scale, size=(n_samples_per_center, centers.shape[1])\\n    )\\n\\n    X = np.concatenate([c + noise for c in centers])\\n    y = np.concatenate([[i] * n_samples_per_center for i in range(n_clusters_true)])\\n    return shuffle(X, y, random_state=random_state)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_stability_low_dim_dense.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================================================\\nEmpirical evaluation of the impact of k-means initialization\\n============================================================\\n\\nEvaluate the ability of k-means initializations strategies to make\\nthe algorithm convergence robust, as measured by the relative standard\\ndeviation of the inertia of the clustering (i.e. the sum of squared\\ndistances to the nearest cluster center).\\n\\nThe first plot shows the best inertia reached for each combination\\nof the model (``KMeans`` or ``MiniBatchKMeans``), and the init method\\n(``init=\"random\"`` or ``init=\"k-means++\"``) for increasing values of the\\n``n_init`` parameter that controls the number of initializations.\\n\\nThe second plot demonstrates one single run of the ``MiniBatchKMeans``\\nestimator using a ``init=\"random\"`` and ``n_init=1``. This run leads to\\na bad convergence (local optimum), with estimated centers stuck\\nbetween ground truth clusters.\\n\\nThe dataset used for evaluation is a 2D grid of isotropic Gaussian\\nclusters widely spaced.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.cm as cm\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\\nfrom sklearn.utils import check_random_state, shuffle\\n\\nrandom_state = np.random.RandomState(0)\\n\\n# Number of run (with randomly generated dataset) for each strategy so as\\n# to be able to compute an estimate of the standard deviation\\nn_runs = 5\\n\\n# k-means models can do several random inits so as to be able to trade\\n# CPU time for convergence robustness\\nn_init_range = np.array([1, 5, 10, 15, 20])\\n\\n# Datasets generation parameters\\nn_samples_per_center = 100\\ngrid_size = 3\\nscale = 0.1\\nn_clusters = grid_size**2\\n\\n\\n# Code for: def make_data(random_state, n_samples_per_center, grid_size, scale):\\n\\n\\n# Part 1: Quantitative evaluation of various init methods\\n\\n\\nplt.figure()\\nplots = []\\nlegends = []\\n\\ncases = [\\n    (KMeans, \"k-means++\", {}, \"^-\"),\\n    (KMeans, \"random\", {}, \"o-\"),\\n    (MiniBatchKMeans, \"k-means++\", {\"max_no_improvement\": 3}, \"x-\"),\\n    (MiniBatchKMeans, \"random\", {\"max_no_improvement\": 3, \"init_size\": 500}, \"d-\"),\\n]\\n\\nfor factory, init, params, format in cases:\\n    print(\"Evaluation of %s with %s init\" % (factory.__name__, init))\\n    inertia = np.empty((len(n_init_range), n_runs))\\n\\n    for run_id in range(n_runs):\\n        X, y = make_data(run_id, n_samples_per_center, grid_size, scale)\\n        for i, n_init in enumerate(n_init_range):\\n            km = factory(\\n                n_clusters=n_clusters,\\n                init=init,\\n                random_state=run_id,\\n                n_init=n_init,\\n                **params,\\n            ).fit(X)\\n            inertia[i, run_id] = km.inertia_\\n    p = plt.errorbar(\\n        n_init_range, inertia.mean(axis=1), inertia.std(axis=1), fmt=format\\n    )\\n    plots.append(p[0])\\n    legends.append(\"%s with %s init\" % (factory.__name__, init))'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_stability_low_dim_dense.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.xlabel(\"n_init\")\\nplt.ylabel(\"inertia\")\\nplt.legend(plots, legends)\\nplt.title(\"Mean inertia for various k-means init across %d runs\" % n_runs)\\n\\n# Part 2: Qualitative visual inspection of the convergence\\n\\nX, y = make_data(random_state, n_samples_per_center, grid_size, scale)\\nkm = MiniBatchKMeans(\\n    n_clusters=n_clusters, init=\"random\", n_init=1, random_state=random_state\\n).fit(X)\\n\\nplt.figure()\\nfor k in range(n_clusters):\\n    my_members = km.labels_ == k\\n    color = cm.nipy_spectral(float(k) / n_clusters, 1)\\n    plt.plot(X[my_members, 0], X[my_members, 1], \".\", c=color)\\n    cluster_center = km.cluster_centers_[k]\\n    plt.plot(\\n        cluster_center[0],\\n        cluster_center[1],\\n        \"o\",\\n        markerfacecolor=color,\\n        markeredgecolor=\"k\",\\n        markersize=6,\\n    )\\n    plt.title(\\n        \"Example cluster allocation with a single random init\\\\nwith MiniBatchKMeans\"\\n    )\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_coin_ward_segmentation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================================================\\nA demo of structured Ward hierarchical clustering on an image of coins\\n======================================================================\\n\\nCompute the segmentation of a 2D image with Ward hierarchical\\nclustering. The clustering is spatially constrained in order\\nfor each segmented region to be in one piece.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate data\\n# -------------\\n\\nfrom skimage.data import coins\\n\\norig_coins = coins()\\n\\n# %%\\n# Resize it to 20% of the original size to speed up the processing\\n# Applying a Gaussian filter for smoothing prior to down-scaling\\n# reduces aliasing artifacts.\\n\\nimport numpy as np\\nfrom scipy.ndimage import gaussian_filter\\nfrom skimage.transform import rescale\\n\\nsmoothened_coins = gaussian_filter(orig_coins, sigma=2)\\nrescaled_coins = rescale(\\n    smoothened_coins,\\n    0.2,\\n    mode=\"reflect\",\\n    anti_aliasing=False,\\n)\\n\\nX = np.reshape(rescaled_coins, (-1, 1))\\n\\n# %%\\n# Define structure of the data\\n# ----------------------------\\n#\\n# Pixels are connected to their neighbors.\\n\\nfrom sklearn.feature_extraction.image import grid_to_graph\\n\\nconnectivity = grid_to_graph(*rescaled_coins.shape)\\n\\n# %%\\n# Compute clustering\\n# ------------------\\n\\nimport time as time\\n\\nfrom sklearn.cluster import AgglomerativeClustering\\n\\nprint(\"Compute structured hierarchical clustering...\")\\nst = time.time()\\nn_clusters = 27  # number of regions\\nward = AgglomerativeClustering(\\n    n_clusters=n_clusters, linkage=\"ward\", connectivity=connectivity\\n)\\nward.fit(X)\\nlabel = np.reshape(ward.labels_, rescaled_coins.shape)\\nprint(f\"Elapsed time: {time.time() - st:.3f}s\")\\nprint(f\"Number of pixels: {label.size}\")\\nprint(f\"Number of clusters: {np.unique(label).size}\")\\n\\n# %%\\n# Plot the results on an image\\n# ----------------------------\\n#\\n# Agglomerative clustering is able to segment each coin however, we have had to\\n# use a ``n_cluster`` larger than the number of coins because the segmentation\\n# is finding a large in the background.\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.figure(figsize=(5, 5))\\nplt.imshow(rescaled_coins, cmap=plt.cm.gray)\\nfor l in range(n_clusters):\\n    plt.contour(\\n        label == l,\\n        colors=[\\n            plt.cm.nipy_spectral(l / float(n_clusters)),\\n        ],\\n    )\\nplt.axis(\"off\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_coin_segmentation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================================\\nSegmenting the picture of greek coins in regions\\n================================================\\n\\nThis example uses :ref:`spectral_clustering` on a graph created from\\nvoxel-to-voxel difference on an image to break this image into multiple\\npartly-homogeneous regions.\\n\\nThis procedure (spectral clustering on an image) is an efficient\\napproximate solution for finding normalized graph cuts.\\n\\nThere are three options to assign labels:\\n\\n* \\'kmeans\\' spectral clustering clusters samples in the embedding space\\n  using a kmeans algorithm\\n* \\'discrete\\' iteratively searches for the closest partition\\n  space to the embedding space of spectral clustering.\\n* \\'cluster_qr\\' assigns labels using the QR factorization with pivoting\\n  that directly determines the partition in the embedding space.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom scipy.ndimage import gaussian_filter\\nfrom skimage.data import coins\\nfrom skimage.transform import rescale\\n\\nfrom sklearn.cluster import spectral_clustering\\nfrom sklearn.feature_extraction import image\\n\\n# load the coins as a numpy array\\norig_coins = coins()\\n\\n# Resize it to 20% of the original size to speed up the processing\\n# Applying a Gaussian filter for smoothing prior to down-scaling\\n# reduces aliasing artifacts.\\nsmoothened_coins = gaussian_filter(orig_coins, sigma=2)\\nrescaled_coins = rescale(smoothened_coins, 0.2, mode=\"reflect\", anti_aliasing=False)\\n\\n# Convert the image into a graph with the value of the gradient on the\\n# edges.\\ngraph = image.img_to_graph(rescaled_coins)\\n\\n# Take a decreasing function of the gradient: an exponential\\n# The smaller beta is, the more independent the segmentation is of the\\n# actual image. For beta=1, the segmentation is close to a voronoi\\nbeta = 10\\neps = 1e-6\\ngraph.data = np.exp(-beta * graph.data / graph.data.std()) + eps\\n\\n# The number of segmented regions to display needs to be chosen manually.\\n# The current version of \\'spectral_clustering\\' does not support determining\\n# the number of good quality clusters automatically.\\nn_regions = 26\\n\\n# %%\\n# Compute and visualize the resulting regions\\n\\n# Computing a few extra eigenvectors may speed up the eigen_solver.\\n# The spectral clustering quality may also benefit from requesting\\n# extra regions for segmentation.\\nn_regions_plus = 3'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_coin_segmentation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Compute and visualize the resulting regions\\n\\n# Computing a few extra eigenvectors may speed up the eigen_solver.\\n# The spectral clustering quality may also benefit from requesting\\n# extra regions for segmentation.\\nn_regions_plus = 3\\n\\n# Apply spectral clustering using the default eigen_solver=\\'arpack\\'.\\n# Any implemented solver can be used: eigen_solver=\\'arpack\\', \\'lobpcg\\', or \\'amg\\'.\\n# Choosing eigen_solver=\\'amg\\' requires an extra package called \\'pyamg\\'.\\n# The quality of segmentation and the speed of calculations is mostly determined\\n# by the choice of the solver and the value of the tolerance \\'eigen_tol\\'.\\n# TODO: varying eigen_tol seems to have no effect for \\'lobpcg\\' and \\'amg\\' #21243.\\nfor assign_labels in (\"kmeans\", \"discretize\", \"cluster_qr\"):\\n    t0 = time.time()\\n    labels = spectral_clustering(\\n        graph,\\n        n_clusters=(n_regions + n_regions_plus),\\n        eigen_tol=1e-7,\\n        assign_labels=assign_labels,\\n        random_state=42,\\n    )\\n\\n    t1 = time.time()\\n    labels = labels.reshape(rescaled_coins.shape)\\n    plt.figure(figsize=(5, 5))\\n    plt.imshow(rescaled_coins, cmap=plt.cm.gray)\\n\\n    plt.xticks(())\\n    plt.yticks(())\\n    title = \"Spectral clustering: %s, %.2fs\" % (assign_labels, (t1 - t0))\\n    print(title)\\n    plt.title(title)\\n    for l in range(n_regions):\\n        colors = [plt.cm.nipy_spectral((l + 4) / float(n_regions + 4))]\\n        plt.contour(labels == l, colors=colors)\\n        # To view individual segments as appear comment in plt.pause(0.5)\\nplt.show()\\n\\n# TODO: After #21194 is merged and #21243 is fixed, check which eigen_solver\\n# is the best and set eigen_solver=\\'arpack\\', \\'lobpcg\\', or \\'amg\\' and eigen_tol\\n# explicitly in this example.'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_adjusted_for_chance_measures.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def random_labels(n_samples, n_classes):\\n    return rng.randint(low=0, high=n_classes, size=n_samples)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_adjusted_for_chance_measures.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def fixed_classes_uniform_labelings_scores(\\n    score_func, n_samples, n_clusters_range, n_classes, n_runs=5\\n):\\n    scores = np.zeros((len(n_clusters_range), n_runs))\\n    labels_a = random_labels(n_samples=n_samples, n_classes=n_classes)\\n\\n    for i, n_clusters in enumerate(n_clusters_range):\\n        for j in range(n_runs):\\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\\n            scores[i, j] = score_func(labels_a, labels_b)\\n    return scores'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_adjusted_for_chance_measures.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\\n    scores = np.zeros((len(n_clusters_range), n_runs))\\n\\n    for i, n_clusters in enumerate(n_clusters_range):\\n        for j in range(n_runs):\\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\\n            scores[i, j] = score_func(labels_a, labels_b)\\n    return scores'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_adjusted_for_chance_measures.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================================================\\nAdjustment for chance in clustering performance evaluation\\n==========================================================\\nThis notebook explores the impact of uniformly-distributed random labeling on\\nthe behavior of some clustering evaluation metrics. For such purpose, the\\nmetrics are computed with a fixed number of samples and as a function of the number\\nof clusters assigned by the estimator. The example is divided into two\\nexperiments:\\n\\n- a first experiment with fixed \"ground truth labels\" (and therefore fixed\\n  number of classes) and randomly \"predicted labels\";\\n- a second experiment with varying \"ground truth labels\", randomly \"predicted\\n  labels\". The \"predicted labels\" have the same number of classes and clusters\\n  as the \"ground truth labels\".\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Defining the list of metrics to evaluate\\n# ----------------------------------------\\n#\\n# Clustering algorithms are fundamentally unsupervised learning methods.\\n# However, since we assign class labels for the synthetic clusters in this\\n# example, it is possible to use evaluation metrics that leverage this\\n# \"supervised\" ground truth information to quantify the quality of the resulting\\n# clusters. Examples of such metrics are the following:\\n#\\n# - V-measure, the harmonic mean of completeness and homogeneity;\\n#\\n# - Rand index, which measures how frequently pairs of data points are grouped\\n#   consistently according to the result of the clustering algorithm and the\\n#   ground truth class assignment;\\n#\\n# - Adjusted Rand index (ARI), a chance-adjusted Rand index such that a random\\n#   cluster assignment has an ARI of 0.0 in expectation;\\n#\\n# - Mutual Information (MI) is an information theoretic measure that quantifies\\n#   how dependent are the two labelings. Note that the maximum value of MI for\\n#   perfect labelings depends on the number of clusters and samples;\\n#\\n# - Normalized Mutual Information (NMI), a Mutual Information defined between 0\\n#   (no mutual information) in the limit of large number of data points and 1\\n#   (perfectly matching label assignments, up to a permutation of the labels).\\n#   It is not adjusted for chance: then the number of clustered data points is\\n#   not large enough, the expected values of MI or NMI for random labelings can\\n#   be significantly non-zero;\\n#\\n# - Adjusted Mutual Information (AMI), a chance-adjusted Mutual Information.\\n#   Similarly to ARI, random cluster assignment has an AMI of 0.0 in\\n#   expectation.\\n#\\n# For more information, see the :ref:`clustering_evaluation` module.\\n\\nfrom sklearn import metrics\\n\\nscore_funcs = [\\n    (\"V-measure\", metrics.v_measure_score),\\n    (\"Rand index\", metrics.rand_score),\\n    (\"ARI\", metrics.adjusted_rand_score),\\n    (\"MI\", metrics.mutual_info_score),\\n    (\"NMI\", metrics.normalized_mutual_info_score),\\n    (\"AMI\", metrics.adjusted_mutual_info_score),\\n]'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_adjusted_for_chance_measures.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='score_funcs = [\\n    (\"V-measure\", metrics.v_measure_score),\\n    (\"Rand index\", metrics.rand_score),\\n    (\"ARI\", metrics.adjusted_rand_score),\\n    (\"MI\", metrics.mutual_info_score),\\n    (\"NMI\", metrics.normalized_mutual_info_score),\\n    (\"AMI\", metrics.adjusted_mutual_info_score),\\n]\\n\\n# %%\\n# First experiment: fixed ground truth labels and growing number of clusters\\n# --------------------------------------------------------------------------\\n#\\n# We first define a function that creates uniformly-distributed random labeling.\\n\\nimport numpy as np\\n\\nrng = np.random.RandomState(0)\\n\\n\\n# Code for: def random_labels(n_samples, n_classes):\\n\\n\\n# %%\\n# Another function will use the `random_labels` function to create a fixed set\\n# of ground truth labels (`labels_a`) distributed in `n_classes` and then score\\n# several sets of randomly \"predicted\" labels (`labels_b`) to assess the\\n# variability of a given metric at a given `n_clusters`.\\n\\n\\n# Code for: def fixed_classes_uniform_labelings_scores(\\n\\n\\n# %%\\n# In this first example we set the number of classes (true number of clusters) to\\n# `n_classes=10`. The number of clusters varies over the values provided by\\n# `n_clusters_range`.\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\nn_samples = 1000\\nn_classes = 10\\nn_clusters_range = np.linspace(2, 100, 10).astype(int)\\nplots = []\\nnames = []\\n\\nsns.color_palette(\"colorblind\")\\nplt.figure(1)\\n\\nfor marker, (score_name, score_func) in zip(\"d^vx.,\", score_funcs):\\n    scores = fixed_classes_uniform_labelings_scores(\\n        score_func, n_samples, n_clusters_range, n_classes=n_classes\\n    )\\n    plots.append(\\n        plt.errorbar(\\n            n_clusters_range,\\n            scores.mean(axis=1),\\n            scores.std(axis=1),\\n            alpha=0.8,\\n            linewidth=1,\\n            marker=marker,\\n        )[0]\\n    )\\n    names.append(score_name)\\n\\nplt.title(\\n    \"Clustering measures for random uniform labeling\\\\n\"\\n    f\"against reference assignment with {n_classes} classes\"\\n)\\nplt.xlabel(f\"Number of clusters (Number of samples is fixed to {n_samples})\")\\nplt.ylabel(\"Score value\")\\nplt.ylim(bottom=-0.05, top=1.05)\\nplt.legend(plots, names, bbox_to_anchor=(0.5, 0.5))\\nplt.show()\\n\\n# %%\\n# The Rand index saturates for `n_clusters` > `n_classes`. Other non-adjusted\\n# measures such as the V-Measure show a linear dependency between the number of\\n# clusters and the number of samples.\\n#\\n# Adjusted for chance measure, such as ARI and AMI, display some random\\n# variations centered around a mean score of 0.0, independently of the number of\\n# samples and clusters.\\n#\\n# Second experiment: varying number of classes and clusters\\n# ---------------------------------------------------------\\n#\\n# In this section we define a similar function that uses several metrics to\\n# score 2 uniformly-distributed random labelings. In this case, the number of\\n# classes and assigned number of clusters are matched for each possible value in\\n# `n_clusters_range`.'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_adjusted_for_chance_measures.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\\n\\n\\n# %%\\n# In this case, we use `n_samples=100` to show the effect of having a number of\\n# clusters similar or equal to the number of samples.\\n\\nn_samples = 100\\nn_clusters_range = np.linspace(2, n_samples, 10).astype(int)\\n\\nplt.figure(2)\\n\\nplots = []\\nnames = []\\n\\nfor marker, (score_name, score_func) in zip(\"d^vx.,\", score_funcs):\\n    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)\\n    plots.append(\\n        plt.errorbar(\\n            n_clusters_range,\\n            np.median(scores, axis=1),\\n            scores.std(axis=1),\\n            alpha=0.8,\\n            linewidth=2,\\n            marker=marker,\\n        )[0]\\n    )\\n    names.append(score_name)\\n\\nplt.title(\\n    \"Clustering measures for 2 random uniform labelings\\\\nwith equal number of clusters\"\\n)\\nplt.xlabel(f\"Number of clusters (Number of samples is fixed to {n_samples})\")\\nplt.ylabel(\"Score value\")\\nplt.legend(plots, names)\\nplt.ylim(bottom=-0.05, top=1.05)\\nplt.show()\\n\\n# %%\\n# We observe similar results as for the first experiment: adjusted for chance\\n# metrics stay constantly near zero while other metrics tend to get larger with\\n# finer-grained labelings. The mean V-measure of random labeling increases\\n# significantly as the number of clusters is closer to the total number of\\n# samples used to compute the measure. Furthermore, raw Mutual Information is\\n# unbounded from above and its scale depends on the dimensions of the clustering\\n# problem and the cardinality of the ground truth classes. This is why the\\n# curve goes off the chart.\\n#\\n# Only adjusted measures can hence be safely used as a consensus index to\\n# evaluate the average stability of clustering algorithms for a given value of k\\n# on various overlapping sub-samples of the dataset.\\n#\\n# Non-adjusted clustering evaluation metric can therefore be misleading as they\\n# output large values for fine-grained labelings, one could be lead to think\\n# that the labeling has captured meaningful groups while they can be totally\\n# random. In particular, such non-adjusted metrics should not be used to compare\\n# the results of different clustering algorithms that output a different number\\n# of clusters.'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_agglomerative_clustering_metrics.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def sqr(x):\\n    return np.sign(np.cos(x))'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_agglomerative_clustering_metrics.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\nAgglomerative clustering with different metrics\\n===============================================\\n\\nDemonstrates the effect of different metrics on the hierarchical clustering.\\n\\nThe example is engineered to show the effect of the choice of different\\nmetrics. It is applied to waveforms, which can be seen as\\nhigh-dimensional vector. Indeed, the difference between metrics is\\nusually more pronounced in high dimension (in particular for euclidean\\nand cityblock).\\n\\nWe generate data from three groups of waveforms. Two of the waveforms\\n(waveform 1 and waveform 2) are proportional one to the other. The cosine\\ndistance is invariant to a scaling of the data, as a result, it cannot\\ndistinguish these two waveforms. Thus even with no noise, clustering\\nusing this distance will not separate out waveform 1 and 2.\\n\\nWe add observation noise to these waveforms. We generate very sparse\\nnoise: only 6% of the time points contain noise. As a result, the\\nl1 norm of this noise (ie \"cityblock\" distance) is much smaller than it\\'s\\nl2 norm (\"euclidean\" distance). This can be seen on the inter-class\\ndistance matrices: the values on the diagonal, that characterize the\\nspread of the class, are much bigger for the Euclidean distance than for\\nthe cityblock distance.\\n\\nWhen we apply clustering to the data, we find that the clustering\\nreflects what was in the distance matrices. Indeed, for the Euclidean\\ndistance, the classes are ill-separated because of the noise, and thus\\nthe clustering does not separate the waveforms. For the cityblock\\ndistance, the separation is good and the waveform classes are recovered.\\nFinally, the cosine distance does not separate at all waveform 1 and 2,\\nthus the clustering puts them in the same cluster.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.patheffects as PathEffects\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.cluster import AgglomerativeClustering\\nfrom sklearn.metrics import pairwise_distances\\n\\nnp.random.seed(0)\\n\\n# Generate waveform data\\nn_features = 2000\\nt = np.pi * np.linspace(0, 1, n_features)\\n\\n\\n# Code for: def sqr(x):\\n\\n\\nX = list()\\ny = list()\\nfor i, (phi, a) in enumerate([(0.5, 0.15), (0.5, 0.6), (0.3, 0.2)]):\\n    for _ in range(30):\\n        phase_noise = 0.01 * np.random.normal()\\n        amplitude_noise = 0.04 * np.random.normal()\\n        additional_noise = 1 - 2 * np.random.rand(n_features)\\n        # Make the noise sparse\\n        additional_noise[np.abs(additional_noise) < 0.997] = 0\\n\\n        X.append(\\n            12\\n            * (\\n                (a + amplitude_noise) * (sqr(6 * (t + phi + phase_noise)))\\n                + additional_noise\\n            )\\n        )\\n        y.append(i)\\n\\nX = np.array(X)\\ny = np.array(y)\\n\\nn_clusters = 3\\n\\nlabels = (\"Waveform 1\", \"Waveform 2\", \"Waveform 3\")\\n\\ncolors = [\"#f7bd01\", \"#377eb8\", \"#f781bf\"]'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_agglomerative_clustering_metrics.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X = np.array(X)\\ny = np.array(y)\\n\\nn_clusters = 3\\n\\nlabels = (\"Waveform 1\", \"Waveform 2\", \"Waveform 3\")\\n\\ncolors = [\"#f7bd01\", \"#377eb8\", \"#f781bf\"]\\n\\n# Plot the ground-truth labelling\\nplt.figure()\\nplt.axes([0, 0, 1, 1])\\nfor l, color, n in zip(range(n_clusters), colors, labels):\\n    lines = plt.plot(X[y == l].T, c=color, alpha=0.5)\\n    lines[0].set_label(n)\\n\\nplt.legend(loc=\"best\")\\n\\nplt.axis(\"tight\")\\nplt.axis(\"off\")\\nplt.suptitle(\"Ground truth\", size=20, y=1)\\n\\n\\n# Plot the distances\\nfor index, metric in enumerate([\"cosine\", \"euclidean\", \"cityblock\"]):\\n    avg_dist = np.zeros((n_clusters, n_clusters))\\n    plt.figure(figsize=(5, 4.5))\\n    for i in range(n_clusters):\\n        for j in range(n_clusters):\\n            avg_dist[i, j] = pairwise_distances(\\n                X[y == i], X[y == j], metric=metric\\n            ).mean()\\n    avg_dist /= avg_dist.max()\\n    for i in range(n_clusters):\\n        for j in range(n_clusters):\\n            t = plt.text(\\n                i,\\n                j,\\n                \"%5.3f\" % avg_dist[i, j],\\n                verticalalignment=\"center\",\\n                horizontalalignment=\"center\",\\n            )\\n            t.set_path_effects(\\n                [PathEffects.withStroke(linewidth=5, foreground=\"w\", alpha=0.5)]\\n            )\\n\\n    plt.imshow(avg_dist, interpolation=\"nearest\", cmap=\"cividis\", vmin=0)\\n    plt.xticks(range(n_clusters), labels, rotation=45)\\n    plt.yticks(range(n_clusters), labels)\\n    plt.colorbar()\\n    plt.suptitle(\"Interclass %s distances\" % metric, size=18, y=1)\\n    plt.tight_layout()\\n\\n\\n# Plot clustering results\\nfor index, metric in enumerate([\"cosine\", \"euclidean\", \"cityblock\"]):\\n    model = AgglomerativeClustering(\\n        n_clusters=n_clusters, linkage=\"average\", metric=metric\\n    )\\n    model.fit(X)\\n    plt.figure()\\n    plt.axes([0, 0, 1, 1])\\n    for l, color in zip(np.arange(model.n_clusters), colors):\\n        plt.plot(X[model.labels_ == l].T, c=color, alpha=0.5)\\n    plt.axis(\"tight\")\\n    plt.axis(\"off\")\\n    plt.suptitle(\"AgglomerativeClustering(metric=%s)\" % metric, size=20, y=1)\\n\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_inductive_clustering.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def _classifier_has(attr):\\n    \"\"\"Check if we can delegate a method to the underlying classifier.\\n\\n    First, we check the first fitted classifier if available, otherwise we\\n    check the unfitted classifier.\\n    \"\"\"\\n    return lambda estimator: (\\n        hasattr(estimator.classifier_, attr)\\n        if hasattr(estimator, \"classifier_\")\\n        else hasattr(estimator.classifier, attr)\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_inductive_clustering.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class InductiveClusterer(BaseEstimator):\\n    def __init__(self, clusterer, classifier):\\n        self.clusterer = clusterer\\n        self.classifier = classifier\\n\\n    def fit(self, X, y=None):\\n        self.clusterer_ = clone(self.clusterer)\\n        self.classifier_ = clone(self.classifier)\\n        y = self.clusterer_.fit_predict(X)\\n        self.classifier_.fit(X, y)\\n        return self\\n\\n    @available_if(_classifier_has(\"predict\"))\\n    def predict(self, X):\\n        check_is_fitted(self)\\n        return self.classifier_.predict(X)\\n\\n    @available_if(_classifier_has(\"decision_function\"))\\n    def decision_function(self, X):\\n        check_is_fitted(self)\\n        return self.classifier_.decision_function(X)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_inductive_clustering.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_scatter(X, color, alpha=0.5):\\n    return plt.scatter(X[:, 0], X[:, 1], c=color, alpha=alpha, edgecolor=\"k\")'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_inductive_clustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================\\nInductive Clustering\\n====================\\n\\nClustering can be expensive, especially when our dataset contains millions\\nof datapoints. Many clustering algorithms are not :term:`inductive` and so\\ncannot be directly applied to new data samples without recomputing the\\nclustering, which may be intractable. Instead, we can use clustering to then\\nlearn an inductive model with a classifier, which has several benefits:\\n\\n- it allows the clusters to scale and apply to new data\\n- unlike re-fitting the clusters to new samples, it makes sure the labelling\\n  procedure is consistent over time\\n- it allows us to use the inferential capabilities of the classifier to\\n  describe or explain the clusters\\n\\nThis example illustrates a generic implementation of a meta-estimator which\\nextends clustering by inducing a classifier from the cluster labels.\\n\\n\"\"\"\\n\\n# Authors: Chirag Nagpal\\n#          Christos Aridas\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.base import BaseEstimator, clone\\nfrom sklearn.cluster import AgglomerativeClustering\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.utils.metaestimators import available_if\\nfrom sklearn.utils.validation import check_is_fitted\\n\\nN_SAMPLES = 5000\\nRANDOM_STATE = 42\\n\\n\\n# Code for: def _classifier_has(attr):\\n\\n\\n# Code for: class InductiveClusterer(BaseEstimator):\\n\\n\\n# Code for: def plot_scatter(X, color, alpha=0.5):\\n\\n\\n# Generate some training data from clustering\\nX, y = make_blobs(\\n    n_samples=N_SAMPLES,\\n    cluster_std=[1.0, 1.0, 0.5],\\n    centers=[(-5, -5), (0, 0), (5, 5)],\\n    random_state=RANDOM_STATE,\\n)\\n\\n\\n# Train a clustering algorithm on the training data and get the cluster labels\\nclusterer = AgglomerativeClustering(n_clusters=3)\\ncluster_labels = clusterer.fit_predict(X)\\n\\nplt.figure(figsize=(12, 4))\\n\\nplt.subplot(131)\\nplot_scatter(X, cluster_labels)\\nplt.title(\"Ward Linkage\")\\n\\n\\n# Generate new samples and plot them along with the original dataset\\nX_new, y_new = make_blobs(\\n    n_samples=10, centers=[(-7, -1), (-2, 4), (3, 6)], random_state=RANDOM_STATE\\n)\\n\\nplt.subplot(132)\\nplot_scatter(X, cluster_labels)\\nplot_scatter(X_new, \"black\", 1)\\nplt.title(\"Unknown instances\")\\n\\n\\n# Declare the inductive learning model that it will be used to\\n# predict cluster membership for unknown instances\\nclassifier = RandomForestClassifier(random_state=RANDOM_STATE)\\ninductive_learner = InductiveClusterer(clusterer, classifier).fit(X)\\n\\nprobable_clusters = inductive_learner.predict(X_new)\\n\\n\\nax = plt.subplot(133)\\nplot_scatter(X, cluster_labels)\\nplot_scatter(X_new, probable_clusters)\\n\\n# Plotting decision regions\\nDecisionBoundaryDisplay.from_estimator(\\n    inductive_learner, X, response_method=\"predict\", alpha=0.4, ax=ax\\n)\\nplt.title(\"Classify unknown instances\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_linkage_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================================================\\nComparing different hierarchical linkage methods on toy datasets\\n================================================================\\n\\nThis example shows characteristics of different linkage\\nmethods for hierarchical clustering on datasets that are\\n\"interesting\" but still in 2D.\\n\\nThe main observations to make are:\\n\\n- single linkage is fast, and can perform well on\\n  non-globular data, but it performs poorly in the\\n  presence of noise.\\n- average and complete linkage perform well on\\n  cleanly separated globular clusters, but have mixed\\n  results otherwise.\\n- Ward is the most effective method for noisy data.\\n\\nWhile these examples give some intuition about the\\nalgorithms, this intuition might not apply to very high\\ndimensional data.\\n\\n\"\"\"\\n\\nimport time\\nimport warnings\\nfrom itertools import cycle, islice\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import cluster, datasets\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# %%\\n# Generate datasets. We choose the size big enough to see the scalability\\n# of the algorithms, but not too big to avoid too long running times\\n\\nn_samples = 1500\\nnoisy_circles = datasets.make_circles(\\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=170\\n)\\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=170)\\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=170)\\nrng = np.random.RandomState(170)\\nno_structure = rng.rand(n_samples, 2), None\\n\\n# Anisotropicly distributed data\\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=170)\\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\\nX_aniso = np.dot(X, transformation)\\naniso = (X_aniso, y)\\n\\n# blobs with varied variances\\nvaried = datasets.make_blobs(\\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=170\\n)\\n\\n# %%\\n# Run the clustering and plot\\n\\n# Set up cluster parameters\\nplt.figure(figsize=(9 * 1.3 + 2, 14.5))\\nplt.subplots_adjust(\\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\\n)\\n\\nplot_num = 1\\n\\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\\n\\ndatasets = [\\n    (noisy_circles, {\"n_clusters\": 2}),\\n    (noisy_moons, {\"n_clusters\": 2}),\\n    (varied, {\"n_neighbors\": 2}),\\n    (aniso, {\"n_neighbors\": 2}),\\n    (blobs, {}),\\n    (no_structure, {}),\\n]\\n\\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\\n    # update parameters with dataset-specific values\\n    params = default_base.copy()\\n    params.update(algo_params)\\n\\n    X, y = dataset\\n\\n    # normalize dataset for easier parameter selection\\n    X = StandardScaler().fit_transform(X)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_linkage_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for i_dataset, (dataset, algo_params) in enumerate(datasets):\\n    # update parameters with dataset-specific values\\n    params = default_base.copy()\\n    params.update(algo_params)\\n\\n    X, y = dataset\\n\\n    # normalize dataset for easier parameter selection\\n    X = StandardScaler().fit_transform(X)\\n\\n    # ============\\n    # Create cluster objects\\n    # ============\\n    ward = cluster.AgglomerativeClustering(\\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\\n    )\\n    complete = cluster.AgglomerativeClustering(\\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\\n    )\\n    average = cluster.AgglomerativeClustering(\\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\\n    )\\n    single = cluster.AgglomerativeClustering(\\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\\n    )\\n\\n    clustering_algorithms = (\\n        (\"Single Linkage\", single),\\n        (\"Average Linkage\", average),\\n        (\"Complete Linkage\", complete),\\n        (\"Ward Linkage\", ward),\\n    )\\n\\n    for name, algorithm in clustering_algorithms:\\n        t0 = time.time()\\n\\n        # catch warnings related to kneighbors_graph\\n        with warnings.catch_warnings():\\n            warnings.filterwarnings(\\n                \"ignore\",\\n                message=\"the number of connected components of the \"\\n                + \"connectivity matrix is [0-9]{1,2}\"\\n                + \" > 1. Completing it to avoid stopping the tree early.\",\\n                category=UserWarning,\\n            )\\n            algorithm.fit(X)\\n\\n        t1 = time.time()\\n        if hasattr(algorithm, \"labels_\"):\\n            y_pred = algorithm.labels_.astype(int)\\n        else:\\n            y_pred = algorithm.predict(X)\\n\\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\\n        if i_dataset == 0:\\n            plt.title(name, size=18)\\n\\n        colors = np.array(\\n            list(\\n                islice(\\n                    cycle(\\n                        [\\n                            \"#377eb8\",\\n                            \"#ff7f00\",\\n                            \"#4daf4a\",\\n                            \"#f781bf\",\\n                            \"#a65628\",\\n                            \"#984ea3\",\\n                            \"#999999\",\\n                            \"#e41a1c\",\\n                            \"#dede00\",\\n                        ]\\n                    ),\\n                    int(max(y_pred) + 1),\\n                )\\n            )\\n        )\\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\\n\\n        plt.xlim(-2.5, 2.5)\\n        plt.ylim(-2.5, 2.5)\\n        plt.xticks(())\\n        plt.yticks(())\\n        plt.text(\\n            0.99,\\n            0.01,\\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\\n            transform=plt.gca().transAxes,\\n            size=15,\\n            horizontalalignment=\"right\",\\n        )\\n        plot_num += 1\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_bisect_kmeans.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================================\\nBisecting K-Means and Regular K-Means Performance Comparison\\n=============================================================\\n\\nThis example shows differences between Regular K-Means algorithm and Bisecting K-Means.\\n\\nWhile K-Means clusterings are different when increasing n_clusters,\\nBisecting K-Means clustering builds on top of the previous ones. As a result, it\\ntends to create clusters that have a more regular large-scale structure. This\\ndifference can be visually observed: for all numbers of clusters, there is a\\ndividing line cutting the overall data cloud in two for BisectingKMeans, which is not\\npresent for regular K-Means.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.cluster import BisectingKMeans, KMeans\\nfrom sklearn.datasets import make_blobs\\n\\nprint(__doc__)\\n\\n\\n# Generate sample data\\nn_samples = 10000\\nrandom_state = 0\\n\\nX, _ = make_blobs(n_samples=n_samples, centers=2, random_state=random_state)\\n\\n# Number of cluster centers for KMeans and BisectingKMeans\\nn_clusters_list = [4, 8, 16]\\n\\n# Algorithms to compare\\nclustering_algorithms = {\\n    \"Bisecting K-Means\": BisectingKMeans,\\n    \"K-Means\": KMeans,\\n}\\n\\n# Make subplots for each variant\\nfig, axs = plt.subplots(\\n    len(clustering_algorithms), len(n_clusters_list), figsize=(12, 5)\\n)\\n\\naxs = axs.T\\n\\nfor i, (algorithm_name, Algorithm) in enumerate(clustering_algorithms.items()):\\n    for j, n_clusters in enumerate(n_clusters_list):\\n        algo = Algorithm(n_clusters=n_clusters, random_state=random_state, n_init=3)\\n        algo.fit(X)\\n        centers = algo.cluster_centers_\\n\\n        axs[j, i].scatter(X[:, 0], X[:, 1], s=10, c=algo.labels_)\\n        axs[j, i].scatter(centers[:, 0], centers[:, 1], c=\"r\", s=20)\\n\\n        axs[j, i].set_title(f\"{algorithm_name} : {n_clusters} clusters\")\\n\\n\\n# Hide x labels and tick labels for top plots and y ticks for right plots.\\nfor ax in axs.flat:\\n    ax.label_outer()\\n    ax.set_xticks([])\\n    ax.set_yticks([])\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_mean_shift.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================\\nA demo of the mean-shift clustering algorithm\\n=============================================\\n\\nReference:\\n\\nDorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\\nfeature space analysis\". IEEE Transactions on Pattern Analysis and\\nMachine Intelligence. 2002. pp. 603-619.\\n\\n\"\"\"\\n\\nimport numpy as np\\n\\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\\nfrom sklearn.datasets import make_blobs\\n\\n# %%\\n# Generate sample data\\n# --------------------\\ncenters = [[1, 1], [-1, -1], [1, -1]]\\nX, _ = make_blobs(n_samples=10000, centers=centers, cluster_std=0.6)\\n\\n# %%\\n# Compute clustering with MeanShift\\n# ---------------------------------\\n\\n# The following bandwidth can be automatically detected using\\nbandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\\n\\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\\nms.fit(X)\\nlabels = ms.labels_\\ncluster_centers = ms.cluster_centers_\\n\\nlabels_unique = np.unique(labels)\\nn_clusters_ = len(labels_unique)\\n\\nprint(\"number of estimated clusters : %d\" % n_clusters_)\\n\\n# %%\\n# Plot result\\n# -----------\\nimport matplotlib.pyplot as plt\\n\\nplt.figure(1)\\nplt.clf()\\n\\ncolors = [\"#dede00\", \"#377eb8\", \"#f781bf\"]\\nmarkers = [\"x\", \"o\", \"^\"]\\n\\nfor k, col in zip(range(n_clusters_), colors):\\n    my_members = labels == k\\n    cluster_center = cluster_centers[k]\\n    plt.plot(X[my_members, 0], X[my_members, 1], markers[k], color=col)\\n    plt.plot(\\n        cluster_center[0],\\n        cluster_center[1],\\n        markers[k],\\n        markerfacecolor=col,\\n        markeredgecolor=\"k\",\\n        markersize=14,\\n    )\\nplt.title(\"Estimated number of clusters: %d\" % n_clusters_)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_assumptions.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================\\nDemonstration of k-means assumptions\\n====================================\\n\\nThis example is meant to illustrate situations where k-means produces\\nunintuitive and possibly undesirable clusters.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Data generation\\n# ---------------\\n#\\n# The function :func:`~sklearn.datasets.make_blobs` generates isotropic\\n# (spherical) gaussian blobs. To obtain anisotropic (elliptical) gaussian blobs\\n# one has to define a linear `transformation`.\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_blobs\\n\\nn_samples = 1500\\nrandom_state = 170\\ntransformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\\n\\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\\nX_aniso = np.dot(X, transformation)  # Anisotropic blobs\\nX_varied, y_varied = make_blobs(\\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\\n)  # Unequal variance\\nX_filtered = np.vstack(\\n    (X[y == 0][:500], X[y == 1][:100], X[y == 2][:10])\\n)  # Unevenly sized blobs\\ny_filtered = [0] * 500 + [1] * 100 + [2] * 10\\n\\n# %%\\n# We can visualize the resulting data:\\n\\nimport matplotlib.pyplot as plt\\n\\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\\n\\naxs[0, 0].scatter(X[:, 0], X[:, 1], c=y)\\naxs[0, 0].set_title(\"Mixture of Gaussian Blobs\")\\n\\naxs[0, 1].scatter(X_aniso[:, 0], X_aniso[:, 1], c=y)\\naxs[0, 1].set_title(\"Anisotropically Distributed Blobs\")\\n\\naxs[1, 0].scatter(X_varied[:, 0], X_varied[:, 1], c=y_varied)\\naxs[1, 0].set_title(\"Unequal Variance\")\\n\\naxs[1, 1].scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_filtered)\\naxs[1, 1].set_title(\"Unevenly Sized Blobs\")\\n\\nplt.suptitle(\"Ground truth clusters\").set_y(0.95)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_assumptions.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='axs[1, 0].scatter(X_varied[:, 0], X_varied[:, 1], c=y_varied)\\naxs[1, 0].set_title(\"Unequal Variance\")\\n\\naxs[1, 1].scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_filtered)\\naxs[1, 1].set_title(\"Unevenly Sized Blobs\")\\n\\nplt.suptitle(\"Ground truth clusters\").set_y(0.95)\\nplt.show()\\n\\n# %%\\n# Fit models and plot results\\n# ---------------------------\\n#\\n# The previously generated data is now used to show how\\n# :class:`~sklearn.cluster.KMeans` behaves in the following scenarios:\\n#\\n# - Non-optimal number of clusters: in a real setting there is no uniquely\\n#   defined **true** number of clusters. An appropriate number of clusters has\\n#   to be decided from data-based criteria and knowledge of the intended goal.\\n# - Anisotropically distributed blobs: k-means consists of minimizing sample\\'s\\n#   euclidean distances to the centroid of the cluster they are assigned to. As\\n#   a consequence, k-means is more appropriate for clusters that are isotropic\\n#   and normally distributed (i.e. spherical gaussians).\\n# - Unequal variance: k-means is equivalent to taking the maximum likelihood\\n#   estimator for a \"mixture\" of k gaussian distributions with the same\\n#   variances but with possibly different means.\\n# - Unevenly sized blobs: there is no theoretical result about k-means that\\n#   states that it requires similar cluster sizes to perform well, yet\\n#   minimizing euclidean distances does mean that the more sparse and\\n#   high-dimensional the problem is, the higher is the need to run the algorithm\\n#   with different centroid seeds to ensure a global minimal inertia.\\n\\nfrom sklearn.cluster import KMeans\\n\\ncommon_params = {\\n    \"n_init\": \"auto\",\\n    \"random_state\": random_state,\\n}\\n\\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\\n\\ny_pred = KMeans(n_clusters=2, **common_params).fit_predict(X)\\naxs[0, 0].scatter(X[:, 0], X[:, 1], c=y_pred)\\naxs[0, 0].set_title(\"Non-optimal Number of Clusters\")\\n\\ny_pred = KMeans(n_clusters=3, **common_params).fit_predict(X_aniso)\\naxs[0, 1].scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\\naxs[0, 1].set_title(\"Anisotropically Distributed Blobs\")\\n\\ny_pred = KMeans(n_clusters=3, **common_params).fit_predict(X_varied)\\naxs[1, 0].scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\\naxs[1, 0].set_title(\"Unequal Variance\")\\n\\ny_pred = KMeans(n_clusters=3, **common_params).fit_predict(X_filtered)\\naxs[1, 1].scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\\naxs[1, 1].set_title(\"Unevenly Sized Blobs\")\\n\\nplt.suptitle(\"Unexpected KMeans clusters\").set_y(0.95)\\nplt.show()\\n\\n# %%\\n# Possible solutions\\n# ------------------\\n#\\n# For an example on how to find a correct number of blobs, see\\n# :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\\n# In this case it suffices to set `n_clusters=3`.\\n\\ny_pred = KMeans(n_clusters=3, **common_params).fit_predict(X)\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\\nplt.title(\"Optimal Number of Clusters\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_assumptions.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='y_pred = KMeans(n_clusters=3, **common_params).fit_predict(X)\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\\nplt.title(\"Optimal Number of Clusters\")\\nplt.show()\\n\\n# %%\\n# To deal with unevenly sized blobs one can increase the number of random\\n# initializations. In this case we set `n_init=10` to avoid finding a\\n# sub-optimal local minimum. For more details see :ref:`kmeans_sparse_high_dim`.\\n\\ny_pred = KMeans(n_clusters=3, n_init=10, random_state=random_state).fit_predict(\\n    X_filtered\\n)\\nplt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\\nplt.title(\"Unevenly Sized Blobs \\\\nwith several initializations\")\\nplt.show()\\n\\n# %%\\n# As anisotropic and unequal variances are real limitations of the k-means\\n# algorithm, here we propose instead the use of\\n# :class:`~sklearn.mixture.GaussianMixture`, which also assumes gaussian\\n# clusters but does not impose any constraints on their variances. Notice that\\n# one still has to find the correct number of blobs (see\\n# :ref:`sphx_glr_auto_examples_mixture_plot_gmm_selection.py`).\\n#\\n# For an example on how other clustering methods deal with anisotropic or\\n# unequal variance blobs, see the example\\n# :ref:`sphx_glr_auto_examples_cluster_plot_cluster_comparison.py`.\\n\\nfrom sklearn.mixture import GaussianMixture\\n\\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\\n\\ny_pred = GaussianMixture(n_components=3).fit_predict(X_aniso)\\nax1.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\\nax1.set_title(\"Anisotropically Distributed Blobs\")\\n\\ny_pred = GaussianMixture(n_components=3).fit_predict(X_varied)\\nax2.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\\nax2.set_title(\"Unequal Variance\")\\n\\nplt.suptitle(\"Gaussian mixture clusters\").set_y(0.95)\\nplt.show()\\n\\n# %%\\n# Final remarks\\n# -------------\\n#\\n# In high-dimensional spaces, Euclidean distances tend to become inflated\\n# (not shown in this example). Running a dimensionality reduction algorithm\\n# prior to k-means clustering can alleviate this problem and speed up the\\n# computations (see the example\\n# :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`).\\n#\\n# In the case where clusters are known to be isotropic, have similar variance\\n# and are not too sparse, the k-means algorithm is quite effective and is one of\\n# the fastest clustering algorithms available. This advantage is lost if one has\\n# to restart it several times to avoid convergence to a local minimum.'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================\\nFeature agglomeration vs. univariate selection\\n==============================================\\n\\nThis example compares 2 dimensionality reduction strategies:\\n\\n- univariate feature selection with Anova\\n\\n- feature agglomeration with Ward hierarchical clustering\\n\\nBoth methods are compared in a regression problem using\\na BayesianRidge as supervised estimator.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\nimport shutil\\nimport tempfile\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom joblib import Memory\\nfrom scipy import linalg, ndimage\\n\\nfrom sklearn import feature_selection\\nfrom sklearn.cluster import FeatureAgglomeration\\nfrom sklearn.feature_extraction.image import grid_to_graph\\nfrom sklearn.linear_model import BayesianRidge\\nfrom sklearn.model_selection import GridSearchCV, KFold\\nfrom sklearn.pipeline import Pipeline\\n\\n# %%\\n# Set parameters\\nn_samples = 200\\nsize = 40  # image size\\nroi_size = 15\\nsnr = 5.0\\nnp.random.seed(0)\\n\\n# %%\\n# Generate data\\ncoef = np.zeros((size, size))\\ncoef[0:roi_size, 0:roi_size] = -1.0\\ncoef[-roi_size:, -roi_size:] = 1.0\\n\\nX = np.random.randn(n_samples, size**2)\\nfor x in X:  # smooth data\\n    x[:] = ndimage.gaussian_filter(x.reshape(size, size), sigma=1.0).ravel()\\nX -= X.mean(axis=0)\\nX /= X.std(axis=0)\\n\\ny = np.dot(X, coef.ravel())\\n\\n# %%\\n# add noise\\nnoise = np.random.randn(y.shape[0])\\nnoise_coef = (linalg.norm(y, 2) / np.exp(snr / 20.0)) / linalg.norm(noise, 2)\\ny += noise_coef * noise\\n\\n# %%\\n# Compute the coefs of a Bayesian Ridge with GridSearch\\ncv = KFold(2)  # cross-validation generator for model selection\\nridge = BayesianRidge()\\ncachedir = tempfile.mkdtemp()\\nmem = Memory(location=cachedir, verbose=1)\\n\\n# %%\\n# Ward agglomeration followed by BayesianRidge\\nconnectivity = grid_to_graph(n_x=size, n_y=size)\\nward = FeatureAgglomeration(n_clusters=10, connectivity=connectivity, memory=mem)\\nclf = Pipeline([(\"ward\", ward), (\"ridge\", ridge)])\\n# Select the optimal number of parcels with grid search\\nclf = GridSearchCV(clf, {\"ward__n_clusters\": [10, 20, 30]}, n_jobs=1, cv=cv)\\nclf.fit(X, y)  # set the best parameters\\ncoef_ = clf.best_estimator_.steps[-1][1].coef_\\ncoef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_)\\ncoef_agglomeration_ = coef_.reshape(size, size)\\n\\n# %%\\n# Anova univariate feature selection followed by BayesianRidge\\nf_regression = mem.cache(feature_selection.f_regression)  # caching function\\nanova = feature_selection.SelectPercentile(f_regression)\\nclf = Pipeline([(\"anova\", anova), (\"ridge\", ridge)])\\n# Select the optimal percentage of features with grid search\\nclf = GridSearchCV(clf, {\"anova__percentile\": [5, 10, 20]}, cv=cv)\\nclf.fit(X, y)  # set the best parameters\\ncoef_ = clf.best_estimator_.steps[-1][1].coef_\\ncoef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_.reshape(1, -1))\\ncoef_selection_ = coef_.reshape(size, size)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Inverse the transformation to plot the results on an image\\nplt.close(\"all\")\\nplt.figure(figsize=(7.3, 2.7))\\nplt.subplot(1, 3, 1)\\nplt.imshow(coef, interpolation=\"nearest\", cmap=plt.cm.RdBu_r)\\nplt.title(\"True weights\")\\nplt.subplot(1, 3, 2)\\nplt.imshow(coef_selection_, interpolation=\"nearest\", cmap=plt.cm.RdBu_r)\\nplt.title(\"Feature Selection\")\\nplt.subplot(1, 3, 3)\\nplt.imshow(coef_agglomeration_, interpolation=\"nearest\", cmap=plt.cm.RdBu_r)\\nplt.title(\"Feature Agglomeration\")\\nplt.subplots_adjust(0.04, 0.0, 0.98, 0.94, 0.16, 0.26)\\nplt.show()\\n\\n# %%\\n# Attempt to remove the temporary cachedir, but don\\'t worry if it fails\\nshutil.rmtree(cachedir, ignore_errors=True)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_cluster_iris.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nK-means Clustering\\n=========================================================\\n\\nThe plot shows:\\n\\n- top left: What a K-means algorithm would yield using 8 clusters.\\n\\n- top right: What using three clusters would deliver.\\n\\n- bottom left: What the effect of a bad initialization is\\n  on the classification process: By setting n_init to only 1\\n  (default is 10), the amount of times that the algorithm will\\n  be run with different centroid seeds is reduced.\\n\\n- bottom right: The ground truth.\\n\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# Modified for documentation by Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\n\\n# Though the following import is not directly being used, it is required\\n# for 3D projection to work with matplotlib < 3.2\\nimport mpl_toolkits.mplot3d  # noqa: F401\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.cluster import KMeans\\n\\nnp.random.seed(5)\\n\\niris = datasets.load_iris()\\nX = iris.data\\ny = iris.target\\n\\nestimators = [\\n    (\"k_means_iris_8\", KMeans(n_clusters=8)),\\n    (\"k_means_iris_3\", KMeans(n_clusters=3)),\\n    (\"k_means_iris_bad_init\", KMeans(n_clusters=3, n_init=1, init=\"random\")),\\n]\\n\\nfig = plt.figure(figsize=(10, 8))\\ntitles = [\"8 clusters\", \"3 clusters\", \"3 clusters, bad initialization\"]\\nfor idx, ((name, est), title) in enumerate(zip(estimators, titles)):\\n    ax = fig.add_subplot(2, 2, idx + 1, projection=\"3d\", elev=48, azim=134)\\n    est.fit(X)\\n    labels = est.labels_\\n\\n    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(float), edgecolor=\"k\")\\n\\n    ax.xaxis.set_ticklabels([])\\n    ax.yaxis.set_ticklabels([])\\n    ax.zaxis.set_ticklabels([])\\n    ax.set_xlabel(\"Petal width\")\\n    ax.set_ylabel(\"Sepal length\")\\n    ax.set_zlabel(\"Petal length\")\\n    ax.set_title(title)\\n\\n# Plot the ground truth\\nax = fig.add_subplot(2, 2, 4, projection=\"3d\", elev=48, azim=134)\\n\\nfor name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\\n    ax.text3D(\\n        X[y == label, 3].mean(),\\n        X[y == label, 0].mean(),\\n        X[y == label, 2].mean() + 2,\\n        name,\\n        horizontalalignment=\"center\",\\n        bbox=dict(alpha=0.2, edgecolor=\"w\", facecolor=\"w\"),\\n    )\\n\\nax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor=\"k\")\\n\\nax.xaxis.set_ticklabels([])\\nax.yaxis.set_ticklabels([])\\nax.zaxis.set_ticklabels([])\\nax.set_xlabel(\"Petal width\")\\nax.set_ylabel(\"Sepal length\")\\nax.set_zlabel(\"Petal length\")\\nax.set_title(\"Ground Truth\")\\n\\nplt.subplots_adjust(wspace=0.25, hspace=0.25)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_agglomerative_clustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\nAgglomerative clustering with and without structure\\n===================================================\\n\\nThis example shows the effect of imposing a connectivity graph to capture\\nlocal structure in the data. The graph is simply the graph of 20 nearest\\nneighbors.\\n\\nThere are two advantages of imposing a connectivity. First, clustering\\nwith sparse connectivity matrices is faster in general.\\n\\nSecond, when using a connectivity matrix, single, average and complete\\nlinkage are unstable and tend to create a few clusters that grow very\\nquickly. Indeed, average and complete linkage fight this percolation behavior\\nby considering all the distances between two clusters when merging them (\\nwhile single linkage exaggerates the behaviour by considering only the\\nshortest distance between clusters). The connectivity graph breaks this\\nmechanism for average and complete linkage, making them resemble the more\\nbrittle single linkage. This effect is more pronounced for very sparse graphs\\n(try decreasing the number of neighbors in kneighbors_graph) and with\\ncomplete linkage. In particular, having a very small number of neighbors in\\nthe graph, imposes a geometry that is close to that of single linkage,\\nwhich is well known to have this percolation instability.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.cluster import AgglomerativeClustering\\nfrom sklearn.neighbors import kneighbors_graph\\n\\n# Generate sample data\\nn_samples = 1500\\nnp.random.seed(0)\\nt = 1.5 * np.pi * (1 + 3 * np.random.rand(1, n_samples))\\nx = t * np.cos(t)\\ny = t * np.sin(t)\\n\\n\\nX = np.concatenate((x, y))\\nX += 0.7 * np.random.randn(2, n_samples)\\nX = X.T\\n\\n# Create a graph capturing local connectivity. Larger number of neighbors\\n# will give more homogeneous clusters to the cost of computation\\n# time. A very large number of neighbors gives more evenly distributed\\n# cluster sizes, but may not impose the local manifold structure of\\n# the data\\nknn_graph = kneighbors_graph(X, 30, include_self=False)\\n\\nfor connectivity in (None, knn_graph):\\n    for n_clusters in (30, 3):\\n        plt.figure(figsize=(10, 4))\\n        for index, linkage in enumerate((\"average\", \"complete\", \"ward\", \"single\")):\\n            plt.subplot(1, 4, index + 1)\\n            model = AgglomerativeClustering(\\n                linkage=linkage, connectivity=connectivity, n_clusters=n_clusters\\n            )\\n            t0 = time.time()\\n            model.fit(X)\\n            elapsed_time = time.time() - t0\\n            plt.scatter(X[:, 0], X[:, 1], c=model.labels_, cmap=plt.cm.nipy_spectral)\\n            plt.title(\\n                \"linkage=%s\\\\n(time %.2fs)\" % (linkage, elapsed_time),\\n                fontdict=dict(verticalalignment=\"top\"),\\n            )\\n            plt.axis(\"equal\")\\n            plt.axis(\"off\")'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_agglomerative_clustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.subplots_adjust(bottom=0, top=0.83, wspace=0, left=0, right=1)\\n            plt.suptitle(\\n                \"n_cluster=%i, connectivity=%r\"\\n                % (n_clusters, connectivity is not None),\\n                size=17,\\n            )\\n\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_optics.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================\\nDemo of OPTICS clustering algorithm\\n===================================\\n\\n.. currentmodule:: sklearn\\n\\nFinds core samples of high density and expands clusters from them.\\nThis example uses data that is generated so that the clusters have\\ndifferent densities.\\n\\nThe :class:`~cluster.OPTICS` is first used with its Xi cluster detection\\nmethod, and then setting specific thresholds on the reachability, which\\ncorresponds to :class:`~cluster.DBSCAN`. We can see that the different\\nclusters of OPTICS\\'s Xi method can be recovered with different choices of\\nthresholds in DBSCAN.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.gridspec as gridspec\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.cluster import OPTICS, cluster_optics_dbscan\\n\\n# Generate sample data\\n\\nnp.random.seed(0)\\nn_points_per_cluster = 250\\n\\nC1 = [-5, -2] + 0.8 * np.random.randn(n_points_per_cluster, 2)\\nC2 = [4, -1] + 0.1 * np.random.randn(n_points_per_cluster, 2)\\nC3 = [1, -2] + 0.2 * np.random.randn(n_points_per_cluster, 2)\\nC4 = [-2, 3] + 0.3 * np.random.randn(n_points_per_cluster, 2)\\nC5 = [3, -2] + 1.6 * np.random.randn(n_points_per_cluster, 2)\\nC6 = [5, 6] + 2 * np.random.randn(n_points_per_cluster, 2)\\nX = np.vstack((C1, C2, C3, C4, C5, C6))\\n\\nclust = OPTICS(min_samples=50, xi=0.05, min_cluster_size=0.05)\\n\\n# Run the fit\\nclust.fit(X)\\n\\nlabels_050 = cluster_optics_dbscan(\\n    reachability=clust.reachability_,\\n    core_distances=clust.core_distances_,\\n    ordering=clust.ordering_,\\n    eps=0.5,\\n)\\nlabels_200 = cluster_optics_dbscan(\\n    reachability=clust.reachability_,\\n    core_distances=clust.core_distances_,\\n    ordering=clust.ordering_,\\n    eps=2,\\n)\\n\\nspace = np.arange(len(X))\\nreachability = clust.reachability_[clust.ordering_]\\nlabels = clust.labels_[clust.ordering_]\\n\\nplt.figure(figsize=(10, 7))\\nG = gridspec.GridSpec(2, 3)\\nax1 = plt.subplot(G[0, :])\\nax2 = plt.subplot(G[1, 0])\\nax3 = plt.subplot(G[1, 1])\\nax4 = plt.subplot(G[1, 2])\\n\\n# Reachability plot\\ncolors = [\"g.\", \"r.\", \"b.\", \"y.\", \"c.\"]\\nfor klass, color in enumerate(colors):\\n    Xk = space[labels == klass]\\n    Rk = reachability[labels == klass]\\n    ax1.plot(Xk, Rk, color, alpha=0.3)\\nax1.plot(space[labels == -1], reachability[labels == -1], \"k.\", alpha=0.3)\\nax1.plot(space, np.full_like(space, 2.0, dtype=float), \"k-\", alpha=0.5)\\nax1.plot(space, np.full_like(space, 0.5, dtype=float), \"k-.\", alpha=0.5)\\nax1.set_ylabel(\"Reachability (epsilon distance)\")\\nax1.set_title(\"Reachability Plot\")\\n\\n# OPTICS\\ncolors = [\"g.\", \"r.\", \"b.\", \"y.\", \"c.\"]\\nfor klass, color in enumerate(colors):\\n    Xk = X[clust.labels_ == klass]\\n    ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\\nax2.plot(X[clust.labels_ == -1, 0], X[clust.labels_ == -1, 1], \"k+\", alpha=0.1)\\nax2.set_title(\"Automatic Clustering\\\\nOPTICS\")'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_optics.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# DBSCAN at 0.5\\ncolors = [\"g.\", \"r.\", \"b.\", \"c.\"]\\nfor klass, color in enumerate(colors):\\n    Xk = X[labels_050 == klass]\\n    ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\\nax3.plot(X[labels_050 == -1, 0], X[labels_050 == -1, 1], \"k+\", alpha=0.1)\\nax3.set_title(\"Clustering at 0.5 epsilon cut\\\\nDBSCAN\")\\n\\n# DBSCAN at 2.\\ncolors = [\"g.\", \"m.\", \"y.\", \"c.\"]\\nfor klass, color in enumerate(colors):\\n    Xk = X[labels_200 == klass]\\n    ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\\nax4.plot(X[labels_200 == -1, 0], X[labels_200 == -1, 1], \"k+\", alpha=0.1)\\nax4.set_title(\"Clustering at 2.0 epsilon cut\\\\nDBSCAN\")\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_agglomerative_dendrogram.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_dendrogram(model, **kwargs):\\n    # Create linkage matrix and then plot the dendrogram\\n\\n    # create the counts of samples under each node\\n    counts = np.zeros(model.children_.shape[0])\\n    n_samples = len(model.labels_)\\n    for i, merge in enumerate(model.children_):\\n        current_count = 0\\n        for child_idx in merge:\\n            if child_idx < n_samples:\\n                current_count += 1  # leaf node\\n            else:\\n                current_count += counts[child_idx - n_samples]\\n        counts[i] = current_count\\n\\n    linkage_matrix = np.column_stack(\\n        [model.children_, model.distances_, counts]\\n    ).astype(float)\\n\\n    # Plot the corresponding dendrogram\\n    dendrogram(linkage_matrix, **kwargs)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_agglomerative_dendrogram.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\"\"\"\\n=========================================\\nPlot Hierarchical Clustering Dendrogram\\n=========================================\\nThis example plots the corresponding dendrogram of a hierarchical clustering\\nusing AgglomerativeClustering and the dendrogram method available in scipy.\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nfrom scipy.cluster.hierarchy import dendrogram\\n\\nfrom sklearn.cluster import AgglomerativeClustering\\nfrom sklearn.datasets import load_iris\\n\\n\\n# Code for: def plot_dendrogram(model, **kwargs):\\n\\n\\niris = load_iris()\\nX = iris.data\\n\\n# setting distance_threshold=0 ensures we compute the full tree.\\nmodel = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\\n\\nmodel = model.fit(X)\\nplt.title(\"Hierarchical Clustering Dendrogram\")\\n# plot the top three levels of the dendrogram\\nplot_dendrogram(model, truncate_mode=\"level\", p=3)\\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_cluster_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nComparing different clustering algorithms on toy datasets\\n=========================================================\\n\\nThis example shows characteristics of different\\nclustering algorithms on datasets that are \"interesting\"\\nbut still in 2D. With the exception of the last dataset,\\nthe parameters of each of these dataset-algorithm pairs\\nhas been tuned to produce good clustering results. Some\\nalgorithms are more sensitive to parameter values than\\nothers.\\n\\nThe last dataset is an example of a \\'null\\' situation for\\nclustering: the data is homogeneous, and there is no good\\nclustering. For this example, the null dataset uses the\\nsame parameters as the dataset in the row above it, which\\nrepresents a mismatch in the parameter values and the\\ndata structure.\\n\\nWhile these examples give some intuition about the\\nalgorithms, this intuition might not apply to very high\\ndimensional data.\\n\\n\"\"\"\\n\\nimport time\\nimport warnings\\nfrom itertools import cycle, islice\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import cluster, datasets, mixture\\nfrom sklearn.neighbors import kneighbors_graph\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# ============\\n# Generate datasets. We choose the size big enough to see the scalability\\n# of the algorithms, but not too big to avoid too long running times\\n# ============\\nn_samples = 500\\nseed = 30\\nnoisy_circles = datasets.make_circles(\\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\\n)\\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\\nrng = np.random.RandomState(seed)\\nno_structure = rng.rand(n_samples, 2), None\\n\\n# Anisotropicly distributed data\\nrandom_state = 170\\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\\nX_aniso = np.dot(X, transformation)\\naniso = (X_aniso, y)\\n\\n# blobs with varied variances\\nvaried = datasets.make_blobs(\\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\\n)\\n\\n# ============\\n# Set up cluster parameters\\n# ============\\nplt.figure(figsize=(9 * 2 + 3, 13))\\nplt.subplots_adjust(\\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\\n)\\n\\nplot_num = 1\\n\\ndefault_base = {\\n    \"quantile\": 0.3,\\n    \"eps\": 0.3,\\n    \"damping\": 0.9,\\n    \"preference\": -200,\\n    \"n_neighbors\": 3,\\n    \"n_clusters\": 3,\\n    \"min_samples\": 7,\\n    \"xi\": 0.05,\\n    \"min_cluster_size\": 0.1,\\n    \"allow_single_cluster\": True,\\n    \"hdbscan_min_cluster_size\": 15,\\n    \"hdbscan_min_samples\": 3,\\n    \"random_state\": 42,\\n}'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_cluster_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='datasets = [\\n    (\\n        noisy_circles,\\n        {\\n            \"damping\": 0.77,\\n            \"preference\": -240,\\n            \"quantile\": 0.2,\\n            \"n_clusters\": 2,\\n            \"min_samples\": 7,\\n            \"xi\": 0.08,\\n        },\\n    ),\\n    (\\n        noisy_moons,\\n        {\\n            \"damping\": 0.75,\\n            \"preference\": -220,\\n            \"n_clusters\": 2,\\n            \"min_samples\": 7,\\n            \"xi\": 0.1,\\n        },\\n    ),\\n    (\\n        varied,\\n        {\\n            \"eps\": 0.18,\\n            \"n_neighbors\": 2,\\n            \"min_samples\": 7,\\n            \"xi\": 0.01,\\n            \"min_cluster_size\": 0.2,\\n        },\\n    ),\\n    (\\n        aniso,\\n        {\\n            \"eps\": 0.15,\\n            \"n_neighbors\": 2,\\n            \"min_samples\": 7,\\n            \"xi\": 0.1,\\n            \"min_cluster_size\": 0.2,\\n        },\\n    ),\\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\\n    (no_structure, {}),\\n]\\n\\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\\n    # update parameters with dataset-specific values\\n    params = default_base.copy()\\n    params.update(algo_params)\\n\\n    X, y = dataset\\n\\n    # normalize dataset for easier parameter selection\\n    X = StandardScaler().fit_transform(X)\\n\\n    # estimate bandwidth for mean shift\\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\\n\\n    # connectivity matrix for structured Ward\\n    connectivity = kneighbors_graph(\\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\\n    )\\n    # make connectivity symmetric\\n    connectivity = 0.5 * (connectivity + connectivity.T)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_cluster_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# connectivity matrix for structured Ward\\n    connectivity = kneighbors_graph(\\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\\n    )\\n    # make connectivity symmetric\\n    connectivity = 0.5 * (connectivity + connectivity.T)\\n\\n    # ============\\n    # Create cluster objects\\n    # ============\\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\\n    two_means = cluster.MiniBatchKMeans(\\n        n_clusters=params[\"n_clusters\"],\\n        random_state=params[\"random_state\"],\\n    )\\n    ward = cluster.AgglomerativeClustering(\\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\\n    )\\n    spectral = cluster.SpectralClustering(\\n        n_clusters=params[\"n_clusters\"],\\n        eigen_solver=\"arpack\",\\n        affinity=\"nearest_neighbors\",\\n        random_state=params[\"random_state\"],\\n    )\\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\\n    hdbscan = cluster.HDBSCAN(\\n        min_samples=params[\"hdbscan_min_samples\"],\\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\\n        allow_single_cluster=params[\"allow_single_cluster\"],\\n    )\\n    optics = cluster.OPTICS(\\n        min_samples=params[\"min_samples\"],\\n        xi=params[\"xi\"],\\n        min_cluster_size=params[\"min_cluster_size\"],\\n    )\\n    affinity_propagation = cluster.AffinityPropagation(\\n        damping=params[\"damping\"],\\n        preference=params[\"preference\"],\\n        random_state=params[\"random_state\"],\\n    )\\n    average_linkage = cluster.AgglomerativeClustering(\\n        linkage=\"average\",\\n        metric=\"cityblock\",\\n        n_clusters=params[\"n_clusters\"],\\n        connectivity=connectivity,\\n    )\\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\\n    gmm = mixture.GaussianMixture(\\n        n_components=params[\"n_clusters\"],\\n        covariance_type=\"full\",\\n        random_state=params[\"random_state\"],\\n    )\\n\\n    clustering_algorithms = (\\n        (\"MiniBatch\\\\nKMeans\", two_means),\\n        (\"Affinity\\\\nPropagation\", affinity_propagation),\\n        (\"MeanShift\", ms),\\n        (\"Spectral\\\\nClustering\", spectral),\\n        (\"Ward\", ward),\\n        (\"Agglomerative\\\\nClustering\", average_linkage),\\n        (\"DBSCAN\", dbscan),\\n        (\"HDBSCAN\", hdbscan),\\n        (\"OPTICS\", optics),\\n        (\"BIRCH\", birch),\\n        (\"Gaussian\\\\nMixture\", gmm),\\n    )\\n\\n    for name, algorithm in clustering_algorithms:\\n        t0 = time.time()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_cluster_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for name, algorithm in clustering_algorithms:\\n        t0 = time.time()\\n\\n        # catch warnings related to kneighbors_graph\\n        with warnings.catch_warnings():\\n            warnings.filterwarnings(\\n                \"ignore\",\\n                message=\"the number of connected components of the \"\\n                + \"connectivity matrix is [0-9]{1,2}\"\\n                + \" > 1. Completing it to avoid stopping the tree early.\",\\n                category=UserWarning,\\n            )\\n            warnings.filterwarnings(\\n                \"ignore\",\\n                message=\"Graph is not fully connected, spectral embedding\"\\n                + \" may not work as expected.\",\\n                category=UserWarning,\\n            )\\n            algorithm.fit(X)\\n\\n        t1 = time.time()\\n        if hasattr(algorithm, \"labels_\"):\\n            y_pred = algorithm.labels_.astype(int)\\n        else:\\n            y_pred = algorithm.predict(X)\\n\\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\\n        if i_dataset == 0:\\n            plt.title(name, size=18)\\n\\n        colors = np.array(\\n            list(\\n                islice(\\n                    cycle(\\n                        [\\n                            \"#377eb8\",\\n                            \"#ff7f00\",\\n                            \"#4daf4a\",\\n                            \"#f781bf\",\\n                            \"#a65628\",\\n                            \"#984ea3\",\\n                            \"#999999\",\\n                            \"#e41a1c\",\\n                            \"#dede00\",\\n                        ]\\n                    ),\\n                    int(max(y_pred) + 1),\\n                )\\n            )\\n        )\\n        # add black color for outliers (if any)\\n        colors = np.append(colors, [\"#000000\"])\\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\\n\\n        plt.xlim(-2.5, 2.5)\\n        plt.ylim(-2.5, 2.5)\\n        plt.xticks(())\\n        plt.yticks(())\\n        plt.text(\\n            0.99,\\n            0.01,\\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\\n            transform=plt.gca().transAxes,\\n            size=15,\\n            horizontalalignment=\"right\",\\n        )\\n        plot_num += 1\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_affinity_propagation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================\\nDemo of affinity propagation clustering algorithm\\n=================================================\\n\\nReference:\\nBrendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\\nBetween Data Points\", Science Feb. 2007\\n\\n\"\"\"\\n\\nimport numpy as np\\n\\nfrom sklearn import metrics\\nfrom sklearn.cluster import AffinityPropagation\\nfrom sklearn.datasets import make_blobs\\n\\n# %%\\n# Generate sample data\\n# --------------------\\ncenters = [[1, 1], [-1, -1], [1, -1]]\\nX, labels_true = make_blobs(\\n    n_samples=300, centers=centers, cluster_std=0.5, random_state=0\\n)\\n\\n# %%\\n# Compute Affinity Propagation\\n# ----------------------------\\naf = AffinityPropagation(preference=-50, random_state=0).fit(X)\\ncluster_centers_indices = af.cluster_centers_indices_\\nlabels = af.labels_\\n\\nn_clusters_ = len(cluster_centers_indices)\\n\\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\\nprint(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))\\nprint(\\n    \"Adjusted Mutual Information: %0.3f\"\\n    % metrics.adjusted_mutual_info_score(labels_true, labels)\\n)\\nprint(\\n    \"Silhouette Coefficient: %0.3f\"\\n    % metrics.silhouette_score(X, labels, metric=\"sqeuclidean\")\\n)\\n\\n# %%\\n# Plot result\\n# -----------\\nimport matplotlib.pyplot as plt\\n\\nplt.close(\"all\")\\nplt.figure(1)\\nplt.clf()\\n\\ncolors = plt.cycler(\"color\", plt.cm.viridis(np.linspace(0, 1, 4)))\\n\\nfor k, col in zip(range(n_clusters_), colors):\\n    class_members = labels == k\\n    cluster_center = X[cluster_centers_indices[k]]\\n    plt.scatter(\\n        X[class_members, 0], X[class_members, 1], color=col[\"color\"], marker=\".\"\\n    )\\n    plt.scatter(\\n        cluster_center[0], cluster_center[1], s=14, color=col[\"color\"], marker=\"o\"\\n    )\\n    for x in X[class_members]:\\n        plt.plot(\\n            [cluster_center[0], x[0]], [cluster_center[1], x[1]], color=col[\"color\"]\\n        )\\n\\nplt.title(\"Estimated number of clusters: %d\" % n_clusters_)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_dict_face_patches.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\nOnline learning of a dictionary of parts of faces\\n=================================================\\n\\nThis example uses a large dataset of faces to learn a set of 20 x 20\\nimages patches that constitute faces.\\n\\nFrom the programming standpoint, it is interesting because it shows how\\nto use the online API of the scikit-learn to process a very large\\ndataset by chunks. The way we proceed is that we load an image at a time\\nand extract randomly 50 patches from this image. Once we have accumulated\\n500 of these patches (using 10 images), we run the\\n:func:`~sklearn.cluster.MiniBatchKMeans.partial_fit` method\\nof the online KMeans object, MiniBatchKMeans.\\n\\nThe verbose setting on the MiniBatchKMeans enables us to see that some\\nclusters are reassigned during the successive calls to\\npartial-fit. This is because the number of patches that they represent\\nhas become too low, and it is better to choose a random new\\ncluster.\\n\\n\"\"\"\\n\\n# %%\\n# Load the data\\n# -------------\\n\\nfrom sklearn import datasets\\n\\nfaces = datasets.fetch_olivetti_faces()\\n\\n# %%\\n# Learn the dictionary of images\\n# ------------------------------\\n\\nimport time\\n\\nimport numpy as np\\n\\nfrom sklearn.cluster import MiniBatchKMeans\\nfrom sklearn.feature_extraction.image import extract_patches_2d\\n\\nprint(\"Learning the dictionary... \")\\nrng = np.random.RandomState(0)\\nkmeans = MiniBatchKMeans(n_clusters=81, random_state=rng, verbose=True, n_init=3)\\npatch_size = (20, 20)\\n\\nbuffer = []\\nt0 = time.time()\\n\\n# The online learning part: cycle over the whole dataset 6 times\\nindex = 0\\nfor _ in range(6):\\n    for img in faces.images:\\n        data = extract_patches_2d(img, patch_size, max_patches=50, random_state=rng)\\n        data = np.reshape(data, (len(data), -1))\\n        buffer.append(data)\\n        index += 1\\n        if index % 10 == 0:\\n            data = np.concatenate(buffer, axis=0)\\n            data -= np.mean(data, axis=0)\\n            data /= np.std(data, axis=0)\\n            kmeans.partial_fit(data)\\n            buffer = []\\n        if index % 100 == 0:\\n            print(\"Partial fit of %4i out of %i\" % (index, 6 * len(faces.images)))\\n\\ndt = time.time() - t0\\nprint(\"done in %.2fs.\" % dt)\\n\\n# %%\\n# Plot the results\\n# ----------------\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.figure(figsize=(4.2, 4))\\nfor i, patch in enumerate(kmeans.cluster_centers_):\\n    plt.subplot(9, 9, i + 1)\\n    plt.imshow(patch.reshape(patch_size), cmap=plt.cm.gray, interpolation=\"nearest\")\\n    plt.xticks(())\\n    plt.yticks(())\\n\\n\\nplt.suptitle(\\n    \"Patches of faces\\\\nTrain time %.1fs on %d patches\" % (dt, 8 * len(faces.images)),\\n    fontsize=16,\\n)\\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_color_quantization.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def recreate_image(codebook, labels, w, h):\\n    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\\n    return codebook[labels].reshape(w, h, -1)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_color_quantization.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================\\nColor Quantization using K-Means\\n==================================\\n\\nPerforms a pixel-wise Vector Quantization (VQ) of an image of the summer palace\\n(China), reducing the number of colors required to show the image from 96,615\\nunique colors to 64, while preserving the overall appearance quality.\\n\\nIn this example, pixels are represented in a 3D-space and K-means is used to\\nfind 64 color clusters. In the image processing literature, the codebook\\nobtained from K-means (the cluster centers) is called the color palette. Using\\na single byte, up to 256 colors can be addressed, whereas an RGB encoding\\nrequires 3 bytes per pixel. The GIF file format, for example, uses such a\\npalette.\\n\\nFor comparison, a quantized image using a random codebook (colors picked up\\nrandomly) is also shown.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom time import time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.datasets import load_sample_image\\nfrom sklearn.metrics import pairwise_distances_argmin\\nfrom sklearn.utils import shuffle\\n\\nn_colors = 64\\n\\n# Load the Summer Palace photo\\nchina = load_sample_image(\"china.jpg\")\\n\\n# Convert to floats instead of the default 8 bits integer coding. Dividing by\\n# 255 is important so that plt.imshow works well on float data (need to\\n# be in the range [0-1])\\nchina = np.array(china, dtype=np.float64) / 255\\n\\n# Load Image and transform to a 2D numpy array.\\nw, h, d = original_shape = tuple(china.shape)\\nassert d == 3\\nimage_array = np.reshape(china, (w * h, d))\\n\\nprint(\"Fitting model on a small sub-sample of the data\")\\nt0 = time()\\nimage_array_sample = shuffle(image_array, random_state=0, n_samples=1_000)\\nkmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)\\nprint(f\"done in {time() - t0:0.3f}s.\")\\n\\n# Get labels for all points\\nprint(\"Predicting color indices on the full image (k-means)\")\\nt0 = time()\\nlabels = kmeans.predict(image_array)\\nprint(f\"done in {time() - t0:0.3f}s.\")\\n\\n\\ncodebook_random = shuffle(image_array, random_state=0, n_samples=n_colors)\\nprint(\"Predicting color indices on the full image (random)\")\\nt0 = time()\\nlabels_random = pairwise_distances_argmin(codebook_random, image_array, axis=0)\\nprint(f\"done in {time() - t0:0.3f}s.\")\\n\\n\\n# Code for: def recreate_image(codebook, labels, w, h):\\n\\n\\n# Display all results, alongside original image\\nplt.figure(1)\\nplt.clf()\\nplt.axis(\"off\")\\nplt.title(\"Original image (96,615 colors)\")\\nplt.imshow(china)\\n\\nplt.figure(2)\\nplt.clf()\\nplt.axis(\"off\")\\nplt.title(f\"Quantized image ({n_colors} colors, K-Means)\")\\nplt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\\n\\nplt.figure(3)\\nplt.clf()\\nplt.axis(\"off\")\\nplt.title(f\"Quantized image ({n_colors} colors, Random)\")\\nplt.imshow(recreate_image(codebook_random, labels_random, w, h))\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_plusplus.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================================\\nAn example of K-Means++ initialization\\n===========================================================\\n\\nAn example to show the output of the :func:`sklearn.cluster.kmeans_plusplus`\\nfunction for generating initial seeds for clustering.\\n\\nK-Means++ is used as the default initialization for :ref:`k_means`.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.cluster import kmeans_plusplus\\nfrom sklearn.datasets import make_blobs\\n\\n# Generate sample data\\nn_samples = 4000\\nn_components = 4\\n\\nX, y_true = make_blobs(\\n    n_samples=n_samples, centers=n_components, cluster_std=0.60, random_state=0\\n)\\nX = X[:, ::-1]\\n\\n# Calculate seeds from k-means++\\ncenters_init, indices = kmeans_plusplus(X, n_clusters=4, random_state=0)\\n\\n# Plot init seeds along side sample data\\nplt.figure(1)\\ncolors = [\"#4EACC5\", \"#FF9C34\", \"#4E9A06\", \"m\"]\\n\\nfor k, col in enumerate(colors):\\n    cluster_data = y_true == k\\n    plt.scatter(X[cluster_data, 0], X[cluster_data, 1], c=col, marker=\".\", s=10)\\n\\nplt.scatter(centers_init[:, 0], centers_init[:, 1], c=\"b\", s=50)\\nplt.title(\"K-Means++ Initialization\")\\nplt.xticks([])\\nplt.yticks([])\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_face_compress.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================\\nVector Quantization Example\\n===========================\\n\\nThis example shows how one can use :class:`~sklearn.preprocessing.KBinsDiscretizer`\\nto perform vector quantization on a set of toy image, the raccoon face.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Original image\\n# --------------\\n#\\n# We start by loading the raccoon face image from SciPy. We will additionally check\\n# a couple of information regarding the image, such as the shape and data type used\\n# to store the image.\\n#\\n# Note that depending of the SciPy version, we have to adapt the import since the\\n# function returning the image is not located in the same module. Also, SciPy >= 1.10\\n# requires the package `pooch` to be installed.\\ntry:  # Scipy >= 1.10\\n    from scipy.datasets import face\\nexcept ImportError:\\n    from scipy.misc import face\\n\\nraccoon_face = face(gray=True)\\n\\nprint(f\"The dimension of the image is {raccoon_face.shape}\")\\nprint(f\"The data used to encode the image is of type {raccoon_face.dtype}\")\\nprint(f\"The number of bytes taken in RAM is {raccoon_face.nbytes}\")\\n\\n# %%\\n# Thus the image is a 2D array of 768 pixels in height and 1024 pixels in width. Each\\n# value is a 8-bit unsigned integer, which means that the image is encoded using 8\\n# bits per pixel. The total memory usage of the image is 786 kilobytes (1 byte equals\\n# 8 bits).\\n#\\n# Using 8-bit unsigned integer means that the image is encoded using 256 different\\n# shades of gray, at most. We can check the distribution of these values.\\nimport matplotlib.pyplot as plt\\n\\nfig, ax = plt.subplots(ncols=2, figsize=(12, 4))\\n\\nax[0].imshow(raccoon_face, cmap=plt.cm.gray)\\nax[0].axis(\"off\")\\nax[0].set_title(\"Rendering of the image\")\\nax[1].hist(raccoon_face.ravel(), bins=256)\\nax[1].set_xlabel(\"Pixel value\")\\nax[1].set_ylabel(\"Count of pixels\")\\nax[1].set_title(\"Distribution of the pixel values\")\\n_ = fig.suptitle(\"Original image of a raccoon face\")\\n\\n# %%\\n# Compression via vector quantization\\n# -----------------------------------\\n#\\n# The idea behind compression via vector quantization is to reduce the number of\\n# gray levels to represent an image. For instance, we can use 8 values instead\\n# of 256 values. Therefore, it means that we could efficiently use 3 bits instead\\n# of 8 bits to encode a single pixel and therefore reduce the memory usage by a\\n# factor of approximately 2.5. We will later discuss about this memory usage.\\n#\\n# Encoding strategy\\n# \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\n#\\n# The compression can be done using a\\n# :class:`~sklearn.preprocessing.KBinsDiscretizer`. We need to choose a strategy\\n# to define the 8 gray values to sub-sample. The simplest strategy is to define\\n# them equally spaced, which correspond to setting `strategy=\"uniform\"`. From\\n# the previous histogram, we know that this strategy is certainly not optimal.\\n\\nfrom sklearn.preprocessing import KBinsDiscretizer'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_face_compress.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.preprocessing import KBinsDiscretizer\\n\\nn_bins = 8\\nencoder = KBinsDiscretizer(\\n    n_bins=n_bins,\\n    encode=\"ordinal\",\\n    strategy=\"uniform\",\\n    random_state=0,\\n)\\ncompressed_raccoon_uniform = encoder.fit_transform(raccoon_face.reshape(-1, 1)).reshape(\\n    raccoon_face.shape\\n)\\n\\nfig, ax = plt.subplots(ncols=2, figsize=(12, 4))\\nax[0].imshow(compressed_raccoon_uniform, cmap=plt.cm.gray)\\nax[0].axis(\"off\")\\nax[0].set_title(\"Rendering of the image\")\\nax[1].hist(compressed_raccoon_uniform.ravel(), bins=256)\\nax[1].set_xlabel(\"Pixel value\")\\nax[1].set_ylabel(\"Count of pixels\")\\nax[1].set_title(\"Sub-sampled distribution of the pixel values\")\\n_ = fig.suptitle(\"Raccoon face compressed using 3 bits and a uniform strategy\")\\n\\n# %%\\n# Qualitatively, we can spot some small regions where we see the effect of the\\n# compression (e.g. leaves on the bottom right corner). But after all, the resulting\\n# image is still looking good.\\n#\\n# We observe that the distribution of pixels values have been mapped to 8\\n# different values. We can check the correspondence between such values and the\\n# original pixel values.\\n\\nbin_edges = encoder.bin_edges_[0]\\nbin_center = bin_edges[:-1] + (bin_edges[1:] - bin_edges[:-1]) / 2\\nbin_center\\n\\n# %%\\n_, ax = plt.subplots()\\nax.hist(raccoon_face.ravel(), bins=256)\\ncolor = \"tab:orange\"\\nfor center in bin_center:\\n    ax.axvline(center, color=color)\\n    ax.text(center - 10, ax.get_ybound()[1] + 100, f\"{center:.1f}\", color=color)\\n\\n# %%\\n# As previously stated, the uniform sampling strategy is not optimal. Notice for\\n# instance that the pixels mapped to the value 7 will encode a rather small\\n# amount of information, whereas the mapped value 3 will represent a large\\n# amount of counts. We can instead use a clustering strategy such as k-means to\\n# find a more optimal mapping.\\n\\nencoder = KBinsDiscretizer(\\n    n_bins=n_bins,\\n    encode=\"ordinal\",\\n    strategy=\"kmeans\",\\n    random_state=0,\\n)\\ncompressed_raccoon_kmeans = encoder.fit_transform(raccoon_face.reshape(-1, 1)).reshape(\\n    raccoon_face.shape\\n)\\n\\nfig, ax = plt.subplots(ncols=2, figsize=(12, 4))\\nax[0].imshow(compressed_raccoon_kmeans, cmap=plt.cm.gray)\\nax[0].axis(\"off\")\\nax[0].set_title(\"Rendering of the image\")\\nax[1].hist(compressed_raccoon_kmeans.ravel(), bins=256)\\nax[1].set_xlabel(\"Pixel value\")\\nax[1].set_ylabel(\"Number of pixels\")\\nax[1].set_title(\"Distribution of the pixel values\")\\n_ = fig.suptitle(\"Raccoon face compressed using 3 bits and a K-means strategy\")\\n\\n# %%\\nbin_edges = encoder.bin_edges_[0]\\nbin_center = bin_edges[:-1] + (bin_edges[1:] - bin_edges[:-1]) / 2\\nbin_center\\n\\n# %%\\n_, ax = plt.subplots()\\nax.hist(raccoon_face.ravel(), bins=256)\\ncolor = \"tab:orange\"\\nfor center in bin_center:\\n    ax.axvline(center, color=color)\\n    ax.text(center - 10, ax.get_ybound()[1] + 100, f\"{center:.1f}\", color=color)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_face_compress.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n_, ax = plt.subplots()\\nax.hist(raccoon_face.ravel(), bins=256)\\ncolor = \"tab:orange\"\\nfor center in bin_center:\\n    ax.axvline(center, color=color)\\n    ax.text(center - 10, ax.get_ybound()[1] + 100, f\"{center:.1f}\", color=color)\\n\\n# %%\\n# The counts in the bins are now more balanced and their centers are no longer\\n# equally spaced. Note that we could enforce the same number of pixels per bin\\n# by using the `strategy=\"quantile\"` instead of `strategy=\"kmeans\"`.\\n#\\n# Memory footprint\\n# \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\n#\\n# We previously stated that we should save 8 times less memory. Let\\'s verify it.\\n\\nprint(f\"The number of bytes taken in RAM is {compressed_raccoon_kmeans.nbytes}\")\\nprint(f\"Compression ratio: {compressed_raccoon_kmeans.nbytes / raccoon_face.nbytes}\")\\n\\n# %%\\n# It is quite surprising to see that our compressed image is taking x8 more\\n# memory than the original image. This is indeed the opposite of what we\\n# expected. The reason is mainly due to the type of data used to encode the\\n# image.\\n\\nprint(f\"Type of the compressed image: {compressed_raccoon_kmeans.dtype}\")\\n\\n# %%\\n# Indeed, the output of the :class:`~sklearn.preprocessing.KBinsDiscretizer` is\\n# an array of 64-bit float. It means that it takes x8 more memory. However, we\\n# use this 64-bit float representation to encode 8 values. Indeed, we will save\\n# memory only if we cast the compressed image into an array of 3-bits integers. We\\n# could use the method `numpy.ndarray.astype`. However, a 3-bits integer\\n# representation does not exist and to encode the 8 values, we would need to use\\n# the 8-bit unsigned integer representation as well.\\n#\\n# In practice, observing a memory gain would require the original image to be in\\n# a 64-bit float representation.'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_segmentation_toy.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================\\nSpectral clustering for image segmentation\\n===========================================\\n\\nIn this example, an image with connected circles is generated and\\nspectral clustering is used to separate the circles.\\n\\nIn these settings, the :ref:`spectral_clustering` approach solves the problem\\nknow as \\'normalized graph cuts\\': the image is seen as a graph of\\nconnected voxels, and the spectral clustering algorithm amounts to\\nchoosing graph cuts defining regions while minimizing the ratio of the\\ngradient along the cut, and the volume of the region.\\n\\nAs the algorithm tries to balance the volume (ie balance the region\\nsizes), if we take circles with different sizes, the segmentation fails.\\n\\nIn addition, as there is no useful information in the intensity of the image,\\nor its gradient, we choose to perform the spectral clustering on a graph\\nthat is only weakly informed by the gradient. This is close to performing\\na Voronoi partition of the graph.\\n\\nIn addition, we use the mask of the objects to restrict the graph to the\\noutline of the objects. In this example, we are interested in\\nseparating the objects one from the other, and not from the background.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate the data\\n# -----------------\\nimport numpy as np\\n\\nl = 100\\nx, y = np.indices((l, l))\\n\\ncenter1 = (28, 24)\\ncenter2 = (40, 50)\\ncenter3 = (67, 58)\\ncenter4 = (24, 70)\\n\\nradius1, radius2, radius3, radius4 = 16, 14, 15, 14\\n\\ncircle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1**2\\ncircle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2**2\\ncircle3 = (x - center3[0]) ** 2 + (y - center3[1]) ** 2 < radius3**2\\ncircle4 = (x - center4[0]) ** 2 + (y - center4[1]) ** 2 < radius4**2\\n\\n# %%\\n# Plotting four circles\\n# ---------------------\\nimg = circle1 + circle2 + circle3 + circle4\\n\\n# We use a mask that limits to the foreground: the problem that we are\\n# interested in here is not separating the objects from the background,\\n# but separating them one from the other.\\nmask = img.astype(bool)\\n\\nimg = img.astype(float)\\nimg += 1 + 0.2 * np.random.randn(*img.shape)\\n\\n# %%\\n# Convert the image into a graph with the value of the gradient on the\\n# edges.\\nfrom sklearn.feature_extraction import image\\n\\ngraph = image.img_to_graph(img, mask=mask)\\n\\n# %%\\n# Take a decreasing function of the gradient resulting in a segmentation\\n# that is close to a Voronoi partition\\ngraph.data = np.exp(-graph.data / graph.data.std())\\n\\n# %%\\n# Here we perform spectral clustering using the arpack solver since amg is\\n# numerically unstable on this example. We then plot the results.\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.cluster import spectral_clustering\\n\\nlabels = spectral_clustering(graph, n_clusters=4, eigen_solver=\"arpack\")\\nlabel_im = np.full(mask.shape, -1.0)\\nlabel_im[mask] = labels\\n\\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\\naxs[0].matshow(img)\\naxs[1].matshow(label_im)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_segmentation_toy.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='labels = spectral_clustering(graph, n_clusters=4, eigen_solver=\"arpack\")\\nlabel_im = np.full(mask.shape, -1.0)\\nlabel_im[mask] = labels\\n\\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\\naxs[0].matshow(img)\\naxs[1].matshow(label_im)\\n\\nplt.show()\\n\\n# %%\\n# Plotting two circles\\n# --------------------\\n# Here we repeat the above process but only consider the first two circles\\n# we generated. Note that this results in a cleaner separation between the\\n# circles as the region sizes are easier to balance in this case.\\n\\nimg = circle1 + circle2\\nmask = img.astype(bool)\\nimg = img.astype(float)\\n\\nimg += 1 + 0.2 * np.random.randn(*img.shape)\\n\\ngraph = image.img_to_graph(img, mask=mask)\\ngraph.data = np.exp(-graph.data / graph.data.std())\\n\\nlabels = spectral_clustering(graph, n_clusters=2, eigen_solver=\"arpack\")\\nlabel_im = np.full(mask.shape, -1.0)\\nlabel_im[mask] = labels\\n\\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\\naxs[0].matshow(img)\\naxs[1].matshow(label_im)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_silhouette_analysis.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================================================================\\nSelecting the number of clusters with silhouette analysis on KMeans clustering\\n===============================================================================\\n\\nSilhouette analysis can be used to study the separation distance between the\\nresulting clusters. The silhouette plot displays a measure of how close each\\npoint in one cluster is to points in the neighboring clusters and thus provides\\na way to assess parameters like number of clusters visually. This measure has a\\nrange of [-1, 1].\\n\\nSilhouette coefficients (as these values are referred to as) near +1 indicate\\nthat the sample is far away from the neighboring clusters. A value of 0\\nindicates that the sample is on or very close to the decision boundary between\\ntwo neighboring clusters and negative values indicate that those samples might\\nhave been assigned to the wrong cluster.\\n\\nIn this example the silhouette analysis is used to choose an optimal value for\\n``n_clusters``. The silhouette plot shows that the ``n_clusters`` value of 3, 5\\nand 6 are a bad pick for the given data due to the presence of clusters with\\nbelow average silhouette scores and also due to wide fluctuations in the size\\nof the silhouette plots. Silhouette analysis is more ambivalent in deciding\\nbetween 2 and 4.\\n\\nAlso from the thickness of the silhouette plot the cluster size can be\\nvisualized. The silhouette plot for cluster 0 when ``n_clusters`` is equal to\\n2, is bigger in size owing to the grouping of the 3 sub clusters into one big\\ncluster. However when the ``n_clusters`` is equal to 4, all the plots are more\\nor less of similar thickness and hence are of similar sizes as can be also\\nverified from the labelled scatter plot on the right.\\n\\n\"\"\"\\n\\nimport matplotlib.cm as cm\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.metrics import silhouette_samples, silhouette_score\\n\\n# Generating the sample data from make_blobs\\n# This particular setting has one distinct cluster and 3 clusters placed close\\n# together.\\nX, y = make_blobs(\\n    n_samples=500,\\n    n_features=2,\\n    centers=4,\\n    cluster_std=1,\\n    center_box=(-10.0, 10.0),\\n    shuffle=True,\\n    random_state=1,\\n)  # For reproducibility\\n\\nrange_n_clusters = [2, 3, 4, 5, 6]\\n\\nfor n_clusters in range_n_clusters:\\n    # Create a subplot with 1 row and 2 columns\\n    fig, (ax1, ax2) = plt.subplots(1, 2)\\n    fig.set_size_inches(18, 7)\\n\\n    # The 1st subplot is the silhouette plot\\n    # The silhouette coefficient can range from -1, 1 but in this example all\\n    # lie within [-0.1, 1]\\n    ax1.set_xlim([-0.1, 1])\\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\\n    # plots of individual clusters, to demarcate them clearly.\\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_silhouette_analysis.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Initialize the clusterer with n_clusters value and a random generator\\n    # seed of 10 for reproducibility.\\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\\n    cluster_labels = clusterer.fit_predict(X)\\n\\n    # The silhouette_score gives the average value for all the samples.\\n    # This gives a perspective into the density and separation of the formed\\n    # clusters\\n    silhouette_avg = silhouette_score(X, cluster_labels)\\n    print(\\n        \"For n_clusters =\",\\n        n_clusters,\\n        \"The average silhouette_score is :\",\\n        silhouette_avg,\\n    )\\n\\n    # Compute the silhouette scores for each sample\\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\\n\\n    y_lower = 10\\n    for i in range(n_clusters):\\n        # Aggregate the silhouette scores for samples belonging to\\n        # cluster i, and sort them\\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\\n\\n        ith_cluster_silhouette_values.sort()\\n\\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\\n        y_upper = y_lower + size_cluster_i\\n\\n        color = cm.nipy_spectral(float(i) / n_clusters)\\n        ax1.fill_betweenx(\\n            np.arange(y_lower, y_upper),\\n            0,\\n            ith_cluster_silhouette_values,\\n            facecolor=color,\\n            edgecolor=color,\\n            alpha=0.7,\\n        )\\n\\n        # Label the silhouette plots with their cluster numbers at the middle\\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\\n\\n        # Compute the new y_lower for next plot\\n        y_lower = y_upper + 10  # 10 for the 0 samples\\n\\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\\n    ax1.set_xlabel(\"The silhouette coefficient values\")\\n    ax1.set_ylabel(\"Cluster label\")\\n\\n    # The vertical line for average silhouette score of all the values\\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\\n\\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\\n\\n    # 2nd Plot showing the actual clusters formed\\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\\n    ax2.scatter(\\n        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\\n    )\\n\\n    # Labeling the clusters\\n    centers = clusterer.cluster_centers_\\n    # Draw white circles at cluster centers\\n    ax2.scatter(\\n        centers[:, 0],\\n        centers[:, 1],\\n        marker=\"o\",\\n        c=\"white\",\\n        alpha=1,\\n        s=200,\\n        edgecolor=\"k\",\\n    )\\n\\n    for i, c in enumerate(centers):\\n        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\\n\\n    ax2.set_title(\"The visualization of the clustered data.\")\\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_kmeans_silhouette_analysis.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for i, c in enumerate(centers):\\n        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\\n\\n    ax2.set_title(\"The visualization of the clustered data.\")\\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\\n\\n    plt.suptitle(\\n        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\\n        % n_clusters,\\n        fontsize=14,\\n        fontweight=\"bold\",\\n    )\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_mini_batch_kmeans.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================================================\\nComparison of the K-Means and MiniBatchKMeans clustering algorithms\\n====================================================================\\n\\nWe want to compare the performance of the MiniBatchKMeans and KMeans:\\nthe MiniBatchKMeans is faster, but gives slightly different results (see\\n:ref:`mini_batch_kmeans`).\\n\\nWe will cluster a set of data, first with KMeans and then with\\nMiniBatchKMeans, and plot the results.\\nWe will also plot the points that are labelled differently between the two\\nalgorithms.\\n\\n\"\"\"\\n\\n# %%\\n# Generate the data\\n# -----------------\\n#\\n# We start by generating the blobs of data to be clustered.\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_blobs\\n\\nnp.random.seed(0)\\n\\nbatch_size = 45\\ncenters = [[1, 1], [-1, -1], [1, -1]]\\nn_clusters = len(centers)\\nX, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)\\n\\n# %%\\n# Compute clustering with KMeans\\n# ------------------------------\\n\\nimport time\\n\\nfrom sklearn.cluster import KMeans\\n\\nk_means = KMeans(init=\"k-means++\", n_clusters=3, n_init=10)\\nt0 = time.time()\\nk_means.fit(X)\\nt_batch = time.time() - t0\\n\\n# %%\\n# Compute clustering with MiniBatchKMeans\\n# ---------------------------------------\\n\\nfrom sklearn.cluster import MiniBatchKMeans\\n\\nmbk = MiniBatchKMeans(\\n    init=\"k-means++\",\\n    n_clusters=3,\\n    batch_size=batch_size,\\n    n_init=10,\\n    max_no_improvement=10,\\n    verbose=0,\\n)\\nt0 = time.time()\\nmbk.fit(X)\\nt_mini_batch = time.time() - t0\\n\\n# %%\\n# Establishing parity between clusters\\n# ------------------------------------\\n#\\n# We want to have the same color for the same cluster from both the\\n# MiniBatchKMeans and the KMeans algorithm. Let\\'s pair the cluster centers per\\n# closest one.\\n\\nfrom sklearn.metrics.pairwise import pairwise_distances_argmin\\n\\nk_means_cluster_centers = k_means.cluster_centers_\\norder = pairwise_distances_argmin(k_means.cluster_centers_, mbk.cluster_centers_)\\nmbk_means_cluster_centers = mbk.cluster_centers_[order]\\n\\nk_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)\\nmbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)\\n\\n# %%\\n# Plotting the results\\n# --------------------\\n\\nimport matplotlib.pyplot as plt\\n\\nfig = plt.figure(figsize=(8, 3))\\nfig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\\ncolors = [\"#4EACC5\", \"#FF9C34\", \"#4E9A06\"]\\n\\n# KMeans\\nax = fig.add_subplot(1, 3, 1)\\nfor k, col in zip(range(n_clusters), colors):\\n    my_members = k_means_labels == k\\n    cluster_center = k_means_cluster_centers[k]\\n    ax.plot(X[my_members, 0], X[my_members, 1], \"w\", markerfacecolor=col, marker=\".\")\\n    ax.plot(\\n        cluster_center[0],\\n        cluster_center[1],\\n        \"o\",\\n        markerfacecolor=col,\\n        markeredgecolor=\"k\",\\n        markersize=6,\\n    )\\nax.set_title(\"KMeans\")\\nax.set_xticks(())\\nax.set_yticks(())\\nplt.text(-3.5, 1.8, \"train time: %.2fs\\\\ninertia: %f\" % (t_batch, k_means.inertia_))'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_mini_batch_kmeans.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# MiniBatchKMeans\\nax = fig.add_subplot(1, 3, 2)\\nfor k, col in zip(range(n_clusters), colors):\\n    my_members = mbk_means_labels == k\\n    cluster_center = mbk_means_cluster_centers[k]\\n    ax.plot(X[my_members, 0], X[my_members, 1], \"w\", markerfacecolor=col, marker=\".\")\\n    ax.plot(\\n        cluster_center[0],\\n        cluster_center[1],\\n        \"o\",\\n        markerfacecolor=col,\\n        markeredgecolor=\"k\",\\n        markersize=6,\\n    )\\nax.set_title(\"MiniBatchKMeans\")\\nax.set_xticks(())\\nax.set_yticks(())\\nplt.text(-3.5, 1.8, \"train time: %.2fs\\\\ninertia: %f\" % (t_mini_batch, mbk.inertia_))\\n\\n# Initialize the different array to all False\\ndifferent = mbk_means_labels == 4\\nax = fig.add_subplot(1, 3, 3)\\n\\nfor k in range(n_clusters):\\n    different += (k_means_labels == k) != (mbk_means_labels == k)\\n\\nidentical = np.logical_not(different)\\nax.plot(X[identical, 0], X[identical, 1], \"w\", markerfacecolor=\"#bbbbbb\", marker=\".\")\\nax.plot(X[different, 0], X[different, 1], \"w\", markerfacecolor=\"m\", marker=\".\")\\nax.set_title(\"Difference\")\\nax.set_xticks(())\\nax.set_yticks(())\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_hdbscan.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot(X, labels, probabilities=None, parameters=None, ground_truth=False, ax=None):\\n    if ax is None:\\n        _, ax = plt.subplots(figsize=(10, 4))\\n    labels = labels if labels is not None else np.ones(X.shape[0])\\n    probabilities = probabilities if probabilities is not None else np.ones(X.shape[0])\\n    # Black removed and is used for noise instead.\\n    unique_labels = set(labels)\\n    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\\n    # The probability of a point belonging to its labeled cluster determines\\n    # the size of its marker\\n    proba_map = {idx: probabilities[idx] for idx in range(len(labels))}\\n    for k, col in zip(unique_labels, colors):\\n        if k == -1:\\n            # Black used for noise.\\n            col = [0, 0, 0, 1]\\n\\n        class_index = np.where(labels == k)[0]\\n        for ci in class_index:\\n            ax.plot(\\n                X[ci, 0],\\n                X[ci, 1],\\n                \"x\" if k == -1 else \"o\",\\n                markerfacecolor=tuple(col),\\n                markeredgecolor=\"k\",\\n                markersize=4 if k == -1 else 1 + 5 * proba_map[ci],\\n            )\\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\\n    preamble = \"True\" if ground_truth else \"Estimated\"\\n    title = f\"{preamble} number of clusters: {n_clusters_}\"\\n    if parameters is not None:\\n        parameters_str = \", \".join(f\"{k}={v}\" for k, v in parameters.items())\\n        title += f\" | {parameters_str}\"\\n    ax.set_title(title)\\n    plt.tight_layout()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_hdbscan.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# -*- coding: utf-8 -*-\\n\"\"\"\\n====================================\\nDemo of HDBSCAN clustering algorithm\\n====================================\\n.. currentmodule:: sklearn\\n\\nIn this demo we will take a look at :class:`cluster.HDBSCAN` from the\\nperspective of generalizing the :class:`cluster.DBSCAN` algorithm.\\nWe\\'ll compare both algorithms on specific datasets. Finally we\\'ll evaluate\\nHDBSCAN\\'s sensitivity to certain hyperparameters.\\n\\nWe first define a couple utility functions for convenience.\\n\"\"\"\\n# %%\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.cluster import DBSCAN, HDBSCAN\\nfrom sklearn.datasets import make_blobs\\n\\n\\n# Code for: def plot(X, labels, probabilities=None, parameters=None, ground_truth=False, ax=None):\\n\\n\\n# %%\\n# Generate sample data\\n# --------------------\\n# One of the greatest advantages of HDBSCAN over DBSCAN is its out-of-the-box\\n# robustness. It\\'s especially remarkable on heterogeneous mixtures of data.\\n# Like DBSCAN, it can model arbitrary shapes and distributions, however unlike\\n# DBSCAN it does not require specification of an arbitrary and sensitive\\n# `eps` hyperparameter.\\n#\\n# For example, below we generate a dataset from a mixture of three bi-dimensional\\n# and isotropic Gaussian distributions.\\ncenters = [[1, 1], [-1, -1], [1.5, -1.5]]\\nX, labels_true = make_blobs(\\n    n_samples=750, centers=centers, cluster_std=[0.4, 0.1, 0.75], random_state=0\\n)\\nplot(X, labels=labels_true, ground_truth=True)\\n# %%\\n# Scale Invariance\\n# -----------------\\n# It\\'s worth remembering that, while DBSCAN provides a default value for `eps`\\n# parameter, it hardly has a proper default value and must be tuned for the\\n# specific dataset at use.\\n#\\n# As a simple demonstration, consider the clustering for a `eps` value tuned\\n# for one dataset, and clustering obtained with the same value but applied to\\n# rescaled versions of the dataset.\\nfig, axes = plt.subplots(3, 1, figsize=(10, 12))\\ndbs = DBSCAN(eps=0.3)\\nfor idx, scale in enumerate([1, 0.5, 3]):\\n    dbs.fit(X * scale)\\n    plot(X * scale, dbs.labels_, parameters={\"scale\": scale, \"eps\": 0.3}, ax=axes[idx])'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_hdbscan.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Indeed, in order to maintain the same results we would have to scale `eps` by\\n# the same factor.\\nfig, axis = plt.subplots(1, 1, figsize=(12, 5))\\ndbs = DBSCAN(eps=0.9).fit(3 * X)\\nplot(3 * X, dbs.labels_, parameters={\"scale\": 3, \"eps\": 0.9}, ax=axis)\\n# %%\\n# While standardizing data (e.g. using\\n# :class:`sklearn.preprocessing.StandardScaler`) helps mitigate this problem,\\n# great care must be taken to select the appropriate value for `eps`.\\n#\\n# HDBSCAN is much more robust in this sense: HDBSCAN can be seen as\\n# clustering over all possible values of `eps` and extracting the best\\n# clusters from all possible clusters (see :ref:`User Guide <HDBSCAN>`).\\n# One immediate advantage is that HDBSCAN is scale-invariant.\\nfig, axes = plt.subplots(3, 1, figsize=(10, 12))\\nhdb = HDBSCAN()\\nfor idx, scale in enumerate([1, 0.5, 3]):\\n    hdb.fit(X * scale)\\n    plot(\\n        X * scale,\\n        hdb.labels_,\\n        hdb.probabilities_,\\n        ax=axes[idx],\\n        parameters={\"scale\": scale},\\n    )\\n# %%\\n# Multi-Scale Clustering\\n# ----------------------\\n# HDBSCAN is much more than scale invariant though -- it is capable of\\n# multi-scale clustering, which accounts for clusters with varying density.\\n# Traditional DBSCAN assumes that any potential clusters are homogeneous in\\n# density. HDBSCAN is free from such constraints. To demonstrate this we\\n# consider the following dataset\\ncenters = [[-0.85, -0.85], [-0.85, 0.85], [3, 3], [3, -3]]\\nX, labels_true = make_blobs(\\n    n_samples=750, centers=centers, cluster_std=[0.2, 0.35, 1.35, 1.35], random_state=0\\n)\\nplot(X, labels=labels_true, ground_truth=True)\\n\\n# %%\\n# This dataset is more difficult for DBSCAN due to the varying densities and\\n# spatial separation:\\n#\\n# - If `eps` is too large then we risk falsely clustering the two dense\\n#   clusters as one since their mutual reachability will extend\\n#   clusters.\\n# - If `eps` is too small, then we risk fragmenting the sparser clusters\\n#   into many false clusters.\\n#\\n# Not to mention this requires manually tuning choices of `eps` until we\\n# find a tradeoff that we are comfortable with.\\nfig, axes = plt.subplots(2, 1, figsize=(10, 8))\\nparams = {\"eps\": 0.7}\\ndbs = DBSCAN(**params).fit(X)\\nplot(X, dbs.labels_, parameters=params, ax=axes[0])\\nparams = {\"eps\": 0.3}\\ndbs = DBSCAN(**params).fit(X)\\nplot(X, dbs.labels_, parameters=params, ax=axes[1])\\n\\n# %%\\n# To properly cluster the two dense clusters, we would need a smaller value of\\n# epsilon, however at `eps=0.3` we are already fragmenting the sparse clusters,\\n# which would only become more severe as we decrease epsilon. Indeed it seems\\n# that DBSCAN is incapable of simultaneously separating the two dense clusters\\n# while preventing the sparse clusters from fragmenting. Let\\'s compare with\\n# HDBSCAN.\\nhdb = HDBSCAN().fit(X)\\nplot(X, hdb.labels_, hdb.probabilities_)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_hdbscan.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# HDBSCAN is able to adapt to the multi-scale structure of the dataset without\\n# requiring parameter tuning. While any sufficiently interesting dataset will\\n# require tuning, this case demonstrates that HDBSCAN can yield qualitatively\\n# better classes of clusterings without users\\' intervention which are\\n# inaccessible via DBSCAN.\\n\\n# %%\\n# Hyperparameter Robustness\\n# -------------------------\\n# Ultimately tuning will be an important step in any real world application, so\\n# let\\'s take a look at some of the most important hyperparameters for HDBSCAN.\\n# While HDBSCAN is free from the `eps` parameter of DBSCAN, it does still have\\n# some hyperparameters like `min_cluster_size` and `min_samples` which tune its\\n# results regarding density. We will however see that HDBSCAN is relatively robust\\n# to various real world examples thanks to those parameters whose clear meaning\\n# helps tuning them.\\n#\\n# `min_cluster_size`\\n# ^^^^^^^^^^^^^^^^^^\\n# `min_cluster_size` is the minimum number of samples in a group for that\\n# group to be considered a cluster.\\n#\\n# Clusters smaller than the ones of this size will be left as noise.\\n# The default value is 5. This parameter is generally tuned to\\n# larger values as needed. Smaller values will likely to lead to results with\\n# fewer points labeled as noise. However values which too small will lead to\\n# false sub-clusters being picked up and preferred. Larger values tend to be\\n# more robust with respect to noisy datasets, e.g. high-variance clusters with\\n# significant overlap.\\n\\nPARAM = ({\"min_cluster_size\": 5}, {\"min_cluster_size\": 3}, {\"min_cluster_size\": 25})\\nfig, axes = plt.subplots(3, 1, figsize=(10, 12))\\nfor i, param in enumerate(PARAM):\\n    hdb = HDBSCAN(**param).fit(X)\\n    labels = hdb.labels_\\n\\n    plot(X, labels, hdb.probabilities_, param, ax=axes[i])\\n\\n# %%\\n# `min_samples`\\n# ^^^^^^^^^^^^^\\n# `min_samples` is the number of samples in a neighborhood for a point to\\n# be considered as a core point, including the point itself.\\n# `min_samples` defaults to `min_cluster_size`.\\n# Similarly to `min_cluster_size`, larger values for `min_samples` increase\\n# the model\\'s robustness to noise, but risks ignoring or discarding\\n# potentially valid but small clusters.\\n# `min_samples` better be tuned after finding a good value for `min_cluster_size`.\\n\\nPARAM = (\\n    {\"min_cluster_size\": 20, \"min_samples\": 5},\\n    {\"min_cluster_size\": 20, \"min_samples\": 3},\\n    {\"min_cluster_size\": 20, \"min_samples\": 25},\\n)\\nfig, axes = plt.subplots(3, 1, figsize=(10, 12))\\nfor i, param in enumerate(PARAM):\\n    hdb = HDBSCAN(**param).fit(X)\\n    labels = hdb.labels_\\n\\n    plot(X, labels, hdb.probabilities_, param, ax=axes[i])'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_hdbscan.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plot(X, labels, hdb.probabilities_, param, ax=axes[i])\\n\\n# %%\\n# `dbscan_clustering`\\n# ^^^^^^^^^^^^^^^^^^^\\n# During `fit`, `HDBSCAN` builds a single-linkage tree which encodes the\\n# clustering of all points across all values of :class:`~cluster.DBSCAN`\\'s\\n# `eps` parameter.\\n# We can thus plot and evaluate these clusterings efficiently without fully\\n# recomputing intermediate values such as core-distances, mutual-reachability,\\n# and the minimum spanning tree. All we need to do is specify the `cut_distance`\\n# (equivalent to `eps`) we want to cluster with.\\n\\nPARAM = (\\n    {\"cut_distance\": 0.1},\\n    {\"cut_distance\": 0.5},\\n    {\"cut_distance\": 1.0},\\n)\\nhdb = HDBSCAN()\\nhdb.fit(X)\\nfig, axes = plt.subplots(len(PARAM), 1, figsize=(10, 12))\\nfor i, param in enumerate(PARAM):\\n    labels = hdb.dbscan_clustering(**param)\\n\\n    plot(X, labels, hdb.probabilities_, param, ax=axes[i])'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_birch_vs_minibatchkmeans.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================\\nCompare BIRCH and MiniBatchKMeans\\n=================================\\n\\nThis example compares the timing of BIRCH (with and without the global\\nclustering step) and MiniBatchKMeans on a synthetic dataset having\\n25,000 samples and 2 features generated using make_blobs.\\n\\nBoth ``MiniBatchKMeans`` and ``BIRCH`` are very scalable algorithms and could\\nrun efficiently on hundreds of thousands or even millions of datapoints. We\\nchose to limit the dataset size of this example in the interest of keeping\\nour Continuous Integration resource usage reasonable but the interested\\nreader might enjoy editing this script to rerun it with a larger value for\\n`n_samples`.\\n\\nIf ``n_clusters`` is set to None, the data is reduced from 25,000\\nsamples to a set of 158 clusters. This can be viewed as a preprocessing\\nstep before the final (global) clustering step that further reduces these\\n158 clusters to 100 clusters.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom itertools import cycle\\nfrom time import time\\n\\nimport matplotlib.colors as colors\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom joblib import cpu_count\\n\\nfrom sklearn.cluster import Birch, MiniBatchKMeans\\nfrom sklearn.datasets import make_blobs\\n\\n# Generate centers for the blobs so that it forms a 10 X 10 grid.\\nxx = np.linspace(-22, 22, 10)\\nyy = np.linspace(-22, 22, 10)\\nxx, yy = np.meshgrid(xx, yy)\\nn_centers = np.hstack((np.ravel(xx)[:, np.newaxis], np.ravel(yy)[:, np.newaxis]))\\n\\n# Generate blobs to do a comparison between MiniBatchKMeans and BIRCH.\\nX, y = make_blobs(n_samples=25000, centers=n_centers, random_state=0)\\n\\n# Use all colors that matplotlib provides by default.\\ncolors_ = cycle(colors.cnames.keys())\\n\\nfig = plt.figure(figsize=(12, 4))\\nfig.subplots_adjust(left=0.04, right=0.98, bottom=0.1, top=0.9)\\n\\n# Compute clustering with BIRCH with and without the final clustering step\\n# and plot.\\nbirch_models = [\\n    Birch(threshold=1.7, n_clusters=None),\\n    Birch(threshold=1.7, n_clusters=100),\\n]\\nfinal_step = [\"without global clustering\", \"with global clustering\"]\\n\\nfor ind, (birch_model, info) in enumerate(zip(birch_models, final_step)):\\n    t = time()\\n    birch_model.fit(X)\\n    print(\"BIRCH %s as the final step took %0.2f seconds\" % (info, (time() - t)))\\n\\n    # Plot result\\n    labels = birch_model.labels_\\n    centroids = birch_model.subcluster_centers_\\n    n_clusters = np.unique(labels).size\\n    print(\"n_clusters : %d\" % n_clusters)\\n\\n    ax = fig.add_subplot(1, 3, ind + 1)\\n    for this_centroid, k, col in zip(centroids, range(n_clusters), colors_):\\n        mask = labels == k\\n        ax.scatter(X[mask, 0], X[mask, 1], c=\"w\", edgecolor=col, marker=\".\", alpha=0.5)\\n        if birch_model.n_clusters is None:\\n            ax.scatter(this_centroid[0], this_centroid[1], marker=\"+\", c=\"k\", s=25)\\n    ax.set_ylim([-25, 25])\\n    ax.set_xlim([-25, 25])\\n    ax.set_autoscaley_on(False)\\n    ax.set_title(\"BIRCH %s\" % info)'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_birch_vs_minibatchkmeans.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Compute clustering with MiniBatchKMeans.\\nmbk = MiniBatchKMeans(\\n    init=\"k-means++\",\\n    n_clusters=100,\\n    batch_size=256 * cpu_count(),\\n    n_init=10,\\n    max_no_improvement=10,\\n    verbose=0,\\n    random_state=0,\\n)\\nt0 = time()\\nmbk.fit(X)\\nt_mini_batch = time() - t0\\nprint(\"Time taken to run MiniBatchKMeans %0.2f seconds\" % t_mini_batch)\\nmbk_means_labels_unique = np.unique(mbk.labels_)\\n\\nax = fig.add_subplot(1, 3, 3)\\nfor this_centroid, k, col in zip(mbk.cluster_centers_, range(n_clusters), colors_):\\n    mask = mbk.labels_ == k\\n    ax.scatter(X[mask, 0], X[mask, 1], marker=\".\", c=\"w\", edgecolor=col, alpha=0.5)\\n    ax.scatter(this_centroid[0], this_centroid[1], marker=\"+\", c=\"k\", s=25)\\nax.set_xlim([-25, 25])\\nax.set_ylim([-25, 25])\\nax.set_title(\"MiniBatchKMeans\")\\nax.set_autoscaley_on(False)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_digits_linkage.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_clustering(X_red, labels, title=None):\\n    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\\n    X_red = (X_red - x_min) / (x_max - x_min)\\n\\n    plt.figure(figsize=(6, 4))\\n    for digit in digits.target_names:\\n        plt.scatter(\\n            *X_red[y == digit].T,\\n            marker=f\"${digit}$\",\\n            s=50,\\n            c=plt.cm.nipy_spectral(labels[y == digit] / 10),\\n            alpha=0.5,\\n        )\\n\\n    plt.xticks([])\\n    plt.yticks([])\\n    if title is not None:\\n        plt.title(title, size=17)\\n    plt.axis(\"off\")\\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])'), Document(metadata={'source': '/content/local_copy_repo/examples/cluster/plot_digits_linkage.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================================================\\nVarious Agglomerative Clustering on a 2D embedding of digits\\n=============================================================================\\n\\nAn illustration of various linkage option for agglomerative clustering on\\na 2D embedding of the digits dataset.\\n\\nThe goal of this example is to show intuitively how the metrics behave, and\\nnot to find good clusters for the digits. This is why the example works on a\\n2D embedding.\\n\\nWhat this example shows us is the behavior \"rich getting richer\" of\\nagglomerative clustering that tends to create uneven cluster sizes.\\n\\nThis behavior is pronounced for the average linkage strategy,\\nthat ends up with a couple of clusters with few datapoints.\\n\\nThe case of single linkage is even more pathologic with a very\\nlarge cluster covering most digits, an intermediate size (clean)\\ncluster with most zero digits and all other clusters being drawn\\nfrom noise points around the fringes.\\n\\nThe other linkage strategies lead to more evenly distributed\\nclusters that are therefore likely to be less sensible to a\\nrandom resampling of the dataset.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom time import time\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn import datasets, manifold\\n\\ndigits = datasets.load_digits()\\nX, y = digits.data, digits.target\\nn_samples, n_features = X.shape\\n\\nnp.random.seed(0)\\n\\n\\n# ----------------------------------------------------------------------\\n# Visualize the clustering\\n# Code for: def plot_clustering(X_red, labels, title=None):\\n\\n\\n# ----------------------------------------------------------------------\\n# 2D embedding of the digits dataset\\nprint(\"Computing embedding\")\\nX_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)\\nprint(\"Done.\")\\n\\nfrom sklearn.cluster import AgglomerativeClustering\\n\\nfor linkage in (\"ward\", \"average\", \"complete\", \"single\"):\\n    clustering = AgglomerativeClustering(linkage=linkage, n_clusters=10)\\n    t0 = time()\\n    clustering.fit(X_red)\\n    print(\"%s :\\\\t%.2fs\" % (linkage, time() - t0))\\n\\n    plot_clustering(X_red, clustering.labels_, \"%s linkage\" % linkage)\\n\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_roc_curve_visualization_api.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================\\nROC Curve with Visualization API\\n================================\\nScikit-learn defines a simple API for creating visualizations for machine\\nlearning. The key features of this API is to allow for quick plotting and\\nvisual adjustments without recalculation. In this example, we will demonstrate\\nhow to use the visualization API by comparing ROC curves.\\n\\n\"\"\"\\n\\n# %%\\n# Load Data and Train a SVC\\n# -------------------------\\n# First, we load the wine dataset and convert it to a binary classification\\n# problem. Then, we train a support vector classifier on a training dataset.\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import RocCurveDisplay\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\n\\nX, y = load_wine(return_X_y=True)\\ny = y == 2\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\nsvc = SVC(random_state=42)\\nsvc.fit(X_train, y_train)\\n\\n# %%\\n# Plotting the ROC Curve\\n# ----------------------\\n# Next, we plot the ROC curve with a single call to\\n# :func:`sklearn.metrics.RocCurveDisplay.from_estimator`. The returned\\n# `svc_disp` object allows us to continue using the already computed ROC curve\\n# for the SVC in future plots.\\nsvc_disp = RocCurveDisplay.from_estimator(svc, X_test, y_test)\\nplt.show()\\n\\n# %%\\n# Training a Random Forest and Plotting the ROC Curve\\n# ---------------------------------------------------\\n# We train a random forest classifier and create a plot comparing it to the SVC\\n# ROC curve. Notice how `svc_disp` uses\\n# :func:`~sklearn.metrics.RocCurveDisplay.plot` to plot the SVC ROC curve\\n# without recomputing the values of the roc curve itself. Furthermore, we\\n# pass `alpha=0.8` to the plot functions to adjust the alpha values of the\\n# curves.\\nrfc = RandomForestClassifier(n_estimators=10, random_state=42)\\nrfc.fit(X_train, y_train)\\nax = plt.gca()\\nrfc_disp = RocCurveDisplay.from_estimator(rfc, X_test, y_test, ax=ax, alpha=0.8)\\nsvc_disp.plot(ax=ax, alpha=0.8)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_kernel_approximation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================================\\nExplicit feature map approximation for RBF kernels\\n==================================================\\n\\nAn example illustrating the approximation of the feature map\\nof an RBF kernel.\\n\\n.. currentmodule:: sklearn.kernel_approximation\\n\\nIt shows how to use :class:`RBFSampler` and :class:`Nystroem` to\\napproximate the feature map of an RBF kernel for classification with an SVM on\\nthe digits dataset. Results using a linear SVM in the original space, a linear\\nSVM using the approximate mappings and using a kernelized SVM are compared.\\nTimings and accuracy for varying amounts of Monte Carlo samplings (in the case\\nof :class:`RBFSampler`, which uses random Fourier features) and different sized\\nsubsets of the training set (for :class:`Nystroem`) for the approximate mapping\\nare shown.\\n\\nPlease note that the dataset here is not large enough to show the benefits\\nof kernel approximation, as the exact SVM is still reasonably fast.\\n\\nSampling more dimensions clearly leads to better classification results, but\\ncomes at a greater cost. This means there is a tradeoff between runtime and\\naccuracy, given by the parameter n_components. Note that solving the Linear\\nSVM and also the approximate kernel SVM could be greatly accelerated by using\\nstochastic gradient descent via :class:`~sklearn.linear_model.SGDClassifier`.\\nThis is not easily possible for the case of the kernelized SVM.\\n\\n\"\"\"\\n\\n# %%\\n# Python package and dataset imports, load dataset\\n# ---------------------------------------------------\\n\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# Standard scientific Python imports\\nfrom time import time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# Import datasets, classifiers and performance metrics\\nfrom sklearn import datasets, pipeline, svm\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.kernel_approximation import Nystroem, RBFSampler\\n\\n# The digits dataset\\ndigits = datasets.load_digits(n_class=9)\\n\\n\\n# %%\\n# Timing and accuracy plots\\n# --------------------------------------------------\\n# To apply an classifier on this data, we need to flatten the image, to\\n# turn the data in a (samples, feature) matrix:\\nn_samples = len(digits.data)\\ndata = digits.data / 16.0\\ndata -= data.mean(axis=0)\\n\\n# We learn the digits on the first half of the digits\\ndata_train, targets_train = (data[: n_samples // 2], digits.target[: n_samples // 2])\\n\\n\\n# Now predict the value of the digit on the second half:\\ndata_test, targets_test = (data[n_samples // 2 :], digits.target[n_samples // 2 :])\\n# data_test = scaler.transform(data_test)\\n\\n# Create a classifier: a support vector classifier\\nkernel_svm = svm.SVC(gamma=0.2)\\nlinear_svm = svm.LinearSVC(random_state=42)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_kernel_approximation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Create a classifier: a support vector classifier\\nkernel_svm = svm.SVC(gamma=0.2)\\nlinear_svm = svm.LinearSVC(random_state=42)\\n\\n# create pipeline from kernel approximation\\n# and linear svm\\nfeature_map_fourier = RBFSampler(gamma=0.2, random_state=1)\\nfeature_map_nystroem = Nystroem(gamma=0.2, random_state=1)\\nfourier_approx_svm = pipeline.Pipeline(\\n    [\\n        (\"feature_map\", feature_map_fourier),\\n        (\"svm\", svm.LinearSVC(random_state=42)),\\n    ]\\n)\\n\\nnystroem_approx_svm = pipeline.Pipeline(\\n    [\\n        (\"feature_map\", feature_map_nystroem),\\n        (\"svm\", svm.LinearSVC(random_state=42)),\\n    ]\\n)\\n\\n# fit and predict using linear and kernel svm:\\n\\nkernel_svm_time = time()\\nkernel_svm.fit(data_train, targets_train)\\nkernel_svm_score = kernel_svm.score(data_test, targets_test)\\nkernel_svm_time = time() - kernel_svm_time\\n\\nlinear_svm_time = time()\\nlinear_svm.fit(data_train, targets_train)\\nlinear_svm_score = linear_svm.score(data_test, targets_test)\\nlinear_svm_time = time() - linear_svm_time\\n\\nsample_sizes = 30 * np.arange(1, 10)\\nfourier_scores = []\\nnystroem_scores = []\\nfourier_times = []\\nnystroem_times = []\\n\\nfor D in sample_sizes:\\n    fourier_approx_svm.set_params(feature_map__n_components=D)\\n    nystroem_approx_svm.set_params(feature_map__n_components=D)\\n    start = time()\\n    nystroem_approx_svm.fit(data_train, targets_train)\\n    nystroem_times.append(time() - start)\\n\\n    start = time()\\n    fourier_approx_svm.fit(data_train, targets_train)\\n    fourier_times.append(time() - start)\\n\\n    fourier_score = fourier_approx_svm.score(data_test, targets_test)\\n    nystroem_score = nystroem_approx_svm.score(data_test, targets_test)\\n    nystroem_scores.append(nystroem_score)\\n    fourier_scores.append(fourier_score)\\n\\n# plot the results:\\nplt.figure(figsize=(16, 4))\\naccuracy = plt.subplot(121)\\n# second y axis for timings\\ntimescale = plt.subplot(122)\\n\\naccuracy.plot(sample_sizes, nystroem_scores, label=\"Nystroem approx. kernel\")\\ntimescale.plot(sample_sizes, nystroem_times, \"--\", label=\"Nystroem approx. kernel\")\\n\\naccuracy.plot(sample_sizes, fourier_scores, label=\"Fourier approx. kernel\")\\ntimescale.plot(sample_sizes, fourier_times, \"--\", label=\"Fourier approx. kernel\")\\n\\n# horizontal lines for exact rbf and linear kernels:\\naccuracy.plot(\\n    [sample_sizes[0], sample_sizes[-1]],\\n    [linear_svm_score, linear_svm_score],\\n    label=\"linear svm\",\\n)\\ntimescale.plot(\\n    [sample_sizes[0], sample_sizes[-1]],\\n    [linear_svm_time, linear_svm_time],\\n    \"--\",\\n    label=\"linear svm\",\\n)\\n\\naccuracy.plot(\\n    [sample_sizes[0], sample_sizes[-1]],\\n    [kernel_svm_score, kernel_svm_score],\\n    label=\"rbf svm\",\\n)\\ntimescale.plot(\\n    [sample_sizes[0], sample_sizes[-1]],\\n    [kernel_svm_time, kernel_svm_time],\\n    \"--\",\\n    label=\"rbf svm\",\\n)\\n\\n# vertical line for dataset dimensionality = 64\\naccuracy.plot([64, 64], [0.7, 1], label=\"n_features\")'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_kernel_approximation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# vertical line for dataset dimensionality = 64\\naccuracy.plot([64, 64], [0.7, 1], label=\"n_features\")\\n\\n# legends and labels\\naccuracy.set_title(\"Classification accuracy\")\\ntimescale.set_title(\"Training times\")\\naccuracy.set_xlim(sample_sizes[0], sample_sizes[-1])\\naccuracy.set_xticks(())\\naccuracy.set_ylim(np.min(fourier_scores), 1)\\ntimescale.set_xlabel(\"Sampling steps = transformed feature dimension\")\\naccuracy.set_ylabel(\"Classification accuracy\")\\ntimescale.set_ylabel(\"Training time in seconds\")\\naccuracy.legend(loc=\"best\")\\ntimescale.legend(loc=\"best\")\\nplt.tight_layout()\\nplt.show()\\n\\n\\n# %%\\n# Decision Surfaces of RBF Kernel SVM and Linear SVM\\n# --------------------------------------------------------\\n# The second plot visualized the decision surfaces of the RBF kernel SVM and\\n# the linear SVM with approximate kernel maps.\\n# The plot shows decision surfaces of the classifiers projected onto\\n# the first two principal components of the data. This visualization should\\n# be taken with a grain of salt since it is just an interesting slice through\\n# the decision surface in 64 dimensions. In particular note that\\n# a datapoint (represented as a dot) does not necessarily be classified\\n# into the region it is lying in, since it will not lie on the plane\\n# that the first two principal components span.\\n# The usage of :class:`RBFSampler` and :class:`Nystroem` is described in detail\\n# in :ref:`kernel_approximation`.\\n\\n# visualize the decision surface, projected down to the first\\n# two principal components of the dataset\\npca = PCA(n_components=8, random_state=42).fit(data_train)\\n\\nX = pca.transform(data_train)\\n\\n# Generate grid along first two principal components\\nmultiples = np.arange(-2, 2, 0.1)\\n# steps along first component\\nfirst = multiples[:, np.newaxis] * pca.components_[0, :]\\n# steps along second component\\nsecond = multiples[:, np.newaxis] * pca.components_[1, :]\\n# combine\\ngrid = first[np.newaxis, :, :] + second[:, np.newaxis, :]\\nflat_grid = grid.reshape(-1, data.shape[1])\\n\\n# title for the plots\\ntitles = [\\n    \"SVC with rbf kernel\",\\n    \"SVC (linear kernel)\\\\n with Fourier rbf feature map\\\\nn_components=100\",\\n    \"SVC (linear kernel)\\\\n with Nystroem rbf feature map\\\\nn_components=100\",\\n]\\n\\nplt.figure(figsize=(18, 7.5))\\nplt.rcParams.update({\"font.size\": 14})\\n# predict and plot\\nfor i, clf in enumerate((kernel_svm, nystroem_approx_svm, fourier_approx_svm)):\\n    # Plot the decision boundary. For that, we will assign a color to each\\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\\n    plt.subplot(1, 3, i + 1)\\n    Z = clf.predict(flat_grid)\\n\\n    # Put the result into a color plot\\n    Z = Z.reshape(grid.shape[:-1])\\n    levels = np.arange(10)\\n    lv_eps = 0.01  # Adjust a mapping from calculated contour levels to color.\\n    plt.contourf(\\n        multiples,\\n        multiples,\\n        Z,\\n        levels=levels - lv_eps,\\n        cmap=plt.cm.tab10,\\n        vmin=0,\\n        vmax=10,\\n        alpha=0.7,\\n    )\\n    plt.axis(\"off\")'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_kernel_approximation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Plot also the training points\\n    plt.scatter(\\n        X[:, 0],\\n        X[:, 1],\\n        c=targets_train,\\n        cmap=plt.cm.tab10,\\n        edgecolors=(0, 0, 0),\\n        vmin=0,\\n        vmax=10,\\n    )\\n\\n    plt.title(titles[i])\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_display_object_visualization.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================\\nVisualizations with Display Objects\\n===================================\\n\\n.. currentmodule:: sklearn.metrics\\n\\nIn this example, we will construct display objects,\\n:class:`ConfusionMatrixDisplay`, :class:`RocCurveDisplay`, and\\n:class:`PrecisionRecallDisplay` directly from their respective metrics. This\\nis an alternative to using their corresponding plot functions when\\na model\\'s predictions are already computed or expensive to compute. Note that\\nthis is advanced usage, and in general we recommend using their respective\\nplot functions.\\n\\n\"\"\"\\n\\n# %%\\n# Load Data and train model\\n# -------------------------\\n# For this example, we load a blood transfusion service center data set from\\n# `OpenML <https://www.openml.org/d/1464>`. This is a binary classification\\n# problem where the target is whether an individual donated blood. Then the\\n# data is split into a train and test dataset and a logistic regression is\\n# fitted with the train dataset.\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nX, y = fetch_openml(data_id=1464, return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\\n\\nclf = make_pipeline(StandardScaler(), LogisticRegression(random_state=0))\\nclf.fit(X_train, y_train)\\n\\n# %%\\n# Create :class:`ConfusionMatrixDisplay`\\n##############################################################################\\n# With the fitted model, we compute the predictions of the model on the test\\n# dataset. These predictions are used to compute the confusion matrix which\\n# is plotted with the :class:`ConfusionMatrixDisplay`\\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\\n\\ny_pred = clf.predict(X_test)\\ncm = confusion_matrix(y_test, y_pred)\\n\\ncm_display = ConfusionMatrixDisplay(cm).plot()\\n\\n\\n# %%\\n# Create :class:`RocCurveDisplay`\\n##############################################################################\\n# The roc curve requires either the probabilities or the non-thresholded\\n# decision values from the estimator. Since the logistic regression provides\\n# a decision function, we will use it to plot the roc curve:\\nfrom sklearn.metrics import RocCurveDisplay, roc_curve\\n\\ny_score = clf.decision_function(X_test)\\n\\nfpr, tpr, _ = roc_curve(y_test, y_score, pos_label=clf.classes_[1])\\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\\n\\n# %%\\n# Create :class:`PrecisionRecallDisplay`\\n##############################################################################\\n# Similarly, the precision recall curve can be plotted using `y_score` from\\n# the prevision sections.\\nfrom sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve\\n\\nprec, recall, _ = precision_recall_curve(y_test, y_score, pos_label=clf.classes_[1])\\npr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_display_object_visualization.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"prec, recall, _ = precision_recall_curve(y_test, y_score, pos_label=clf.classes_[1])\\npr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()\\n\\n# %%\\n# Combining the display objects into a single plot\\n##############################################################################\\n# The display objects store the computed values that were passed as arguments.\\n# This allows for the visualizations to be easliy combined using matplotlib's\\n# API. In the following example, we place the displays next to each other in a\\n# row.\\n\\n# sphinx_gallery_thumbnail_number = 4\\nimport matplotlib.pyplot as plt\\n\\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\\n\\nroc_display.plot(ax=ax1)\\npr_display.plot(ax=ax2)\\nplt.show()\"), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_multilabel.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_hyperplane(clf, min_x, max_x, linestyle, label):\\n    # get the separating hyperplane\\n    w = clf.coef_[0]\\n    a = -w[0] / w[1]\\n    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough\\n    yy = a * xx - (clf.intercept_[0]) / w[1]\\n    plt.plot(xx, yy, linestyle, label=label)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_multilabel.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_subfigure(X, Y, subplot, title, transform):\\n    if transform == \"pca\":\\n        X = PCA(n_components=2).fit_transform(X)\\n    elif transform == \"cca\":\\n        X = CCA(n_components=2).fit(X, Y).transform(X)\\n    else:\\n        raise ValueError\\n\\n    min_x = np.min(X[:, 0])\\n    max_x = np.max(X[:, 0])\\n\\n    min_y = np.min(X[:, 1])\\n    max_y = np.max(X[:, 1])\\n\\n    classif = OneVsRestClassifier(SVC(kernel=\"linear\"))\\n    classif.fit(X, Y)\\n\\n    plt.subplot(2, 2, subplot)\\n    plt.title(title)\\n\\n    zero_class = np.where(Y[:, 0])\\n    one_class = np.where(Y[:, 1])\\n    plt.scatter(X[:, 0], X[:, 1], s=40, c=\"gray\", edgecolors=(0, 0, 0))\\n    plt.scatter(\\n        X[zero_class, 0],\\n        X[zero_class, 1],\\n        s=160,\\n        edgecolors=\"b\",\\n        facecolors=\"none\",\\n        linewidths=2,\\n        label=\"Class 1\",\\n    )\\n    plt.scatter(\\n        X[one_class, 0],\\n        X[one_class, 1],\\n        s=80,\\n        edgecolors=\"orange\",\\n        facecolors=\"none\",\\n        linewidths=2,\\n        label=\"Class 2\",\\n    )\\n\\n    plot_hyperplane(\\n        classif.estimators_[0], min_x, max_x, \"k--\", \"Boundary\\\\nfor class 1\"\\n    )\\n    plot_hyperplane(\\n        classif.estimators_[1], min_x, max_x, \"k-.\", \"Boundary\\\\nfor class 2\"\\n    )\\n    plt.xticks(())\\n    plt.yticks(())\\n\\n    plt.xlim(min_x - 0.5 * max_x, max_x + 0.5 * max_x)\\n    plt.ylim(min_y - 0.5 * max_y, max_y + 0.5 * max_y)\\n    if subplot == 2:\\n        plt.xlabel(\"First principal component\")\\n        plt.ylabel(\"Second principal component\")\\n        plt.legend(loc=\"upper left\")'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_multilabel.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================\\nMultilabel classification\\n=========================\\n\\nThis example simulates a multi-label document classification problem. The\\ndataset is generated randomly based on the following process:\\n\\n    - pick the number of labels: n ~ Poisson(n_labels)\\n    - n times, choose a class c: c ~ Multinomial(theta)\\n    - pick the document length: k ~ Poisson(length)\\n    - k times, choose a word: w ~ Multinomial(theta_c)\\n\\nIn the above process, rejection sampling is used to make sure that n is more\\nthan 2, and that the document length is never zero. Likewise, we reject classes\\nwhich have already been chosen.  The documents that are assigned to both\\nclasses are plotted surrounded by two colored circles.\\n\\nThe classification is performed by projecting to the first two principal\\ncomponents found by PCA and CCA for visualisation purposes, followed by using\\nthe :class:`~sklearn.multiclass.OneVsRestClassifier` metaclassifier using two\\nSVCs with linear kernels to learn a discriminative model for each class.\\nNote that PCA is used to perform an unsupervised dimensionality reduction,\\nwhile CCA is used to perform a supervised one.\\n\\nNote: in the plot, \"unlabeled samples\" does not mean that we don\\'t know the\\nlabels (as in semi-supervised learning) but that the samples simply do *not*\\nhave a label.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.cross_decomposition import CCA\\nfrom sklearn.datasets import make_multilabel_classification\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.multiclass import OneVsRestClassifier\\nfrom sklearn.svm import SVC\\n\\n\\n# Code for: def plot_hyperplane(clf, min_x, max_x, linestyle, label):\\n\\n\\n# Code for: def plot_subfigure(X, Y, subplot, title, transform):\\n\\n\\nplt.figure(figsize=(8, 6))\\n\\nX, Y = make_multilabel_classification(\\n    n_classes=2, n_labels=1, allow_unlabeled=True, random_state=1\\n)\\n\\nplot_subfigure(X, Y, 1, \"With unlabeled samples + CCA\", \"cca\")\\nplot_subfigure(X, Y, 2, \"With unlabeled samples + PCA\", \"pca\")\\n\\nX, Y = make_multilabel_classification(\\n    n_classes=2, n_labels=1, allow_unlabeled=False, random_state=1\\n)\\n\\nplot_subfigure(X, Y, 3, \"Without unlabeled samples + CCA\", \"cca\")\\nplot_subfigure(X, Y, 4, \"Without unlabeled samples + PCA\", \"pca\")\\n\\nplt.subplots_adjust(0.04, 0.02, 0.97, 0.94, 0.09, 0.2)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_anomaly_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================================================================\\nComparing anomaly detection algorithms for outlier detection on toy datasets\\n============================================================================\\n\\nThis example shows characteristics of different anomaly detection algorithms\\non 2D datasets. Datasets contain one or two modes (regions of high density)\\nto illustrate the ability of algorithms to cope with multimodal data.\\n\\nFor each dataset, 15% of samples are generated as random uniform noise. This\\nproportion is the value given to the nu parameter of the OneClassSVM and the\\ncontamination parameter of the other outlier detection algorithms.\\nDecision boundaries between inliers and outliers are displayed in black\\nexcept for Local Outlier Factor (LOF) as it has no predict method to be applied\\non new data when it is used for outlier detection.\\n\\nThe :class:`~sklearn.svm.OneClassSVM` is known to be sensitive to outliers and\\nthus does not perform very well for outlier detection. This estimator is best\\nsuited for novelty detection when the training set is not contaminated by\\noutliers. That said, outlier detection in high-dimension, or without any\\nassumptions on the distribution of the inlying data is very challenging, and a\\nOne-class SVM might give useful results in these situations depending on the\\nvalue of its hyperparameters.\\n\\nThe :class:`sklearn.linear_model.SGDOneClassSVM` is an implementation of the\\nOne-Class SVM based on stochastic gradient descent (SGD). Combined with kernel\\napproximation, this estimator can be used to approximate the solution\\nof a kernelized :class:`sklearn.svm.OneClassSVM`. We note that, although not\\nidentical, the decision boundaries of the\\n:class:`sklearn.linear_model.SGDOneClassSVM` and the ones of\\n:class:`sklearn.svm.OneClassSVM` are very similar. The main advantage of using\\n:class:`sklearn.linear_model.SGDOneClassSVM` is that it scales linearly with\\nthe number of samples.\\n\\n:class:`sklearn.covariance.EllipticEnvelope` assumes the data is Gaussian and\\nlearns an ellipse. It thus degrades when the data is not unimodal. Notice\\nhowever that this estimator is robust to outliers.\\n\\n:class:`~sklearn.ensemble.IsolationForest` and\\n:class:`~sklearn.neighbors.LocalOutlierFactor` seem to perform reasonably well\\nfor multi-modal data sets. The advantage of\\n:class:`~sklearn.neighbors.LocalOutlierFactor` over the other estimators is\\nshown for the third data set, where the two modes have different densities.\\nThis advantage is explained by the local aspect of LOF, meaning that it only\\ncompares the score of abnormality of one sample with the scores of its\\nneighbors.'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_anomaly_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='Finally, for the last data set, it is hard to say that one sample is more\\nabnormal than another sample as they are uniformly distributed in a\\nhypercube. Except for the :class:`~sklearn.svm.OneClassSVM` which overfits a\\nlittle, all estimators present decent solutions for this situation. In such a\\ncase, it would be wise to look more closely at the scores of abnormality of\\nthe samples as a good estimator should assign similar scores to all the\\nsamples.\\n\\nWhile these examples give some intuition about the algorithms, this\\nintuition might not apply to very high dimensional data.\\n\\nFinally, note that parameters of the models have been here handpicked but\\nthat in practice they need to be adjusted. In the absence of labelled data,\\nthe problem is completely unsupervised so model selection can be a challenge.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport time\\n\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import svm\\nfrom sklearn.covariance import EllipticEnvelope\\nfrom sklearn.datasets import make_blobs, make_moons\\nfrom sklearn.ensemble import IsolationForest\\nfrom sklearn.kernel_approximation import Nystroem\\nfrom sklearn.linear_model import SGDOneClassSVM\\nfrom sklearn.neighbors import LocalOutlierFactor\\nfrom sklearn.pipeline import make_pipeline\\n\\nmatplotlib.rcParams[\"contour.negative_linestyle\"] = \"solid\"\\n\\n# Example settings\\nn_samples = 300\\noutliers_fraction = 0.15\\nn_outliers = int(outliers_fraction * n_samples)\\nn_inliers = n_samples - n_outliers\\n\\n# define outlier/anomaly detection methods to be compared.\\n# the SGDOneClassSVM must be used in a pipeline with a kernel approximation\\n# to give similar results to the OneClassSVM\\nanomaly_algorithms = [\\n    (\\n        \"Robust covariance\",\\n        EllipticEnvelope(contamination=outliers_fraction, random_state=42),\\n    ),\\n    (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\", gamma=0.1)),\\n    (\\n        \"One-Class SVM (SGD)\",\\n        make_pipeline(\\n            Nystroem(gamma=0.1, random_state=42, n_components=150),\\n            SGDOneClassSVM(\\n                nu=outliers_fraction,\\n                shuffle=True,\\n                fit_intercept=True,\\n                random_state=42,\\n                tol=1e-6,\\n            ),\\n        ),\\n    ),\\n    (\\n        \"Isolation Forest\",\\n        IsolationForest(contamination=outliers_fraction, random_state=42),\\n    ),\\n    (\\n        \"Local Outlier Factor\",\\n        LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction),\\n    ),\\n]'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_anomaly_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Define datasets\\nblobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\\ndatasets = [\\n    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5, **blobs_params)[0],\\n    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5], **blobs_params)[0],\\n    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, 0.3], **blobs_params)[0],\\n    4.0\\n    * (\\n        make_moons(n_samples=n_samples, noise=0.05, random_state=0)[0]\\n        - np.array([0.5, 0.25])\\n    ),\\n    14.0 * (np.random.RandomState(42).rand(n_samples, 2) - 0.5),\\n]\\n\\n# Compare given classifiers under given settings\\nxx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\\n\\nplt.figure(figsize=(len(anomaly_algorithms) * 2 + 4, 12.5))\\nplt.subplots_adjust(\\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\\n)\\n\\nplot_num = 1\\nrng = np.random.RandomState(42)\\n\\nfor i_dataset, X in enumerate(datasets):\\n    # Add outliers\\n    X = np.concatenate([X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)\\n\\n    for name, algorithm in anomaly_algorithms:\\n        t0 = time.time()\\n        algorithm.fit(X)\\n        t1 = time.time()\\n        plt.subplot(len(datasets), len(anomaly_algorithms), plot_num)\\n        if i_dataset == 0:\\n            plt.title(name, size=18)\\n\\n        # fit the data and tag outliers\\n        if name == \"Local Outlier Factor\":\\n            y_pred = algorithm.fit_predict(X)\\n        else:\\n            y_pred = algorithm.fit(X).predict(X)\\n\\n        # plot the levels lines and the points\\n        if name != \"Local Outlier Factor\":  # LOF does not implement predict\\n            Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\\n            Z = Z.reshape(xx.shape)\\n            plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\"black\")\\n\\n        colors = np.array([\"#377eb8\", \"#ff7f00\"])\\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])\\n\\n        plt.xlim(-7, 7)\\n        plt.ylim(-7, 7)\\n        plt.xticks(())\\n        plt.yticks(())\\n        plt.text(\\n            0.99,\\n            0.01,\\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\\n            transform=plt.gca().transAxes,\\n            size=15,\\n            horizontalalignment=\"right\",\\n        )\\n        plot_num += 1\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def check_metadata(obj, **kwargs):\\n    for key, value in kwargs.items():\\n        if value is not None:\\n            print(\\n                f\"Received {key} of length = {len(value)} in {obj.__class__.__name__}.\"\\n            )\\n        else:\\n            print(f\"{key} is None in {obj.__class__.__name__}.\")'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def print_routing(obj):\\n    pprint(obj.get_metadata_routing()._serialize())'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content=\"class ExampleClassifier(ClassifierMixin, BaseEstimator):\\n    def fit(self, X, y, sample_weight=None):\\n        check_metadata(self, sample_weight=sample_weight)\\n        # all classifiers need to expose a classes_ attribute once they're fit.\\n        self.classes_ = np.array([0, 1])\\n        return self\\n\\n    def predict(self, X, groups=None):\\n        check_metadata(self, groups=groups)\\n        # return a constant value of 1, not a very smart classifier!\\n        return np.ones(len(X))\"), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class MetaClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):\\n    def __init__(self, estimator):\\n        self.estimator = estimator\\n\\n    def get_metadata_routing(self):\\n        # This method defines the routing for this meta-estimator.\\n        # In order to do so, a `MetadataRouter` instance is created, and the\\n        # routing is added to it. More explanations follow below.\\n        router = MetadataRouter(owner=self.__class__.__name__).add(\\n            estimator=self.estimator,\\n            method_mapping=MethodMapping()\\n            .add(caller=\"fit\", callee=\"fit\")\\n            .add(caller=\"predict\", callee=\"predict\")\\n            .add(caller=\"score\", callee=\"score\"),\\n        )\\n        return router\\n\\n    def fit(self, X, y, **fit_params):\\n        # `get_routing_for_object` returns a copy of the `MetadataRouter`\\n        # constructed by the above `get_metadata_routing` method, that is\\n        # internally called.\\n        request_router = get_routing_for_object(self)\\n        # Meta-estimators are responsible for validating the given metadata.\\n        # `method` refers to the parent\\'s method, i.e. `fit` in this example.\\n        request_router.validate_metadata(params=fit_params, method=\"fit\")\\n        # `MetadataRouter.route_params` maps the given metadata to the metadata\\n        # required by the underlying estimator based on the routing information\\n        # defined by the MetadataRouter. The output of type `Bunch` has a key\\n        # for each consuming object and those hold keys for their consuming\\n        # methods, which then contain key for the metadata which should be\\n        # routed to them.\\n        routed_params = request_router.route_params(params=fit_params, caller=\"fit\")\\n\\n        # A sub-estimator is fitted and its classes are attributed to the\\n        # meta-estimator.\\n        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)\\n        self.classes_ = self.estimator_.classes_\\n        return self\\n\\n    def predict(self, X, **predict_params):\\n        check_is_fitted(self)\\n        # As in `fit`, we get a copy of the object\\'s MetadataRouter,\\n        request_router = get_routing_for_object(self)\\n        # then we validate the given metadata,\\n        request_router.validate_metadata(params=predict_params, method=\"predict\")\\n        # and then prepare the input to the underlying `predict` method.\\n        routed_params = request_router.route_params(\\n            params=predict_params, caller=\"predict\"\\n        )\\n        return self.estimator_.predict(X, **routed_params.estimator.predict)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class RouterConsumerClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):\\n    def __init__(self, estimator):\\n        self.estimator = estimator\\n\\n    def get_metadata_routing(self):\\n        router = (\\n            MetadataRouter(owner=self.__class__.__name__)\\n            # defining metadata routing request values for usage in the meta-estimator\\n            .add_self_request(self)\\n            # defining metadata routing request values for usage in the sub-estimator\\n            .add(\\n                estimator=self.estimator,\\n                method_mapping=MethodMapping()\\n                .add(caller=\"fit\", callee=\"fit\")\\n                .add(caller=\"predict\", callee=\"predict\")\\n                .add(caller=\"score\", callee=\"score\"),\\n            )\\n        )\\n        return router\\n\\n    # Since `sample_weight` is used and consumed here, it should be defined as\\n    # an explicit argument in the method\\'s signature. All other metadata which\\n    # are only routed, will be passed as `**fit_params`:\\n    def fit(self, X, y, sample_weight, **fit_params):\\n        if self.estimator is None:\\n            raise ValueError(\"estimator cannot be None!\")\\n\\n        check_metadata(self, sample_weight=sample_weight)\\n\\n        # We add `sample_weight` to the `fit_params` dictionary.\\n        if sample_weight is not None:\\n            fit_params[\"sample_weight\"] = sample_weight\\n\\n        request_router = get_routing_for_object(self)\\n        request_router.validate_metadata(params=fit_params, method=\"fit\")\\n        routed_params = request_router.route_params(params=fit_params, caller=\"fit\")\\n        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)\\n        self.classes_ = self.estimator_.classes_\\n        return self\\n\\n    def predict(self, X, **predict_params):\\n        check_is_fitted(self)\\n        # As in `fit`, we get a copy of the object\\'s MetadataRouter,\\n        request_router = get_routing_for_object(self)\\n        # we validate the given metadata,\\n        request_router.validate_metadata(params=predict_params, method=\"predict\")\\n        # and then prepare the input to the underlying ``predict`` method.\\n        routed_params = request_router.route_params(\\n            params=predict_params, caller=\"predict\"\\n        )\\n        return self.estimator_.predict(X, **routed_params.estimator.predict)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class SimplePipeline(ClassifierMixin, BaseEstimator):\\n    def __init__(self, transformer, classifier):\\n        self.transformer = transformer\\n        self.classifier = classifier\\n\\n    def get_metadata_routing(self):\\n        router = (\\n            MetadataRouter(owner=self.__class__.__name__)\\n            # We add the routing for the transformer.\\n            .add(\\n                transformer=self.transformer,\\n                method_mapping=MethodMapping()\\n                # The metadata is routed such that it retraces how\\n                # `SimplePipeline` internally calls the transformer\\'s `fit` and\\n                # `transform` methods in its own methods (`fit` and `predict`).\\n                .add(caller=\"fit\", callee=\"fit\")\\n                .add(caller=\"fit\", callee=\"transform\")\\n                .add(caller=\"predict\", callee=\"transform\"),\\n            )\\n            # We add the routing for the classifier.\\n            .add(\\n                classifier=self.classifier,\\n                method_mapping=MethodMapping()\\n                .add(caller=\"fit\", callee=\"fit\")\\n                .add(caller=\"predict\", callee=\"predict\"),\\n            )\\n        )\\n        return router\\n\\n    def fit(self, X, y, **fit_params):\\n        routed_params = process_routing(self, \"fit\", **fit_params)\\n\\n        self.transformer_ = clone(self.transformer).fit(\\n            X, y, **routed_params.transformer.fit\\n        )\\n        X_transformed = self.transformer_.transform(\\n            X, **routed_params.transformer.transform\\n        )\\n\\n        self.classifier_ = clone(self.classifier).fit(\\n            X_transformed, y, **routed_params.classifier.fit\\n        )\\n        return self\\n\\n    def predict(self, X, **predict_params):\\n        routed_params = process_routing(self, \"predict\", **predict_params)\\n\\n        X_transformed = self.transformer_.transform(\\n            X, **routed_params.transformer.transform\\n        )\\n        return self.classifier_.predict(\\n            X_transformed, **routed_params.classifier.predict\\n        )'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class ExampleTransformer(TransformerMixin, BaseEstimator):\\n    def fit(self, X, y, sample_weight=None):\\n        check_metadata(self, sample_weight=sample_weight)\\n        return self\\n\\n    def transform(self, X, groups=None):\\n        check_metadata(self, groups=groups)\\n        return X\\n\\n    def fit_transform(self, X, y, sample_weight=None, groups=None):\\n        return self.fit(X, y, sample_weight).transform(X, groups)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class MetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):\\n    def __init__(self, estimator):\\n        self.estimator = estimator\\n\\n    def fit(self, X, y, **fit_params):\\n        routed_params = process_routing(self, \"fit\", **fit_params)\\n        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)\\n\\n    def get_metadata_routing(self):\\n        router = MetadataRouter(owner=self.__class__.__name__).add(\\n            estimator=self.estimator,\\n            method_mapping=MethodMapping().add(caller=\"fit\", callee=\"fit\"),\\n        )\\n        return router'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class WeightedMetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):\\n    # show warning to remind user to explicitly set the value with\\n    # `.set_{method}_request(sample_weight={boolean})`\\n    __metadata_request__fit = {\"sample_weight\": metadata_routing.WARN}\\n\\n    def __init__(self, estimator):\\n        self.estimator = estimator\\n\\n    def fit(self, X, y, sample_weight=None, **fit_params):\\n        routed_params = process_routing(\\n            self, \"fit\", sample_weight=sample_weight, **fit_params\\n        )\\n        check_metadata(self, sample_weight=sample_weight)\\n        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)\\n\\n    def get_metadata_routing(self):\\n        router = (\\n            MetadataRouter(owner=self.__class__.__name__)\\n            .add_self_request(self)\\n            .add(\\n                estimator=self.estimator,\\n                method_mapping=MethodMapping().add(caller=\"fit\", callee=\"fit\"),\\n            )\\n        )\\n        return router'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class ExampleRegressor(RegressorMixin, BaseEstimator):\\n    __metadata_request__fit = {\"sample_weight\": metadata_routing.WARN}\\n\\n    def fit(self, X, y, sample_weight=None):\\n        check_metadata(self, sample_weight=sample_weight)\\n        return self\\n\\n    def predict(self, X):\\n        return np.zeros(shape=(len(X)))'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================\\nMetadata Routing\\n================\\n\\n.. currentmodule:: sklearn\\n\\nThis document shows how you can use the :ref:`metadata routing mechanism\\n<metadata_routing>` in scikit-learn to route metadata to the estimators,\\nscorers, and CV splitters consuming them.\\n\\nTo better understand the following document, we need to introduce two concepts:\\nrouters and consumers. A router is an object which forwards some given data and\\nmetadata to other objects. In most cases, a router is a :term:`meta-estimator`,\\ni.e. an estimator which takes another estimator as a parameter. A function such\\nas :func:`sklearn.model_selection.cross_validate` which takes an estimator as a\\nparameter and forwards data and metadata, is also a router.\\n\\nA consumer, on the other hand, is an object which accepts and uses some given\\nmetadata. For instance, an estimator taking into account ``sample_weight`` in\\nits :term:`fit` method is a consumer of ``sample_weight``.\\n\\nIt is possible for an object to be both a router and a consumer. For instance,\\na meta-estimator may take into account ``sample_weight`` in certain\\ncalculations, but it may also route it to the underlying estimator.\\n\\nFirst a few imports and some random data for the rest of the script.\\n\"\"\"\\n\\n# %%\\n\\nimport warnings\\nfrom pprint import pprint\\n\\nimport numpy as np\\n\\nfrom sklearn import set_config\\nfrom sklearn.base import (\\n    BaseEstimator,\\n    ClassifierMixin,\\n    MetaEstimatorMixin,\\n    RegressorMixin,\\n    TransformerMixin,\\n    clone,\\n)\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.utils import metadata_routing\\nfrom sklearn.utils.metadata_routing import (\\n    MetadataRouter,\\n    MethodMapping,\\n    get_routing_for_object,\\n    process_routing,\\n)\\nfrom sklearn.utils.validation import check_is_fitted\\n\\nn_samples, n_features = 100, 4\\nrng = np.random.RandomState(42)\\nX = rng.rand(n_samples, n_features)\\ny = rng.randint(0, 2, size=n_samples)\\nmy_groups = rng.randint(0, 10, size=n_samples)\\nmy_weights = rng.rand(n_samples)\\nmy_other_weights = rng.rand(n_samples)\\n\\n# %%\\n# Metadata routing is only available if explicitly enabled:\\nset_config(enable_metadata_routing=True)\\n\\n\\n# %%\\n# This utility function is a dummy to check if a metadata is passed:\\n# Code for: def check_metadata(obj, **kwargs):\\n\\n\\n# %%\\n# A utility function to nicely print the routing information of an object:\\n# Code for: def print_routing(obj):\\n\\n\\n# %%\\n# Consuming Estimator\\n# -------------------\\n# Here we demonstrate how an estimator can expose the required API to support\\n# metadata routing as a consumer. Imagine a simple classifier accepting\\n# ``sample_weight`` as a metadata on its ``fit`` and ``groups`` in its\\n# ``predict`` method:\\n\\n\\n# Code for: class ExampleClassifier(ClassifierMixin, BaseEstimator):'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"# Code for: class ExampleClassifier(ClassifierMixin, BaseEstimator):\\n\\n\\n# %%\\n# The above estimator now has all it needs to consume metadata. This is\\n# accomplished by some magic done in :class:`~base.BaseEstimator`. There are\\n# now three methods exposed by the above class: ``set_fit_request``,\\n# ``set_predict_request``, and ``get_metadata_routing``. There is also a\\n# ``set_score_request`` for ``sample_weight`` which is present since\\n# :class:`~base.ClassifierMixin` implements a ``score`` method accepting\\n# ``sample_weight``. The same applies to regressors which inherit from\\n# :class:`~base.RegressorMixin`.\\n#\\n# By default, no metadata is requested, which we can see as:\\n\\nprint_routing(ExampleClassifier())\\n\\n# %%\\n# The above output means that ``sample_weight`` and ``groups`` are not\\n# requested by `ExampleClassifier`, and if a router is given those metadata, it\\n# should raise an error, since the user has not explicitly set whether they are\\n# required or not. The same is true for ``sample_weight`` in the ``score``\\n# method, which is inherited from :class:`~base.ClassifierMixin`. In order to\\n# explicitly set request values for those metadata, we can use these methods:\\n\\nest = (\\n    ExampleClassifier()\\n    .set_fit_request(sample_weight=False)\\n    .set_predict_request(groups=True)\\n    .set_score_request(sample_weight=False)\\n)\\nprint_routing(est)\\n\\n# %%\\n# .. note ::\\n#     Please note that as long as the above estimator is not used in a\\n#     meta-estimator, the user does not need to set any requests for the\\n#     metadata and the set values are ignored, since a consumer does not\\n#     validate or route given metadata. A simple usage of the above estimator\\n#     would work as expected.\\n\\nest = ExampleClassifier()\\nest.fit(X, y, sample_weight=my_weights)\\nest.predict(X[:3, :], groups=my_groups)\\n\\n# %%\\n# Routing Meta-Estimator\\n# ----------------------\\n# Now, we show how to design a meta-estimator to be a router. As a simplified\\n# example, here is a meta-estimator, which doesn't do much other than routing\\n# the metadata.\\n\\n\\n# Code for: class MetaClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):\"), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: class MetaClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):\\n\\n\\n# %%\\n# Let\\'s break down different parts of the above code.\\n#\\n# First, the :meth:`~utils.metadata_routing.get_routing_for_object` takes our\\n# meta-estimator (``self``) and returns a\\n# :class:`~utils.metadata_routing.MetadataRouter` or, a\\n# :class:`~utils.metadata_routing.MetadataRequest` if the object is a consumer,\\n# based on the output of the estimator\\'s ``get_metadata_routing`` method.\\n#\\n# Then in each method, we use the ``route_params`` method to construct a\\n# dictionary of the form ``{\"object_name\": {\"method_name\": {\"metadata\":\\n# value}}}`` to pass to the underlying estimator\\'s method. The ``object_name``\\n# (``estimator`` in the above ``routed_params.estimator.fit`` example) is the\\n# same as the one added in the ``get_metadata_routing``. ``validate_metadata``\\n# makes sure all given metadata are requested to avoid silent bugs.\\n#\\n# Next, we illustrate the different behaviors and notably the type of errors\\n# raised.\\n\\nmeta_est = MetaClassifier(\\n    estimator=ExampleClassifier().set_fit_request(sample_weight=True)\\n)\\nmeta_est.fit(X, y, sample_weight=my_weights)\\n\\n# %%\\n# Note that the above example is calling our utility function\\n# `check_metadata()` via the `ExampleClassifier`. It checks that\\n# ``sample_weight`` is correctly passed to it. If it is not, like in the\\n# following example, it would print that ``sample_weight`` is ``None``:\\n\\nmeta_est.fit(X, y)\\n\\n# %%\\n# If we pass an unknown metadata, an error is raised:\\ntry:\\n    meta_est.fit(X, y, test=my_weights)\\nexcept TypeError as e:\\n    print(e)\\n\\n# %%\\n# And if we pass a metadata which is not explicitly requested:\\ntry:\\n    meta_est.fit(X, y, sample_weight=my_weights).predict(X, groups=my_groups)\\nexcept ValueError as e:\\n    print(e)\\n\\n# %%\\n# Also, if we explicitly set it as not requested, but it is provided:\\nmeta_est = MetaClassifier(\\n    estimator=ExampleClassifier()\\n    .set_fit_request(sample_weight=True)\\n    .set_predict_request(groups=False)\\n)\\ntry:\\n    meta_est.fit(X, y, sample_weight=my_weights).predict(X[:3, :], groups=my_groups)\\nexcept TypeError as e:\\n    print(e)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Another concept to introduce is **aliased metadata**. This is when an\\n# estimator requests a metadata with a different variable name than the default\\n# variable name. For instance, in a setting where there are two estimators in a\\n# pipeline, one could request ``sample_weight1`` and the other\\n# ``sample_weight2``. Note that this doesn\\'t change what the estimator expects,\\n# it only tells the meta-estimator how to map the provided metadata to what is\\n# required. Here\\'s an example, where we pass ``aliased_sample_weight`` to the\\n# meta-estimator, but the meta-estimator understands that\\n# ``aliased_sample_weight`` is an alias for ``sample_weight``, and passes it as\\n# ``sample_weight`` to the underlying estimator:\\nmeta_est = MetaClassifier(\\n    estimator=ExampleClassifier().set_fit_request(sample_weight=\"aliased_sample_weight\")\\n)\\nmeta_est.fit(X, y, aliased_sample_weight=my_weights)\\n\\n# %%\\n# Passing ``sample_weight`` here will fail since it is requested with an\\n# alias and ``sample_weight`` with that name is not requested:\\ntry:\\n    meta_est.fit(X, y, sample_weight=my_weights)\\nexcept TypeError as e:\\n    print(e)\\n\\n# %%\\n# This leads us to the ``get_metadata_routing``. The way routing works in\\n# scikit-learn is that consumers request what they need, and routers pass that\\n# along. Additionally, a router exposes what it requires itself so that it can\\n# be used inside another router, e.g. a pipeline inside a grid search object.\\n# The output of the ``get_metadata_routing`` which is a dictionary\\n# representation of a :class:`~utils.metadata_routing.MetadataRouter`, includes\\n# the complete tree of requested metadata by all nested objects and their\\n# corresponding method routings, i.e. which method of a sub-estimator is used\\n# in which method of a meta-estimator:\\n\\nprint_routing(meta_est)\\n\\n# %%\\n# As you can see, the only metadata requested for method ``fit`` is\\n# ``\"sample_weight\"`` with ``\"aliased_sample_weight\"`` as the alias. The\\n# ``~utils.metadata_routing.MetadataRouter`` class enables us to easily create\\n# the routing object which would create the output we need for our\\n# ``get_metadata_routing``.\\n#\\n# In order to understand how aliases work in meta-estimators, imagine our\\n# meta-estimator inside another one:\\n\\nmeta_meta_est = MetaClassifier(estimator=meta_est).fit(\\n    X, y, aliased_sample_weight=my_weights\\n)\\n\\n# %%\\n# In the above example, this is how the ``fit`` method of `meta_meta_est`\\n# will call their sub-estimator\\'s ``fit`` methods::\\n#\\n#     # user feeds `my_weights` as `aliased_sample_weight` into `meta_meta_est`:\\n#     meta_meta_est.fit(X, y, aliased_sample_weight=my_weights):\\n#         ...\\n#\\n#         # the first sub-estimator (`meta_est`) expects `aliased_sample_weight`\\n#         self.estimator_.fit(X, y, aliased_sample_weight=aliased_sample_weight):\\n#             ...\\n#\\n#             # the second sub-estimator (`est`) expects `sample_weight`\\n#             self.estimator_.fit(X, y, sample_weight=aliased_sample_weight):\\n#                 ...'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Consuming and routing Meta-Estimator\\n# ------------------------------------\\n# For a slightly more complex example, consider a meta-estimator that routes\\n# metadata to an underlying estimator as before, but it also uses some metadata\\n# in its own methods. This meta-estimator is a consumer and a router at the\\n# same time. Implementing one is very similar to what we had before, but with a\\n# few tweaks.\\n\\n\\n# Code for: class RouterConsumerClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):\\n\\n\\n# %%\\n# The key parts where the above meta-estimator differs from our previous\\n# meta-estimator is accepting ``sample_weight`` explicitly in ``fit`` and\\n# including it in ``fit_params``. Since ``sample_weight`` is an explicit\\n# argument, we can be sure that ``set_fit_request(sample_weight=...)`` is\\n# present for this method. The meta-estimator is both a consumer, as well as a\\n# router of ``sample_weight``.\\n#\\n# In ``get_metadata_routing``, we add ``self`` to the routing using\\n# ``add_self_request`` to indicate this estimator is consuming\\n# ``sample_weight`` as well as being a router; which also adds a\\n# ``$self_request`` key to the routing info as illustrated below. Now let\\'s\\n# look at some examples:\\n\\n# %%\\n# - No metadata requested\\nmeta_est = RouterConsumerClassifier(estimator=ExampleClassifier())\\nprint_routing(meta_est)\\n\\n\\n# %%\\n# - ``sample_weight`` requested by sub-estimator\\nmeta_est = RouterConsumerClassifier(\\n    estimator=ExampleClassifier().set_fit_request(sample_weight=True)\\n)\\nprint_routing(meta_est)\\n\\n# %%\\n# - ``sample_weight`` requested by meta-estimator\\nmeta_est = RouterConsumerClassifier(estimator=ExampleClassifier()).set_fit_request(\\n    sample_weight=True\\n)\\nprint_routing(meta_est)\\n\\n# %%\\n# Note the difference in the requested metadata representations above.\\n#\\n# - We can also alias the metadata to pass different values to the fit methods\\n#   of the meta- and the sub-estimator:\\n\\nmeta_est = RouterConsumerClassifier(\\n    estimator=ExampleClassifier().set_fit_request(sample_weight=\"clf_sample_weight\"),\\n).set_fit_request(sample_weight=\"meta_clf_sample_weight\")\\nprint_routing(meta_est)\\n\\n# %%\\n# However, ``fit`` of the meta-estimator only needs the alias for the\\n# sub-estimator and addresses their own sample weight as `sample_weight`, since\\n# it doesn\\'t validate and route its own required metadata:\\nmeta_est.fit(X, y, sample_weight=my_weights, clf_sample_weight=my_other_weights)\\n\\n# %%\\n# - Alias only on the sub-estimator:\\n#\\n# This is useful when we don\\'t want the meta-estimator to use the metadata, but\\n# the sub-estimator should.\\nmeta_est = RouterConsumerClassifier(\\n    estimator=ExampleClassifier().set_fit_request(sample_weight=\"aliased_sample_weight\")\\n)\\nprint_routing(meta_est)\\n# %%\\n# The meta-estimator cannot use `aliased_sample_weight`, because it expects\\n# it passed as `sample_weight`. This would apply even if\\n# `set_fit_request(sample_weight=True)` was set on it.'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"# %%\\n# Simple Pipeline\\n# ---------------\\n# A slightly more complicated use-case is a meta-estimator resembling a\\n# :class:`~pipeline.Pipeline`. Here is a meta-estimator, which accepts a\\n# transformer and a classifier. When calling its `fit` method, it applies the\\n# transformer's `fit` and `transform` before running the classifier on the\\n# transformed data. Upon `predict`, it applies the transformer's `transform`\\n# before predicting with the classifier's `predict` method on the transformed\\n# new data.\\n\\n\\n# Code for: class SimplePipeline(ClassifierMixin, BaseEstimator):\\n\\n\\n# %%\\n# Note the usage of :class:`~utils.metadata_routing.MethodMapping` to\\n# declare which methods of the child estimator (callee) are used in which\\n# methods of the meta estimator (caller). As you can see, `SimplePipeline` uses\\n# the transformer's ``transform`` and ``fit`` methods in ``fit``, and its\\n# ``transform`` method in ``predict``, and that's what you see implemented in\\n# the routing structure of the pipeline class.\\n#\\n# Another difference in the above example with the previous ones is the usage\\n# of :func:`~utils.metadata_routing.process_routing`, which processes the input\\n# parameters, does the required validation, and returns the `routed_params`\\n# which we had created in previous examples. This reduces the boilerplate code\\n# a developer needs to write in each meta-estimator's method. Developers are\\n# strongly recommended to use this function unless there is a good reason\\n# against it.\\n#\\n# In order to test the above pipeline, let's add an example transformer.\\n\\n\\n# Code for: class ExampleTransformer(TransformerMixin, BaseEstimator):\\n\\n\\n# %%\\n# Note that in the above example, we have implemented ``fit_transform`` which\\n# calls ``fit`` and ``transform`` with the appropriate metadata. This is only\\n# required if ``transform`` accepts metadata, since the default ``fit_transform``\\n# implementation in :class:`~base.TransformerMixin` doesn't pass metadata to\\n# ``transform``.\\n#\\n# Now we can test our pipeline, and see if metadata is correctly passed around.\\n# This example uses our `SimplePipeline`, our `ExampleTransformer`, and our\\n# `RouterConsumerClassifier` which uses our `ExampleClassifier`.\\n\\npipe = SimplePipeline(\\n    transformer=ExampleTransformer()\\n    # we set transformer's fit to receive sample_weight\\n    .set_fit_request(sample_weight=True)\\n    # we set transformer's transform to receive groups\\n    .set_transform_request(groups=True),\\n    classifier=RouterConsumerClassifier(\\n        estimator=ExampleClassifier()\\n        # we want this sub-estimator to receive sample_weight in fit\\n        .set_fit_request(sample_weight=True)\\n        # but not groups in predict\\n        .set_predict_request(groups=False),\\n    )\\n    # and we want the meta-estimator to receive sample_weight as well\\n    .set_fit_request(sample_weight=True),\\n)\\npipe.fit(X, y, sample_weight=my_weights, groups=my_groups).predict(\\n    X[:3], groups=my_groups\\n)\"), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_metadata_routing.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"# %%\\n# Deprecation / Default Value Change\\n# ----------------------------------\\n# In this section we show how one should handle the case where a router becomes\\n# also a consumer, especially when it consumes the same metadata as its\\n# sub-estimator, or a consumer starts consuming a metadata which it wasn't in\\n# an older release. In this case, a warning should be raised for a while, to\\n# let users know the behavior is changed from previous versions.\\n\\n\\n# Code for: class MetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):\\n\\n\\n# %%\\n# As explained above, this is a valid usage if `my_weights` aren't supposed\\n# to be passed as `sample_weight` to `MetaRegressor`:\\n\\nreg = MetaRegressor(estimator=LinearRegression().set_fit_request(sample_weight=True))\\nreg.fit(X, y, sample_weight=my_weights)\\n\\n\\n# %%\\n# Now imagine we further develop ``MetaRegressor`` and it now also *consumes*\\n# ``sample_weight``:\\n\\n\\n# Code for: class WeightedMetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):\\n\\n\\n# %%\\n# The above implementation is almost the same as ``MetaRegressor``, and\\n# because of the default request value defined in ``__metadata_request__fit``\\n# there is a warning raised when fitted.\\n\\nwith warnings.catch_warnings(record=True) as record:\\n    WeightedMetaRegressor(\\n        estimator=LinearRegression().set_fit_request(sample_weight=False)\\n    ).fit(X, y, sample_weight=my_weights)\\nfor w in record:\\n    print(w.message)\\n\\n\\n# %%\\n# When an estimator consumes a metadata which it didn't consume before, the\\n# following pattern can be used to warn the users about it.\\n\\n\\n# Code for: class ExampleRegressor(RegressorMixin, BaseEstimator):\\n\\n\\nwith warnings.catch_warnings(record=True) as record:\\n    MetaRegressor(estimator=ExampleRegressor()).fit(X, y, sample_weight=my_weights)\\nfor w in record:\\n    print(w.message)\\n\\n# %%\\n# At the end we disable the configuration flag for metadata routing:\\n\\nset_config(enable_metadata_routing=False)\\n\\n# %%\\n# Third Party Development and scikit-learn Dependency\\n# ---------------------------------------------------\\n#\\n# As seen above, information is communicated between classes using\\n# :class:`~utils.metadata_routing.MetadataRequest` and\\n# :class:`~utils.metadata_routing.MetadataRouter`. It is strongly not advised,\\n# but possible to vendor the tools related to metadata-routing if you strictly\\n# want to have a scikit-learn compatible estimator, without depending on the\\n# scikit-learn package. If all of the following conditions are met, you do NOT\\n# need to modify your code at all:\\n#\\n# - your estimator inherits from :class:`~base.BaseEstimator`\\n# - the parameters consumed by your estimator's methods, e.g. ``fit``, are\\n#   explicitly defined in the method's signature, as opposed to being\\n#   ``*args`` or ``*kwargs``.\\n# - your estimator does not route any metadata to the underlying objects, i.e.\\n#   it's not a *router*.\"), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='r\"\"\"\\n=====================================================================\\nThe Johnson-Lindenstrauss bound for embedding with random projections\\n=====================================================================\\n\\n\\nThe `Johnson-Lindenstrauss lemma`_ states that any high dimensional\\ndataset can be randomly projected into a lower dimensional Euclidean\\nspace while controlling the distortion in the pairwise distances.\\n\\n.. _`Johnson-Lindenstrauss lemma`: https://en.wikipedia.org/wiki/\\\\\\n    Johnson%E2%80%93Lindenstrauss_lemma\\n\\n\"\"\"\\n\\nimport sys\\nfrom time import time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_20newsgroups_vectorized, load_digits\\nfrom sklearn.metrics.pairwise import euclidean_distances\\nfrom sklearn.random_projection import (\\n    SparseRandomProjection,\\n    johnson_lindenstrauss_min_dim,\\n)\\n\\n# %%\\n# Theoretical bounds\\n# ==================\\n# The distortion introduced by a random projection `p` is asserted by\\n# the fact that `p` is defining an eps-embedding with good probability\\n# as defined by:\\n#\\n# .. math::\\n#    (1 - eps) \\\\|u - v\\\\|^2 < \\\\|p(u) - p(v)\\\\|^2 < (1 + eps) \\\\|u - v\\\\|^2\\n#\\n# Where `u` and `v` are any rows taken from a dataset of shape `(n_samples,\\n# n_features)` and `p` is a projection by a random Gaussian `N(0, 1)` matrix\\n# of shape `(n_components, n_features)` (or a sparse Achlioptas matrix).\\n#\\n# The minimum number of components to guarantees the eps-embedding is\\n# given by:\\n#\\n# .. math::\\n#    n\\\\_components \\\\geq 4 log(n\\\\_samples) / (eps^2 / 2 - eps^3 / 3)\\n#\\n#\\n# The first plot shows that with an increasing number of samples ``n_samples``,\\n# the minimal number of dimensions ``n_components`` increased logarithmically\\n# in order to guarantee an ``eps``-embedding.\\n\\n# range of admissible distortions\\neps_range = np.linspace(0.1, 0.99, 5)\\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(eps_range)))\\n\\n# range of number of samples (observation) to embed\\nn_samples_range = np.logspace(1, 9, 9)\\n\\nplt.figure()\\nfor eps, color in zip(eps_range, colors):\\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples_range, eps=eps)\\n    plt.loglog(n_samples_range, min_n_components, color=color)\\n\\nplt.legend([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\\nplt.xlabel(\"Number of observations to eps-embed\")\\nplt.ylabel(\"Minimum number of dimensions\")\\nplt.title(\"Johnson-Lindenstrauss bounds:\\\\nn_samples vs n_components\")\\nplt.show()\\n\\n\\n# %%\\n# The second plot shows that an increase of the admissible\\n# distortion ``eps`` allows to reduce drastically the minimal number of\\n# dimensions ``n_components`` for a given number of samples ``n_samples``\\n\\n# range of admissible distortions\\neps_range = np.linspace(0.01, 0.99, 100)\\n\\n# range of number of samples (observation) to embed\\nn_samples_range = np.logspace(2, 6, 5)\\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# range of admissible distortions\\neps_range = np.linspace(0.01, 0.99, 100)\\n\\n# range of number of samples (observation) to embed\\nn_samples_range = np.logspace(2, 6, 5)\\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))\\n\\nplt.figure()\\nfor n_samples, color in zip(n_samples_range, colors):\\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples, eps=eps_range)\\n    plt.semilogy(eps_range, min_n_components, color=color)\\n\\nplt.legend([f\"n_samples = {n}\" for n in n_samples_range], loc=\"upper right\")\\nplt.xlabel(\"Distortion eps\")\\nplt.ylabel(\"Minimum number of dimensions\")\\nplt.title(\"Johnson-Lindenstrauss bounds:\\\\nn_components vs eps\")\\nplt.show()\\n\\n# %%\\n# Empirical validation\\n# ====================\\n#\\n# We validate the above bounds on the 20 newsgroups text document\\n# (TF-IDF word frequencies) dataset or on the digits dataset:\\n#\\n# - for the 20 newsgroups dataset some 300 documents with 100k\\n#   features in total are projected using a sparse random matrix to smaller\\n#   euclidean spaces with various values for the target number of dimensions\\n#   ``n_components``.\\n#\\n# - for the digits dataset, some 8x8 gray level pixels data for 300\\n#   handwritten digits pictures are randomly projected to spaces for various\\n#   larger number of dimensions ``n_components``.\\n#\\n# The default dataset is the 20 newsgroups dataset. To run the example on the\\n# digits dataset, pass the ``--use-digits-dataset`` command line argument to\\n# this script.\\n\\nif \"--use-digits-dataset\" in sys.argv:\\n    data = load_digits().data[:300]\\nelse:\\n    data = fetch_20newsgroups_vectorized().data[:300]\\n\\n# %%\\n# For each value of ``n_components``, we plot:\\n#\\n# - 2D distribution of sample pairs with pairwise distances in original\\n#   and projected spaces as x- and y-axis respectively.\\n#\\n# - 1D histogram of the ratio of those distances (projected / original).\\n\\nn_samples, n_features = data.shape\\nprint(\\n    f\"Embedding {n_samples} samples with dim {n_features} using various \"\\n    \"random projections\"\\n)\\n\\nn_components_range = np.array([300, 1_000, 10_000])\\ndists = euclidean_distances(data, squared=True).ravel()\\n\\n# select only non-identical samples pairs\\nnonzero = dists != 0\\ndists = dists[nonzero]\\n\\nfor n_components in n_components_range:\\n    t0 = time()\\n    rp = SparseRandomProjection(n_components=n_components)\\n    projected_data = rp.fit_transform(data)\\n    print(\\n        f\"Projected {n_samples} samples from {n_features} to {n_components} in \"\\n        f\"{time() - t0:0.3f}s\"\\n    )\\n    if hasattr(rp, \"components_\"):\\n        n_bytes = rp.components_.data.nbytes\\n        n_bytes += rp.components_.indices.nbytes\\n        print(f\"Random matrix with size: {n_bytes / 1e6:0.3f} MB\")\\n\\n    projected_dists = euclidean_distances(projected_data, squared=True).ravel()[nonzero]'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='projected_dists = euclidean_distances(projected_data, squared=True).ravel()[nonzero]\\n\\n    plt.figure()\\n    min_dist = min(projected_dists.min(), dists.min())\\n    max_dist = max(projected_dists.max(), dists.max())\\n    plt.hexbin(\\n        dists,\\n        projected_dists,\\n        gridsize=100,\\n        cmap=plt.cm.PuBu,\\n        extent=[min_dist, max_dist, min_dist, max_dist],\\n    )\\n    plt.xlabel(\"Pairwise squared distances in original space\")\\n    plt.ylabel(\"Pairwise squared distances in projected space\")\\n    plt.title(\"Pairwise distances distribution for n_components=%d\" % n_components)\\n    cb = plt.colorbar()\\n    cb.set_label(\"Sample pairs counts\")\\n\\n    rates = projected_dists / dists\\n    print(f\"Mean distances rate: {np.mean(rates):.2f} ({np.std(rates):.2f})\")\\n\\n    plt.figure()\\n    plt.hist(rates, bins=50, range=(0.0, 2.0), edgecolor=\"k\", density=True)\\n    plt.xlabel(\"Squared distances rate: projected / original\")\\n    plt.ylabel(\"Distribution of samples pairs\")\\n    plt.title(\"Histogram of pairwise distance rates for n_components=%d\" % n_components)\\n\\n    # TODO: compute the expected value of eps and add them to the previous plot\\n    # as vertical lines / region\\n\\nplt.show()\\n\\n\\n# %%\\n# We can see that for low values of ``n_components`` the distribution is wide\\n# with many distorted pairs and a skewed distribution (due to the hard\\n# limit of zero ratio on the left as distances are always positives)\\n# while for larger values of `n_components` the distortion is controlled\\n# and the distances are well preserved by the random projection.\\n#\\n# Remarks\\n# =======\\n#\\n# According to the JL lemma, projecting 300 samples without too much distortion\\n# will require at least several thousands dimensions, irrespective of the\\n# number of features of the original dataset.\\n#\\n# Hence using random projections on the digits dataset which only has 64\\n# features in the input space does not make sense: it does not allow\\n# for dimensionality reduction in this case.\\n#\\n# On the twenty newsgroups on the other hand the dimensionality can be\\n# decreased from 56,436 down to 10,000 while reasonably preserving\\n# pairwise distances.'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_partial_dependence_visualization_api.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================\\nAdvanced Plotting With Partial Dependence\\n=========================================\\nThe :class:`~sklearn.inspection.PartialDependenceDisplay` object can be used\\nfor plotting without needing to recalculate the partial dependence. In this\\nexample, we show how to plot partial dependence plots and how to quickly\\ncustomize the plot with the visualization API.\\n\\n.. note::\\n\\n    See also :ref:`sphx_glr_auto_examples_miscellaneous_plot_roc_curve_visualization_api.py`\\n\\n\"\"\"  # noqa: E501\\n\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.inspection import PartialDependenceDisplay\\nfrom sklearn.neural_network import MLPRegressor\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\n# %%\\n# Train models on the diabetes dataset\\n# ================================================\\n#\\n# First, we train a decision tree and a multi-layer perceptron on the diabetes\\n# dataset.\\n\\ndiabetes = load_diabetes()\\nX = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\\ny = diabetes.target\\n\\ntree = DecisionTreeRegressor()\\nmlp = make_pipeline(\\n    StandardScaler(),\\n    MLPRegressor(hidden_layer_sizes=(100, 100), tol=1e-2, max_iter=500, random_state=0),\\n)\\ntree.fit(X, y)\\nmlp.fit(X, y)\\n\\n# %%\\n# Plotting partial dependence for two features\\n# ============================================\\n#\\n# We plot partial dependence curves for features \"age\" and \"bmi\" (body mass\\n# index) for the decision tree. With two features,\\n# :func:`~sklearn.inspection.PartialDependenceDisplay.from_estimator` expects to plot\\n# two curves. Here the plot function place a grid of two plots using the space\\n# defined by `ax` .\\nfig, ax = plt.subplots(figsize=(12, 6))\\nax.set_title(\"Decision Tree\")\\ntree_disp = PartialDependenceDisplay.from_estimator(tree, X, [\"age\", \"bmi\"], ax=ax)\\n\\n# %%\\n# The partial dependence curves can be plotted for the multi-layer perceptron.\\n# In this case, `line_kw` is passed to\\n# :func:`~sklearn.inspection.PartialDependenceDisplay.from_estimator` to change the\\n# color of the curve.\\nfig, ax = plt.subplots(figsize=(12, 6))\\nax.set_title(\"Multi-layer Perceptron\")\\nmlp_disp = PartialDependenceDisplay.from_estimator(\\n    mlp, X, [\"age\", \"bmi\"], ax=ax, line_kw={\"color\": \"red\"}\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_partial_dependence_visualization_api.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Plotting partial dependence of the two models together\\n# ======================================================\\n#\\n# The `tree_disp` and `mlp_disp`\\n# :class:`~sklearn.inspection.PartialDependenceDisplay` objects contain all the\\n# computed information needed to recreate the partial dependence curves. This\\n# means we can easily create additional plots without needing to recompute the\\n# curves.\\n#\\n# One way to plot the curves is to place them in the same figure, with the\\n# curves of each model on each row. First, we create a figure with two axes\\n# within two rows and one column. The two axes are passed to the\\n# :func:`~sklearn.inspection.PartialDependenceDisplay.plot` functions of\\n# `tree_disp` and `mlp_disp`. The given axes will be used by the plotting\\n# function to draw the partial dependence. The resulting plot places the\\n# decision tree partial dependence curves in the first row of the\\n# multi-layer perceptron in the second row.\\n\\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\\ntree_disp.plot(ax=ax1)\\nax1.set_title(\"Decision Tree\")\\nmlp_disp.plot(ax=ax2, line_kw={\"color\": \"red\"})\\nax2.set_title(\"Multi-layer Perceptron\")\\n\\n# %%\\n# Another way to compare the curves is to plot them on top of each other. Here,\\n# we create a figure with one row and two columns. The axes are passed into the\\n# :func:`~sklearn.inspection.PartialDependenceDisplay.plot` function as a list,\\n# which will plot the partial dependence curves of each model on the same axes.\\n# The length of the axes list must be equal to the number of plots drawn.\\n\\n# sphinx_gallery_thumbnail_number = 4\\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\\ntree_disp.plot(ax=[ax1, ax2], line_kw={\"label\": \"Decision Tree\"})\\nmlp_disp.plot(\\n    ax=[ax1, ax2], line_kw={\"label\": \"Multi-layer Perceptron\", \"color\": \"red\"}\\n)\\nax1.legend()\\nax2.legend()\\n\\n# %%\\n# `tree_disp.axes_` is a numpy array container the axes used to draw the\\n# partial dependence plots. This can be passed to `mlp_disp` to have the same\\n# affect of drawing the plots on top of each other. Furthermore, the\\n# `mlp_disp.figure_` stores the figure, which allows for resizing the figure\\n# after calling `plot`. In this case `tree_disp.axes_` has two dimensions, thus\\n# `plot` will only show the y label and y ticks on the left most plot.\\n\\ntree_disp.plot(line_kw={\"label\": \"Decision Tree\"})\\nmlp_disp.plot(\\n    line_kw={\"label\": \"Multi-layer Perceptron\", \"color\": \"red\"}, ax=tree_disp.axes_\\n)\\ntree_disp.figure_.set_size_inches(10, 6)\\ntree_disp.axes_[0, 0].legend()\\ntree_disp.axes_[0, 1].legend()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_partial_dependence_visualization_api.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='tree_disp.plot(line_kw={\"label\": \"Decision Tree\"})\\nmlp_disp.plot(\\n    line_kw={\"label\": \"Multi-layer Perceptron\", \"color\": \"red\"}, ax=tree_disp.axes_\\n)\\ntree_disp.figure_.set_size_inches(10, 6)\\ntree_disp.axes_[0, 0].legend()\\ntree_disp.axes_[0, 1].legend()\\nplt.show()\\n\\n# %%\\n# Plotting partial dependence for one feature\\n# ===========================================\\n#\\n# Here, we plot the partial dependence curves for a single feature, \"age\", on\\n# the same axes. In this case, `tree_disp.axes_` is passed into the second\\n# plot function.\\ntree_disp = PartialDependenceDisplay.from_estimator(tree, X, [\"age\"])\\nmlp_disp = PartialDependenceDisplay.from_estimator(\\n    mlp, X, [\"age\"], ax=tree_disp.axes_, line_kw={\"color\": \"red\"}\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_multioutput_face_completion.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================\\nFace completion with a multi-output estimators\\n==============================================\\n\\nThis example shows the use of multi-output estimator to complete images.\\nThe goal is to predict the lower half of a face given its upper half.\\n\\nThe first column of images shows true faces. The next columns illustrate\\nhow extremely randomized trees, k nearest neighbors, linear\\nregression and ridge regression complete the lower half of those faces.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_olivetti_faces\\nfrom sklearn.ensemble import ExtraTreesRegressor\\nfrom sklearn.linear_model import LinearRegression, RidgeCV\\nfrom sklearn.neighbors import KNeighborsRegressor\\nfrom sklearn.utils.validation import check_random_state\\n\\n# Load the faces datasets\\ndata, targets = fetch_olivetti_faces(return_X_y=True)\\n\\ntrain = data[targets < 30]\\ntest = data[targets >= 30]  # Test on independent people\\n\\n# Test on a subset of people\\nn_faces = 5\\nrng = check_random_state(4)\\nface_ids = rng.randint(test.shape[0], size=(n_faces,))\\ntest = test[face_ids, :]\\n\\nn_pixels = data.shape[1]\\n# Upper half of the faces\\nX_train = train[:, : (n_pixels + 1) // 2]\\n# Lower half of the faces\\ny_train = train[:, n_pixels // 2 :]\\nX_test = test[:, : (n_pixels + 1) // 2]\\ny_test = test[:, n_pixels // 2 :]\\n\\n# Fit estimators\\nESTIMATORS = {\\n    \"Extra trees\": ExtraTreesRegressor(\\n        n_estimators=10, max_features=32, random_state=0\\n    ),\\n    \"K-nn\": KNeighborsRegressor(),\\n    \"Linear regression\": LinearRegression(),\\n    \"Ridge\": RidgeCV(),\\n}\\n\\ny_test_predict = dict()\\nfor name, estimator in ESTIMATORS.items():\\n    estimator.fit(X_train, y_train)\\n    y_test_predict[name] = estimator.predict(X_test)\\n\\n# Plot the completed faces\\nimage_shape = (64, 64)\\n\\nn_cols = 1 + len(ESTIMATORS)\\nplt.figure(figsize=(2.0 * n_cols, 2.26 * n_faces))\\nplt.suptitle(\"Face completion with multi-output estimators\", size=16)\\n\\nfor i in range(n_faces):\\n    true_face = np.hstack((X_test[i], y_test[i]))\\n\\n    if i:\\n        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1)\\n    else:\\n        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1, title=\"true faces\")\\n\\n    sub.axis(\"off\")\\n    sub.imshow(\\n        true_face.reshape(image_shape), cmap=plt.cm.gray, interpolation=\"nearest\"\\n    )\\n\\n    for j, est in enumerate(sorted(ESTIMATORS)):\\n        completed_face = np.hstack((X_test[i], y_test_predict[est][i]))\\n\\n        if i:\\n            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j)\\n\\n        else:\\n            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j, title=est)\\n\\n        sub.axis(\"off\")\\n        sub.imshow(\\n            completed_face.reshape(image_shape),\\n            cmap=plt.cm.gray,\\n            interpolation=\"nearest\",\\n        )\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_outlier_detection_bench.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def make_estimator(name, categorical_columns=None, iforest_kw=None, lof_kw=None):\\n    \"\"\"Create an outlier detection estimator based on its name.\"\"\"\\n    if name == \"LOF\":\\n        outlier_detector = LocalOutlierFactor(**(lof_kw or {}))\\n        if categorical_columns is None:\\n            preprocessor = RobustScaler()\\n        else:\\n            preprocessor = ColumnTransformer(\\n                transformers=[(\"categorical\", OneHotEncoder(), categorical_columns)],\\n                remainder=RobustScaler(),\\n            )\\n    else:  # name == \"IForest\"\\n        outlier_detector = IsolationForest(**(iforest_kw or {}))\\n        if categorical_columns is None:\\n            preprocessor = None\\n        else:\\n            ordinal_encoder = OrdinalEncoder(\\n                handle_unknown=\"use_encoded_value\", unknown_value=-1\\n            )\\n            preprocessor = ColumnTransformer(\\n                transformers=[\\n                    (\"categorical\", ordinal_encoder, categorical_columns),\\n                ],\\n                remainder=\"passthrough\",\\n            )\\n\\n    return make_pipeline(preprocessor, outlier_detector)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_outlier_detection_bench.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def fit_predict(estimator, X):\\n    tic = perf_counter()\\n    if estimator[-1].__class__.__name__ == \"LocalOutlierFactor\":\\n        estimator.fit(X)\\n        y_pred = estimator[-1].negative_outlier_factor_\\n    else:  # \"IsolationForest\"\\n        y_pred = estimator.fit(X).decision_function(X)\\n    toc = perf_counter()\\n    print(f\"Duration for {model_name}: {toc - tic:.2f} s\")\\n    return y_pred'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_outlier_detection_bench.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================================\\nEvaluation of outlier detection estimators\\n==========================================\\n\\nThis example compares two outlier detection algorithms, namely\\n:ref:`local_outlier_factor` (LOF) and :ref:`isolation_forest` (IForest), on\\nreal-world datasets available in :class:`sklearn.datasets`. The goal is to show\\nthat different algorithms perform well on different datasets and contrast their\\ntraining speed and sensitivity to hyperparameters.\\n\\nThe algorithms are trained (without labels) on the whole dataset assumed to\\ncontain outliers.\\n\\n1. The ROC curves are computed using knowledge of the ground-truth labels\\nand displayed using :class:`~sklearn.metrics.RocCurveDisplay`.\\n\\n2. The performance is assessed in terms of the ROC-AUC.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Dataset preprocessing and model training\\n# ========================================\\n#\\n# Different outlier detection models require different preprocessing. In the\\n# presence of categorical variables,\\n# :class:`~sklearn.preprocessing.OrdinalEncoder` is often a good strategy for\\n# tree-based models such as :class:`~sklearn.ensemble.IsolationForest`, whereas\\n# neighbors-based models such as :class:`~sklearn.neighbors.LocalOutlierFactor`\\n# would be impacted by the ordering induced by ordinal encoding. To avoid\\n# inducing an ordering, on should rather use\\n# :class:`~sklearn.preprocessing.OneHotEncoder`.\\n#\\n# Neighbors-based models may also require scaling of the numerical features (see\\n# for instance :ref:`neighbors_scaling`). In the presence of outliers, a good\\n# option is to use a :class:`~sklearn.preprocessing.RobustScaler`.\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.ensemble import IsolationForest\\nfrom sklearn.neighbors import LocalOutlierFactor\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import (\\n    OneHotEncoder,\\n    OrdinalEncoder,\\n    RobustScaler,\\n)\\n\\n\\n# Code for: def make_estimator(name, categorical_columns=None, iforest_kw=None, lof_kw=None):\\n\\n\\n# %%\\n# The following `fit_predict` function returns the average outlier score of X.\\n\\nfrom time import perf_counter\\n\\n\\n# Code for: def fit_predict(estimator, X):'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_outlier_detection_bench.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def make_estimator(name, categorical_columns=None, iforest_kw=None, lof_kw=None):\\n\\n\\n# %%\\n# The following `fit_predict` function returns the average outlier score of X.\\n\\nfrom time import perf_counter\\n\\n\\n# Code for: def fit_predict(estimator, X):\\n\\n\\n# %%\\n# On the rest of the example we process one dataset per section. After loading\\n# the data, the targets are modified to consist of two classes: 0 representing\\n# inliers and 1 representing outliers. Due to computational constraints of the\\n# scikit-learn documentation, the sample size of some datasets is reduced using\\n# a stratified :class:`~sklearn.model_selection.train_test_split`.\\n#\\n# Furthermore, we set `n_neighbors` to match the expected number of anomalies\\n# `expected_n_anomalies = n_samples * expected_anomaly_fraction`. This is a good\\n# heuristic as long as the proportion of outliers is not very low, the reason\\n# being that `n_neighbors` should be at least greater than the number of samples\\n# in the less populated cluster (see\\n# :ref:`sphx_glr_auto_examples_neighbors_plot_lof_outlier_detection.py`).\\n#\\n# KDDCup99 - SA dataset\\n# ---------------------\\n#\\n# The :ref:`kddcup99_dataset` was generated using a closed network and\\n# hand-injected attacks. The SA dataset is a subset of it obtained by simply\\n# selecting all the normal data and an anomaly proportion of around 3%.\\n\\n# %%\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_kddcup99\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = fetch_kddcup99(\\n    subset=\"SA\", percent10=True, random_state=42, return_X_y=True, as_frame=True\\n)\\ny = (y != b\"normal.\").astype(np.int32)\\nX, _, y, _ = train_test_split(X, y, train_size=0.1, stratify=y, random_state=42)\\n\\nn_samples, anomaly_frac = X.shape[0], y.mean()\\nprint(f\"{n_samples} datapoints with {y.sum()} anomalies ({anomaly_frac:.02%})\")\\n\\n# %%\\n# The SA dataset contains 41 features out of which 3 are categorical:\\n# \"protocol_type\", \"service\" and \"flag\".\\n\\n# %%\\ny_true = {}\\ny_pred = {\"LOF\": {}, \"IForest\": {}}\\nmodel_names = [\"LOF\", \"IForest\"]\\ncat_columns = [\"protocol_type\", \"service\", \"flag\"]\\n\\ny_true[\"KDDCup99 - SA\"] = y\\nfor model_name in model_names:\\n    model = make_estimator(\\n        name=model_name,\\n        categorical_columns=cat_columns,\\n        lof_kw={\"n_neighbors\": int(n_samples * anomaly_frac)},\\n        iforest_kw={\"random_state\": 42},\\n    )\\n    y_pred[model_name][\"KDDCup99 - SA\"] = fit_predict(model, X)\\n\\n# %%\\n# Forest covertypes dataset\\n# -------------------------\\n#\\n# The :ref:`covtype_dataset` is a multiclass dataset where the target is the\\n# dominant species of tree in a given patch of forest. It contains 54 features,\\n# some of which (\"Wilderness_Area\" and \"Soil_Type\") are already binary encoded.\\n# Though originally meant as a classification task, one can regard inliers as\\n# samples encoded with label 2 and outliers as those with label 4.\\n\\n# %%\\nfrom sklearn.datasets import fetch_covtype'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_outlier_detection_bench.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\nfrom sklearn.datasets import fetch_covtype\\n\\nX, y = fetch_covtype(return_X_y=True, as_frame=True)\\ns = (y == 2) + (y == 4)\\nX = X.loc[s]\\ny = y.loc[s]\\ny = (y != 2).astype(np.int32)\\n\\nX, _, y, _ = train_test_split(X, y, train_size=0.05, stratify=y, random_state=42)\\nX_forestcover = X  # save X for later use\\n\\nn_samples, anomaly_frac = X.shape[0], y.mean()\\nprint(f\"{n_samples} datapoints with {y.sum()} anomalies ({anomaly_frac:.02%})\")\\n\\n# %%\\ny_true[\"forestcover\"] = y\\nfor model_name in model_names:\\n    model = make_estimator(\\n        name=model_name,\\n        lof_kw={\"n_neighbors\": int(n_samples * anomaly_frac)},\\n        iforest_kw={\"random_state\": 42},\\n    )\\n    y_pred[model_name][\"forestcover\"] = fit_predict(model, X)\\n\\n# %%\\n# Ames Housing dataset\\n# --------------------\\n#\\n# The `Ames housing dataset <http://www.openml.org/d/43926>`_ is originally a\\n# regression dataset where the target are sales prices of houses in Ames, Iowa.\\n# Here we convert it into an outlier detection problem by regarding houses with\\n# price over 70 USD/sqft. To make the problem easier, we drop intermediate\\n# prices between 40 and 70 USD/sqft.\\n\\n# %%\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import fetch_openml\\n\\nX, y = fetch_openml(name=\"ames_housing\", version=1, return_X_y=True, as_frame=True)\\ny = y.div(X[\"Lot_Area\"])\\n\\n# None values in pandas 1.5.1 were mapped to np.nan in pandas 2.0.1\\nX[\"Misc_Feature\"] = X[\"Misc_Feature\"].cat.add_categories(\"NoInfo\").fillna(\"NoInfo\")\\nX[\"Mas_Vnr_Type\"] = X[\"Mas_Vnr_Type\"].cat.add_categories(\"NoInfo\").fillna(\"NoInfo\")\\n\\nX.drop(columns=\"Lot_Area\", inplace=True)\\nmask = (y < 40) | (y > 70)\\nX = X.loc[mask]\\ny = y.loc[mask]\\ny.hist(bins=20, edgecolor=\"black\")\\nplt.xlabel(\"House price in USD/sqft\")\\n_ = plt.title(\"Distribution of house prices in Ames\")\\n\\n# %%\\ny = (y > 70).astype(np.int32)\\n\\nn_samples, anomaly_frac = X.shape[0], y.mean()\\nprint(f\"{n_samples} datapoints with {y.sum()} anomalies ({anomaly_frac:.02%})\")\\n\\n# %%\\n# The dataset contains 46 categorical features. In this case it is easier use a\\n# :class:`~sklearn.compose.make_column_selector` to find them instead of passing\\n# a list made by hand.\\n\\n# %%\\nfrom sklearn.compose import make_column_selector as selector\\n\\ncategorical_columns_selector = selector(dtype_include=\"category\")\\ncat_columns = categorical_columns_selector(X)\\n\\ny_true[\"ames_housing\"] = y\\nfor model_name in model_names:\\n    model = make_estimator(\\n        name=model_name,\\n        categorical_columns=cat_columns,\\n        lof_kw={\"n_neighbors\": int(n_samples * anomaly_frac)},\\n        iforest_kw={\"random_state\": 42},\\n    )\\n    y_pred[model_name][\"ames_housing\"] = fit_predict(model, X)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_outlier_detection_bench.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Cardiotocography dataset\\n# ------------------------\\n#\\n# The `Cardiotocography dataset <http://www.openml.org/d/1466>`_ is a multiclass\\n# dataset of fetal cardiotocograms, the classes being the fetal heart rate (FHR)\\n# pattern encoded with labels from 1 to 10. Here we set class 3 (the minority\\n# class) to represent the outliers. It contains 30 numerical features, some of\\n# which are binary encoded and some are continuous.\\n\\n# %%\\nX, y = fetch_openml(name=\"cardiotocography\", version=1, return_X_y=True, as_frame=False)\\nX_cardiotocography = X  # save X for later use\\ns = y == \"3\"\\ny = s.astype(np.int32)\\n\\nn_samples, anomaly_frac = X.shape[0], y.mean()\\nprint(f\"{n_samples} datapoints with {y.sum()} anomalies ({anomaly_frac:.02%})\")\\n\\n# %%\\ny_true[\"cardiotocography\"] = y\\nfor model_name in model_names:\\n    model = make_estimator(\\n        name=model_name,\\n        lof_kw={\"n_neighbors\": int(n_samples * anomaly_frac)},\\n        iforest_kw={\"random_state\": 42},\\n    )\\n    y_pred[model_name][\"cardiotocography\"] = fit_predict(model, X)\\n\\n# %%\\n# Plot and interpret results\\n# ==========================\\n#\\n# The algorithm performance relates to how good the true positive rate (TPR) is\\n# at low value of the false positive rate (FPR). The best algorithms have the\\n# curve on the top-left of the plot and the area under curve (AUC) close to 1.\\n# The diagonal dashed line represents a random classification of outliers and\\n# inliers.\\n\\n# %%\\nimport math\\n\\nfrom sklearn.metrics import RocCurveDisplay\\n\\ncols = 2\\npos_label = 0  # mean 0 belongs to positive class\\ndatasets_names = y_true.keys()\\nrows = math.ceil(len(datasets_names) / cols)\\n\\nfig, axs = plt.subplots(nrows=rows, ncols=cols, squeeze=False, figsize=(10, rows * 4))\\n\\nfor ax, dataset_name in zip(axs.ravel(), datasets_names):\\n    for model_idx, model_name in enumerate(model_names):\\n        display = RocCurveDisplay.from_predictions(\\n            y_true[dataset_name],\\n            y_pred[model_name][dataset_name],\\n            pos_label=pos_label,\\n            name=model_name,\\n            ax=ax,\\n            plot_chance_level=(model_idx == len(model_names) - 1),\\n            chance_level_kw={\"linestyle\": \":\"},\\n        )\\n    ax.set_title(dataset_name)\\n_ = plt.tight_layout(pad=2.0)  # spacing between subplots'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_outlier_detection_bench.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We observe that once the number of neighbors is tuned, LOF and IForest perform\\n# similarly in terms of ROC AUC for the forestcover and cardiotocography\\n# datasets. The score for IForest is slightly better for the SA dataset and LOF\\n# performs considerably better on the Ames housing dataset than IForest.\\n#\\n# Recall however that Isolation Forest tends to train much faster than LOF on\\n# datasets with a large number of samples. LOF needs to compute pairwise\\n# distances to find nearest neighbors, which has a quadratic complexity with respect\\n# to the number of observations. This can make this method prohibitive on large\\n# datasets.\\n#\\n# Ablation study\\n# ==============\\n#\\n# In this section we explore the impact of the hyperparameter `n_neighbors` and\\n# the choice of scaling the numerical variables on the LOF model. Here we use\\n# the :ref:`covtype_dataset` dataset as the binary encoded categories introduce\\n# a natural scale of euclidean distances between 0 and 1. We then want a scaling\\n# method to avoid granting a privilege to non-binary features and that is robust\\n# enough to outliers so that the task of finding them does not become too\\n# difficult.\\n\\n# %%\\nX = X_forestcover\\ny = y_true[\"forestcover\"]\\n\\nn_samples = X.shape[0]\\nn_neighbors_list = (n_samples * np.array([0.2, 0.02, 0.01, 0.001])).astype(np.int32)\\nmodel = make_pipeline(RobustScaler(), LocalOutlierFactor())\\n\\nlinestyles = [\"solid\", \"dashed\", \"dashdot\", \":\", (5, (10, 3))]\\n\\nfig, ax = plt.subplots()\\nfor model_idx, (linestyle, n_neighbors) in enumerate(zip(linestyles, n_neighbors_list)):\\n    model.set_params(localoutlierfactor__n_neighbors=n_neighbors)\\n    model.fit(X)\\n    y_pred = model[-1].negative_outlier_factor_\\n    display = RocCurveDisplay.from_predictions(\\n        y,\\n        y_pred,\\n        pos_label=pos_label,\\n        name=f\"n_neighbors = {n_neighbors}\",\\n        ax=ax,\\n        plot_chance_level=(model_idx == len(n_neighbors_list) - 1),\\n        chance_level_kw={\"linestyle\": (0, (1, 10))},\\n        linestyle=linestyle,\\n        linewidth=2,\\n    )\\n_ = ax.set_title(\"RobustScaler with varying n_neighbors\\\\non forestcover dataset\")\\n\\n# %%\\n# We observe that the number of neighbors has a big impact on the performance of\\n# the model. If one has access to (at least some) ground truth labels, it is\\n# then important to tune `n_neighbors` accordingly. A convenient way to do so is\\n# to explore values for `n_neighbors` of the order of magnitud of the expected\\n# contamination.\\n\\n# %%\\nfrom sklearn.preprocessing import MinMaxScaler, SplineTransformer, StandardScaler\\n\\npreprocessor_list = [\\n    None,\\n    RobustScaler(),\\n    StandardScaler(),\\n    MinMaxScaler(),\\n    SplineTransformer(),\\n]\\nexpected_anomaly_fraction = 0.02\\nlof = LocalOutlierFactor(n_neighbors=int(n_samples * expected_anomaly_fraction))'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_outlier_detection_bench.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='preprocessor_list = [\\n    None,\\n    RobustScaler(),\\n    StandardScaler(),\\n    MinMaxScaler(),\\n    SplineTransformer(),\\n]\\nexpected_anomaly_fraction = 0.02\\nlof = LocalOutlierFactor(n_neighbors=int(n_samples * expected_anomaly_fraction))\\n\\nfig, ax = plt.subplots()\\nfor model_idx, (linestyle, preprocessor) in enumerate(\\n    zip(linestyles, preprocessor_list)\\n):\\n    model = make_pipeline(preprocessor, lof)\\n    model.fit(X)\\n    y_pred = model[-1].negative_outlier_factor_\\n    display = RocCurveDisplay.from_predictions(\\n        y,\\n        y_pred,\\n        pos_label=pos_label,\\n        name=str(preprocessor).split(\"(\")[0],\\n        ax=ax,\\n        plot_chance_level=(model_idx == len(preprocessor_list) - 1),\\n        chance_level_kw={\"linestyle\": (0, (1, 10))},\\n        linestyle=linestyle,\\n        linewidth=2,\\n    )\\n_ = ax.set_title(\"Fixed n_neighbors with varying preprocessing\\\\non forestcover dataset\")\\n\\n# %%\\n# On the one hand, :class:`~sklearn.preprocessing.RobustScaler` scales each\\n# feature independently by using the interquartile range (IQR) by default, which\\n# is the range between the 25th and 75th percentiles of the data. It centers the\\n# data by subtracting the median and then scale it by dividing by the IQR. The\\n# IQR is robust to outliers: the median and interquartile range are less\\n# affected by extreme values than the range, the mean and the standard\\n# deviation. Furthermore, :class:`~sklearn.preprocessing.RobustScaler` does not\\n# squash marginal outlier values, contrary to\\n# :class:`~sklearn.preprocessing.StandardScaler`.\\n#\\n# On the other hand, :class:`~sklearn.preprocessing.MinMaxScaler` scales each\\n# feature individually such that its range maps into the range between zero and\\n# one. If there are outliers in the data, they can skew it towards either the\\n# minimum or maximum values, leading to a completely different distribution of\\n# data with large marginal outliers: all non-outlier values can be collapsed\\n# almost together as a result.\\n#\\n# We also evaluated no preprocessing at all (by passing `None` to the pipeline),\\n# :class:`~sklearn.preprocessing.StandardScaler` and\\n# :class:`~sklearn.preprocessing.SplineTransformer`. Please refer to their\\n# respective documentation for more details.\\n#\\n# Note that the optimal preprocessing depends on the dataset, as shown below:\\n\\n# %%\\nX = X_cardiotocography\\ny = y_true[\"cardiotocography\"]\\n\\nn_samples, expected_anomaly_fraction = X.shape[0], 0.025\\nlof = LocalOutlierFactor(n_neighbors=int(n_samples * expected_anomaly_fraction))'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_outlier_detection_bench.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\nX = X_cardiotocography\\ny = y_true[\"cardiotocography\"]\\n\\nn_samples, expected_anomaly_fraction = X.shape[0], 0.025\\nlof = LocalOutlierFactor(n_neighbors=int(n_samples * expected_anomaly_fraction))\\n\\nfig, ax = plt.subplots()\\nfor model_idx, (linestyle, preprocessor) in enumerate(\\n    zip(linestyles, preprocessor_list)\\n):\\n    model = make_pipeline(preprocessor, lof)\\n    model.fit(X)\\n    y_pred = model[-1].negative_outlier_factor_\\n    display = RocCurveDisplay.from_predictions(\\n        y,\\n        y_pred,\\n        pos_label=pos_label,\\n        name=str(preprocessor).split(\"(\")[0],\\n        ax=ax,\\n        plot_chance_level=(model_idx == len(preprocessor_list) - 1),\\n        chance_level_kw={\"linestyle\": (0, (1, 10))},\\n        linestyle=linestyle,\\n        linewidth=2,\\n    )\\nax.set_title(\\n    \"Fixed n_neighbors with varying preprocessing\\\\non cardiotocography dataset\"\\n)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_kernel_ridge_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================\\nComparison of kernel ridge regression and SVR\\n=============================================\\n\\nBoth kernel ridge regression (KRR) and SVR learn a non-linear function by\\nemploying the kernel trick, i.e., they learn a linear function in the space\\ninduced by the respective kernel which corresponds to a non-linear function in\\nthe original space. They differ in the loss functions (ridge versus\\nepsilon-insensitive loss). In contrast to SVR, fitting a KRR can be done in\\nclosed-form and is typically faster for medium-sized datasets. On the other\\nhand, the learned model is non-sparse and thus slower than SVR at\\nprediction-time.\\n\\nThis example illustrates both methods on an artificial dataset, which\\nconsists of a sinusoidal target function and strong noise added to every fifth\\ndatapoint.\\n\\n\"\"\"\\n\\n# %%\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate sample data\\n# --------------------\\nimport numpy as np\\n\\nrng = np.random.RandomState(42)\\n\\nX = 5 * rng.rand(10000, 1)\\ny = np.sin(X).ravel()\\n\\n# Add noise to targets\\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\\n\\nX_plot = np.linspace(0, 5, 100000)[:, None]\\n\\n# %%\\n# Construct the kernel-based regression models\\n# --------------------------------------------\\n\\nfrom sklearn.kernel_ridge import KernelRidge\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.svm import SVR\\n\\ntrain_size = 100\\n\\nsvr = GridSearchCV(\\n    SVR(kernel=\"rbf\", gamma=0.1),\\n    param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)},\\n)\\n\\nkr = GridSearchCV(\\n    KernelRidge(kernel=\"rbf\", gamma=0.1),\\n    param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3], \"gamma\": np.logspace(-2, 2, 5)},\\n)\\n\\n# %%\\n# Compare times of SVR and Kernel Ridge Regression\\n# ------------------------------------------------\\n\\nimport time\\n\\nt0 = time.time()\\nsvr.fit(X[:train_size], y[:train_size])\\nsvr_fit = time.time() - t0\\nprint(f\"Best SVR with params: {svr.best_params_} and R2 score: {svr.best_score_:.3f}\")\\nprint(\"SVR complexity and bandwidth selected and model fitted in %.3f s\" % svr_fit)\\n\\nt0 = time.time()\\nkr.fit(X[:train_size], y[:train_size])\\nkr_fit = time.time() - t0\\nprint(f\"Best KRR with params: {kr.best_params_} and R2 score: {kr.best_score_:.3f}\")\\nprint(\"KRR complexity and bandwidth selected and model fitted in %.3f s\" % kr_fit)\\n\\nsv_ratio = svr.best_estimator_.support_.shape[0] / train_size\\nprint(\"Support vector ratio: %.3f\" % sv_ratio)\\n\\nt0 = time.time()\\ny_svr = svr.predict(X_plot)\\nsvr_predict = time.time() - t0\\nprint(\"SVR prediction for %d inputs in %.3f s\" % (X_plot.shape[0], svr_predict))\\n\\nt0 = time.time()\\ny_kr = kr.predict(X_plot)\\nkr_predict = time.time() - t0\\nprint(\"KRR prediction for %d inputs in %.3f s\" % (X_plot.shape[0], kr_predict))\\n\\n# %%\\n# Look at the results\\n# -------------------\\n\\nimport matplotlib.pyplot as plt'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_kernel_ridge_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='t0 = time.time()\\ny_kr = kr.predict(X_plot)\\nkr_predict = time.time() - t0\\nprint(\"KRR prediction for %d inputs in %.3f s\" % (X_plot.shape[0], kr_predict))\\n\\n# %%\\n# Look at the results\\n# -------------------\\n\\nimport matplotlib.pyplot as plt\\n\\nsv_ind = svr.best_estimator_.support_\\nplt.scatter(\\n    X[sv_ind],\\n    y[sv_ind],\\n    c=\"r\",\\n    s=50,\\n    label=\"SVR support vectors\",\\n    zorder=2,\\n    edgecolors=(0, 0, 0),\\n)\\nplt.scatter(X[:100], y[:100], c=\"k\", label=\"data\", zorder=1, edgecolors=(0, 0, 0))\\nplt.plot(\\n    X_plot,\\n    y_svr,\\n    c=\"r\",\\n    label=\"SVR (fit: %.3fs, predict: %.3fs)\" % (svr_fit, svr_predict),\\n)\\nplt.plot(\\n    X_plot, y_kr, c=\"g\", label=\"KRR (fit: %.3fs, predict: %.3fs)\" % (kr_fit, kr_predict)\\n)\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\nplt.title(\"SVR versus Kernel Ridge\")\\n_ = plt.legend()\\n\\n# %%\\n# The previous figure compares the learned model of KRR and SVR when both\\n# complexity/regularization and bandwidth of the RBF kernel are optimized using\\n# grid-search. The learned functions are very similar; however, fitting KRR is\\n# approximately 3-4 times faster than fitting SVR (both with grid-search).\\n#\\n# Prediction of 100000 target values could be in theory approximately three\\n# times faster with SVR since it has learned a sparse model using only\\n# approximately 1/3 of the training datapoints as support vectors. However, in\\n# practice, this is not necessarily the case because of implementation details\\n# in the way the kernel function is computed for each model that can make the\\n# KRR model as fast or even faster despite computing more arithmetic\\n# operations.\\n\\n# %%\\n# Visualize training and prediction times\\n# ---------------------------------------\\n\\nplt.figure()\\n\\nsizes = np.logspace(1, 3.8, 7).astype(int)\\nfor name, estimator in {\\n    \"KRR\": KernelRidge(kernel=\"rbf\", alpha=0.01, gamma=10),\\n    \"SVR\": SVR(kernel=\"rbf\", C=1e2, gamma=10),\\n}.items():\\n    train_time = []\\n    test_time = []\\n    for train_test_size in sizes:\\n        t0 = time.time()\\n        estimator.fit(X[:train_test_size], y[:train_test_size])\\n        train_time.append(time.time() - t0)\\n\\n        t0 = time.time()\\n        estimator.predict(X_plot[:1000])\\n        test_time.append(time.time() - t0)\\n\\n    plt.plot(\\n        sizes,\\n        train_time,\\n        \"o-\",\\n        color=\"r\" if name == \"SVR\" else \"g\",\\n        label=\"%s (train)\" % name,\\n    )\\n    plt.plot(\\n        sizes,\\n        test_time,\\n        \"o--\",\\n        color=\"r\" if name == \"SVR\" else \"g\",\\n        label=\"%s (test)\" % name,\\n    )\\n\\nplt.xscale(\"log\")\\nplt.yscale(\"log\")\\nplt.xlabel(\"Train size\")\\nplt.ylabel(\"Time (seconds)\")\\nplt.title(\"Execution Time\")\\n_ = plt.legend(loc=\"best\")'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_kernel_ridge_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.xscale(\"log\")\\nplt.yscale(\"log\")\\nplt.xlabel(\"Train size\")\\nplt.ylabel(\"Time (seconds)\")\\nplt.title(\"Execution Time\")\\n_ = plt.legend(loc=\"best\")\\n\\n# %%\\n# This figure compares the time for fitting and prediction of KRR and SVR for\\n# different sizes of the training set. Fitting KRR is faster than SVR for\\n# medium-sized training sets (less than a few thousand samples); however, for\\n# larger training sets SVR scales better. With regard to prediction time, SVR\\n# should be faster than KRR for all sizes of the training set because of the\\n# learned sparse solution, however this is not necessarily the case in practice\\n# because of implementation details. Note that the degree of sparsity and thus\\n# the prediction time depends on the parameters epsilon and C of the SVR.\\n\\n# %%\\n# Visualize the learning curves\\n# -----------------------------\\nfrom sklearn.model_selection import LearningCurveDisplay\\n\\n_, ax = plt.subplots()\\n\\nsvr = SVR(kernel=\"rbf\", C=1e1, gamma=0.1)\\nkr = KernelRidge(kernel=\"rbf\", alpha=0.1, gamma=0.1)\\n\\ncommon_params = {\\n    \"X\": X[:100],\\n    \"y\": y[:100],\\n    \"train_sizes\": np.linspace(0.1, 1, 10),\\n    \"scoring\": \"neg_mean_squared_error\",\\n    \"negate_score\": True,\\n    \"score_name\": \"Mean Squared Error\",\\n    \"score_type\": \"test\",\\n    \"std_display_style\": None,\\n    \"ax\": ax,\\n}\\n\\nLearningCurveDisplay.from_estimator(svr, **common_params)\\nLearningCurveDisplay.from_estimator(kr, **common_params)\\nax.set_title(\"Learning curves\")\\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_set_output.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================\\nIntroducing the `set_output` API\\n================================\\n\\n.. currentmodule:: sklearn\\n\\nThis example will demonstrate the `set_output` API to configure transformers to\\noutput pandas DataFrames. `set_output` can be configured per estimator by calling\\nthe `set_output` method or globally by setting `set_config(transform_output=\"pandas\")`.\\nFor details, see\\n`SLEP018 <https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep018/proposal.html>`__.\\n\"\"\"  # noqa\\n\\n# %%\\n# First, we load the iris dataset as a DataFrame to demonstrate the `set_output` API.\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_iris(as_frame=True, return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\nX_train.head()\\n\\n# %%\\n# To configure an estimator such as :class:`preprocessing.StandardScaler` to return\\n# DataFrames, call `set_output`. This feature requires pandas to be installed.\\n\\nfrom sklearn.preprocessing import StandardScaler\\n\\nscaler = StandardScaler().set_output(transform=\"pandas\")\\n\\nscaler.fit(X_train)\\nX_test_scaled = scaler.transform(X_test)\\nX_test_scaled.head()\\n\\n# %%\\n# `set_output` can be called after `fit` to configure `transform` after the fact.\\nscaler2 = StandardScaler()\\n\\nscaler2.fit(X_train)\\nX_test_np = scaler2.transform(X_test)\\nprint(f\"Default output type: {type(X_test_np).__name__}\")\\n\\nscaler2.set_output(transform=\"pandas\")\\nX_test_df = scaler2.transform(X_test)\\nprint(f\"Configured pandas output type: {type(X_test_df).__name__}\")\\n\\n# %%\\n# In a :class:`pipeline.Pipeline`, `set_output` configures all steps to output\\n# DataFrames.\\nfrom sklearn.feature_selection import SelectPercentile\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import make_pipeline\\n\\nclf = make_pipeline(\\n    StandardScaler(), SelectPercentile(percentile=75), LogisticRegression()\\n)\\nclf.set_output(transform=\"pandas\")\\nclf.fit(X_train, y_train)\\n\\n# %%\\n# Each transformer in the pipeline is configured to return DataFrames. This\\n# means that the final logistic regression step contains the feature names of the input.\\nclf[-1].feature_names_in_\\n\\n# %%\\n# .. note:: If one uses the method `set_params`, the transformer will be\\n#    replaced by a new one with the default output format.\\nclf.set_params(standardscaler=StandardScaler())\\nclf.fit(X_train, y_train)\\nclf[-1].feature_names_in_\\n\\n# %%\\n# To keep the intended behavior, use `set_output` on the new transformer\\n# beforehand\\nscaler = StandardScaler().set_output(transform=\"pandas\")\\nclf.set_params(standardscaler=scaler)\\nclf.fit(X_train, y_train)\\nclf[-1].feature_names_in_\\n\\n# %%\\n# Next we load the titanic dataset to demonstrate `set_output` with\\n# :class:`compose.ColumnTransformer` and heterogeneous data.\\nfrom sklearn.datasets import fetch_openml\\n\\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_set_output.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\\n\\n# %%\\n# The `set_output` API can be configured globally by using :func:`set_config` and\\n# setting `transform_output` to `\"pandas\"`.\\nfrom sklearn import set_config\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\nset_config(transform_output=\"pandas\")\\n\\nnum_pipe = make_pipeline(SimpleImputer(), StandardScaler())\\nnum_cols = [\"age\", \"fare\"]\\nct = ColumnTransformer(\\n    (\\n        (\"numerical\", num_pipe, num_cols),\\n        (\\n            \"categorical\",\\n            OneHotEncoder(\\n                sparse_output=False, drop=\"if_binary\", handle_unknown=\"ignore\"\\n            ),\\n            [\"embarked\", \"sex\", \"pclass\"],\\n        ),\\n    ),\\n    verbose_feature_names_out=False,\\n)\\nclf = make_pipeline(ct, SelectPercentile(percentile=50), LogisticRegression())\\nclf.fit(X_train, y_train)\\nclf.score(X_test, y_test)\\n\\n# %%\\n# With the global configuration, all transformers output DataFrames. This allows us to\\n# easily plot the logistic regression coefficients with the corresponding feature names.\\nimport pandas as pd\\n\\nlog_reg = clf[-1]\\ncoef = pd.Series(log_reg.coef_.ravel(), index=log_reg.feature_names_in_)\\n_ = coef.sort_values().plot.barh()\\n\\n# %%\\n# In order to demonstrate the :func:`config_context` functionality below, let\\n# us first reset `transform_output` to its default value.\\nset_config(transform_output=\"default\")\\n\\n# %%\\n# When configuring the output type with :func:`config_context` the\\n# configuration at the time when `transform` or `fit_transform` are\\n# called is what counts. Setting these only when you construct or fit\\n# the transformer has no effect.\\nfrom sklearn import config_context\\n\\nscaler = StandardScaler()\\nscaler.fit(X_train[num_cols])\\n\\n# %%\\nwith config_context(transform_output=\"pandas\"):\\n    # the output of transform will be a Pandas DataFrame\\n    X_test_scaled = scaler.transform(X_test[num_cols])\\nX_test_scaled.head()\\n\\n# %%\\n# outside of the context manager, the output will be a NumPy array\\nX_test_scaled = scaler.transform(X_test[num_cols])\\nX_test_scaled[:5]'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_isotonic_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================\\nIsotonic Regression\\n===================\\n\\nAn illustration of the isotonic regression on generated data (non-linear\\nmonotonic trend with homoscedastic uniform noise).\\n\\nThe isotonic regression algorithm finds a non-decreasing approximation of a\\nfunction while minimizing the mean squared error on the training data. The\\nbenefit of such a non-parametric model is that it does not assume any shape for\\nthe target function besides monotonicity. For comparison a linear regression is\\nalso presented.\\n\\nThe plot on the right-hand side shows the model prediction function that\\nresults from the linear interpolation of thresholds points. The thresholds\\npoints are a subset of the training input observations and their matching\\ntarget values are computed by the isotonic non-parametric fit.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib.collections import LineCollection\\n\\nfrom sklearn.isotonic import IsotonicRegression\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.utils import check_random_state\\n\\nn = 100\\nx = np.arange(n)\\nrs = check_random_state(0)\\ny = rs.randint(-50, 50, size=(n,)) + 50.0 * np.log1p(np.arange(n))\\n\\n# %%\\n# Fit IsotonicRegression and LinearRegression models:\\n\\nir = IsotonicRegression(out_of_bounds=\"clip\")\\ny_ = ir.fit_transform(x, y)\\n\\nlr = LinearRegression()\\nlr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\\n\\n# %%\\n# Plot results:\\n\\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\\nlc = LineCollection(segments, zorder=0)\\nlc.set_array(np.ones(len(y)))\\nlc.set_linewidths(np.full(n, 0.5))\\n\\nfig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(12, 6))\\n\\nax0.plot(x, y, \"C0.\", markersize=12)\\nax0.plot(x, y_, \"C1.-\", markersize=12)\\nax0.plot(x, lr.predict(x[:, np.newaxis]), \"C2-\")\\nax0.add_collection(lc)\\nax0.legend((\"Training data\", \"Isotonic fit\", \"Linear fit\"), loc=\"lower right\")\\nax0.set_title(\"Isotonic regression fit on noisy data (n=%d)\" % n)\\n\\nx_test = np.linspace(-10, 110, 1000)\\nax1.plot(x_test, ir.predict(x_test), \"C1-\")\\nax1.plot(ir.X_thresholds_, ir.y_thresholds_, \"C1.\", markersize=12)\\nax1.set_title(\"Prediction function (%d thresholds)\" % len(ir.X_thresholds_))\\n\\nplt.show()\\n\\n# %%\\n# Note that we explicitly passed `out_of_bounds=\"clip\"` to the constructor of\\n# `IsotonicRegression` to control the way the model extrapolates outside of the\\n# range of data observed in the training set. This \"clipping\" extrapolation can\\n# be seen on the plot of the decision function on the right-hand.'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_pipeline_display.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================================\\nDisplaying Pipelines\\n=================================================================\\n\\nThe default configuration for displaying a pipeline in a Jupyter Notebook is\\n`\\'diagram\\'` where `set_config(display=\\'diagram\\')`. To deactivate HTML representation,\\nuse `set_config(display=\\'text\\')`.\\n\\nTo see more detailed steps in the visualization of the pipeline, click on the\\nsteps in the pipeline.\\n\"\"\"\\n\\n# %%\\n# Displaying a Pipeline with a Preprocessing Step and Classifier\\n################################################################################\\n# This section constructs a :class:`~sklearn.pipeline.Pipeline` with a preprocessing\\n# step, :class:`~sklearn.preprocessing.StandardScaler`, and classifier,\\n# :class:`~sklearn.linear_model.LogisticRegression`, and displays its visual\\n# representation.\\n\\nfrom sklearn import set_config\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nsteps = [\\n    (\"preprocessing\", StandardScaler()),\\n    (\"classifier\", LogisticRegression()),\\n]\\npipe = Pipeline(steps)\\n\\n# %%\\n# To visualize the diagram, the default is `display=\\'diagram\\'`.\\nset_config(display=\"diagram\")\\npipe  # click on the diagram below to see the details of each step\\n\\n# %%\\n# To view the text pipeline, change to `display=\\'text\\'`.\\nset_config(display=\"text\")\\npipe\\n\\n# %%\\n# Put back the default display\\nset_config(display=\"diagram\")\\n\\n# %%\\n# Displaying a Pipeline Chaining Multiple Preprocessing Steps & Classifier\\n################################################################################\\n# This section constructs a :class:`~sklearn.pipeline.Pipeline` with multiple\\n# preprocessing steps, :class:`~sklearn.preprocessing.PolynomialFeatures` and\\n# :class:`~sklearn.preprocessing.StandardScaler`, and a classifier step,\\n# :class:`~sklearn.linear_model.LogisticRegression`, and displays its visual\\n# representation.\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\\n\\nsteps = [\\n    (\"standard_scaler\", StandardScaler()),\\n    (\"polynomial\", PolynomialFeatures(degree=3)),\\n    (\"classifier\", LogisticRegression(C=2.0)),\\n]\\npipe = Pipeline(steps)\\npipe  # click on the diagram below to see the details of each step\\n\\n# %%\\n# Displaying a Pipeline and Dimensionality Reduction and Classifier\\n################################################################################\\n# This section constructs a :class:`~sklearn.pipeline.Pipeline` with a\\n# dimensionality reduction step, :class:`~sklearn.decomposition.PCA`,\\n# a classifier, :class:`~sklearn.svm.SVC`, and displays its visual\\n# representation.\\n\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.svm import SVC'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_pipeline_display.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.decomposition import PCA\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.svm import SVC\\n\\nsteps = [(\"reduce_dim\", PCA(n_components=4)), (\"classifier\", SVC(kernel=\"linear\"))]\\npipe = Pipeline(steps)\\npipe  # click on the diagram below to see the details of each step\\n\\n# %%\\n# Displaying a Complex Pipeline Chaining a Column Transformer\\n################################################################################\\n# This section constructs a complex :class:`~sklearn.pipeline.Pipeline` with a\\n# :class:`~sklearn.compose.ColumnTransformer` and a classifier,\\n# :class:`~sklearn.linear_model.LogisticRegression`, and displays its visual\\n# representation.\\n\\nimport numpy as np\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import Pipeline, make_pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\nnumeric_preprocessor = Pipeline(\\n    steps=[\\n        (\"imputation_mean\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\\n        (\"scaler\", StandardScaler()),\\n    ]\\n)\\n\\ncategorical_preprocessor = Pipeline(\\n    steps=[\\n        (\\n            \"imputation_constant\",\\n            SimpleImputer(fill_value=\"missing\", strategy=\"constant\"),\\n        ),\\n        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\\n    ]\\n)\\n\\npreprocessor = ColumnTransformer(\\n    [\\n        (\"categorical\", categorical_preprocessor, [\"state\", \"gender\"]),\\n        (\"numerical\", numeric_preprocessor, [\"age\", \"weight\"]),\\n    ]\\n)\\n\\npipe = make_pipeline(preprocessor, LogisticRegression(max_iter=500))\\npipe  # click on the diagram below to see the details of each step\\n\\n# %%\\n# Displaying a Grid Search over a Pipeline with a Classifier\\n################################################################################\\n# This section constructs a :class:`~sklearn.model_selection.GridSearchCV`\\n# over a :class:`~sklearn.pipeline.Pipeline` with\\n# :class:`~sklearn.ensemble.RandomForestClassifier` and displays its visual\\n# representation.\\n\\nimport numpy as np\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline, make_pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\nnumeric_preprocessor = Pipeline(\\n    steps=[\\n        (\"imputation_mean\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\\n        (\"scaler\", StandardScaler()),\\n    ]\\n)\\n\\ncategorical_preprocessor = Pipeline(\\n    steps=[\\n        (\\n            \"imputation_constant\",\\n            SimpleImputer(fill_value=\"missing\", strategy=\"constant\"),\\n        ),\\n        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\\n    ]\\n)\\n\\npreprocessor = ColumnTransformer(\\n    [\\n        (\"categorical\", categorical_preprocessor, [\"state\", \"gender\"]),\\n        (\"numerical\", numeric_preprocessor, [\"age\", \"weight\"]),\\n    ]\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_pipeline_display.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='preprocessor = ColumnTransformer(\\n    [\\n        (\"categorical\", categorical_preprocessor, [\"state\", \"gender\"]),\\n        (\"numerical\", numeric_preprocessor, [\"age\", \"weight\"]),\\n    ]\\n)\\n\\npipe = Pipeline(\\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\\n)\\n\\nparam_grid = {\\n    \"classifier__n_estimators\": [200, 500],\\n    \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\\n    \"classifier__max_depth\": [4, 5, 6, 7, 8],\\n    \"classifier__criterion\": [\"gini\", \"entropy\"],\\n}\\n\\ngrid_search = GridSearchCV(pipe, param_grid=param_grid, n_jobs=1)\\ngrid_search  # click on the diagram below to see the details of each step'), Document(metadata={'source': '/content/local_copy_repo/examples/miscellaneous/plot_estimator_representation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================\\nDisplaying estimators and complex pipelines\\n===========================================\\n\\nThis example illustrates different ways estimators and pipelines can be\\ndisplayed.\\n\"\"\"\\n\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\n# %%\\n# Compact text representation\\n# ---------------------------\\n#\\n# Estimators will only show the parameters that have been set to non-default\\n# values when displayed as a string. This reduces the visual noise and makes it\\n# easier to spot what the differences are when comparing instances.\\n\\nlr = LogisticRegression(penalty=\"l1\")\\nprint(lr)\\n\\n# %%\\n# Rich HTML representation\\n# ------------------------\\n# In notebooks estimators and pipelines will use a rich HTML representation.\\n# This is particularly useful to summarise the\\n# structure of pipelines and other composite estimators, with interactivity to\\n# provide detail.  Click on the example image below to expand Pipeline\\n# elements.  See :ref:`visualizing_composite_estimators` for how you can use\\n# this feature.\\n\\nnum_proc = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\\n\\ncat_proc = make_pipeline(\\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\\n    OneHotEncoder(handle_unknown=\"ignore\"),\\n)\\n\\npreprocessor = make_column_transformer(\\n    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\\n)\\n\\nclf = make_pipeline(preprocessor, LogisticRegression())\\nclf'), Document(metadata={'source': '/content/local_copy_repo/examples/multiclass/plot_multiclass_overview.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================================\\nOverview of multiclass training meta-estimators\\n===============================================\\n\\nIn this example, we discuss the problem of classification when the target\\nvariable is composed of more than two classes. This is called multiclass\\nclassification.\\n\\nIn scikit-learn, all estimators support multiclass classification out of the\\nbox: the most sensible strategy was implemented for the end-user. The\\n:mod:`sklearn.multiclass` module implements various strategies that one can use\\nfor experimenting or developing third-party estimators that only support binary\\nclassification.\\n\\n:mod:`sklearn.multiclass` includes OvO/OvR strategies used to train a\\nmulticlass classifier by fitting a set of binary classifiers (the\\n:class:`~sklearn.multiclass.OneVsOneClassifier` and\\n:class:`~sklearn.multiclass.OneVsRestClassifier` meta-estimators). This example\\nwill review them.\\n\"\"\"\\n\\n# %%\\n# The Yeast UCI dataset\\n# ---------------------\\n#\\n# In this example, we use a UCI dataset [1]_, generally referred as the Yeast\\n# dataset. We use the :func:`sklearn.datasets.fetch_openml` function to load\\n# the dataset from OpenML.\\nfrom sklearn.datasets import fetch_openml\\n\\nX, y = fetch_openml(data_id=181, as_frame=True, return_X_y=True)\\n\\n# %%\\n# To know the type of data science problem we are dealing with, we can check\\n# the target for which we want to build a predictive model.\\ny.value_counts().sort_index()'), Document(metadata={'source': '/content/local_copy_repo/examples/multiclass/plot_multiclass_overview.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X, y = fetch_openml(data_id=181, as_frame=True, return_X_y=True)\\n\\n# %%\\n# To know the type of data science problem we are dealing with, we can check\\n# the target for which we want to build a predictive model.\\ny.value_counts().sort_index()\\n\\n# %%\\n# We see that the target is discrete and composed of 10 classes. We therefore\\n# deal with a multiclass classification problem.\\n#\\n# Strategies comparison\\n# ---------------------\\n#\\n# In the following experiment, we use a\\n# :class:`~sklearn.tree.DecisionTreeClassifier` and a\\n# :class:`~sklearn.model_selection.RepeatedStratifiedKFold` cross-validation\\n# with 3 splits and 5 repetitions.\\n#\\n# We compare the following strategies:\\n#\\n# * :class:~sklearn.tree.DecisionTreeClassifier can handle multiclass\\n#   classification without needing any special adjustments. It works by breaking\\n#   down the training data into smaller subsets and focusing on the most common\\n#   class in each subset. By repeating this process, the model can accurately\\n#   classify input data into multiple different classes.\\n# * :class:`~sklearn.multiclass.OneVsOneClassifier` trains a set of binary\\n#   classifiers where each classifier is trained to distinguish between\\n#   two classes.\\n# * :class:`~sklearn.multiclass.OneVsRestClassifier`: trains a set of binary\\n#   classifiers where each classifier is trained to distinguish between\\n#   one class and the rest of the classes.\\n# * :class:`~sklearn.multiclass.OutputCodeClassifier`: trains a set of binary\\n#   classifiers where each classifier is trained to distinguish between\\n#   a set of classes from the rest of the classes. The set of classes is\\n#   defined by a codebook, which is randomly generated in scikit-learn. This\\n#   method exposes a parameter `code_size` to control the size of the codebook.\\n#   We set it above one since we are not interested in compressing the class\\n#   representation.\\nimport pandas as pd\\n\\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\\nfrom sklearn.multiclass import (\\n    OneVsOneClassifier,\\n    OneVsRestClassifier,\\n    OutputCodeClassifier,\\n)\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=5, random_state=0)\\n\\ntree = DecisionTreeClassifier(random_state=0)\\novo_tree = OneVsOneClassifier(tree)\\novr_tree = OneVsRestClassifier(tree)\\necoc = OutputCodeClassifier(tree, code_size=2)\\n\\ncv_results_tree = cross_validate(tree, X, y, cv=cv, n_jobs=2)\\ncv_results_ovo = cross_validate(ovo_tree, X, y, cv=cv, n_jobs=2)\\ncv_results_ovr = cross_validate(ovr_tree, X, y, cv=cv, n_jobs=2)\\ncv_results_ecoc = cross_validate(ecoc, X, y, cv=cv, n_jobs=2)\\n\\n# %%\\n# We can now compare the statistical performance of the different strategies.\\n# We plot the score distribution of the different strategies.\\nfrom matplotlib import pyplot as plt'), Document(metadata={'source': '/content/local_copy_repo/examples/multiclass/plot_multiclass_overview.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We can now compare the statistical performance of the different strategies.\\n# We plot the score distribution of the different strategies.\\nfrom matplotlib import pyplot as plt\\n\\nscores = pd.DataFrame(\\n    {\\n        \"DecisionTreeClassifier\": cv_results_tree[\"test_score\"],\\n        \"OneVsOneClassifier\": cv_results_ovo[\"test_score\"],\\n        \"OneVsRestClassifier\": cv_results_ovr[\"test_score\"],\\n        \"OutputCodeClassifier\": cv_results_ecoc[\"test_score\"],\\n    }\\n)\\nax = scores.plot.kde(legend=True)\\nax.set_xlabel(\"Accuracy score\")\\nax.set_xlim([0, 0.7])\\n_ = ax.set_title(\\n    \"Density of the accuracy scores for the different multiclass strategies\"\\n)\\n\\n# %%\\n# At a first glance, we can see that the built-in strategy of the decision\\n# tree classifier is working quite well. One-vs-one and the error-correcting\\n# output code strategies are working even better. However, the\\n# one-vs-rest strategy is not working as well as the other strategies.\\n#\\n# Indeed, these results reproduce something reported in the literature\\n# as in [2]_. However, the story is not as simple as it seems.\\n#\\n# The importance of hyperparameters search\\n# ----------------------------------------\\n#\\n# It was later shown in [3]_ that the multiclass strategies would show similar\\n# scores if the hyperparameters of the base classifiers are first optimized.\\n#\\n# Here we try to reproduce such result by at least optimizing the depth of the\\n# base decision tree.\\nfrom sklearn.model_selection import GridSearchCV\\n\\nparam_grid = {\"max_depth\": [3, 5, 8]}\\ntree_optimized = GridSearchCV(tree, param_grid=param_grid, cv=3)\\novo_tree = OneVsOneClassifier(tree_optimized)\\novr_tree = OneVsRestClassifier(tree_optimized)\\necoc = OutputCodeClassifier(tree_optimized, code_size=2)\\n\\ncv_results_tree = cross_validate(tree_optimized, X, y, cv=cv, n_jobs=2)\\ncv_results_ovo = cross_validate(ovo_tree, X, y, cv=cv, n_jobs=2)\\ncv_results_ovr = cross_validate(ovr_tree, X, y, cv=cv, n_jobs=2)\\ncv_results_ecoc = cross_validate(ecoc, X, y, cv=cv, n_jobs=2)\\n\\nscores = pd.DataFrame(\\n    {\\n        \"DecisionTreeClassifier\": cv_results_tree[\"test_score\"],\\n        \"OneVsOneClassifier\": cv_results_ovo[\"test_score\"],\\n        \"OneVsRestClassifier\": cv_results_ovr[\"test_score\"],\\n        \"OutputCodeClassifier\": cv_results_ecoc[\"test_score\"],\\n    }\\n)\\nax = scores.plot.kde(legend=True)\\nax.set_xlabel(\"Accuracy score\")\\nax.set_xlim([0, 0.7])\\n_ = ax.set_title(\\n    \"Density of the accuracy scores for the different multiclass strategies\"\\n)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/multiclass/plot_multiclass_overview.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.show()\\n\\n# %%\\n# We can see that once the hyperparameters are optimized, all multiclass\\n# strategies have similar performance as discussed in [3]_.\\n#\\n# Conclusion\\n# ----------\\n#\\n# We can get some intuition behind those results.\\n#\\n# First, the reason for which one-vs-one and error-correcting output code are\\n# outperforming the tree when the hyperparameters are not optimized relies on\\n# fact that they ensemble a larger number of classifiers. The ensembling\\n# improves the generalization performance. This is a bit similar why a bagging\\n# classifier generally performs better than a single decision tree if no care\\n# is taken to optimize the hyperparameters.\\n#\\n# Then, we see the importance of optimizing the hyperparameters. Indeed, it\\n# should be regularly explored when developing predictive models even if\\n# techniques such as ensembling help at reducing this impact.\\n#\\n# Finally, it is important to recall that the estimators in scikit-learn\\n# are developed with a specific strategy to handle multiclass classification\\n# out of the box. So for these estimators, it means that there is no need to\\n# use different strategies. These strategies are mainly useful for third-party\\n# estimators supporting only binary classification. In all cases, we also show\\n# that the hyperparameters should be optimized.\\n#\\n# References\\n# ----------\\n#\\n#   .. [1] https://archive.ics.uci.edu/ml/datasets/Yeast\\n#\\n#   .. [2] `\"Reducing multiclass to binary: A unifying approach for margin classifiers.\"\\n#      Allwein, Erin L., Robert E. Schapire, and Yoram Singer.\\n#      Journal of machine learning research 1\\n#      Dec (2000): 113-141.\\n#      <https://www.jmlr.org/papers/volume1/allwein00a/allwein00a.pdf>`_.\\n#\\n#   .. [3] `\"In defense of one-vs-all classification.\"\\n#      Journal of Machine Learning Research 5\\n#      Jan (2004): 101-141.\\n#      <https://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf>`_.'), Document(metadata={'source': '/content/local_copy_repo/examples/bicluster/plot_spectral_biclustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================\\nA demo of the Spectral Biclustering algorithm\\n=============================================\\n\\nThis example demonstrates how to generate a checkerboard dataset and bicluster\\nit using the :class:`~sklearn.cluster.SpectralBiclustering` algorithm. The\\nspectral biclustering algorithm is specifically designed to cluster data by\\nsimultaneously considering both the rows (samples) and columns (features) of a\\nmatrix. It aims to identify patterns not only between samples but also within\\nsubsets of samples, allowing for the detection of localized structure within the\\ndata. This makes spectral biclustering particularly well-suited for datasets\\nwhere the order or arrangement of features is fixed, such as in images, time\\nseries, or genomes.\\n\\nThe data is generated, then shuffled and passed to the spectral biclustering\\nalgorithm. The rows and columns of the shuffled matrix are then rearranged to\\nplot the biclusters found.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate sample data\\n# --------------------\\n# We generate the sample data using the\\n# :func:`~sklearn.datasets.make_checkerboard` function. Each pixel within\\n# `shape=(300, 300)` represents with it\\'s color a value from a uniform\\n# distribution. The noise is added from a normal distribution, where the value\\n# chosen for `noise` is the standard deviation.\\n#\\n# As you can see, the data is distributed over 12 cluster cells and is\\n# relatively well distinguishable.\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn.datasets import make_checkerboard\\n\\nn_clusters = (4, 3)\\ndata, rows, columns = make_checkerboard(\\n    shape=(300, 300), n_clusters=n_clusters, noise=10, shuffle=False, random_state=42\\n)\\n\\nplt.matshow(data, cmap=plt.cm.Blues)\\nplt.title(\"Original dataset\")\\n_ = plt.show()\\n\\n# %%\\n# We shuffle the data and the goal is to reconstruct it afterwards using\\n# :class:`~sklearn.cluster.SpectralBiclustering`.\\nimport numpy as np\\n\\n# Creating lists of shuffled row and column indices\\nrng = np.random.RandomState(0)\\nrow_idx_shuffled = rng.permutation(data.shape[0])\\ncol_idx_shuffled = rng.permutation(data.shape[1])\\n\\n# %%\\n# We redefine the shuffled data and plot it. We observe that we lost the\\n# structure of original data matrix.\\ndata = data[row_idx_shuffled][:, col_idx_shuffled]\\n\\nplt.matshow(data, cmap=plt.cm.Blues)\\nplt.title(\"Shuffled dataset\")\\n_ = plt.show()\\n\\n# %%\\n# Fitting `SpectralBiclustering`\\n# ------------------------------\\n# We fit the model and compare the obtained clusters with the ground truth. Note\\n# that when creating the model we specify the same number of clusters that we\\n# used to create the dataset (`n_clusters = (4, 3)`), which will contribute to\\n# obtain a good result.\\nfrom sklearn.cluster import SpectralBiclustering\\nfrom sklearn.metrics import consensus_score\\n\\nmodel = SpectralBiclustering(n_clusters=n_clusters, method=\"log\", random_state=0)\\nmodel.fit(data)'), Document(metadata={'source': '/content/local_copy_repo/examples/bicluster/plot_spectral_biclustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='model = SpectralBiclustering(n_clusters=n_clusters, method=\"log\", random_state=0)\\nmodel.fit(data)\\n\\n# Compute the similarity of two sets of biclusters\\nscore = consensus_score(\\n    model.biclusters_, (rows[:, row_idx_shuffled], columns[:, col_idx_shuffled])\\n)\\nprint(f\"consensus score: {score:.1f}\")\\n\\n# %%\\n# The score is between 0 and 1, where 1 corresponds to a perfect matching. It\\n# shows the quality of the biclustering.\\n\\n# %%\\n# Plotting results\\n# ----------------\\n# Now, we rearrange the data based on the row and column labels assigned by the\\n# :class:`~sklearn.cluster.SpectralBiclustering` model in ascending order and\\n# plot again. The `row_labels_` range from 0 to 3, while the `column_labels_`\\n# range from 0 to 2, representing a total of 4 clusters per row and 3 clusters\\n# per column.\\n\\n# Reordering first the rows and then the columns.\\nreordered_rows = data[np.argsort(model.row_labels_)]\\nreordered_data = reordered_rows[:, np.argsort(model.column_labels_)]\\n\\nplt.matshow(reordered_data, cmap=plt.cm.Blues)\\nplt.title(\"After biclustering; rearranged to show biclusters\")\\n_ = plt.show()\\n\\n# %%\\n# As a last step, we want to demonstrate the relationships between the row\\n# and column labels assigned by the model. Therefore, we create a grid with\\n# :func:`numpy.outer`, which takes the sorted `row_labels_` and `column_labels_`\\n# and adds 1 to each to ensure that the labels start from 1 instead of 0 for\\n# better visualization.\\nplt.matshow(\\n    np.outer(np.sort(model.row_labels_) + 1, np.sort(model.column_labels_) + 1),\\n    cmap=plt.cm.Blues,\\n)\\nplt.title(\"Checkerboard structure of rearranged data\")\\nplt.show()\\n\\n# %%\\n# The outer product of the row and column label vectors shows a representation\\n# of the checkerboard structure, where different combinations of row and column\\n# labels are represented by different shades of blue.'), Document(metadata={'source': '/content/local_copy_repo/examples/bicluster/plot_spectral_coclustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================\\nA demo of the Spectral Co-Clustering algorithm\\n==============================================\\n\\nThis example demonstrates how to generate a dataset and bicluster it\\nusing the Spectral Co-Clustering algorithm.\\n\\nThe dataset is generated using the ``make_biclusters`` function, which\\ncreates a matrix of small values and implants bicluster with large\\nvalues. The rows and columns are then shuffled and passed to the\\nSpectral Co-Clustering algorithm. Rearranging the shuffled matrix to\\nmake biclusters contiguous shows how accurately the algorithm found\\nthe biclusters.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn.cluster import SpectralCoclustering\\nfrom sklearn.datasets import make_biclusters\\nfrom sklearn.metrics import consensus_score\\n\\ndata, rows, columns = make_biclusters(\\n    shape=(300, 300), n_clusters=5, noise=5, shuffle=False, random_state=0\\n)\\n\\nplt.matshow(data, cmap=plt.cm.Blues)\\nplt.title(\"Original dataset\")\\n\\n# shuffle clusters\\nrng = np.random.RandomState(0)\\nrow_idx = rng.permutation(data.shape[0])\\ncol_idx = rng.permutation(data.shape[1])\\ndata = data[row_idx][:, col_idx]\\n\\nplt.matshow(data, cmap=plt.cm.Blues)\\nplt.title(\"Shuffled dataset\")\\n\\nmodel = SpectralCoclustering(n_clusters=5, random_state=0)\\nmodel.fit(data)\\nscore = consensus_score(model.biclusters_, (rows[:, row_idx], columns[:, col_idx]))\\n\\nprint(\"consensus score: {:.3f}\".format(score))\\n\\nfit_data = data[np.argsort(model.row_labels_)]\\nfit_data = fit_data[:, np.argsort(model.column_labels_)]\\n\\nplt.matshow(fit_data, cmap=plt.cm.Blues)\\nplt.title(\"After biclustering; rearranged to show biclusters\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/bicluster/plot_bicluster_newsgroups.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def number_normalizer(tokens):\\n    \"\"\"Map all numeric tokens to a placeholder.\\n\\n    For many applications, tokens that begin with a number are not directly\\n    useful, but the fact that such a token exists can be relevant.  By applying\\n    this form of dimensionality reduction, some methods may perform better.\\n    \"\"\"\\n    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)'), Document(metadata={'source': '/content/local_copy_repo/examples/bicluster/plot_bicluster_newsgroups.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class NumberNormalizingVectorizer(TfidfVectorizer):\\n    def build_tokenizer(self):\\n        tokenize = super().build_tokenizer()\\n        return lambda doc: list(number_normalizer(tokenize(doc)))'), Document(metadata={'source': '/content/local_copy_repo/examples/bicluster/plot_bicluster_newsgroups.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def bicluster_ncut(i):\\n    rows, cols = cocluster.get_indices(i)\\n    if not (np.any(rows) and np.any(cols)):\\n        import sys\\n\\n        return sys.float_info.max\\n    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\\n    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\\n    # Note: the following is identical to X[rows[:, np.newaxis],\\n    # cols].sum() but much faster in scipy <= 0.16\\n    weight = X[rows][:, cols].sum()\\n    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()\\n    return cut / weight'), Document(metadata={'source': '/content/local_copy_repo/examples/bicluster/plot_bicluster_newsgroups.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def most_common(d):\\n    \"\"\"Items of a defaultdict(int) with the highest values.\\n\\n    Like Counter.most_common in Python >=2.7.\\n    \"\"\"\\n    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)'), Document(metadata={'source': '/content/local_copy_repo/examples/bicluster/plot_bicluster_newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================================================\\nBiclustering documents with the Spectral Co-clustering algorithm\\n================================================================\\n\\nThis example demonstrates the Spectral Co-clustering algorithm on the\\ntwenty newsgroups dataset. The \\'comp.os.ms-windows.misc\\' category is\\nexcluded because it contains many posts containing nothing but data.\\n\\nThe TF-IDF vectorized posts form a word frequency matrix, which is\\nthen biclustered using Dhillon\\'s Spectral Co-Clustering algorithm. The\\nresulting document-word biclusters indicate subsets words used more\\noften in those subsets documents.\\n\\nFor a few of the best biclusters, its most common document categories\\nand its ten most important words get printed. The best biclusters are\\ndetermined by their normalized cut. The best words are determined by\\ncomparing their sums inside and outside the bicluster.\\n\\nFor comparison, the documents are also clustered using\\nMiniBatchKMeans. The document clusters derived from the biclusters\\nachieve a better V-measure than clusters found by MiniBatchKMeans.\\n\\n\"\"\"\\n\\nimport operator\\nfrom collections import defaultdict\\nfrom time import time\\n\\nimport numpy as np\\n\\nfrom sklearn.cluster import MiniBatchKMeans, SpectralCoclustering\\nfrom sklearn.datasets import fetch_20newsgroups\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics.cluster import v_measure_score\\n\\n\\n# Code for: def number_normalizer(tokens):\\n\\n\\n# Code for: class NumberNormalizingVectorizer(TfidfVectorizer):\\n\\n\\n# exclude \\'comp.os.ms-windows.misc\\'\\ncategories = [\\n    \"alt.atheism\",\\n    \"comp.graphics\",\\n    \"comp.sys.ibm.pc.hardware\",\\n    \"comp.sys.mac.hardware\",\\n    \"comp.windows.x\",\\n    \"misc.forsale\",\\n    \"rec.autos\",\\n    \"rec.motorcycles\",\\n    \"rec.sport.baseball\",\\n    \"rec.sport.hockey\",\\n    \"sci.crypt\",\\n    \"sci.electronics\",\\n    \"sci.med\",\\n    \"sci.space\",\\n    \"soc.religion.christian\",\\n    \"talk.politics.guns\",\\n    \"talk.politics.mideast\",\\n    \"talk.politics.misc\",\\n    \"talk.religion.misc\",\\n]\\nnewsgroups = fetch_20newsgroups(categories=categories)\\ny_true = newsgroups.target\\n\\nvectorizer = NumberNormalizingVectorizer(stop_words=\"english\", min_df=5)\\ncocluster = SpectralCoclustering(\\n    n_clusters=len(categories), svd_method=\"arpack\", random_state=0\\n)\\nkmeans = MiniBatchKMeans(\\n    n_clusters=len(categories), batch_size=20000, random_state=0, n_init=3\\n)\\n\\nprint(\"Vectorizing...\")\\nX = vectorizer.fit_transform(newsgroups.data)\\n\\nprint(\"Coclustering...\")\\nstart_time = time()\\ncocluster.fit(X)\\ny_cocluster = cocluster.row_labels_\\nprint(\\n    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\\n        time() - start_time, v_measure_score(y_cocluster, y_true)\\n    )\\n)\\n\\nprint(\"MiniBatchKMeans...\")\\nstart_time = time()\\ny_kmeans = kmeans.fit_predict(X)\\nprint(\\n    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\\n        time() - start_time, v_measure_score(y_kmeans, y_true)\\n    )\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/bicluster/plot_bicluster_newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(\"MiniBatchKMeans...\")\\nstart_time = time()\\ny_kmeans = kmeans.fit_predict(X)\\nprint(\\n    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\\n        time() - start_time, v_measure_score(y_kmeans, y_true)\\n    )\\n)\\n\\nfeature_names = vectorizer.get_feature_names_out()\\ndocument_names = list(newsgroups.target_names[i] for i in newsgroups.target)\\n\\n\\n# Code for: def bicluster_ncut(i):\\n\\n\\n# Code for: def most_common(d):\\n\\n\\nbicluster_ncuts = list(bicluster_ncut(i) for i in range(len(newsgroups.target_names)))\\nbest_idx = np.argsort(bicluster_ncuts)[:5]\\n\\nprint()\\nprint(\"Best biclusters:\")\\nprint(\"----------------\")\\nfor idx, cluster in enumerate(best_idx):\\n    n_rows, n_cols = cocluster.get_shape(cluster)\\n    cluster_docs, cluster_words = cocluster.get_indices(cluster)\\n    if not len(cluster_docs) or not len(cluster_words):\\n        continue\\n\\n    # categories\\n    counter = defaultdict(int)\\n    for i in cluster_docs:\\n        counter[document_names[i]] += 1\\n    cat_string = \", \".join(\\n        \"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\\n        for name, c in most_common(counter)[:3]\\n    )\\n\\n    # words\\n    out_of_cluster_docs = cocluster.row_labels_ != cluster\\n    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\\n    word_col = X[:, cluster_words]\\n    word_scores = np.array(\\n        word_col[cluster_docs, :].sum(axis=0)\\n        - word_col[out_of_cluster_docs, :].sum(axis=0)\\n    )\\n    word_scores = word_scores.ravel()\\n    important_words = list(\\n        feature_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1]\\n    )\\n\\n    print(\"bicluster {} : {} documents, {} words\".format(idx, n_rows, n_cols))\\n    print(\"categories   : {}\".format(cat_string))\\n    print(\"words        : {}\\\\n\".format(\", \".join(important_words)))'), Document(metadata={'source': '/content/local_copy_repo/examples/feature_selection/plot_rfe_digits.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================\\nRecursive feature elimination\\n=============================\\n\\nThis example demonstrates how Recursive Feature Elimination\\n(:class:`~sklearn.feature_selection.RFE`) can be used to determine the\\nimportance of individual pixels for classifying handwritten digits.\\n:class:`~sklearn.feature_selection.RFE` recursively removes the least\\nsignificant features, assigning ranks based on their importance, where higher\\n`ranking_` values denote lower importance. The ranking is visualized using both\\nshades of blue and pixel annotations for clarity. As expected, pixels positioned\\nat the center of the image tend to be more predictive than those near the edges.\\n\\n.. note::\\n\\n    See also :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`\\n\\n\"\"\"  # noqa: E501\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.feature_selection import RFE\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# Load the digits dataset\\ndigits = load_digits()\\nX = digits.images.reshape((len(digits.images), -1))\\ny = digits.target\\n\\npipe = Pipeline(\\n    [\\n        (\"scaler\", MinMaxScaler()),\\n        (\"rfe\", RFE(estimator=LogisticRegression(), n_features_to_select=1, step=1)),\\n    ]\\n)\\n\\npipe.fit(X, y)\\nranking = pipe.named_steps[\"rfe\"].ranking_.reshape(digits.images[0].shape)\\n\\n# Plot pixel ranking\\nplt.matshow(ranking, cmap=plt.cm.Blues)\\n\\n# Add annotations for pixel numbers\\nfor i in range(ranking.shape[0]):\\n    for j in range(ranking.shape[1]):\\n        plt.text(j, i, str(ranking[i, j]), ha=\"center\", va=\"center\", color=\"black\")\\n\\nplt.colorbar()\\nplt.title(\"Ranking of pixels with RFE\\\\n(Logistic Regression)\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/feature_selection/plot_feature_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================\\nUnivariate Feature Selection\\n============================\\n\\nThis notebook is an example of using univariate feature selection\\nto improve classification accuracy on a noisy dataset.\\n\\nIn this example, some noisy (non informative) features are added to\\nthe iris dataset. Support vector machine (SVM) is used to classify the\\ndataset both before and after applying univariate feature selection.\\nFor each feature, we plot the p-values for the univariate feature selection\\nand the corresponding weights of SVMs. With this, we will compare model\\naccuracy and examine the impact of univariate feature selection on model\\nweights.\\n\\n\"\"\"\\n\\n# %%\\n# Generate sample data\\n# --------------------\\n#\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\n\\n# The iris dataset\\nX, y = load_iris(return_X_y=True)\\n\\n# Some noisy data not correlated\\nE = np.random.RandomState(42).uniform(0, 0.1, size=(X.shape[0], 20))\\n\\n# Add the noisy data to the informative features\\nX = np.hstack((X, E))\\n\\n# Split dataset to select feature and evaluate the classifier\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n\\n# %%\\n# Univariate feature selection\\n# ----------------------------\\n#\\n# Univariate feature selection with F-test for feature scoring.\\n# We use the default selection function to select\\n# the four most significant features.\\nfrom sklearn.feature_selection import SelectKBest, f_classif\\n\\nselector = SelectKBest(f_classif, k=4)\\nselector.fit(X_train, y_train)\\nscores = -np.log10(selector.pvalues_)\\nscores /= scores.max()\\n\\n# %%\\nimport matplotlib.pyplot as plt\\n\\nX_indices = np.arange(X.shape[-1])\\nplt.figure(1)\\nplt.clf()\\nplt.bar(X_indices - 0.05, scores, width=0.2)\\nplt.title(\"Feature univariate score\")\\nplt.xlabel(\"Feature number\")\\nplt.ylabel(r\"Univariate score ($-Log(p_{value})$)\")\\nplt.show()\\n\\n# %%\\n# In the total set of features, only the 4 of the original features are significant.\\n# We can see that they have the highest score with univariate feature\\n# selection.\\n\\n# %%\\n# Compare with SVMs\\n# -----------------\\n#\\n# Without univariate feature selection\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.svm import LinearSVC\\n\\nclf = make_pipeline(MinMaxScaler(), LinearSVC())\\nclf.fit(X_train, y_train)\\nprint(\\n    \"Classification accuracy without selecting features: {:.3f}\".format(\\n        clf.score(X_test, y_test)\\n    )\\n)\\n\\nsvm_weights = np.abs(clf[-1].coef_).sum(axis=0)\\nsvm_weights /= svm_weights.sum()\\n\\n# %%\\n# After univariate feature selection\\nclf_selected = make_pipeline(SelectKBest(f_classif, k=4), MinMaxScaler(), LinearSVC())\\nclf_selected.fit(X_train, y_train)\\nprint(\\n    \"Classification accuracy after univariate feature selection: {:.3f}\".format(\\n        clf_selected.score(X_test, y_test)\\n    )\\n)\\n\\nsvm_weights_selected = np.abs(clf_selected[-1].coef_).sum(axis=0)\\nsvm_weights_selected /= svm_weights_selected.sum()'), Document(metadata={'source': '/content/local_copy_repo/examples/feature_selection/plot_feature_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='svm_weights_selected = np.abs(clf_selected[-1].coef_).sum(axis=0)\\nsvm_weights_selected /= svm_weights_selected.sum()\\n\\n# %%\\nplt.bar(\\n    X_indices - 0.45, scores, width=0.2, label=r\"Univariate score ($-Log(p_{value})$)\"\\n)\\n\\nplt.bar(X_indices - 0.25, svm_weights, width=0.2, label=\"SVM weight\")\\n\\nplt.bar(\\n    X_indices[selector.get_support()] - 0.05,\\n    svm_weights_selected,\\n    width=0.2,\\n    label=\"SVM weights after selection\",\\n)\\n\\nplt.title(\"Comparing feature selection\")\\nplt.xlabel(\"Feature number\")\\nplt.yticks(())\\nplt.axis(\"tight\")\\nplt.legend(loc=\"upper right\")\\nplt.show()\\n\\n# %%\\n# Without univariate feature selection, the SVM assigns a large weight\\n# to the first 4 original significant features, but also selects many of the\\n# non-informative features. Applying univariate feature selection before\\n# the SVM increases the SVM weight attributed to the significant features,\\n# and will thus improve classification.'), Document(metadata={'source': '/content/local_copy_repo/examples/feature_selection/plot_feature_selection_pipeline.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================\\nPipeline ANOVA SVM\\n==================\\n\\nThis example shows how a feature selection can be easily integrated within\\na machine learning pipeline.\\n\\nWe also show that you can easily inspect part of the pipeline.\\n\\n\"\"\"\\n\\n# %%\\n# We will start by generating a binary classification dataset. Subsequently, we\\n# will divide the dataset into two subsets.\\n\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = make_classification(\\n    n_features=20,\\n    n_informative=3,\\n    n_redundant=0,\\n    n_classes=2,\\n    n_clusters_per_class=2,\\n    random_state=42,\\n)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n\\n# %%\\n# A common mistake done with feature selection is to search a subset of\\n# discriminative features on the full dataset, instead of only using the\\n# training set. The usage of scikit-learn :func:`~sklearn.pipeline.Pipeline`\\n# prevents to make such mistake.\\n#\\n# Here, we will demonstrate how to build a pipeline where the first step will\\n# be the feature selection.\\n#\\n# When calling `fit` on the training data, a subset of feature will be selected\\n# and the index of these selected features will be stored. The feature selector\\n# will subsequently reduce the number of features, and pass this subset to the\\n# classifier which will be trained.\\n\\nfrom sklearn.feature_selection import SelectKBest, f_classif\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.svm import LinearSVC\\n\\nanova_filter = SelectKBest(f_classif, k=3)\\nclf = LinearSVC()\\nanova_svm = make_pipeline(anova_filter, clf)\\nanova_svm.fit(X_train, y_train)\\n\\n# %%\\n# Once the training is complete, we can predict on new unseen samples. In this\\n# case, the feature selector will only select the most discriminative features\\n# based on the information stored during training. Then, the data will be\\n# passed to the classifier which will make the prediction.\\n#\\n# Here, we show the final metrics via a classification report.\\n\\nfrom sklearn.metrics import classification_report\\n\\ny_pred = anova_svm.predict(X_test)\\nprint(classification_report(y_test, y_pred))\\n\\n# %%\\n# Be aware that you can inspect a step in the pipeline. For instance, we might\\n# be interested about the parameters of the classifier. Since we selected\\n# three features, we expect to have three coefficients.\\n\\nanova_svm[-1].coef_\\n\\n# %%\\n# However, we do not know which features were selected from the original\\n# dataset. We could proceed by several manners. Here, we will invert the\\n# transformation of these coefficients to get information about the original\\n# space.\\n\\nanova_svm[:-1].inverse_transform(anova_svm[-1].coef_)\\n\\n# %%\\n# We can see that the features with non-zero coefficients are the selected\\n# features by the first step.'), Document(metadata={'source': '/content/local_copy_repo/examples/feature_selection/plot_f_test_vs_mi.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================\\nComparison of F-test and mutual information\\n===========================================\\n\\nThis example illustrates the differences between univariate F-test statistics\\nand mutual information.\\n\\nWe consider 3 features x_1, x_2, x_3 distributed uniformly over [0, 1], the\\ntarget depends on them as follows:\\n\\ny = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1), that is the third feature is\\ncompletely irrelevant.\\n\\nThe code below plots the dependency of y against individual x_i and normalized\\nvalues of univariate F-tests statistics and mutual information.\\n\\nAs F-test captures only linear dependency, it rates x_1 as the most\\ndiscriminative feature. On the other hand, mutual information can capture any\\nkind of dependency between variables and it rates x_2 as the most\\ndiscriminative feature, which probably agrees better with our intuitive\\nperception for this example. Both methods correctly mark x_3 as irrelevant.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.feature_selection import f_regression, mutual_info_regression\\n\\nnp.random.seed(0)\\nX = np.random.rand(1000, 3)\\ny = X[:, 0] + np.sin(6 * np.pi * X[:, 1]) + 0.1 * np.random.randn(1000)\\n\\nf_test, _ = f_regression(X, y)\\nf_test /= np.max(f_test)\\n\\nmi = mutual_info_regression(X, y)\\nmi /= np.max(mi)\\n\\nplt.figure(figsize=(15, 5))\\nfor i in range(3):\\n    plt.subplot(1, 3, i + 1)\\n    plt.scatter(X[:, i], y, edgecolor=\"black\", s=20)\\n    plt.xlabel(\"$x_{}$\".format(i + 1), fontsize=14)\\n    if i == 0:\\n        plt.ylabel(\"$y$\", fontsize=14)\\n    plt.title(\"F-test={:.2f}, MI={:.2f}\".format(f_test[i], mi[i]), fontsize=16)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/feature_selection/plot_rfe_with_cross_validation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================================\\nRecursive feature elimination with cross-validation\\n===================================================\\n\\nA Recursive Feature Elimination (RFE) example with automatic tuning of the\\nnumber of features selected with cross-validation.\\n\\n\"\"\"\\n\\n# %%\\n# Data generation\\n# ---------------\\n#\\n# We build a classification task using 3 informative features. The introduction\\n# of 2 additional redundant (i.e. correlated) features has the effect that the\\n# selected features vary depending on the cross-validation fold. The remaining\\n# features are non-informative as they are drawn at random.\\n\\nfrom sklearn.datasets import make_classification\\n\\nX, y = make_classification(\\n    n_samples=500,\\n    n_features=15,\\n    n_informative=3,\\n    n_redundant=2,\\n    n_repeated=0,\\n    n_classes=8,\\n    n_clusters_per_class=1,\\n    class_sep=0.8,\\n    random_state=0,\\n)\\n\\n# %%\\n# Model training and selection\\n# ----------------------------\\n#\\n# We create the RFE object and compute the cross-validated scores. The scoring\\n# strategy \"accuracy\" optimizes the proportion of correctly classified samples.\\n\\nfrom sklearn.feature_selection import RFECV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import StratifiedKFold\\n\\nmin_features_to_select = 1  # Minimum number of features to consider\\nclf = LogisticRegression()\\ncv = StratifiedKFold(5)\\n\\nrfecv = RFECV(\\n    estimator=clf,\\n    step=1,\\n    cv=cv,\\n    scoring=\"accuracy\",\\n    min_features_to_select=min_features_to_select,\\n    n_jobs=2,\\n)\\nrfecv.fit(X, y)\\n\\nprint(f\"Optimal number of features: {rfecv.n_features_}\")\\n\\n# %%\\n# In the present case, the model with 3 features (which corresponds to the true\\n# generative model) is found to be the most optimal.\\n#\\n# Plot number of features VS. cross-validation scores\\n# ---------------------------------------------------\\n\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\ncv_results = pd.DataFrame(rfecv.cv_results_)\\nplt.figure()\\nplt.xlabel(\"Number of features selected\")\\nplt.ylabel(\"Mean test accuracy\")\\nplt.errorbar(\\n    x=cv_results[\"n_features\"],\\n    y=cv_results[\"mean_test_score\"],\\n    yerr=cv_results[\"std_test_score\"],\\n)\\nplt.title(\"Recursive Feature Elimination \\\\nwith correlated features\")\\nplt.show()\\n\\n# %%\\n# From the plot above one can further notice a plateau of equivalent scores\\n# (similar mean value and overlapping errorbars) for 3 to 5 selected features.\\n# This is the result of introducing correlated features. Indeed, the optimal\\n# model selected by the RFE can lie within this range, depending on the\\n# cross-validation technique. The test accuracy decreases above 5 selected\\n# features, this is, keeping non-informative features leads to over-fitting and\\n# is therefore detrimental for the statistical performance of the models.'), Document(metadata={'source': '/content/local_copy_repo/examples/feature_selection/plot_select_from_model_diabetes.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================================\\nModel-based and sequential feature selection\\n============================================\\n\\nThis example illustrates and compares two approaches for feature selection:\\n:class:`~sklearn.feature_selection.SelectFromModel` which is based on feature\\nimportance, and\\n:class:`~sklearn.feature_selection.SequentialFeatureSelector` which relies\\non a greedy approach.\\n\\nWe use the Diabetes dataset, which consists of 10 features collected from 442\\ndiabetes patients.\\n\\nAuthors: `Manoj Kumar <mks542@nyu.edu>`_,\\n`Maria Telenczuk <https://github.com/maikia>`_, Nicolas Hug.\\n\\nLicense: BSD 3 clause\\n\\n\"\"\"\\n\\n# %%\\n# Loading the data\\n# ----------------\\n#\\n# We first load the diabetes dataset which is available from within\\n# scikit-learn, and print its description:\\nfrom sklearn.datasets import load_diabetes\\n\\ndiabetes = load_diabetes()\\nX, y = diabetes.data, diabetes.target\\nprint(diabetes.DESCR)\\n\\n# %%\\n# Feature importance from coefficients\\n# ------------------------------------\\n#\\n# To get an idea of the importance of the features, we are going to use the\\n# :class:`~sklearn.linear_model.RidgeCV` estimator. The features with the\\n# highest absolute `coef_` value are considered the most important.\\n# We can observe the coefficients directly without needing to scale them (or\\n# scale the data) because from the description above, we know that the features\\n# were already standardized.\\n# For a more complete example on the interpretations of the coefficients of\\n# linear models, you may refer to\\n# :ref:`sphx_glr_auto_examples_inspection_plot_linear_model_coefficient_interpretation.py`.  # noqa: E501\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.linear_model import RidgeCV\\n\\nridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(X, y)\\nimportance = np.abs(ridge.coef_)\\nfeature_names = np.array(diabetes.feature_names)\\nplt.bar(height=importance, x=feature_names)\\nplt.title(\"Feature importances via coefficients\")\\nplt.show()\\n\\n# %%\\n# Selecting features based on importance\\n# --------------------------------------\\n#\\n# Now we want to select the two features which are the most important according\\n# to the coefficients. The :class:`~sklearn.feature_selection.SelectFromModel`\\n# is meant just for that. :class:`~sklearn.feature_selection.SelectFromModel`\\n# accepts a `threshold` parameter and will select the features whose importance\\n# (defined by the coefficients) are above this threshold.\\n#\\n# Since we want to select only 2 features, we will set this threshold slightly\\n# above the coefficient of third most important feature.\\nfrom time import time\\n\\nfrom sklearn.feature_selection import SelectFromModel\\n\\nthreshold = np.sort(importance)[-3] + 0.01\\n\\ntic = time()\\nsfm = SelectFromModel(ridge, threshold=threshold).fit(X, y)\\ntoc = time()\\nprint(f\"Features selected by SelectFromModel: {feature_names[sfm.get_support()]}\")\\nprint(f\"Done in {toc - tic:.3f}s\")'), Document(metadata={'source': '/content/local_copy_repo/examples/feature_selection/plot_select_from_model_diabetes.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='threshold = np.sort(importance)[-3] + 0.01\\n\\ntic = time()\\nsfm = SelectFromModel(ridge, threshold=threshold).fit(X, y)\\ntoc = time()\\nprint(f\"Features selected by SelectFromModel: {feature_names[sfm.get_support()]}\")\\nprint(f\"Done in {toc - tic:.3f}s\")\\n\\n# %%\\n# Selecting features with Sequential Feature Selection\\n# ----------------------------------------------------\\n#\\n# Another way of selecting features is to use\\n# :class:`~sklearn.feature_selection.SequentialFeatureSelector`\\n# (SFS). SFS is a greedy procedure where, at each iteration, we choose the best\\n# new feature to add to our selected features based a cross-validation score.\\n# That is, we start with 0 features and choose the best single feature with the\\n# highest score. The procedure is repeated until we reach the desired number of\\n# selected features.\\n#\\n# We can also go in the reverse direction (backward SFS), *i.e.* start with all\\n# the features and greedily choose features to remove one by one. We illustrate\\n# both approaches here.\\n\\nfrom sklearn.feature_selection import SequentialFeatureSelector\\n\\ntic_fwd = time()\\nsfs_forward = SequentialFeatureSelector(\\n    ridge, n_features_to_select=2, direction=\"forward\"\\n).fit(X, y)\\ntoc_fwd = time()\\n\\ntic_bwd = time()\\nsfs_backward = SequentialFeatureSelector(\\n    ridge, n_features_to_select=2, direction=\"backward\"\\n).fit(X, y)\\ntoc_bwd = time()\\n\\nprint(\\n    \"Features selected by forward sequential selection: \"\\n    f\"{feature_names[sfs_forward.get_support()]}\"\\n)\\nprint(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\\nprint(\\n    \"Features selected by backward sequential selection: \"\\n    f\"{feature_names[sfs_backward.get_support()]}\"\\n)\\nprint(f\"Done in {toc_bwd - tic_bwd:.3f}s\")'), Document(metadata={'source': '/content/local_copy_repo/examples/feature_selection/plot_select_from_model_diabetes.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Interestingly, forward and backward selection have selected the same set of\\n# features. In general, this isn\\'t the case and the two methods would lead to\\n# different results.\\n#\\n# We also note that the features selected by SFS differ from those selected by\\n# feature importance: SFS selects `bmi` instead of `s1`. This does sound\\n# reasonable though, since `bmi` corresponds to the third most important\\n# feature according to the coefficients. It is quite remarkable considering\\n# that SFS makes no use of the coefficients at all.\\n#\\n# To finish with, we should note that\\n# :class:`~sklearn.feature_selection.SelectFromModel` is significantly faster\\n# than SFS. Indeed, :class:`~sklearn.feature_selection.SelectFromModel` only\\n# needs to fit a model once, while SFS needs to cross-validate many different\\n# models for each of the iterations. SFS however works with any model, while\\n# :class:`~sklearn.feature_selection.SelectFromModel` requires the underlying\\n# estimator to expose a `coef_` attribute or a `feature_importances_`\\n# attribute. The forward SFS is faster than the backward SFS because it only\\n# needs to perform `n_features_to_select = 2` iterations, while the backward\\n# SFS needs to perform `n_features - n_features_to_select = 8` iterations.\\n#\\n# Using negative tolerance values\\n# -------------------------------\\n#\\n# :class:`~sklearn.feature_selection.SequentialFeatureSelector` can be used\\n# to remove features present in the dataset and return a\\n# smaller subset of the original features with `direction=\"backward\"`\\n# and a negative value of `tol`.\\n#\\n# We begin by loading the Breast Cancer dataset, consisting of 30 different\\n# features and 569 samples.\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_breast_cancer\\n\\nbreast_cancer_data = load_breast_cancer()\\nX, y = breast_cancer_data.data, breast_cancer_data.target\\nfeature_names = np.array(breast_cancer_data.feature_names)\\nprint(breast_cancer_data.DESCR)\\n\\n# %%\\n# We will make use of the :class:`~sklearn.linear_model.LogisticRegression`\\n# estimator with :class:`~sklearn.feature_selection.SequentialFeatureSelector`\\n# to perform the feature selection.\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import roc_auc_score\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nfor tol in [-1e-2, -1e-3, -1e-4]:\\n    start = time()\\n    feature_selector = SequentialFeatureSelector(\\n        LogisticRegression(),\\n        n_features_to_select=\"auto\",\\n        direction=\"backward\",\\n        scoring=\"roc_auc\",\\n        tol=tol,\\n        n_jobs=2,\\n    )\\n    model = make_pipeline(StandardScaler(), feature_selector, LogisticRegression())\\n    model.fit(X, y)\\n    end = time()\\n    print(f\"\\\\ntol: {tol}\")\\n    print(f\"Features selected: {feature_names[model[1].get_support()]}\")\\n    print(f\"ROC AUC score: {roc_auc_score(y, model.predict_proba(X)[:, 1]):.3f}\")\\n    print(f\"Done in {end - start:.3f}s\")'), Document(metadata={'source': '/content/local_copy_repo/examples/feature_selection/plot_select_from_model_diabetes.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We can see that the number of features selected tend to increase as negative\\n# values of `tol` approach to zero. The time taken for feature selection also\\n# decreases as the values of `tol` come closer to zero.'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_2_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# ruff: noqa\\n\"\"\"\\n=======================================\\nRelease Highlights for scikit-learn 1.2\\n=======================================\\n\\n.. currentmodule:: sklearn\\n\\nWe are pleased to announce the release of scikit-learn 1.2! Many bug fixes\\nand improvements were added, as well as some new key features. We detail\\nbelow a few of the major features of this release. **For an exhaustive list of\\nall the changes**, please refer to the :ref:`release notes <release_notes_1_2>`.\\n\\nTo install the latest version (with pip)::\\n\\n    pip install --upgrade scikit-learn\\n\\nor with conda::\\n\\n    conda install -c conda-forge scikit-learn\\n\\n\"\"\"\\n\\n# %%\\n# Pandas output with `set_output` API\\n# -----------------------------------\\n# scikit-learn\\'s transformers now support pandas output with the `set_output` API.\\n# To learn more about the `set_output` API see the example:\\n# :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` and\\n# # this `video, pandas DataFrame output for scikit-learn transformers\\n# (some examples) <https://youtu.be/5bCg8VfX2x8>`__.\\n\\nimport numpy as np\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.preprocessing import StandardScaler, KBinsDiscretizer\\nfrom sklearn.compose import ColumnTransformer\\n\\nX, y = load_iris(as_frame=True, return_X_y=True)\\nsepal_cols = [\"sepal length (cm)\", \"sepal width (cm)\"]\\npetal_cols = [\"petal length (cm)\", \"petal width (cm)\"]\\n\\npreprocessor = ColumnTransformer(\\n    [\\n        (\"scaler\", StandardScaler(), sepal_cols),\\n        (\"kbin\", KBinsDiscretizer(encode=\"ordinal\"), petal_cols),\\n    ],\\n    verbose_feature_names_out=False,\\n).set_output(transform=\"pandas\")\\n\\nX_out = preprocessor.fit_transform(X)\\nX_out.sample(n=5, random_state=0)\\n\\n# %%\\n# Interaction constraints in Histogram-based Gradient Boosting Trees\\n# ------------------------------------------------------------------\\n# :class:`~ensemble.HistGradientBoostingRegressor` and\\n# :class:`~ensemble.HistGradientBoostingClassifier` now supports interaction constraints\\n# with the `interaction_cst` parameter. For details, see the\\n# :ref:`User Guide <interaction_cst_hgbt>`. In the following example, features are not\\n# allowed to interact.\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\n\\nX, y = load_diabetes(return_X_y=True, as_frame=True)\\n\\nhist_no_interact = HistGradientBoostingRegressor(\\n    interaction_cst=[[i] for i in range(X.shape[1])], random_state=0\\n)\\nhist_no_interact.fit(X, y)\\n\\n# %%\\n# New and enhanced displays\\n# -------------------------\\n# :class:`~metrics.PredictionErrorDisplay` provides a way to analyze regression\\n# models in a qualitative manner.\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import PredictionErrorDisplay\\n\\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\\n_ = PredictionErrorDisplay.from_estimator(\\n    hist_no_interact, X, y, kind=\"actual_vs_predicted\", ax=axs[0]\\n)\\n_ = PredictionErrorDisplay.from_estimator(\\n    hist_no_interact, X, y, kind=\"residual_vs_predicted\", ax=axs[1]\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_2_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\\n_ = PredictionErrorDisplay.from_estimator(\\n    hist_no_interact, X, y, kind=\"actual_vs_predicted\", ax=axs[0]\\n)\\n_ = PredictionErrorDisplay.from_estimator(\\n    hist_no_interact, X, y, kind=\"residual_vs_predicted\", ax=axs[1]\\n)\\n\\n# %%\\n# :class:`~model_selection.LearningCurveDisplay` is now available to plot\\n# results from :func:`~model_selection.learning_curve`.\\nfrom sklearn.model_selection import LearningCurveDisplay\\n\\n_ = LearningCurveDisplay.from_estimator(\\n    hist_no_interact, X, y, cv=5, n_jobs=2, train_sizes=np.linspace(0.1, 1, 5)\\n)\\n\\n# %%\\n# :class:`~inspection.PartialDependenceDisplay` exposes a new parameter\\n# `categorical_features` to display partial dependence for categorical features\\n# using bar plots and heatmaps.\\nfrom sklearn.datasets import fetch_openml\\n\\nX, y = fetch_openml(\\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\\n)\\nX = X.select_dtypes([\"number\", \"category\"]).drop(columns=[\"body\"])\\n\\n# %%\\nfrom sklearn.preprocessing import OrdinalEncoder\\nfrom sklearn.pipeline import make_pipeline\\n\\ncategorical_features = [\"pclass\", \"sex\", \"embarked\"]\\nmodel = make_pipeline(\\n    ColumnTransformer(\\n        transformers=[(\"cat\", OrdinalEncoder(), categorical_features)],\\n        remainder=\"passthrough\",\\n    ),\\n    HistGradientBoostingRegressor(random_state=0),\\n).fit(X, y)\\n\\n# %%\\nfrom sklearn.inspection import PartialDependenceDisplay\\n\\nfig, ax = plt.subplots(figsize=(14, 4), constrained_layout=True)\\n_ = PartialDependenceDisplay.from_estimator(\\n    model,\\n    X,\\n    features=[\"age\", \"sex\", (\"pclass\", \"sex\")],\\n    categorical_features=categorical_features,\\n    ax=ax,\\n)\\n\\n# %%\\n# Faster parser in :func:`~datasets.fetch_openml`\\n# -----------------------------------------------\\n# :func:`~datasets.fetch_openml` now supports a new `\"pandas\"` parser that is\\n# more memory and CPU efficient. In v1.4, the default will change to\\n# `parser=\"auto\"` which will automatically use the `\"pandas\"` parser for dense\\n# data and `\"liac-arff\"` for sparse data.\\nX, y = fetch_openml(\\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\\n)\\nX.head()\\n\\n# %%\\n# Experimental Array API support in :class:`~discriminant_analysis.LinearDiscriminantAnalysis`\\n# --------------------------------------------------------------------------------------------\\n# Experimental support for the `Array API <https://data-apis.org/array-api/latest/>`_\\n# specification was added to :class:`~discriminant_analysis.LinearDiscriminantAnalysis`.\\n# The estimator can now run on any Array API compliant libraries such as\\n# `CuPy <https://docs.cupy.dev/en/stable/overview.html>`__, a GPU-accelerated array\\n# library. For details, see the :ref:`User Guide <array_api>`.'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_2_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Improved efficiency of many estimators\\n# --------------------------------------\\n# In version 1.1 the efficiency of many estimators relying on the computation of\\n# pairwise distances (essentially estimators related to clustering, manifold\\n# learning and neighbors search algorithms) was greatly improved for float64\\n# dense input. Efficiency improvement especially were a reduced memory footprint\\n# and a much better scalability on multi-core machines.\\n# In version 1.2, the efficiency of these estimators was further improved for all\\n# combinations of dense and sparse inputs on float32 and float64 datasets, except\\n# the sparse-dense and dense-sparse combinations for the Euclidean and Squared\\n# Euclidean Distance metrics.\\n# A detailed list of the impacted estimators can be found in the\\n# :ref:`changelog <release_notes_1_2>`.'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_22_0.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def test_sklearn_compatible_estimator(estimator, check):\\n    check(estimator)'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_22_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n========================================\\nRelease Highlights for scikit-learn 0.22\\n========================================\\n\\n.. currentmodule:: sklearn\\n\\nWe are pleased to announce the release of scikit-learn 0.22, which comes\\nwith many bug fixes and new features! We detail below a few of the major\\nfeatures of this release. For an exhaustive list of all the changes, please\\nrefer to the :ref:`release notes <release_notes_0_22>`.\\n\\nTo install the latest version (with pip)::\\n\\n    pip install --upgrade scikit-learn\\n\\nor with conda::\\n\\n    conda install -c conda-forge scikit-learn\\n\\n\"\"\"\\n\\n# %%\\n# New plotting API\\n# ----------------\\n#\\n# A new plotting API is available for creating visualizations. This new API\\n# allows for quickly adjusting the visuals of a plot without involving any\\n# recomputation. It is also possible to add different plots to the same\\n# figure. The following example illustrates `plot_roc_curve`,\\n# but other plots utilities are supported like\\n# `plot_partial_dependence`,\\n# `plot_precision_recall_curve`, and\\n# `plot_confusion_matrix`. Read more about this new API in the\\n# :ref:`User Guide <visualizations>`.\\n\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\n# from sklearn.metrics import plot_roc_curve\\nfrom sklearn.metrics import RocCurveDisplay\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\nfrom sklearn.utils.fixes import parse_version\\n\\nX, y = make_classification(random_state=0)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n\\nsvc = SVC(random_state=42)\\nsvc.fit(X_train, y_train)\\nrfc = RandomForestClassifier(random_state=42)\\nrfc.fit(X_train, y_train)\\n\\n# plot_roc_curve has been removed in version 1.2. From 1.2, use RocCurveDisplay instead.\\n# svc_disp = plot_roc_curve(svc, X_test, y_test)\\n# rfc_disp = plot_roc_curve(rfc, X_test, y_test, ax=svc_disp.ax_)\\nsvc_disp = RocCurveDisplay.from_estimator(svc, X_test, y_test)\\nrfc_disp = RocCurveDisplay.from_estimator(rfc, X_test, y_test, ax=svc_disp.ax_)\\nrfc_disp.figure_.suptitle(\"ROC curve comparison\")\\n\\nplt.show()\\n\\n# %%\\n# Stacking Classifier and Regressor\\n# ---------------------------------\\n# :class:`~ensemble.StackingClassifier` and\\n# :class:`~ensemble.StackingRegressor`\\n# allow you to have a stack of estimators with a final classifier or\\n# a regressor.\\n# Stacked generalization consists in stacking the output of individual\\n# estimators and use a classifier to compute the final prediction. Stacking\\n# allows to use the strength of each individual estimator by using their output\\n# as input of a final estimator.\\n# Base estimators are fitted on the full ``X`` while\\n# the final estimator is trained using cross-validated predictions of the\\n# base estimators using ``cross_val_predict``.\\n#\\n# Read more in the :ref:`User Guide <stacking>`.'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_22_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.datasets import load_iris\\nfrom sklearn.ensemble import StackingClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import LinearSVC\\n\\nX, y = load_iris(return_X_y=True)\\nestimators = [\\n    (\"rf\", RandomForestClassifier(n_estimators=10, random_state=42)),\\n    (\"svr\", make_pipeline(StandardScaler(), LinearSVC(dual=\"auto\", random_state=42))),\\n]\\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\\nclf.fit(X_train, y_train).score(X_test, y_test)\\n\\n# %%\\n# Permutation-based feature importance\\n# ------------------------------------\\n#\\n# The :func:`inspection.permutation_importance` can be used to get an\\n# estimate of the importance of each feature, for any fitted estimator:\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.inspection import permutation_importance\\n\\nX, y = make_classification(random_state=0, n_features=5, n_informative=3)\\nfeature_names = np.array([f\"x_{i}\" for i in range(X.shape[1])])\\n\\nrf = RandomForestClassifier(random_state=0).fit(X, y)\\nresult = permutation_importance(rf, X, y, n_repeats=10, random_state=0, n_jobs=2)\\n\\nfig, ax = plt.subplots()\\nsorted_idx = result.importances_mean.argsort()\\n\\n# `labels` argument in boxplot is deprecated in matplotlib 3.9 and has been\\n# renamed to `tick_labels`. The following code handles this, but as a\\n# scikit-learn user you probably can write simpler code by using `labels=...`\\n# (matplotlib < 3.9) or `tick_labels=...` (matplotlib >= 3.9).\\ntick_labels_parameter_name = (\\n    \"tick_labels\"\\n    if parse_version(matplotlib.__version__) >= parse_version(\"3.9\")\\n    else \"labels\"\\n)\\ntick_labels_dict = {tick_labels_parameter_name: feature_names[sorted_idx]}\\nax.boxplot(result.importances[sorted_idx].T, vert=False, **tick_labels_dict)\\nax.set_title(\"Permutation Importance of each feature\")\\nax.set_ylabel(\"Features\")\\nfig.tight_layout()\\nplt.show()\\n\\n# %%\\n# Native support for missing values for gradient boosting\\n# -------------------------------------------------------\\n#\\n# The :class:`ensemble.HistGradientBoostingClassifier`\\n# and :class:`ensemble.HistGradientBoostingRegressor` now have native\\n# support for missing values (NaNs). This means that there is no need for\\n# imputing data when training or predicting.\\n\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\nX = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\\ny = [0, 0, 1, 1]\\n\\ngbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\\nprint(gbdt.predict(X))'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_22_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.ensemble import HistGradientBoostingClassifier\\n\\nX = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\\ny = [0, 0, 1, 1]\\n\\ngbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\\nprint(gbdt.predict(X))\\n\\n# %%\\n# Precomputed sparse nearest neighbors graph\\n# ------------------------------------------\\n# Most estimators based on nearest neighbors graphs now accept precomputed\\n# sparse graphs as input, to reuse the same graph for multiple estimator fits.\\n# To use this feature in a pipeline, one can use the `memory` parameter, along\\n# with one of the two new transformers,\\n# :class:`neighbors.KNeighborsTransformer` and\\n# :class:`neighbors.RadiusNeighborsTransformer`. The precomputation\\n# can also be performed by custom estimators to use alternative\\n# implementations, such as approximate nearest neighbors methods.\\n# See more details in the :ref:`User Guide <neighbors_transformer>`.\\n\\nfrom tempfile import TemporaryDirectory\\n\\nfrom sklearn.manifold import Isomap\\nfrom sklearn.neighbors import KNeighborsTransformer\\nfrom sklearn.pipeline import make_pipeline\\n\\nX, y = make_classification(random_state=0)\\n\\nwith TemporaryDirectory(prefix=\"sklearn_cache_\") as tmpdir:\\n    estimator = make_pipeline(\\n        KNeighborsTransformer(n_neighbors=10, mode=\"distance\"),\\n        Isomap(n_neighbors=10, metric=\"precomputed\"),\\n        memory=tmpdir,\\n    )\\n    estimator.fit(X)\\n\\n    # We can decrease the number of neighbors and the graph will not be\\n    # recomputed.\\n    estimator.set_params(isomap__n_neighbors=5)\\n    estimator.fit(X)\\n\\n# %%\\n# KNN Based Imputation\\n# ------------------------------------\\n# We now support imputation for completing missing values using k-Nearest\\n# Neighbors.\\n#\\n# Each sample\\'s missing values are imputed using the mean value from\\n# ``n_neighbors`` nearest neighbors found in the training set. Two samples are\\n# close if the features that neither is missing are close.\\n# By default, a euclidean distance metric\\n# that supports missing values,\\n# :func:`~sklearn.metrics.pairwise.nan_euclidean_distances`, is used to find the nearest\\n# neighbors.\\n#\\n# Read more in the :ref:`User Guide <knnimpute>`.\\n\\nfrom sklearn.impute import KNNImputer\\n\\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\\nimputer = KNNImputer(n_neighbors=2)\\nprint(imputer.fit_transform(X))\\n\\n# %%\\n# Tree pruning\\n# ------------\\n#\\n# It is now possible to prune most tree-based estimators once the trees are\\n# built. The pruning is based on minimal cost-complexity. Read more in the\\n# :ref:`User Guide <minimal_cost_complexity_pruning>` for details.\\n\\nX, y = make_classification(random_state=0)\\n\\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0).fit(X, y)\\nprint(\\n    \"Average number of nodes without pruning {:.1f}\".format(\\n        np.mean([e.tree_.node_count for e in rf.estimators_])\\n    )\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_22_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X, y = make_classification(random_state=0)\\n\\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0).fit(X, y)\\nprint(\\n    \"Average number of nodes without pruning {:.1f}\".format(\\n        np.mean([e.tree_.node_count for e in rf.estimators_])\\n    )\\n)\\n\\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0.05).fit(X, y)\\nprint(\\n    \"Average number of nodes with pruning {:.1f}\".format(\\n        np.mean([e.tree_.node_count for e in rf.estimators_])\\n    )\\n)\\n\\n# %%\\n# Retrieve dataframes from OpenML\\n# -------------------------------\\n# :func:`datasets.fetch_openml` can now return pandas dataframe and thus\\n# properly handle datasets with heterogeneous data:\\n\\nfrom sklearn.datasets import fetch_openml\\n\\ntitanic = fetch_openml(\"titanic\", version=1, as_frame=True, parser=\"pandas\")\\nprint(titanic.data.head()[[\"pclass\", \"embarked\"]])\\n\\n# %%\\n# Checking scikit-learn compatibility of an estimator\\n# ---------------------------------------------------\\n# Developers can check the compatibility of their scikit-learn compatible\\n# estimators using :func:`~utils.estimator_checks.check_estimator`. For\\n# instance, the ``check_estimator(LinearSVC())`` passes.\\n#\\n# We now provide a ``pytest`` specific decorator which allows ``pytest``\\n# to run all checks independently and report the checks that are failing.\\n#\\n# ..note::\\n#   This entry was slightly updated in version 0.24, where passing classes\\n#   isn\\'t supported anymore: pass instances instead.\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.utils.estimator_checks import parametrize_with_checks\\n\\n\\n@parametrize_with_checks([LogisticRegression(), DecisionTreeRegressor()])\\n# Code for: def test_sklearn_compatible_estimator(estimator, check):\\n\\n\\n# %%\\n# ROC AUC now supports multiclass classification\\n# ----------------------------------------------\\n# The :func:`~sklearn.metrics.roc_auc_score` function can also be used in multi-class\\n# classification. Two averaging strategies are currently supported: the\\n# one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and\\n# the one-vs-rest algorithm computes the average of the ROC AUC scores for each\\n# class against all other classes. In both cases, the multiclass ROC AUC scores\\n# are computed from the probability estimates that a sample belongs to a\\n# particular class according to the model. The OvO and OvR algorithms support\\n# weighting uniformly (``average=\\'macro\\'``) and weighting by the prevalence\\n# (``average=\\'weighted\\'``).\\n#\\n# Read more in the :ref:`User Guide <roc_metrics>`.\\n\\n\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.metrics import roc_auc_score\\nfrom sklearn.svm import SVC\\n\\nX, y = make_classification(n_classes=4, n_informative=16)\\nclf = SVC(decision_function_shape=\"ovo\", probability=True).fit(X, y)\\nprint(roc_auc_score(y, clf.predict_proba(X), multi_class=\"ovo\"))'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_5_0.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def custom_score(y_observed, y_pred):\\n    tn, fp, fn, tp = confusion_matrix(y_observed, y_pred, normalize=\"all\").ravel()\\n    return tp - 2 * fn - 0.1 * fp'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_5_0.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def smallest_abs(arr):\\n    \"\"\"Return the smallest absolute value of a 1D array.\"\"\"\\n    return np.min(np.abs(arr))'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_5_0.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def levenshtein_distance(x, y):\\n    \"\"\"Return the Levenshtein distance between two strings.\"\"\"\\n    if x == \"\" or y == \"\":\\n        return max(len(x), len(y))\\n    if x[0] == y[0]:\\n        return levenshtein_distance(x[1:], y[1:])\\n    return 1 + min(\\n        levenshtein_distance(x[1:], y),\\n        levenshtein_distance(x, y[1:]),\\n        levenshtein_distance(x[1:], y[1:]),\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_5_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# ruff: noqa\\n\"\"\"\\n=======================================\\nRelease Highlights for scikit-learn 1.5\\n=======================================\\n\\n.. currentmodule:: sklearn\\n\\nWe are pleased to announce the release of scikit-learn 1.5! Many bug fixes\\nand improvements were added, as well as some key new features. Below we\\ndetail the highlights of this release. **For an exhaustive list of\\nall the changes**, please refer to the :ref:`release notes <release_notes_1_5>`.\\n\\nTo install the latest version (with pip)::\\n\\n    pip install --upgrade scikit-learn\\n\\nor with conda::\\n\\n    conda install -c conda-forge scikit-learn\\n\\n\"\"\"\\n\\n# %%\\n# FixedThresholdClassifier: Setting the decision threshold of a binary classifier\\n# -------------------------------------------------------------------------------\\n# All binary classifiers of scikit-learn use a fixed decision threshold of 0.5\\n# to convert probability estimates (i.e. output of `predict_proba`) into class\\n# predictions. However, 0.5 is almost never the desired threshold for a given\\n# problem. :class:`~model_selection.FixedThresholdClassifier` allows wrapping any\\n# binary classifier and setting a custom decision threshold.\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import ConfusionMatrixDisplay\\n\\n\\nX, y = make_classification(n_samples=10_000, weights=[0.9, 0.1], random_state=0)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\nclassifier_05 = LogisticRegression(C=1e6, random_state=0).fit(X_train, y_train)\\n_ = ConfusionMatrixDisplay.from_estimator(classifier_05, X_test, y_test)\\n\\n# %%\\n# Lowering the threshold, i.e. allowing more samples to be classified as the positive\\n# class, increases the number of true positives at the cost of more false positives\\n# (as is well known from the concavity of the ROC curve).\\nfrom sklearn.model_selection import FixedThresholdClassifier\\n\\nclassifier_01 = FixedThresholdClassifier(classifier_05, threshold=0.1)\\nclassifier_01.fit(X_train, y_train)\\n_ = ConfusionMatrixDisplay.from_estimator(classifier_01, X_test, y_test)'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_5_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='classifier_01 = FixedThresholdClassifier(classifier_05, threshold=0.1)\\nclassifier_01.fit(X_train, y_train)\\n_ = ConfusionMatrixDisplay.from_estimator(classifier_01, X_test, y_test)\\n\\n# %%\\n# TunedThresholdClassifierCV: Tuning the decision threshold of a binary classifier\\n# --------------------------------------------------------------------------------\\n# The decision threshold of a binary classifier can be tuned to optimize a\\n# given metric, using :class:`~model_selection.TunedThresholdClassifierCV`.\\n#\\n# It is particularly useful to find the best decision threshold when the model\\n# is meant to be deployed in a specific application context where we can assign\\n# different gains or costs for true positives, true negatives, false positives,\\n# and false negatives.\\n#\\n# Let\\'s illustrate this by considering an arbitrary case where:\\n#\\n# - each true positive gains 1 unit of profit, e.g. euro, year of life in good\\n#   health, etc.;\\n# - true negatives gain or cost nothing;\\n# - each false negative costs 2;\\n# - each false positive costs 0.1.\\n#\\n# Our metric quantifies the average profit per sample, which is defined by the\\n# following Python function:\\nfrom sklearn.metrics import confusion_matrix\\n\\n\\n# Code for: def custom_score(y_observed, y_pred):\\n\\n\\nprint(\"Untuned decision threshold: 0.5\")\\nprint(f\"Custom score: {custom_score(y_test, classifier_05.predict(X_test)):.2f}\")\\n\\n# %%\\n# It is interesting to observe that the average gain per prediction is negative\\n# which means that this decision system is making a loss on average.\\n#\\n# Tuning the threshold to optimize this custom metric gives a smaller threshold\\n# that allows more samples to be classified as the positive class. As a result,\\n# the average gain per prediction improves.\\nfrom sklearn.model_selection import TunedThresholdClassifierCV\\nfrom sklearn.metrics import make_scorer\\n\\ncustom_scorer = make_scorer(\\n    custom_score, response_method=\"predict\", greater_is_better=True\\n)\\ntuned_classifier = TunedThresholdClassifierCV(\\n    classifier_05, cv=5, scoring=custom_scorer\\n).fit(X, y)\\n\\nprint(f\"Tuned decision threshold: {tuned_classifier.best_threshold_:.3f}\")\\nprint(f\"Custom score: {custom_score(y_test, tuned_classifier.predict(X_test)):.2f}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_5_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(f\"Tuned decision threshold: {tuned_classifier.best_threshold_:.3f}\")\\nprint(f\"Custom score: {custom_score(y_test, tuned_classifier.predict(X_test)):.2f}\")\\n\\n# %%\\n# We observe that tuning the decision threshold can turn a machine\\n# learning-based system that makes a loss on average into a beneficial one.\\n#\\n# In practice, defining a meaningful application-specific metric might involve\\n# making those costs for bad predictions and gains for good predictions depend on\\n# auxiliary metadata specific to each individual data point such as the amount\\n# of a transaction in a fraud detection system.\\n#\\n# To achieve this, :class:`~model_selection.TunedThresholdClassifierCV`\\n# leverages metadata routing support (:ref:`Metadata Routing User\\n# Guide<metadata_routing>`) allowing to optimize complex business metrics as\\n# detailed in :ref:`Post-tuning the decision threshold for cost-sensitive\\n# learning\\n# <sphx_glr_auto_examples_model_selection_plot_cost_sensitive_learning.py>`.\\n\\n# %%\\n# Performance improvements in PCA\\n# -------------------------------\\n# :class:`~decomposition.PCA` has a new solver, `\"covariance_eigh\"`, which is\\n# up to an order of magnitude faster and more memory efficient than the other\\n# solvers for datasets with many data points and few features.\\nfrom sklearn.datasets import make_low_rank_matrix\\nfrom sklearn.decomposition import PCA\\n\\nX = make_low_rank_matrix(\\n    n_samples=10_000, n_features=100, tail_strength=0.1, random_state=0\\n)\\n\\npca = PCA(n_components=10, svd_solver=\"covariance_eigh\").fit(X)\\nprint(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2f}\")\\n\\n\\n# %%\\n# The new solver also accepts sparse input data:\\nfrom scipy.sparse import random\\n\\nX = random(10_000, 100, format=\"csr\", random_state=0)\\n\\npca = PCA(n_components=10, svd_solver=\"covariance_eigh\").fit(X)\\nprint(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2f}\")\\n\\n# %%\\n# The `\"full\"` solver has also been improved to use less memory and allows\\n# faster transformation. The default `svd_solver=\"auto\"`` option takes\\n# advantage of the new solver and is now able to select an appropriate solver\\n# for sparse datasets.\\n#\\n# Similarly to most other PCA solvers, the new `\"covariance_eigh\"` solver can leverage\\n# GPU computation if the input data is passed as a PyTorch or CuPy array by\\n# enabling the experimental support for :ref:`Array API <array_api>`.\\n\\n# %%\\n# ColumnTransformer is subscriptable\\n# ----------------------------------\\n# The transformers of a :class:`~compose.ColumnTransformer` can now be directly\\n# accessed using indexing by name.\\nimport numpy as np\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\\n\\nX = np.array([[0, 1, 2], [3, 4, 5]])\\ncolumn_transformer = ColumnTransformer(\\n    [(\"std_scaler\", StandardScaler(), [0]), (\"one_hot\", OneHotEncoder(), [1, 2])]\\n)\\n\\ncolumn_transformer.fit(X)\\n\\nprint(column_transformer[\"std_scaler\"])\\nprint(column_transformer[\"one_hot\"])'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_5_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X = np.array([[0, 1, 2], [3, 4, 5]])\\ncolumn_transformer = ColumnTransformer(\\n    [(\"std_scaler\", StandardScaler(), [0]), (\"one_hot\", OneHotEncoder(), [1, 2])]\\n)\\n\\ncolumn_transformer.fit(X)\\n\\nprint(column_transformer[\"std_scaler\"])\\nprint(column_transformer[\"one_hot\"])\\n\\n# %%\\n# Custom imputation strategies for the SimpleImputer\\n# --------------------------------------------------\\n# :class:`~impute.SimpleImputer` now supports custom strategies for imputation,\\n# using a callable that computes a scalar value from the non missing values of\\n# a column vector.\\nfrom sklearn.impute import SimpleImputer\\n\\nX = np.array(\\n    [\\n        [-1.1, 1.1, 1.1],\\n        [3.9, -1.2, np.nan],\\n        [np.nan, 1.3, np.nan],\\n        [-0.1, -1.4, -1.4],\\n        [-4.9, 1.5, -1.5],\\n        [np.nan, 1.6, 1.6],\\n    ]\\n)\\n\\n\\n# Code for: def smallest_abs(arr):\\n\\n\\nimputer = SimpleImputer(strategy=smallest_abs)\\n\\nimputer.fit_transform(X)\\n\\n# %%\\n# Pairwise distances with non-numeric arrays\\n# ------------------------------------------\\n# :func:`~metrics.pairwise_distances` can now compute distances between\\n# non-numeric arrays using a callable metric.\\nfrom sklearn.metrics import pairwise_distances\\n\\nX = [\"cat\", \"dog\"]\\nY = [\"cat\", \"fox\"]\\n\\n\\n# Code for: def levenshtein_distance(x, y):\\n\\n\\npairwise_distances(X, Y, metric=levenshtein_distance)'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_24_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# ruff: noqa\\n\"\"\"\\n========================================\\nRelease Highlights for scikit-learn 0.24\\n========================================\\n\\n.. currentmodule:: sklearn\\n\\nWe are pleased to announce the release of scikit-learn 0.24! Many bug fixes\\nand improvements were added, as well as some new key features. We detail\\nbelow a few of the major features of this release. **For an exhaustive list of\\nall the changes**, please refer to the :ref:`release notes <release_notes_0_24>`.\\n\\nTo install the latest version (with pip)::\\n\\n    pip install --upgrade scikit-learn\\n\\nor with conda::\\n\\n    conda install -c conda-forge scikit-learn\\n\\n\"\"\"\\n\\n##############################################################################\\n# Successive Halving estimators for tuning hyper-parameters\\n# ---------------------------------------------------------\\n# Successive Halving, a state of the art method, is now available to\\n# explore the space of the parameters and identify their best combination.\\n# :class:`~sklearn.model_selection.HalvingGridSearchCV` and\\n# :class:`~sklearn.model_selection.HalvingRandomSearchCV` can be\\n# used as drop-in replacement for\\n# :class:`~sklearn.model_selection.GridSearchCV` and\\n# :class:`~sklearn.model_selection.RandomizedSearchCV`.\\n# Successive Halving is an iterative selection process illustrated in the\\n# figure below. The first iteration is run with a small amount of resources,\\n# where the resource typically corresponds to the number of training samples,\\n# but can also be an arbitrary integer parameter such as `n_estimators` in a\\n# random forest. Only a subset of the parameter candidates are selected for the\\n# next iteration, which will be run with an increasing amount of allocated\\n# resources. Only a subset of candidates will last until the end of the\\n# iteration process, and the best parameter candidate is the one that has the\\n# highest score on the last iteration.\\n#\\n# Read more in the :ref:`User Guide <successive_halving_user_guide>` (note:\\n# the Successive Halving estimators are still :term:`experimental\\n# <experimental>`).\\n#\\n# .. figure:: ../model_selection/images/sphx_glr_plot_successive_halving_iterations_001.png\\n#   :target: ../model_selection/plot_successive_halving_iterations.html\\n#   :align: center\\n\\nimport numpy as np\\nfrom scipy.stats import randint\\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\\nfrom sklearn.model_selection import HalvingRandomSearchCV\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.datasets import make_classification\\n\\nrng = np.random.RandomState(0)\\n\\nX, y = make_classification(n_samples=700, random_state=rng)\\n\\nclf = RandomForestClassifier(n_estimators=10, random_state=rng)\\n\\nparam_dist = {\\n    \"max_depth\": [3, None],\\n    \"max_features\": randint(1, 11),\\n    \"min_samples_split\": randint(2, 11),\\n    \"bootstrap\": [True, False],\\n    \"criterion\": [\"gini\", \"entropy\"],\\n}'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_24_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='clf = RandomForestClassifier(n_estimators=10, random_state=rng)\\n\\nparam_dist = {\\n    \"max_depth\": [3, None],\\n    \"max_features\": randint(1, 11),\\n    \"min_samples_split\": randint(2, 11),\\n    \"bootstrap\": [True, False],\\n    \"criterion\": [\"gini\", \"entropy\"],\\n}\\n\\nrsh = HalvingRandomSearchCV(\\n    estimator=clf, param_distributions=param_dist, factor=2, random_state=rng\\n)\\nrsh.fit(X, y)\\nrsh.best_params_\\n\\n##############################################################################\\n# Native support for categorical features in HistGradientBoosting estimators\\n# --------------------------------------------------------------------------\\n# :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor` now have native\\n# support for categorical features: they can consider splits on non-ordered,\\n# categorical data. Read more in the :ref:`User Guide\\n# <categorical_support_gbdt>`.\\n#\\n# .. figure:: ../ensemble/images/sphx_glr_plot_gradient_boosting_categorical_001.png\\n#   :target: ../ensemble/plot_gradient_boosting_categorical.html\\n#   :align: center\\n#\\n# The plot shows that the new native support for categorical features leads to\\n# fitting times that are comparable to models where the categories are treated\\n# as ordered quantities, i.e. simply ordinal-encoded. Native support is also\\n# more expressive than both one-hot encoding and ordinal encoding. However, to\\n# use the new `categorical_features` parameter, it is still required to\\n# preprocess the data within a pipeline as demonstrated in this :ref:`example\\n# <sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py>`.\\n\\n##############################################################################\\n# Improved performances of HistGradientBoosting estimators\\n# --------------------------------------------------------\\n# The memory footprint of :class:`ensemble.HistGradientBoostingRegressor` and\\n# :class:`ensemble.HistGradientBoostingClassifier` has been significantly\\n# improved during calls to `fit`. In addition, histogram initialization is now\\n# done in parallel which results in slight speed improvements.\\n# See more in the `Benchmark page\\n# <https://scikit-learn.org/scikit-learn-benchmarks/>`_.\\n\\n##############################################################################\\n# New self-training meta-estimator\\n# --------------------------------\\n# A new self-training implementation, based on `Yarowski\\'s algorithm\\n# <https://doi.org/10.3115/981658.981684>`_ can now be used with any\\n# classifier that implements :term:`predict_proba`. The sub-classifier\\n# will behave as a\\n# semi-supervised classifier, allowing it to learn from unlabeled data.\\n# Read more in the :ref:`User guide <self_training>`.\\n\\nimport numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.semi_supervised import SelfTrainingClassifier\\nfrom sklearn.svm import SVC'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_24_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.semi_supervised import SelfTrainingClassifier\\nfrom sklearn.svm import SVC\\n\\nrng = np.random.RandomState(42)\\niris = datasets.load_iris()\\nrandom_unlabeled_points = rng.rand(iris.target.shape[0]) < 0.3\\niris.target[random_unlabeled_points] = -1\\nsvc = SVC(probability=True, gamma=\"auto\")\\nself_training_model = SelfTrainingClassifier(svc)\\nself_training_model.fit(iris.data, iris.target)\\n\\n##############################################################################\\n# New SequentialFeatureSelector transformer\\n# -----------------------------------------\\n# A new iterative transformer to select features is available:\\n# :class:`~sklearn.feature_selection.SequentialFeatureSelector`.\\n# Sequential Feature Selection can add features one at a time (forward\\n# selection) or remove features from the list of the available features\\n# (backward selection), based on a cross-validated score maximization.\\n# See the :ref:`User Guide <sequential_feature_selection>`.\\n\\nfrom sklearn.feature_selection import SequentialFeatureSelector\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.datasets import load_iris\\n\\nX, y = load_iris(return_X_y=True, as_frame=True)\\nfeature_names = X.columns\\nknn = KNeighborsClassifier(n_neighbors=3)\\nsfs = SequentialFeatureSelector(knn, n_features_to_select=2)\\nsfs.fit(X, y)\\nprint(\\n    \"Features selected by forward sequential selection: \"\\n    f\"{feature_names[sfs.get_support()].tolist()}\"\\n)\\n\\n##############################################################################\\n# New PolynomialCountSketch kernel approximation function\\n# -------------------------------------------------------\\n# The new :class:`~sklearn.kernel_approximation.PolynomialCountSketch`\\n# approximates a polynomial expansion of a feature space when used with linear\\n# models, but uses much less memory than\\n# :class:`~sklearn.preprocessing.PolynomialFeatures`.\\n\\nfrom sklearn.datasets import fetch_covtype\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.kernel_approximation import PolynomialCountSketch\\nfrom sklearn.linear_model import LogisticRegression\\n\\nX, y = fetch_covtype(return_X_y=True)\\npipe = make_pipeline(\\n    MinMaxScaler(),\\n    PolynomialCountSketch(degree=2, n_components=300),\\n    LogisticRegression(max_iter=1000),\\n)\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, train_size=5000, test_size=10000, random_state=42\\n)\\npipe.fit(X_train, y_train).score(X_test, y_test)\\n\\n##############################################################################\\n# For comparison, here is the score of a linear baseline for the same data:\\n\\nlinear_baseline = make_pipeline(MinMaxScaler(), LogisticRegression(max_iter=1000))\\nlinear_baseline.fit(X_train, y_train).score(X_test, y_test)'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_24_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='##############################################################################\\n# For comparison, here is the score of a linear baseline for the same data:\\n\\nlinear_baseline = make_pipeline(MinMaxScaler(), LogisticRegression(max_iter=1000))\\nlinear_baseline.fit(X_train, y_train).score(X_test, y_test)\\n\\n##############################################################################\\n# Individual Conditional Expectation plots\\n# ----------------------------------------\\n# A new kind of partial dependence plot is available: the Individual\\n# Conditional Expectation (ICE) plot. ICE plots visualize the dependence of the\\n# prediction on a feature for each sample separately, with one line per sample.\\n# See the :ref:`User Guide <individual_conditional>`\\n\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.datasets import fetch_california_housing\\n\\n# from sklearn.inspection import plot_partial_dependence\\nfrom sklearn.inspection import PartialDependenceDisplay\\n\\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\\nfeatures = [\"MedInc\", \"AveOccup\", \"HouseAge\", \"AveRooms\"]\\nest = RandomForestRegressor(n_estimators=10)\\nest.fit(X, y)\\n\\n# plot_partial_dependence has been removed in version 1.2. From 1.2, use\\n# PartialDependenceDisplay instead.\\n# display = plot_partial_dependence(\\ndisplay = PartialDependenceDisplay.from_estimator(\\n    est,\\n    X,\\n    features,\\n    kind=\"individual\",\\n    subsample=50,\\n    n_jobs=3,\\n    grid_resolution=20,\\n    random_state=0,\\n)\\ndisplay.figure_.suptitle(\\n    \"Partial dependence of house value on non-location features\\\\n\"\\n    \"for the California housing dataset, with BayesianRidge\"\\n)\\ndisplay.figure_.subplots_adjust(hspace=0.3)\\n\\n##############################################################################\\n# New Poisson splitting criterion for DecisionTreeRegressor\\n# ---------------------------------------------------------\\n# The integration of Poisson regression estimation continues from version 0.23.\\n# :class:`~sklearn.tree.DecisionTreeRegressor` now supports a new `\\'poisson\\'`\\n# splitting criterion. Setting `criterion=\"poisson\"` might be a good choice\\n# if your target is a count or a frequency.\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\n\\nn_samples, n_features = 1000, 20\\nrng = np.random.RandomState(0)\\nX = rng.randn(n_samples, n_features)\\n# positive integer target correlated with X[:, 5] with many zeros:\\ny = rng.poisson(lam=np.exp(X[:, 5]) / 2)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\\nregressor = DecisionTreeRegressor(criterion=\"poisson\", random_state=0)\\nregressor.fit(X_train, y_train)'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_24_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='##############################################################################\\n# New documentation improvements\\n# ------------------------------\\n#\\n# New examples and documentation pages have been added, in a continuous effort\\n# to improve the understanding of machine learning practices:\\n#\\n# - a new section about :ref:`common pitfalls and recommended\\n#   practices <common_pitfalls>`,\\n# - an example illustrating how to :ref:`statistically compare the performance of\\n#   models <sphx_glr_auto_examples_model_selection_plot_grid_search_stats.py>`\\n#   evaluated using :class:`~sklearn.model_selection.GridSearchCV`,\\n# - an example on how to :ref:`interpret coefficients of linear models\\n#   <sphx_glr_auto_examples_inspection_plot_linear_model_coefficient_interpretation.py>`,\\n# - an :ref:`example\\n#   <sphx_glr_auto_examples_cross_decomposition_plot_pcr_vs_pls.py>`\\n#   comparing Principal Component Regression and Partial Least Squares.'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_0_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# ruff: noqa\\n\"\"\"\\n=======================================\\nRelease Highlights for scikit-learn 1.0\\n=======================================\\n\\n.. currentmodule:: sklearn\\n\\nWe are very pleased to announce the release of scikit-learn 1.0! The library\\nhas been stable for quite some time, releasing version 1.0 is recognizing that\\nand signalling it to our users. This release does not include any breaking\\nchanges apart from the usual two-release deprecation cycle. For the future, we\\ndo our best to keep this pattern.\\n\\nThis release includes some new key features as well as many improvements and\\nbug fixes. We detail below a few of the major features of this release. **For\\nan exhaustive list of all the changes**, please refer to the :ref:`release\\nnotes <release_notes_1_0>`.\\n\\nTo install the latest version (with pip)::\\n\\n    pip install --upgrade scikit-learn\\n\\nor with conda::\\n\\n    conda install -c conda-forge scikit-learn\\n\\n\"\"\"\\n\\n##############################################################################\\n# Keyword and positional arguments\\n# ---------------------------------------------------------\\n# The scikit-learn API exposes many functions and methods which have many input\\n# parameters. For example, before this release, one could instantiate a\\n# :class:`~ensemble.HistGradientBoostingRegressor` as::\\n#\\n#         HistGradientBoostingRegressor(\"squared_error\", 0.1, 100, 31, None,\\n#             20, 0.0, 255, None, None, False, \"auto\", \"loss\", 0.1, 10, 1e-7,\\n#             0, None)\\n#\\n# Understanding the above code requires the reader to go to the API\\n# documentation and to check each and every parameter for its position and\\n# its meaning. To improve the readability of code written based on scikit-learn,\\n# now users have to provide most parameters with their names, as keyword\\n# arguments, instead of positional arguments. For example, the above code would\\n# be::\\n#\\n#     HistGradientBoostingRegressor(\\n#         loss=\"squared_error\",\\n#         learning_rate=0.1,\\n#         max_iter=100,\\n#         max_leaf_nodes=31,\\n#         max_depth=None,\\n#         min_samples_leaf=20,\\n#         l2_regularization=0.0,\\n#         max_bins=255,\\n#         categorical_features=None,\\n#         monotonic_cst=None,\\n#         warm_start=False,\\n#         early_stopping=\"auto\",\\n#         scoring=\"loss\",\\n#         validation_fraction=0.1,\\n#         n_iter_no_change=10,\\n#         tol=1e-7,\\n#         verbose=0,\\n#         random_state=None,\\n#     )\\n#\\n# which is much more readable. Positional arguments have been deprecated since\\n# version 0.23 and will now raise a ``TypeError``. A limited number of\\n# positional arguments are still allowed in some cases, for example in\\n# :class:`~decomposition.PCA`, where ``PCA(10)`` is still allowed, but ``PCA(10,\\n# False)`` is not allowed.'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_0_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"##############################################################################\\n# Spline Transformers\\n# ---------------------------------------------------------\\n# One way to add nonlinear terms to a dataset's feature set is to generate\\n# spline basis functions for continuous/numerical features with the new\\n# :class:`~preprocessing.SplineTransformer`. Splines are piecewise polynomials,\\n# parametrized by their polynomial degree and the positions of the knots. The\\n# :class:`~preprocessing.SplineTransformer` implements a B-spline basis.\\n#\\n# .. figure:: ../linear_model/images/sphx_glr_plot_polynomial_interpolation_001.png\\n#   :target: ../linear_model/plot_polynomial_interpolation.html\\n#   :align: center\\n#\\n# The following code shows splines in action, for more information, please\\n# refer to the :ref:`User Guide <spline_transformer>`.\\n\\nimport numpy as np\\nfrom sklearn.preprocessing import SplineTransformer\\n\\nX = np.arange(5).reshape(5, 1)\\nspline = SplineTransformer(degree=2, n_knots=3)\\nspline.fit_transform(X)\\n\\n\\n##############################################################################\\n# Quantile Regressor\\n# --------------------------------------------------------------------------\\n# Quantile regression estimates the median or other quantiles of :math:`y`\\n# conditional on :math:`X`, while ordinary least squares (OLS) estimates the\\n# conditional mean.\\n#\\n# As a linear model, the new :class:`~linear_model.QuantileRegressor` gives\\n# linear predictions :math:`\\\\hat{y}(w, X) = Xw` for the :math:`q`-th quantile,\\n# :math:`q \\\\in (0, 1)`. The weights or coefficients :math:`w` are then found by\\n# the following minimization problem:\\n#\\n# .. math::\\n#     \\\\min_{w} {\\\\frac{1}{n_{\\\\text{samples}}}\\n#     \\\\sum_i PB_q(y_i - X_i w) + \\\\alpha ||w||_1}.\\n#\\n# This consists of the pinball loss (also known as linear loss),\\n# see also :class:`~sklearn.metrics.mean_pinball_loss`,\\n#\\n# .. math::\\n#     PB_q(t) = q \\\\max(t, 0) + (1 - q) \\\\max(-t, 0) =\\n#     \\\\begin{cases}\\n#         q t, & t > 0, \\\\\\\\\\n#         0,    & t = 0, \\\\\\\\\\n#         (1-q) t, & t < 0\\n#     \\\\end{cases}\\n#\\n# and the L1 penalty controlled by parameter ``alpha``, similar to\\n# :class:`linear_model.Lasso`.\\n#\\n# Please check the following example to see how it works, and the :ref:`User\\n# Guide <quantile_regression>` for more details.\\n#\\n# .. figure:: ../linear_model/images/sphx_glr_plot_quantile_regression_002.png\\n#    :target: ../linear_model/plot_quantile_regression.html\\n#    :align: center\\n#    :scale: 50%\"), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_0_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='##############################################################################\\n# Feature Names Support\\n# --------------------------------------------------------------------------\\n# When an estimator is passed a `pandas\\' dataframe\\n# <https://pandas.pydata.org/docs/user_guide/dsintro.html#dataframe>`_ during\\n# :term:`fit`, the estimator will set a `feature_names_in_` attribute\\n# containing the feature names. Note that feature names support is only enabled\\n# when the column names in the dataframe are all strings. `feature_names_in_`\\n# is used to check that the column names of the dataframe passed in\\n# non-:term:`fit`, such as :term:`predict`, are consistent with features in\\n# :term:`fit`:\\nfrom sklearn.preprocessing import StandardScaler\\nimport pandas as pd\\n\\nX = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=[\"a\", \"b\", \"c\"])\\nscalar = StandardScaler().fit(X)\\nscalar.feature_names_in_\\n\\n# %%\\n# The support of :term:`get_feature_names_out` is available for transformers\\n# that already had `get_feature_names` and transformers with a one-to-one\\n# correspondence between input and output such as\\n# :class:`~preprocessing.StandardScaler`. :term:`get_feature_names_out` support\\n# will be added to all other transformers in future releases. Additionally,\\n# :meth:`compose.ColumnTransformer.get_feature_names_out` is available to\\n# combine feature names of its transformers:\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import OneHotEncoder\\nimport pandas as pd\\n\\nX = pd.DataFrame({\"pet\": [\"dog\", \"cat\", \"fish\"], \"age\": [3, 7, 1]})\\npreprocessor = ColumnTransformer(\\n    [\\n        (\"numerical\", StandardScaler(), [\"age\"]),\\n        (\"categorical\", OneHotEncoder(), [\"pet\"]),\\n    ],\\n    verbose_feature_names_out=False,\\n).fit(X)\\n\\npreprocessor.get_feature_names_out()\\n\\n# %%\\n# When this ``preprocessor`` is used with a pipeline, the feature names used\\n# by the classifier are obtained by slicing and calling\\n# :term:`get_feature_names_out`:\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import make_pipeline\\n\\ny = [1, 0, 1]\\npipe = make_pipeline(preprocessor, LogisticRegression())\\npipe.fit(X, y)\\npipe[:-1].get_feature_names_out()\\n\\n\\n##############################################################################\\n# A more flexible plotting API\\n# --------------------------------------------------------------------------\\n# :class:`metrics.ConfusionMatrixDisplay`,\\n# :class:`metrics.PrecisionRecallDisplay`, :class:`metrics.DetCurveDisplay`,\\n# and :class:`inspection.PartialDependenceDisplay` now expose two class\\n# methods: `from_estimator` and `from_predictions` which allow users to create\\n# a plot given the predictions or an estimator. This means the corresponding\\n# `plot_*` functions are deprecated. Please check :ref:`example one\\n# <sphx_glr_auto_examples_model_selection_plot_confusion_matrix.py>` and\\n# :ref:`example two\\n# <sphx_glr_auto_examples_classification_plot_digits_classification.py>` for\\n# how to use the new plotting functionalities.'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_0_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"##############################################################################\\n# Online One-Class SVM\\n# --------------------------------------------------------------------------\\n# The new class :class:`~linear_model.SGDOneClassSVM` implements an online\\n# linear version of the One-Class SVM using a stochastic gradient descent.\\n# Combined with kernel approximation techniques,\\n# :class:`~linear_model.SGDOneClassSVM` can be used to approximate the solution\\n# of a kernelized One-Class SVM, implemented in :class:`~svm.OneClassSVM`, with\\n# a fit time complexity linear in the number of samples. Note that the\\n# complexity of a kernelized One-Class SVM is at best quadratic in the number\\n# of samples. :class:`~linear_model.SGDOneClassSVM` is thus well suited for\\n# datasets with a large number of training samples (> 10,000) for which the SGD\\n# variant can be several orders of magnitude faster. Please check this\\n# :ref:`example\\n# <sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py>` to see how\\n# it's used, and the :ref:`User Guide <sgd_online_one_class_svm>` for more\\n# details.\\n#\\n# .. figure:: ../miscellaneous/images/sphx_glr_plot_anomaly_comparison_001.png\\n#    :target: ../miscellaneous/plot_anomaly_comparison.html\\n#    :align: center\\n\\n##############################################################################\\n# Histogram-based Gradient Boosting Models are now stable\\n# --------------------------------------------------------------------------\\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor` and\\n# :class:`~ensemble.HistGradientBoostingClassifier` are no longer experimental\\n# and can simply be imported and used as::\\n#\\n#     from sklearn.ensemble import HistGradientBoostingClassifier\\n\\n##############################################################################\\n# New documentation improvements\\n# ------------------------------\\n# This release includes many documentation improvements. Out of over 2100\\n# merged pull requests, about 800 of them are improvements to our\\n# documentation.\"), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_1_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# ruff: noqa\\n\"\"\"\\n=======================================\\nRelease Highlights for scikit-learn 1.1\\n=======================================\\n\\n.. currentmodule:: sklearn\\n\\nWe are pleased to announce the release of scikit-learn 1.1! Many bug fixes\\nand improvements were added, as well as some new key features. We detail\\nbelow a few of the major features of this release. **For an exhaustive list of\\nall the changes**, please refer to the :ref:`release notes <release_notes_1_1>`.\\n\\nTo install the latest version (with pip)::\\n\\n    pip install --upgrade scikit-learn\\n\\nor with conda::\\n\\n    conda install -c conda-forge scikit-learn\\n\\n\"\"\"\\n\\n# %%\\n# .. _quantile_support_hgbdt:\\n#\\n# Quantile loss in :class:`~ensemble.HistGradientBoostingRegressor`\\n# -----------------------------------------------------------------\\n# :class:`~ensemble.HistGradientBoostingRegressor` can model quantiles with\\n# `loss=\"quantile\"` and the new parameter `quantile`.\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Simple regression function for X * cos(X)\\nrng = np.random.RandomState(42)\\nX_1d = np.linspace(0, 10, num=2000)\\nX = X_1d.reshape(-1, 1)\\ny = X_1d * np.cos(X_1d) + rng.normal(scale=X_1d / 3)\\n\\nquantiles = [0.95, 0.5, 0.05]\\nparameters = dict(loss=\"quantile\", max_bins=32, max_iter=50)\\nhist_quantiles = {\\n    f\"quantile={quantile:.2f}\": HistGradientBoostingRegressor(\\n        **parameters, quantile=quantile\\n    ).fit(X, y)\\n    for quantile in quantiles\\n}\\n\\nfig, ax = plt.subplots()\\nax.plot(X_1d, y, \"o\", alpha=0.5, markersize=1)\\nfor quantile, hist in hist_quantiles.items():\\n    ax.plot(X_1d, hist.predict(X), label=quantile)\\n_ = ax.legend(loc=\"lower left\")\\n\\n# %%\\n# For a usecase example, see\\n# :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`\\n\\n# %%\\n# `get_feature_names_out` Available in all Transformers\\n# -----------------------------------------------------\\n# :term:`get_feature_names_out` is now available in all Transformers. This enables\\n# :class:`~pipeline.Pipeline` to construct the output feature names for more complex\\n# pipelines:\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.linear_model import LogisticRegression\\n\\nX, y = fetch_openml(\\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\\n)\\nnumeric_features = [\"age\", \"fare\"]\\nnumeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\\ncategorical_features = [\"embarked\", \"pclass\"]'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_1_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X, y = fetch_openml(\\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\\n)\\nnumeric_features = [\"age\", \"fare\"]\\nnumeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\\ncategorical_features = [\"embarked\", \"pclass\"]\\n\\npreprocessor = ColumnTransformer(\\n    [\\n        (\"num\", numeric_transformer, numeric_features),\\n        (\\n            \"cat\",\\n            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\\n            categorical_features,\\n        ),\\n    ],\\n    verbose_feature_names_out=False,\\n)\\nlog_reg = make_pipeline(preprocessor, SelectKBest(k=7), LogisticRegression())\\nlog_reg.fit(X, y)\\n\\n\\n# %%\\n# Here we slice the pipeline to include all the steps but the last one. The output\\n# feature names of this pipeline slice are the features put into logistic\\n# regression. These names correspond directly to the coefficients in the logistic\\n# regression:\\nimport pandas as pd\\n\\nlog_reg_input_features = log_reg[:-1].get_feature_names_out()\\npd.Series(log_reg[-1].coef_.ravel(), index=log_reg_input_features).plot.bar()\\nplt.tight_layout()\\n\\n\\n# %%\\n# Grouping infrequent categories in :class:`~preprocessing.OneHotEncoder`\\n# -----------------------------------------------------------------------\\n# :class:`~preprocessing.OneHotEncoder` supports aggregating infrequent\\n# categories into a single output for each feature. The parameters to enable\\n# the gathering of infrequent categories are `min_frequency` and\\n# `max_categories`. See the :ref:`User Guide <encoder_infrequent_categories>`\\n# for more details.\\nfrom sklearn.preprocessing import OneHotEncoder\\nimport numpy as np\\n\\nX = np.array(\\n    [[\"dog\"] * 5 + [\"cat\"] * 20 + [\"rabbit\"] * 10 + [\"snake\"] * 3], dtype=object\\n).T\\nenc = OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)\\nenc.infrequent_categories_\\n\\n# %%\\n# Since dog and snake are infrequent categories, they are grouped together when\\n# transformed:\\nencoded = enc.transform(np.array([[\"dog\"], [\"snake\"], [\"cat\"], [\"rabbit\"]]))\\npd.DataFrame(encoded, columns=enc.get_feature_names_out())'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_1_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Since dog and snake are infrequent categories, they are grouped together when\\n# transformed:\\nencoded = enc.transform(np.array([[\"dog\"], [\"snake\"], [\"cat\"], [\"rabbit\"]]))\\npd.DataFrame(encoded, columns=enc.get_feature_names_out())\\n\\n# %%\\n# Performance improvements\\n# ------------------------\\n# Reductions on pairwise distances for dense float64 datasets has been refactored\\n# to better take advantage of non-blocking thread parallelism. For example,\\n# :meth:`neighbors.NearestNeighbors.kneighbors` and\\n# :meth:`neighbors.NearestNeighbors.radius_neighbors` can respectively be up to √ó20 and\\n# √ó5 faster than previously. In summary, the following functions and estimators\\n# now benefit from improved performance:\\n#\\n# - :func:`metrics.pairwise_distances_argmin`\\n# - :func:`metrics.pairwise_distances_argmin_min`\\n# - :class:`cluster.AffinityPropagation`\\n# - :class:`cluster.Birch`\\n# - :class:`cluster.MeanShift`\\n# - :class:`cluster.OPTICS`\\n# - :class:`cluster.SpectralClustering`\\n# - :func:`feature_selection.mutual_info_regression`\\n# - :class:`neighbors.KNeighborsClassifier`\\n# - :class:`neighbors.KNeighborsRegressor`\\n# - :class:`neighbors.RadiusNeighborsClassifier`\\n# - :class:`neighbors.RadiusNeighborsRegressor`\\n# - :class:`neighbors.LocalOutlierFactor`\\n# - :class:`neighbors.NearestNeighbors`\\n# - :class:`manifold.Isomap`\\n# - :class:`manifold.LocallyLinearEmbedding`\\n# - :class:`manifold.TSNE`\\n# - :func:`manifold.trustworthiness`\\n# - :class:`semi_supervised.LabelPropagation`\\n# - :class:`semi_supervised.LabelSpreading`\\n#\\n# To know more about the technical details of this work, you can read\\n# `this suite of blog posts <https://blog.scikit-learn.org/technical/performances/>`_.\\n#\\n# Moreover, the computation of loss functions has been refactored using\\n# Cython resulting in performance improvements for the following estimators:\\n#\\n# - :class:`linear_model.LogisticRegression`\\n# - :class:`linear_model.GammaRegressor`\\n# - :class:`linear_model.PoissonRegressor`\\n# - :class:`linear_model.TweedieRegressor`\\n\\n# %%\\n# :class:`~decomposition.MiniBatchNMF`: an online version of NMF\\n# --------------------------------------------------------------\\n# The new class :class:`~decomposition.MiniBatchNMF` implements a faster but\\n# less accurate version of non-negative matrix factorization\\n# (:class:`~decomposition.NMF`). :class:`~decomposition.MiniBatchNMF` divides the\\n# data into mini-batches and optimizes the NMF model in an online manner by\\n# cycling over the mini-batches, making it better suited for large datasets. In\\n# particular, it implements `partial_fit`, which can be used for online\\n# learning when the data is not readily available from the start, or when the\\n# data does not fit into memory.\\nimport numpy as np\\nfrom sklearn.decomposition import MiniBatchNMF\\n\\nrng = np.random.RandomState(0)\\nn_samples, n_features, n_components = 10, 10, 5\\ntrue_W = rng.uniform(size=(n_samples, n_components))\\ntrue_H = rng.uniform(size=(n_components, n_features))\\nX = true_W @ true_H'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_1_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='rng = np.random.RandomState(0)\\nn_samples, n_features, n_components = 10, 10, 5\\ntrue_W = rng.uniform(size=(n_samples, n_components))\\ntrue_H = rng.uniform(size=(n_components, n_features))\\nX = true_W @ true_H\\n\\nnmf = MiniBatchNMF(n_components=n_components, random_state=0)\\n\\nfor _ in range(10):\\n    nmf.partial_fit(X)\\n\\nW = nmf.transform(X)\\nH = nmf.components_\\nX_reconstructed = W @ H\\n\\nprint(\\n    f\"relative reconstruction error: \",\\n    f\"{np.sum((X - X_reconstructed) ** 2) / np.sum(X**2):.5f}\",\\n)\\n\\n# %%\\n# :class:`~cluster.BisectingKMeans`: divide and cluster\\n# -----------------------------------------------------\\n# The new class :class:`~cluster.BisectingKMeans` is a variant of\\n# :class:`~cluster.KMeans`, using divisive hierarchical clustering. Instead of\\n# creating all centroids at once, centroids are picked progressively based on a\\n# previous clustering: a cluster is split into two new clusters repeatedly\\n# until the target number of clusters is reached, giving a hierarchical\\n# structure to the clustering.\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans, BisectingKMeans\\nimport matplotlib.pyplot as plt\\n\\nX, _ = make_blobs(n_samples=1000, centers=2, random_state=0)\\n\\nkm = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(X)\\nbisect_km = BisectingKMeans(n_clusters=5, random_state=0).fit(X)\\n\\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\\nax[0].scatter(X[:, 0], X[:, 1], s=10, c=km.labels_)\\nax[0].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=20, c=\"r\")\\nax[0].set_title(\"KMeans\")\\n\\nax[1].scatter(X[:, 0], X[:, 1], s=10, c=bisect_km.labels_)\\nax[1].scatter(\\n    bisect_km.cluster_centers_[:, 0], bisect_km.cluster_centers_[:, 1], s=20, c=\"r\"\\n)\\n_ = ax[1].set_title(\"BisectingKMeans\")'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_4_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# ruff: noqa\\n\"\"\"\\n=======================================\\nRelease Highlights for scikit-learn 1.4\\n=======================================\\n\\n.. currentmodule:: sklearn\\n\\nWe are pleased to announce the release of scikit-learn 1.4! Many bug fixes\\nand improvements were added, as well as some new key features. We detail\\nbelow a few of the major features of this release. **For an exhaustive list of\\nall the changes**, please refer to the :ref:`release notes <release_notes_1_4>`.\\n\\nTo install the latest version (with pip)::\\n\\n    pip install --upgrade scikit-learn\\n\\nor with conda::\\n\\n    conda install -c conda-forge scikit-learn\\n\\n\"\"\"\\n\\n# %%\\n# HistGradientBoosting Natively Supports Categorical DTypes in DataFrames\\n# -----------------------------------------------------------------------\\n# :class:`ensemble.HistGradientBoostingClassifier` and\\n# :class:`ensemble.HistGradientBoostingRegressor` now directly supports dataframes with\\n# categorical features.  Here we have a dataset with a mixture of\\n# categorical and numerical features:\\nfrom sklearn.datasets import fetch_openml\\n\\nX_adult, y_adult = fetch_openml(\"adult\", version=2, return_X_y=True)\\n\\n# Remove redundant and non-feature columns\\nX_adult = X_adult.drop([\"education-num\", \"fnlwgt\"], axis=\"columns\")\\nX_adult.dtypes\\n\\n# %%\\n# By setting `categorical_features=\"from_dtype\"`, the gradient boosting classifier\\n# treats the columns with categorical dtypes as categorical features in the\\n# algorithm:\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import roc_auc_score\\n\\nX_train, X_test, y_train, y_test = train_test_split(X_adult, y_adult, random_state=0)\\nhist = HistGradientBoostingClassifier(categorical_features=\"from_dtype\")\\n\\nhist.fit(X_train, y_train)\\ny_decision = hist.decision_function(X_test)\\nprint(f\"ROC AUC score is {roc_auc_score(y_test, y_decision)}\")\\n\\n# %%\\n# Polars output in `set_output`\\n# -----------------------------\\n# scikit-learn\\'s transformers now support polars output with the `set_output` API.\\nimport polars as pl\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\n\\ndf = pl.DataFrame(\\n    {\"height\": [120, 140, 150, 110, 100], \"pet\": [\"dog\", \"cat\", \"dog\", \"cat\", \"cat\"]}\\n)\\npreprocessor = ColumnTransformer(\\n    [\\n        (\"numerical\", StandardScaler(), [\"height\"]),\\n        (\"categorical\", OneHotEncoder(sparse_output=False), [\"pet\"]),\\n    ],\\n    verbose_feature_names_out=False,\\n)\\npreprocessor.set_output(transform=\"polars\")\\n\\ndf_out = preprocessor.fit_transform(df)\\ndf_out\\n\\n# %%\\nprint(f\"Output type: {type(df_out)}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_4_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='df_out = preprocessor.fit_transform(df)\\ndf_out\\n\\n# %%\\nprint(f\"Output type: {type(df_out)}\")\\n\\n# %%\\n# Missing value support for Random Forest\\n# ---------------------------------------\\n# The classes :class:`ensemble.RandomForestClassifier` and\\n# :class:`ensemble.RandomForestRegressor` now support missing values. When training\\n# every individual tree, the splitter evaluates each potential threshold with the\\n# missing values going to the left and right nodes. More details in the\\n# :ref:`User Guide <tree_missing_value_support>`.\\nimport numpy as np\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nX = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\\ny = [0, 0, 1, 1]\\n\\nforest = RandomForestClassifier(random_state=0).fit(X, y)\\nforest.predict(X)\\n\\n# %%\\n# Add support for monotonic constraints in tree-based models\\n# ----------------------------------------------------------\\n# While we added support for monotonic constraints in histogram-based gradient boosting\\n# in scikit-learn 0.23, we now support this feature for all other tree-based models as\\n# trees, random forests, extra-trees, and exact gradient boosting. Here, we show this\\n# feature for random forest on a regression problem.\\nimport matplotlib.pyplot as plt\\nfrom sklearn.inspection import PartialDependenceDisplay\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\nn_samples = 500\\nrng = np.random.RandomState(0)\\nX = rng.randn(n_samples, 2)\\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\\ny = 5 * X[:, 0] + np.sin(10 * np.pi * X[:, 0]) - noise\\n\\nrf_no_cst = RandomForestRegressor().fit(X, y)\\nrf_cst = RandomForestRegressor(monotonic_cst=[1, 0]).fit(X, y)\\n\\ndisp = PartialDependenceDisplay.from_estimator(\\n    rf_no_cst,\\n    X,\\n    features=[0],\\n    feature_names=[\"feature 0\"],\\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\\n)\\nPartialDependenceDisplay.from_estimator(\\n    rf_cst,\\n    X,\\n    features=[0],\\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\\n    ax=disp.axes_,\\n)\\ndisp.axes_[0, 0].plot(\\n    X[:, 0], y, \"o\", alpha=0.5, zorder=-1, label=\"samples\", color=\"tab:green\"\\n)\\ndisp.axes_[0, 0].set_ylim(-3, 3)\\ndisp.axes_[0, 0].set_xlim(-1, 1)\\ndisp.axes_[0, 0].legend()\\nplt.show()\\n\\n# %%\\n# Enriched estimator displays\\n# ---------------------------\\n# Estimators displays have been enriched: if we look at `forest`, defined above:\\nforest\\n\\n# %%\\n# One can access the documentation of the estimator by clicking on the icon \"?\" on\\n# the top right corner of the diagram.\\n#\\n# In addition, the display changes color, from orange to blue, when the estimator is\\n# fitted. You can also get this information by hovering on the icon \"i\".\\nfrom sklearn.base import clone\\n\\nclone(forest)  # the clone is not fitted'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_4_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='clone(forest)  # the clone is not fitted\\n\\n# %%\\n# Metadata Routing Support\\n# ------------------------\\n# Many meta-estimators and cross-validation routines now support metadata\\n# routing, which are listed in the :ref:`user guide\\n# <metadata_routing_models>`. For instance, this is how you can do a nested\\n# cross-validation with sample weights and :class:`~model_selection.GroupKFold`:\\nimport sklearn\\nfrom sklearn.metrics import get_scorer\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.linear_model import Lasso\\nfrom sklearn.model_selection import GridSearchCV, cross_validate, GroupKFold\\n\\n# For now by default metadata routing is disabled, and need to be explicitly\\n# enabled.\\nsklearn.set_config(enable_metadata_routing=True)\\n\\nn_samples = 100\\nX, y = make_regression(n_samples=n_samples, n_features=5, noise=0.5)\\nrng = np.random.RandomState(7)\\ngroups = rng.randint(0, 10, size=n_samples)\\nsample_weights = rng.rand(n_samples)\\nestimator = Lasso().set_fit_request(sample_weight=True)\\nhyperparameter_grid = {\"alpha\": [0.1, 0.5, 1.0, 2.0]}\\nscoring_inner_cv = get_scorer(\"neg_mean_squared_error\").set_score_request(\\n    sample_weight=True\\n)\\ninner_cv = GroupKFold(n_splits=5)\\n\\ngrid_search = GridSearchCV(\\n    estimator=estimator,\\n    param_grid=hyperparameter_grid,\\n    cv=inner_cv,\\n    scoring=scoring_inner_cv,\\n)\\n\\nouter_cv = GroupKFold(n_splits=5)\\nscorers = {\\n    \"mse\": get_scorer(\"neg_mean_squared_error\").set_score_request(sample_weight=True)\\n}\\nresults = cross_validate(\\n    grid_search,\\n    X,\\n    y,\\n    cv=outer_cv,\\n    scoring=scorers,\\n    return_estimator=True,\\n    params={\"sample_weight\": sample_weights, \"groups\": groups},\\n)\\nprint(\"cv error on test sets:\", results[\"test_mse\"])\\n\\n# Setting the flag to the default `False` to avoid interference with other\\n# scripts.\\nsklearn.set_config(enable_metadata_routing=False)\\n\\n# %%\\n# Improved memory and runtime efficiency for PCA on sparse data\\n# -------------------------------------------------------------\\n# PCA is now able to handle sparse matrices natively for the `arpack`\\n# solver by levaraging `scipy.sparse.linalg.LinearOperator` to avoid\\n# materializing large sparse matrices when performing the\\n# eigenvalue decomposition of the data set covariance matrix.\\n#\\nfrom sklearn.decomposition import PCA\\nimport scipy.sparse as sp\\nfrom time import time\\n\\nX_sparse = sp.random(m=1000, n=1000, random_state=0)\\nX_dense = X_sparse.toarray()\\n\\nt0 = time()\\nPCA(n_components=10, svd_solver=\"arpack\").fit(X_sparse)\\ntime_sparse = time() - t0\\n\\nt0 = time()\\nPCA(n_components=10, svd_solver=\"arpack\").fit(X_dense)\\ntime_dense = time() - t0\\n\\nprint(f\"Speedup: {time_dense / time_sparse:.1f}x\")'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_23_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# ruff: noqa\\n\"\"\"\\n========================================\\nRelease Highlights for scikit-learn 0.23\\n========================================\\n\\n.. currentmodule:: sklearn\\n\\nWe are pleased to announce the release of scikit-learn 0.23! Many bug fixes\\nand improvements were added, as well as some new key features. We detail\\nbelow a few of the major features of this release. **For an exhaustive list of\\nall the changes**, please refer to the :ref:`release notes <release_notes_0_23>`.\\n\\nTo install the latest version (with pip)::\\n\\n    pip install --upgrade scikit-learn\\n\\nor with conda::\\n\\n    conda install -c conda-forge scikit-learn\\n\\n\"\"\"\\n\\n##############################################################################\\n# Generalized Linear Models, and Poisson loss for gradient boosting\\n# -----------------------------------------------------------------\\n# Long-awaited Generalized Linear Models with non-normal loss functions are now\\n# available. In particular, three new regressors were implemented:\\n# :class:`~sklearn.linear_model.PoissonRegressor`,\\n# :class:`~sklearn.linear_model.GammaRegressor`, and\\n# :class:`~sklearn.linear_model.TweedieRegressor`. The Poisson regressor can be\\n# used to model positive integer counts, or relative frequencies. Read more in\\n# the :ref:`User Guide <Generalized_linear_regression>`. Additionally,\\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor` supports a new\\n# \\'poisson\\' loss as well.\\n\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import PoissonRegressor\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\n\\nn_samples, n_features = 1000, 20\\nrng = np.random.RandomState(0)\\nX = rng.randn(n_samples, n_features)\\n# positive integer target correlated with X[:, 5] with many zeros:\\ny = rng.poisson(lam=np.exp(X[:, 5]) / 2)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\\nglm = PoissonRegressor()\\ngbdt = HistGradientBoostingRegressor(loss=\"poisson\", learning_rate=0.01)\\nglm.fit(X_train, y_train)\\ngbdt.fit(X_train, y_train)\\nprint(glm.score(X_test, y_test))\\nprint(gbdt.score(X_test, y_test))\\n\\n##############################################################################\\n# Rich visual representation of estimators\\n# -----------------------------------------\\n# Estimators can now be visualized in notebooks by enabling the\\n# `display=\\'diagram\\'` option. This is particularly useful to summarise the\\n# structure of pipelines and other composite estimators, with interactivity to\\n# provide detail.  Click on the example image below to expand Pipeline\\n# elements.  See :ref:`visualizing_composite_estimators` for how you can use\\n# this feature.\\n\\nfrom sklearn import set_config\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.linear_model import LogisticRegression\\n\\nset_config(display=\"diagram\")'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_23_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='set_config(display=\"diagram\")\\n\\nnum_proc = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\\n\\ncat_proc = make_pipeline(\\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\\n    OneHotEncoder(handle_unknown=\"ignore\"),\\n)\\n\\npreprocessor = make_column_transformer(\\n    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\\n)\\n\\nclf = make_pipeline(preprocessor, LogisticRegression())\\nclf\\n\\n##############################################################################\\n# Scalability and stability improvements to KMeans\\n# ------------------------------------------------\\n# The :class:`~sklearn.cluster.KMeans` estimator was entirely re-worked, and it\\n# is now significantly faster and more stable. In addition, the Elkan algorithm\\n# is now compatible with sparse matrices. The estimator uses OpenMP based\\n# parallelism instead of relying on joblib, so the `n_jobs` parameter has no\\n# effect anymore. For more details on how to control the number of threads,\\n# please refer to our :ref:`parallelism` notes.\\nimport scipy\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.metrics import completeness_score\\n\\nrng = np.random.RandomState(0)\\nX, y = make_blobs(random_state=rng)\\nX = scipy.sparse.csr_matrix(X)\\nX_train, X_test, _, y_test = train_test_split(X, y, random_state=rng)\\nkmeans = KMeans(n_init=\"auto\").fit(X_train)\\nprint(completeness_score(kmeans.predict(X_test), y_test))\\n\\n##############################################################################\\n# Improvements to the histogram-based Gradient Boosting estimators\\n# ----------------------------------------------------------------\\n# Various improvements were made to\\n# :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor`. On top of the\\n# Poisson loss mentioned above, these estimators now support :ref:`sample\\n# weights <sw_hgbdt>`. Also, an automatic early-stopping criterion was added:\\n# early-stopping is enabled by default when the number of samples exceeds 10k.\\n# Finally, users can now define :ref:`monotonic constraints\\n# <monotonic_cst_gbdt>` to constrain the predictions based on the variations of\\n# specific features. In the following example, we construct a target that is\\n# generally positively correlated with the first feature, with some noise.\\n# Applying monotoinc constraints allows the prediction to capture the global\\n# effect of the first feature, instead of fitting the noise. For a usecase\\n# example, see :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`.\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nfrom sklearn.model_selection import train_test_split\\n\\n# from sklearn.inspection import plot_partial_dependence\\nfrom sklearn.inspection import PartialDependenceDisplay\\nfrom sklearn.ensemble import HistGradientBoostingRegressor'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_0_23_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# from sklearn.inspection import plot_partial_dependence\\nfrom sklearn.inspection import PartialDependenceDisplay\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\n\\nn_samples = 500\\nrng = np.random.RandomState(0)\\nX = rng.randn(n_samples, 2)\\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\\ny = 5 * X[:, 0] + np.sin(10 * np.pi * X[:, 0]) - noise\\n\\ngbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)\\ngbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)\\n\\n# plot_partial_dependence has been removed in version 1.2. From 1.2, use\\n# PartialDependenceDisplay instead.\\n# disp = plot_partial_dependence(\\ndisp = PartialDependenceDisplay.from_estimator(\\n    gbdt_no_cst,\\n    X,\\n    features=[0],\\n    feature_names=[\"feature 0\"],\\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\\n)\\n# plot_partial_dependence(\\nPartialDependenceDisplay.from_estimator(\\n    gbdt_cst,\\n    X,\\n    features=[0],\\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\\n    ax=disp.axes_,\\n)\\ndisp.axes_[0, 0].plot(\\n    X[:, 0], y, \"o\", alpha=0.5, zorder=-1, label=\"samples\", color=\"tab:green\"\\n)\\ndisp.axes_[0, 0].set_ylim(-3, 3)\\ndisp.axes_[0, 0].set_xlim(-1, 1)\\nplt.legend()\\nplt.show()\\n\\n##############################################################################\\n# Sample-weight support for Lasso and ElasticNet\\n# ----------------------------------------------\\n# The two linear regressors :class:`~sklearn.linear_model.Lasso` and\\n# :class:`~sklearn.linear_model.ElasticNet` now support sample weights.\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.linear_model import Lasso\\nimport numpy as np\\n\\nn_samples, n_features = 1000, 20\\nrng = np.random.RandomState(0)\\nX, y = make_regression(n_samples, n_features, random_state=rng)\\nsample_weight = rng.rand(n_samples)\\nX_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\\n    X, y, sample_weight, random_state=rng\\n)\\nreg = Lasso()\\nreg.fit(X_train, y_train, sample_weight=sw_train)\\nprint(reg.score(X_test, y_test, sw_test))'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_3_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# ruff: noqa\\n\"\"\"\\n=======================================\\nRelease Highlights for scikit-learn 1.3\\n=======================================\\n\\n.. currentmodule:: sklearn\\n\\nWe are pleased to announce the release of scikit-learn 1.3! Many bug fixes\\nand improvements were added, as well as some new key features. We detail\\nbelow a few of the major features of this release. **For an exhaustive list of\\nall the changes**, please refer to the :ref:`release notes <release_notes_1_3>`.\\n\\nTo install the latest version (with pip)::\\n\\n    pip install --upgrade scikit-learn\\n\\nor with conda::\\n\\n    conda install -c conda-forge scikit-learn\\n\\n\"\"\"\\n\\n# %%\\n# Metadata Routing\\n# ----------------\\n# We are in the process of introducing a new way to route metadata such as\\n# ``sample_weight`` throughout the codebase, which would affect how\\n# meta-estimators such as :class:`pipeline.Pipeline` and\\n# :class:`model_selection.GridSearchCV` route metadata. While the\\n# infrastructure for this feature is already included in this release, the work\\n# is ongoing and not all meta-estimators support this new feature. You can read\\n# more about this feature in the :ref:`Metadata Routing User Guide\\n# <metadata_routing>`. Note that this feature is still under development and\\n# not implemented for most meta-estimators.\\n#\\n# Third party developers can already start incorporating this into their\\n# meta-estimators. For more details, see\\n# :ref:`metadata routing developer guide\\n# <sphx_glr_auto_examples_miscellaneous_plot_metadata_routing.py>`.\\n\\n# %%\\n# HDBSCAN: hierarchical density-based clustering\\n# ----------------------------------------------\\n# Originally hosted in the scikit-learn-contrib repository, :class:`cluster.HDBSCAN`\\n# has been adpoted into scikit-learn. It\\'s missing a few features from the original\\n# implementation which will be added in future releases.\\n# By performing a modified version of :class:`cluster.DBSCAN` over multiple epsilon\\n# values simultaneously, :class:`cluster.HDBSCAN` finds clusters of varying densities\\n# making it more robust to parameter selection than :class:`cluster.DBSCAN`.\\n# More details in the :ref:`User Guide <hdbscan>`.\\nimport numpy as np\\nfrom sklearn.cluster import HDBSCAN\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.metrics import v_measure_score\\n\\nX, true_labels = load_digits(return_X_y=True)\\nprint(f\"number of digits: {len(np.unique(true_labels))}\")\\n\\nhdbscan = HDBSCAN(min_cluster_size=15).fit(X)\\nnon_noisy_labels = hdbscan.labels_[hdbscan.labels_ != -1]\\nprint(f\"number of clusters found: {len(np.unique(non_noisy_labels))}\")\\n\\nprint(v_measure_score(true_labels[hdbscan.labels_ != -1], non_noisy_labels))'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_3_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='hdbscan = HDBSCAN(min_cluster_size=15).fit(X)\\nnon_noisy_labels = hdbscan.labels_[hdbscan.labels_ != -1]\\nprint(f\"number of clusters found: {len(np.unique(non_noisy_labels))}\")\\n\\nprint(v_measure_score(true_labels[hdbscan.labels_ != -1], non_noisy_labels))\\n\\n# %%\\n# TargetEncoder: a new category encoding strategy\\n# -----------------------------------------------\\n# Well suited for categorical features with high cardinality,\\n# :class:`preprocessing.TargetEncoder` encodes the categories based on a shrunk\\n# estimate of the average target values for observations belonging to that category.\\n# More details in the :ref:`User Guide <target_encoder>`.\\nimport numpy as np\\nfrom sklearn.preprocessing import TargetEncoder\\n\\nX = np.array([[\"cat\"] * 30 + [\"dog\"] * 20 + [\"snake\"] * 38], dtype=object).T\\ny = [90.3] * 30 + [20.4] * 20 + [21.2] * 38\\n\\nenc = TargetEncoder(random_state=0)\\nX_trans = enc.fit_transform(X, y)\\n\\nenc.encodings_\\n\\n# %%\\n# Missing values support in decision trees\\n# ----------------------------------------\\n# The classes :class:`tree.DecisionTreeClassifier` and\\n# :class:`tree.DecisionTreeRegressor` now support missing values. For each potential\\n# threshold on the non-missing data, the splitter will evaluate the split with all the\\n# missing values going to the left node or the right node.\\n# See more details in the :ref:`User Guide <tree_missing_value_support>` or see\\n# :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for a usecase\\n# example of this feature in :class:`~ensemble.HistGradientBoostingRegressor`.\\nimport numpy as np\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\nX = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\\ny = [0, 0, 1, 1]\\n\\ntree = DecisionTreeClassifier(random_state=0).fit(X, y)\\ntree.predict(X)\\n\\n# %%\\n# New display :class:`~model_selection.ValidationCurveDisplay`\\n# ------------------------------------------------------------\\n# :class:`model_selection.ValidationCurveDisplay` is now available to plot results\\n# from :func:`model_selection.validation_curve`.\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import ValidationCurveDisplay\\n\\nX, y = make_classification(1000, 10, random_state=0)\\n\\n_ = ValidationCurveDisplay.from_estimator(\\n    LogisticRegression(),\\n    X,\\n    y,\\n    param_name=\"C\",\\n    param_range=np.geomspace(1e-5, 1e3, num=9),\\n    score_type=\"both\",\\n    score_name=\"Accuracy\",\\n)\\n\\n# %%\\n# Gamma loss for gradient boosting\\n# --------------------------------\\n# The class :class:`ensemble.HistGradientBoostingRegressor` supports the\\n# Gamma deviance loss function via `loss=\"gamma\"`. This loss function is useful for\\n# modeling strictly positive targets with a right-skewed distribution.\\nimport numpy as np\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.datasets import make_low_rank_matrix\\nfrom sklearn.ensemble import HistGradientBoostingRegressor'), Document(metadata={'source': '/content/local_copy_repo/examples/release_highlights/plot_release_highlights_1_3_0.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='n_samples, n_features = 500, 10\\nrng = np.random.RandomState(0)\\nX = make_low_rank_matrix(n_samples, n_features, random_state=rng)\\ncoef = rng.uniform(low=-10, high=20, size=n_features)\\ny = rng.gamma(shape=2, scale=np.exp(X @ coef) / 2)\\ngbdt = HistGradientBoostingRegressor(loss=\"gamma\")\\ncross_val_score(gbdt, X, y).mean()\\n\\n# %%\\n# Grouping infrequent categories in :class:`~preprocessing.OrdinalEncoder`\\n# ------------------------------------------------------------------------\\n# Similarly to :class:`preprocessing.OneHotEncoder`, the class\\n# :class:`preprocessing.OrdinalEncoder` now supports aggregating infrequent categories\\n# into a single output for each feature. The parameters to enable the gathering of\\n# infrequent categories are `min_frequency` and `max_categories`.\\n# See the :ref:`User Guide <encoder_infrequent_categories>` for more details.\\nfrom sklearn.preprocessing import OrdinalEncoder\\nimport numpy as np\\n\\nX = np.array(\\n    [[\"dog\"] * 5 + [\"cat\"] * 20 + [\"rabbit\"] * 10 + [\"snake\"] * 3], dtype=object\\n).T\\nenc = OrdinalEncoder(min_frequency=6).fit(X)\\nenc.infrequent_categories_'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_iterative_imputer_variants_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nImputing missing values with variants of IterativeImputer\\n=========================================================\\n\\n.. currentmodule:: sklearn\\n\\nThe :class:`~impute.IterativeImputer` class is very flexible - it can be\\nused with a variety of estimators to do round-robin regression, treating every\\nvariable as an output in turn.\\n\\nIn this example we compare some estimators for the purpose of missing feature\\nimputation with :class:`~impute.IterativeImputer`:\\n\\n* :class:`~linear_model.BayesianRidge`: regularized linear regression\\n* :class:`~ensemble.RandomForestRegressor`: Forests of randomized trees regression\\n* :func:`~pipeline.make_pipeline` (:class:`~kernel_approximation.Nystroem`,\\n  :class:`~linear_model.Ridge`): a pipeline with the expansion of a degree 2\\n  polynomial kernel and regularized linear regression\\n* :class:`~neighbors.KNeighborsRegressor`: comparable to other KNN\\n  imputation approaches\\n\\nOf particular interest is the ability of\\n:class:`~impute.IterativeImputer` to mimic the behavior of missForest, a\\npopular imputation package for R.\\n\\nNote that :class:`~neighbors.KNeighborsRegressor` is different from KNN\\nimputation, which learns from samples with missing values by using a distance\\nmetric that accounts for missing values, rather than imputing them.\\n\\nThe goal is to compare different estimators to see which one is best for the\\n:class:`~impute.IterativeImputer` when using a\\n:class:`~linear_model.BayesianRidge` estimator on the California housing\\ndataset with a single value randomly removed from each row.\\n\\nFor this particular pattern of missing values we see that\\n:class:`~linear_model.BayesianRidge` and\\n:class:`~ensemble.RandomForestRegressor` give the best results.\\n\\nIt should be noted that some estimators such as\\n:class:`~ensemble.HistGradientBoostingRegressor` can natively deal with\\nmissing features and are often recommended over building pipelines with\\ncomplex and costly missing values imputation strategies.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\n# To use this experimental feature, we need to explicitly ask for it:\\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\\nfrom sklearn.impute import IterativeImputer, SimpleImputer\\nfrom sklearn.kernel_approximation import Nystroem\\nfrom sklearn.linear_model import BayesianRidge, Ridge\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.neighbors import KNeighborsRegressor\\nfrom sklearn.pipeline import make_pipeline\\n\\nN_SPLITS = 5\\n\\nrng = np.random.RandomState(0)\\n\\nX_full, y_full = fetch_california_housing(return_X_y=True)\\n# ~2k samples is enough for the purpose of the example.\\n# Remove the following two lines for a slower run with different error bars.\\nX_full = X_full[::10]\\ny_full = y_full[::10]\\nn_samples, n_features = X_full.shape'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_iterative_imputer_variants_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X_full, y_full = fetch_california_housing(return_X_y=True)\\n# ~2k samples is enough for the purpose of the example.\\n# Remove the following two lines for a slower run with different error bars.\\nX_full = X_full[::10]\\ny_full = y_full[::10]\\nn_samples, n_features = X_full.shape\\n\\n# Estimate the score on the entire dataset, with no missing values\\nbr_estimator = BayesianRidge()\\nscore_full_data = pd.DataFrame(\\n    cross_val_score(\\n        br_estimator, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\\n    ),\\n    columns=[\"Full Data\"],\\n)\\n\\n# Add a single missing value to each row\\nX_missing = X_full.copy()\\ny_missing = y_full\\nmissing_samples = np.arange(n_samples)\\nmissing_features = rng.choice(n_features, n_samples, replace=True)\\nX_missing[missing_samples, missing_features] = np.nan\\n\\n# Estimate the score after imputation (mean and median strategies)\\nscore_simple_imputer = pd.DataFrame()\\nfor strategy in (\"mean\", \"median\"):\\n    estimator = make_pipeline(\\n        SimpleImputer(missing_values=np.nan, strategy=strategy), br_estimator\\n    )\\n    score_simple_imputer[strategy] = cross_val_score(\\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\\n    )\\n\\n# Estimate the score after iterative imputation of the missing values\\n# with different estimators\\nestimators = [\\n    BayesianRidge(),\\n    RandomForestRegressor(\\n        # We tuned the hyperparameters of the RandomForestRegressor to get a good\\n        # enough predictive performance for a restricted execution time.\\n        n_estimators=4,\\n        max_depth=10,\\n        bootstrap=True,\\n        max_samples=0.5,\\n        n_jobs=2,\\n        random_state=0,\\n    ),\\n    make_pipeline(\\n        Nystroem(kernel=\"polynomial\", degree=2, random_state=0), Ridge(alpha=1e3)\\n    ),\\n    KNeighborsRegressor(n_neighbors=15),\\n]\\nscore_iterative_imputer = pd.DataFrame()\\n# iterative imputer is sensible to the tolerance and\\n# dependent on the estimator used internally.\\n# we tuned the tolerance to keep this example run with limited computational\\n# resources while not changing the results too much compared to keeping the\\n# stricter default value for the tolerance parameter.\\ntolerances = (1e-3, 1e-1, 1e-1, 1e-2)\\nfor impute_estimator, tol in zip(estimators, tolerances):\\n    estimator = make_pipeline(\\n        IterativeImputer(\\n            random_state=0, estimator=impute_estimator, max_iter=25, tol=tol\\n        ),\\n        br_estimator,\\n    )\\n    score_iterative_imputer[impute_estimator.__class__.__name__] = cross_val_score(\\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\\n    )\\n\\nscores = pd.concat(\\n    [score_full_data, score_simple_imputer, score_iterative_imputer],\\n    keys=[\"Original\", \"SimpleImputer\", \"IterativeImputer\"],\\n    axis=1,\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_iterative_imputer_variants_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='scores = pd.concat(\\n    [score_full_data, score_simple_imputer, score_iterative_imputer],\\n    keys=[\"Original\", \"SimpleImputer\", \"IterativeImputer\"],\\n    axis=1,\\n)\\n\\n# plot california housing results\\nfig, ax = plt.subplots(figsize=(13, 6))\\nmeans = -scores.mean()\\nerrors = scores.std()\\nmeans.plot.barh(xerr=errors, ax=ax)\\nax.set_title(\"California Housing Regression with Different Imputation Methods\")\\nax.set_xlabel(\"MSE (smaller is better)\")\\nax.set_yticks(np.arange(means.shape[0]))\\nax.set_yticklabels([\" w/ \".join(label) for label in means.index.tolist()])\\nplt.tight_layout(pad=1)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_missing_values.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def add_missing_values(X_full, y_full):\\n    n_samples, n_features = X_full.shape\\n\\n    # Add missing values in 75% of the lines\\n    missing_rate = 0.75\\n    n_missing_samples = int(n_samples * missing_rate)\\n\\n    missing_samples = np.zeros(n_samples, dtype=bool)\\n    missing_samples[:n_missing_samples] = True\\n\\n    rng.shuffle(missing_samples)\\n    missing_features = rng.randint(0, n_features, n_missing_samples)\\n    X_missing = X_full.copy()\\n    X_missing[missing_samples, missing_features] = np.nan\\n    y_missing = y_full.copy()\\n\\n    return X_missing, y_missing'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_missing_values.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_scores_for_imputer(imputer, X_missing, y_missing):\\n    estimator = make_pipeline(imputer, regressor)\\n    impute_scores = cross_val_score(\\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\\n    )\\n    return impute_scores'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_missing_values.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_full_score(X_full, y_full):\\n    full_scores = cross_val_score(\\n        regressor, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\\n    )\\n    return full_scores.mean(), full_scores.std()'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_missing_values.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_impute_zero_score(X_missing, y_missing):\\n    imputer = SimpleImputer(\\n        missing_values=np.nan, add_indicator=True, strategy=\"constant\", fill_value=0\\n    )\\n    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\\n    return zero_impute_scores.mean(), zero_impute_scores.std()'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_missing_values.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_impute_knn_score(X_missing, y_missing):\\n    imputer = KNNImputer(missing_values=np.nan, add_indicator=True)\\n    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\\n    return knn_impute_scores.mean(), knn_impute_scores.std()'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_missing_values.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_impute_mean(X_missing, y_missing):\\n    imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\", add_indicator=True)\\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\\n    return mean_impute_scores.mean(), mean_impute_scores.std()'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_missing_values.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_impute_iterative(X_missing, y_missing):\\n    imputer = IterativeImputer(\\n        missing_values=np.nan,\\n        add_indicator=True,\\n        random_state=0,\\n        n_nearest_features=3,\\n        max_iter=1,\\n        sample_posterior=True,\\n    )\\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_missing_values.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================================\\nImputing missing values before building an estimator\\n====================================================\\n\\nMissing values can be replaced by the mean, the median or the most frequent\\nvalue using the basic :class:`~sklearn.impute.SimpleImputer`.\\n\\nIn this example we will investigate different imputation techniques:\\n\\n- imputation by the constant value 0\\n- imputation by the mean value of each feature combined with a missing-ness\\n  indicator auxiliary variable\\n- k nearest neighbor imputation\\n- iterative imputation\\n\\nWe will use two datasets: Diabetes dataset which consists of 10 feature\\nvariables collected from diabetes patients with an aim to predict disease\\nprogression and California Housing dataset for which the target is the median\\nhouse value for California districts.\\n\\nAs neither of these datasets have missing values, we will remove some\\nvalues to create new versions with artificially missing data. The performance\\nof\\n:class:`~sklearn.ensemble.RandomForestRegressor` on the full original dataset\\nis then compared the performance on the altered datasets with the artificially\\nmissing values imputed using different techniques.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Download the data and make missing values sets\\n################################################\\n#\\n# First we download the two datasets. Diabetes dataset is shipped with\\n# scikit-learn. It has 442 entries, each with 10 features. California Housing\\n# dataset is much larger with 20640 entries and 8 features. It needs to be\\n# downloaded. We will only use the first 400 entries for the sake of speeding\\n# up the calculations but feel free to use the whole dataset.\\n#\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_california_housing, load_diabetes\\n\\nrng = np.random.RandomState(42)\\n\\nX_diabetes, y_diabetes = load_diabetes(return_X_y=True)\\nX_california, y_california = fetch_california_housing(return_X_y=True)\\nX_california = X_california[:300]\\ny_california = y_california[:300]\\nX_diabetes = X_diabetes[:300]\\ny_diabetes = y_diabetes[:300]\\n\\n\\n# Code for: def add_missing_values(X_full, y_full):\\n\\n\\nX_miss_california, y_miss_california = add_missing_values(X_california, y_california)\\n\\nX_miss_diabetes, y_miss_diabetes = add_missing_values(X_diabetes, y_diabetes)\\n\\n\\n# %%\\n# Impute the missing data and score\\n# #################################\\n# Now we will write a function which will score the results on the differently\\n# imputed data. Let\\'s look at each imputer separately:\\n#\\n\\nrng = np.random.RandomState(0)\\n\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\n# To use the experimental IterativeImputer, we need to explicitly ask for it:\\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\\nfrom sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.pipeline import make_pipeline'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_missing_values.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='N_SPLITS = 4\\nregressor = RandomForestRegressor(random_state=0)\\n\\n# %%\\n# Missing information\\n# -------------------\\n# In addition to imputing the missing values, the imputers have an\\n# `add_indicator` parameter that marks the values that were missing, which\\n# might carry some information.\\n#\\n\\n\\n# Code for: def get_scores_for_imputer(imputer, X_missing, y_missing):\\n\\n\\nx_labels = []\\n\\nmses_california = np.zeros(5)\\nstds_california = np.zeros(5)\\nmses_diabetes = np.zeros(5)\\nstds_diabetes = np.zeros(5)\\n\\n# %%\\n# Estimate the score\\n# ------------------\\n# First, we want to estimate the score on the original data:\\n#\\n\\n\\n# Code for: def get_full_score(X_full, y_full):\\n\\n\\nmses_california[0], stds_california[0] = get_full_score(X_california, y_california)\\nmses_diabetes[0], stds_diabetes[0] = get_full_score(X_diabetes, y_diabetes)\\nx_labels.append(\"Full data\")\\n\\n\\n# %%\\n# Replace missing values by 0\\n# ---------------------------\\n#\\n# Now we will estimate the score on the data where the missing values are\\n# replaced by 0:\\n#\\n\\n\\n# Code for: def get_impute_zero_score(X_missing, y_missing):\\n\\n\\nmses_california[1], stds_california[1] = get_impute_zero_score(\\n    X_miss_california, y_miss_california\\n)\\nmses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(\\n    X_miss_diabetes, y_miss_diabetes\\n)\\nx_labels.append(\"Zero imputation\")\\n\\n\\n# %%\\n# kNN-imputation of the missing values\\n# ------------------------------------\\n#\\n# :class:`~sklearn.impute.KNNImputer` imputes missing values using the weighted\\n# or unweighted mean of the desired number of nearest neighbors.\\n\\n\\n# Code for: def get_impute_knn_score(X_missing, y_missing):\\n\\n\\nmses_california[2], stds_california[2] = get_impute_knn_score(\\n    X_miss_california, y_miss_california\\n)\\nmses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(\\n    X_miss_diabetes, y_miss_diabetes\\n)\\nx_labels.append(\"KNN Imputation\")\\n\\n\\n# %%\\n# Impute missing values with mean\\n# -------------------------------\\n#\\n\\n\\n# Code for: def get_impute_mean(X_missing, y_missing):\\n\\n\\nmses_california[3], stds_california[3] = get_impute_mean(\\n    X_miss_california, y_miss_california\\n)\\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\\nx_labels.append(\"Mean Imputation\")\\n\\n\\n# %%\\n# Iterative imputation of the missing values\\n# ------------------------------------------\\n#\\n# Another option is the :class:`~sklearn.impute.IterativeImputer`. This uses\\n# round-robin linear regression, modeling each feature with missing values as a\\n# function of other features, in turn.\\n# The version implemented assumes Gaussian (output) variables. If your features\\n# are obviously non-normal, consider transforming them to look more normal\\n# to potentially improve performance.\\n#\\n\\n\\n# Code for: def get_impute_iterative(X_missing, y_missing):'), Document(metadata={'source': '/content/local_copy_repo/examples/impute/plot_missing_values.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def get_impute_iterative(X_missing, y_missing):\\n\\n\\nmses_california[4], stds_california[4] = get_impute_iterative(\\n    X_miss_california, y_miss_california\\n)\\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\\n    X_miss_diabetes, y_miss_diabetes\\n)\\nx_labels.append(\"Iterative Imputation\")\\n\\nmses_diabetes = mses_diabetes * -1\\nmses_california = mses_california * -1\\n\\n# %%\\n# Plot the results\\n# ################\\n#\\n# Finally we are going to visualize the score:\\n#\\n\\nimport matplotlib.pyplot as plt\\n\\nn_bars = len(mses_diabetes)\\nxval = np.arange(n_bars)\\n\\ncolors = [\"r\", \"g\", \"b\", \"orange\", \"black\"]\\n\\n# plot diabetes results\\nplt.figure(figsize=(12, 6))\\nax1 = plt.subplot(121)\\nfor j in xval:\\n    ax1.barh(\\n        j,\\n        mses_diabetes[j],\\n        xerr=stds_diabetes[j],\\n        color=colors[j],\\n        alpha=0.6,\\n        align=\"center\",\\n    )\\n\\nax1.set_title(\"Imputation Techniques with Diabetes Data\")\\nax1.set_xlim(left=np.min(mses_diabetes) * 0.9, right=np.max(mses_diabetes) * 1.1)\\nax1.set_yticks(xval)\\nax1.set_xlabel(\"MSE\")\\nax1.invert_yaxis()\\nax1.set_yticklabels(x_labels)\\n\\n# plot california dataset results\\nax2 = plt.subplot(122)\\nfor j in xval:\\n    ax2.barh(\\n        j,\\n        mses_california[j],\\n        xerr=stds_california[j],\\n        color=colors[j],\\n        alpha=0.6,\\n        align=\"center\",\\n    )\\n\\nax2.set_title(\"Imputation Techniques with California Data\")\\nax2.set_yticks(xval)\\nax2.set_xlabel(\"MSE\")\\nax2.invert_yaxis()\\nax2.set_yticklabels([\"\"] * n_bars)\\n\\nplt.show()\\n\\n# %%\\n# You can also try different techniques. For instance, the median is a more\\n# robust estimator for data with high magnitude variables which could dominate\\n# results (otherwise known as a \\'long tail\\').'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_oneclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================================\\nOne-class SVM with non-linear kernel (RBF)\\n==========================================\\n\\nAn example using a one-class SVM for novelty detection.\\n\\n:ref:`One-class SVM <svm_outlier_detection>` is an unsupervised\\nalgorithm that learns a decision function for novelty detection:\\nclassifying new data as similar or different to the training set.\\n\\n\"\"\"\\n\\n# %%\\nimport numpy as np\\n\\nfrom sklearn import svm\\n\\n# Generate train data\\nX = 0.3 * np.random.randn(100, 2)\\nX_train = np.r_[X + 2, X - 2]\\n# Generate some regular novel observations\\nX = 0.3 * np.random.randn(20, 2)\\nX_test = np.r_[X + 2, X - 2]\\n# Generate some abnormal novel observations\\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\\n\\n# fit the model\\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\\nclf.fit(X_train)\\ny_pred_train = clf.predict(X_train)\\ny_pred_test = clf.predict(X_test)\\ny_pred_outliers = clf.predict(X_outliers)\\nn_error_train = y_pred_train[y_pred_train == -1].size\\nn_error_test = y_pred_test[y_pred_test == -1].size\\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\\n\\n# %%\\nimport matplotlib.font_manager\\nimport matplotlib.lines as mlines\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\n_, ax = plt.subplots()\\n\\n# generate grid for the boundary display\\nxx, yy = np.meshgrid(np.linspace(-5, 5, 10), np.linspace(-5, 5, 10))\\nX = np.concatenate([xx.reshape(-1, 1), yy.reshape(-1, 1)], axis=1)\\nDecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    response_method=\"decision_function\",\\n    plot_method=\"contourf\",\\n    ax=ax,\\n    cmap=\"PuBu\",\\n)\\nDecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    response_method=\"decision_function\",\\n    plot_method=\"contourf\",\\n    ax=ax,\\n    levels=[0, 10000],\\n    colors=\"palevioletred\",\\n)\\nDecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    response_method=\"decision_function\",\\n    plot_method=\"contour\",\\n    ax=ax,\\n    levels=[0],\\n    colors=\"darkred\",\\n    linewidths=2,\\n)\\n\\ns = 40\\nb1 = ax.scatter(X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\\nb2 = ax.scatter(X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\\nc = ax.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\\nplt.legend(\\n    [mlines.Line2D([], [], color=\"darkred\"), b1, b2, c],\\n    [\\n        \"learned frontier\",\\n        \"training observations\",\\n        \"new regular observations\",\\n        \"new abnormal observations\",\\n    ],\\n    loc=\"upper left\",\\n    prop=matplotlib.font_manager.FontProperties(size=11),\\n)\\nax.set(\\n    xlabel=(\\n        f\"error train: {n_error_train}/200 ; errors novel regular: {n_error_test}/40 ;\"\\n        f\" errors novel abnormal: {n_error_outliers}/40\"\\n    ),\\n    title=\"Novelty Detection\",\\n    xlim=(-5, 5),\\n    ylim=(-5, 5),\\n)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_separating_hyperplane.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================\\nSVM: Maximum margin separating hyperplane\\n=========================================\\n\\nPlot the maximum margin separating hyperplane within a two-class\\nseparable dataset using a Support Vector Machine classifier with\\nlinear kernel.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import svm\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\n# we create 40 separable points\\nX, y = make_blobs(n_samples=40, centers=2, random_state=6)\\n\\n# fit the model, don\\'t regularize for illustration purposes\\nclf = svm.SVC(kernel=\"linear\", C=1000)\\nclf.fit(X, y)\\n\\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\\n\\n# plot the decision function\\nax = plt.gca()\\nDecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    plot_method=\"contour\",\\n    colors=\"k\",\\n    levels=[-1, 0, 1],\\n    alpha=0.5,\\n    linestyles=[\"--\", \"-\", \"--\"],\\n    ax=ax,\\n)\\n# plot support vectors\\nax.scatter(\\n    clf.support_vectors_[:, 0],\\n    clf.support_vectors_[:, 1],\\n    s=100,\\n    linewidth=1,\\n    facecolors=\"none\",\\n    edgecolors=\"k\",\\n)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_iris_svc.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================================\\nPlot different SVM classifiers in the iris dataset\\n==================================================\\n\\nComparison of different linear SVM classifiers on a 2D projection of the iris\\ndataset. We only consider the first 2 features of this dataset:\\n\\n- Sepal length\\n- Sepal width\\n\\nThis example shows how to plot the decision surface for four SVM classifiers\\nwith different kernels.\\n\\nThe linear models ``LinearSVC()`` and ``SVC(kernel=\\'linear\\')`` yield slightly\\ndifferent decision boundaries. This can be a consequence of the following\\ndifferences:\\n\\n- ``LinearSVC`` minimizes the squared hinge loss while ``SVC`` minimizes the\\n  regular hinge loss.\\n\\n- ``LinearSVC`` uses the One-vs-All (also known as One-vs-Rest) multiclass\\n  reduction while ``SVC`` uses the One-vs-One multiclass reduction.\\n\\nBoth linear models have linear decision boundaries (intersecting hyperplanes)\\nwhile the non-linear kernel models (polynomial or Gaussian RBF) have more\\nflexible non-linear decision boundaries with shapes that depend on the kind of\\nkernel and its parameters.\\n\\n.. NOTE:: while plotting the decision function of classifiers for toy 2D\\n   datasets can help get an intuitive understanding of their respective\\n   expressive power, be aware that those intuitions don\\'t always generalize to\\n   more realistic high-dimensional problems.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import datasets, svm\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\n# import some data to play with\\niris = datasets.load_iris()\\n# Take the first two features. We could avoid this by using a two-dim dataset\\nX = iris.data[:, :2]\\ny = iris.target\\n\\n# we create an instance of SVM and fit out data. We do not scale our\\n# data since we want to plot the support vectors\\nC = 1.0  # SVM regularization parameter\\nmodels = (\\n    svm.SVC(kernel=\"linear\", C=C),\\n    svm.LinearSVC(C=C, max_iter=10000),\\n    svm.SVC(kernel=\"rbf\", gamma=0.7, C=C),\\n    svm.SVC(kernel=\"poly\", degree=3, gamma=\"auto\", C=C),\\n)\\nmodels = (clf.fit(X, y) for clf in models)\\n\\n# title for the plots\\ntitles = (\\n    \"SVC with linear kernel\",\\n    \"LinearSVC (linear kernel)\",\\n    \"SVC with RBF kernel\",\\n    \"SVC with polynomial (degree 3) kernel\",\\n)\\n\\n# Set-up 2x2 grid for plotting.\\nfig, sub = plt.subplots(2, 2)\\nplt.subplots_adjust(wspace=0.4, hspace=0.4)\\n\\nX0, X1 = X[:, 0], X[:, 1]\\n\\nfor clf, title, ax in zip(models, titles, sub.flatten()):\\n    disp = DecisionBoundaryDisplay.from_estimator(\\n        clf,\\n        X,\\n        response_method=\"predict\",\\n        cmap=plt.cm.coolwarm,\\n        alpha=0.8,\\n        ax=ax,\\n        xlabel=iris.feature_names[0],\\n        ylabel=iris.feature_names[1],\\n    )\\n    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\\n    ax.set_xticks(())\\n    ax.set_yticks(())\\n    ax.set_title(title)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_separating_hyperplane_unbalanced.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================\\nSVM: Separating hyperplane for unbalanced classes\\n=================================================\\n\\nFind the optimal separating hyperplane using an SVC for classes that\\nare unbalanced.\\n\\nWe first find the separating plane with a plain SVC and then plot\\n(dashed) the separating hyperplane with automatically correction for\\nunbalanced classes.\\n\\n.. currentmodule:: sklearn.linear_model\\n\\n.. note::\\n\\n    This example will also work by replacing ``SVC(kernel=\"linear\")``\\n    with ``SGDClassifier(loss=\"hinge\")``. Setting the ``loss`` parameter\\n    of the :class:`SGDClassifier` equal to ``hinge`` will yield behaviour\\n    such as that of a SVC with a linear kernel.\\n\\n    For example try instead of the ``SVC``::\\n\\n        clf = SGDClassifier(n_iter=100, alpha=0.01)\\n\\n\"\"\"\\n\\nimport matplotlib.lines as mlines\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import svm\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\n# we create two clusters of random points\\nn_samples_1 = 1000\\nn_samples_2 = 100\\ncenters = [[0.0, 0.0], [2.0, 2.0]]\\nclusters_std = [1.5, 0.5]\\nX, y = make_blobs(\\n    n_samples=[n_samples_1, n_samples_2],\\n    centers=centers,\\n    cluster_std=clusters_std,\\n    random_state=0,\\n    shuffle=False,\\n)\\n\\n# fit the model and get the separating hyperplane\\nclf = svm.SVC(kernel=\"linear\", C=1.0)\\nclf.fit(X, y)\\n\\n# fit the model and get the separating hyperplane using weighted classes\\nwclf = svm.SVC(kernel=\"linear\", class_weight={1: 10})\\nwclf.fit(X, y)\\n\\n# plot the samples\\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors=\"k\")\\n\\n# plot the decision functions for both classifiers\\nax = plt.gca()\\ndisp = DecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    plot_method=\"contour\",\\n    colors=\"k\",\\n    levels=[0],\\n    alpha=0.5,\\n    linestyles=[\"-\"],\\n    ax=ax,\\n)\\n\\n# plot decision boundary and margins for weighted classes\\nwdisp = DecisionBoundaryDisplay.from_estimator(\\n    wclf,\\n    X,\\n    plot_method=\"contour\",\\n    colors=\"r\",\\n    levels=[0],\\n    alpha=0.5,\\n    linestyles=[\"-\"],\\n    ax=ax,\\n)\\n\\nplt.legend(\\n    [\\n        mlines.Line2D([], [], color=\"k\", label=\"non weighted\"),\\n        mlines.Line2D([], [], color=\"r\", label=\"weighted\"),\\n    ],\\n    [\"non weighted\", \"weighted\"],\\n    loc=\"upper right\",\\n)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_scale_c.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='r\"\"\"\\n==============================================\\nScaling the regularization parameter for SVCs\\n==============================================\\n\\nThe following example illustrates the effect of scaling the regularization\\nparameter when using :ref:`svm` for :ref:`classification <svm_classification>`.\\nFor SVC classification, we are interested in a risk minimization for the\\nequation:\\n\\n\\n.. math::\\n\\n    C \\\\sum_{i=1, n} \\\\mathcal{L} (f(x_i), y_i) + \\\\Omega (w)\\n\\nwhere\\n\\n    - :math:`C` is used to set the amount of regularization\\n    - :math:`\\\\mathcal{L}` is a `loss` function of our samples\\n      and our model parameters.\\n    - :math:`\\\\Omega` is a `penalty` function of our model parameters\\n\\nIf we consider the loss function to be the individual error per sample, then the\\ndata-fit term, or the sum of the error for each sample, increases as we add more\\nsamples. The penalization term, however, does not increase.\\n\\nWhen using, for example, :ref:`cross validation <cross_validation>`, to set the\\namount of regularization with `C`, there would be a different amount of samples\\nbetween the main problem and the smaller problems within the folds of the cross\\nvalidation.\\n\\nSince the loss function dependens on the amount of samples, the latter\\ninfluences the selected value of `C`. The question that arises is \"How do we\\noptimally adjust C to account for the different amount of training samples?\"\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Data generation\\n# ---------------\\n#\\n# In this example we investigate the effect of reparametrizing the regularization\\n# parameter `C` to account for the number of samples when using either L1 or L2\\n# penalty. For such purpose we create a synthetic dataset with a large number of\\n# features, out of which only a few are informative. We therefore expect the\\n# regularization to shrink the coefficients towards zero (L2 penalty) or exactly\\n# zero (L1 penalty).\\n\\nfrom sklearn.datasets import make_classification\\n\\nn_samples, n_features = 100, 300\\nX, y = make_classification(\\n    n_samples=n_samples, n_features=n_features, n_informative=5, random_state=1\\n)\\n\\n# %%\\n# L1-penalty case\\n# ---------------\\n# In the L1 case, theory says that provided a strong regularization, the\\n# estimator cannot predict as well as a model knowing the true distribution\\n# (even in the limit where the sample size grows to infinity) as it may set some\\n# weights of otherwise predictive features to zero, which induces a bias. It does\\n# say, however, that it is possible to find the right set of non-zero parameters\\n# as well as their signs by tuning `C`.\\n#\\n# We define a linear SVC with the L1 penalty.\\n\\nfrom sklearn.svm import LinearSVC\\n\\nmodel_l1 = LinearSVC(penalty=\"l1\", loss=\"squared_hinge\", dual=False, tol=1e-3)\\n\\n# %%\\n# We compute the mean test score for different values of `C` via\\n# cross-validation.\\n\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom sklearn.model_selection import ShuffleSplit, validation_curve'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_scale_c.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='model_l1 = LinearSVC(penalty=\"l1\", loss=\"squared_hinge\", dual=False, tol=1e-3)\\n\\n# %%\\n# We compute the mean test score for different values of `C` via\\n# cross-validation.\\n\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom sklearn.model_selection import ShuffleSplit, validation_curve\\n\\nCs = np.logspace(-2.3, -1.3, 10)\\ntrain_sizes = np.linspace(0.3, 0.7, 3)\\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\\nshuffle_params = {\\n    \"test_size\": 0.3,\\n    \"n_splits\": 150,\\n    \"random_state\": 1,\\n}\\n\\nresults = {\"C\": Cs}\\nfor label, train_size in zip(labels, train_sizes):\\n    cv = ShuffleSplit(train_size=train_size, **shuffle_params)\\n    train_scores, test_scores = validation_curve(\\n        model_l1,\\n        X,\\n        y,\\n        param_name=\"C\",\\n        param_range=Cs,\\n        cv=cv,\\n        n_jobs=2,\\n    )\\n    results[label] = test_scores.mean(axis=1)\\nresults = pd.DataFrame(results)\\n\\n# %%\\nimport matplotlib.pyplot as plt\\n\\nfig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(12, 6))\\n\\n# plot results without scaling C\\nresults.plot(x=\"C\", ax=axes[0], logx=True)\\naxes[0].set_ylabel(\"CV score\")\\naxes[0].set_title(\"No scaling\")\\n\\nfor label in labels:\\n    best_C = results.loc[results[label].idxmax(), \"C\"]\\n    axes[0].axvline(x=best_C, linestyle=\"--\", color=\"grey\", alpha=0.7)\\n\\n# plot results by scaling C\\nfor train_size_idx, label in enumerate(labels):\\n    train_size = train_sizes[train_size_idx]\\n    results_scaled = results[[label]].assign(\\n        C_scaled=Cs * float(n_samples * np.sqrt(train_size))\\n    )\\n    results_scaled.plot(x=\"C_scaled\", ax=axes[1], logx=True, label=label)\\n    best_C_scaled = results_scaled[\"C_scaled\"].loc[results[label].idxmax()]\\n    axes[1].axvline(x=best_C_scaled, linestyle=\"--\", color=\"grey\", alpha=0.7)\\n\\naxes[1].set_title(\"Scaling C by sqrt(1 / n_samples)\")\\n\\n_ = fig.suptitle(\"Effect of scaling C with L1 penalty\")\\n\\n# %%\\n# In the region of small `C` (strong regularization) all the coefficients\\n# learned by the models are zero, leading to severe underfitting. Indeed, the\\n# accuracy in this region is at the chance level.\\n#\\n# Using the default scale results in a somewhat stable optimal value of `C`,\\n# whereas the transition out of the underfitting region depends on the number of\\n# training samples. The reparametrization leads to even more stable results.\\n#\\n# See e.g. theorem 3 of :arxiv:`On the prediction performance of the Lasso\\n# <1402.1700>` or :arxiv:`Simultaneous analysis of Lasso and Dantzig selector\\n# <0801.1095>` where the regularization parameter is always assumed to be\\n# proportional to 1 / sqrt(n_samples).\\n#\\n# L2-penalty case\\n# ---------------\\n# We can do a similar experiment with the L2 penalty. In this case, the\\n# theory says that in order to achieve prediction consistency, the penalty\\n# parameter should be kept constant as the number of samples grow.\\n\\nmodel_l2 = LinearSVC(penalty=\"l2\", loss=\"squared_hinge\", dual=True)\\nCs = np.logspace(-8, 4, 11)'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_scale_c.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='model_l2 = LinearSVC(penalty=\"l2\", loss=\"squared_hinge\", dual=True)\\nCs = np.logspace(-8, 4, 11)\\n\\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\\nresults = {\"C\": Cs}\\nfor label, train_size in zip(labels, train_sizes):\\n    cv = ShuffleSplit(train_size=train_size, **shuffle_params)\\n    train_scores, test_scores = validation_curve(\\n        model_l2,\\n        X,\\n        y,\\n        param_name=\"C\",\\n        param_range=Cs,\\n        cv=cv,\\n        n_jobs=2,\\n    )\\n    results[label] = test_scores.mean(axis=1)\\nresults = pd.DataFrame(results)\\n\\n# %%\\nimport matplotlib.pyplot as plt\\n\\nfig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(12, 6))\\n\\n# plot results without scaling C\\nresults.plot(x=\"C\", ax=axes[0], logx=True)\\naxes[0].set_ylabel(\"CV score\")\\naxes[0].set_title(\"No scaling\")\\n\\nfor label in labels:\\n    best_C = results.loc[results[label].idxmax(), \"C\"]\\n    axes[0].axvline(x=best_C, linestyle=\"--\", color=\"grey\", alpha=0.8)\\n\\n# plot results by scaling C\\nfor train_size_idx, label in enumerate(labels):\\n    results_scaled = results[[label]].assign(\\n        C_scaled=Cs * float(n_samples * np.sqrt(train_sizes[train_size_idx]))\\n    )\\n    results_scaled.plot(x=\"C_scaled\", ax=axes[1], logx=True, label=label)\\n    best_C_scaled = results_scaled[\"C_scaled\"].loc[results[label].idxmax()]\\n    axes[1].axvline(x=best_C_scaled, linestyle=\"--\", color=\"grey\", alpha=0.8)\\naxes[1].set_title(\"Scaling C by sqrt(1 / n_samples)\")\\n\\nfig.suptitle(\"Effect of scaling C with L2 penalty\")\\nplt.show()\\n\\n# %%\\n# For the L2 penalty case, the reparametrization seems to have a smaller impact\\n# on the stability of the optimal value for the regularization. The transition\\n# out of the overfitting region occurs in a more spread range and the accuracy\\n# does not seem to be degraded up to chance level.\\n#\\n# Try increasing the value to `n_splits=1_000` for better results in the L2\\n# case, which is not shown here due to the limitations on the documentation\\n# builder.'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_anova.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================\\nSVM-Anova: SVM with univariate feature selection\\n=================================================\\n\\nThis example shows how to perform univariate feature selection before running a\\nSVC (support vector classifier) to improve the classification scores. We use\\nthe iris dataset (4 features) and add 36 non-informative features. We can find\\nthat our model achieves best performance when we select around 10% of features.\\n\\n\"\"\"\\n\\n# %%\\n# Load some data to play with\\n# ---------------------------\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\nX, y = load_iris(return_X_y=True)\\n\\n# Add non-informative features\\nrng = np.random.RandomState(0)\\nX = np.hstack((X, 2 * rng.random((X.shape[0], 36))))\\n\\n# %%\\n# Create the pipeline\\n# -------------------\\nfrom sklearn.feature_selection import SelectPercentile, f_classif\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\n\\n# Create a feature-selection transform, a scaler and an instance of SVM that we\\n# combine together to have a full-blown estimator\\n\\nclf = Pipeline(\\n    [\\n        (\"anova\", SelectPercentile(f_classif)),\\n        (\"scaler\", StandardScaler()),\\n        (\"svc\", SVC(gamma=\"auto\")),\\n    ]\\n)\\n\\n# %%\\n# Plot the cross-validation score as a function of percentile of features\\n# -----------------------------------------------------------------------\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.model_selection import cross_val_score\\n\\nscore_means = list()\\nscore_stds = list()\\npercentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\\n\\nfor percentile in percentiles:\\n    clf.set_params(anova__percentile=percentile)\\n    this_scores = cross_val_score(clf, X, y)\\n    score_means.append(this_scores.mean())\\n    score_stds.append(this_scores.std())\\n\\nplt.errorbar(percentiles, score_means, np.array(score_stds))\\nplt.title(\"Performance of the SVM-Anova varying the percentile of features selected\")\\nplt.xticks(np.linspace(0, 100, 11, endpoint=True))\\nplt.xlabel(\"Percentile\")\\nplt.ylabel(\"Accuracy Score\")\\nplt.axis(\"tight\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_weighted_samples.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_decision_function(classifier, sample_weight, axis, title):\\n    # plot the decision function\\n    xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\\n\\n    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    # plot the line, the points, and the nearest vectors to the plane\\n    axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)\\n    axis.scatter(\\n        X[:, 0],\\n        X[:, 1],\\n        c=y,\\n        s=100 * sample_weight,\\n        alpha=0.9,\\n        cmap=plt.cm.bone,\\n        edgecolors=\"black\",\\n    )\\n\\n    axis.axis(\"off\")\\n    axis.set_title(title)'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_weighted_samples.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================\\nSVM: Weighted samples\\n=====================\\n\\nPlot decision function of a weighted dataset, where the size of points\\nis proportional to its weight.\\n\\nThe sample weighting rescales the C parameter, which means that the classifier\\nputs more emphasis on getting these points right. The effect might often be\\nsubtle.\\nTo emphasize the effect here, we particularly weight outliers, making the\\ndeformation of the decision boundary very visible.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import svm\\n\\n\\n# Code for: def plot_decision_function(classifier, sample_weight, axis, title):\\n\\n\\n# we create 20 points\\nnp.random.seed(0)\\nX = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\\ny = [1] * 10 + [-1] * 10\\nsample_weight_last_ten = abs(np.random.randn(len(X)))\\nsample_weight_constant = np.ones(len(X))\\n# and bigger weights to some outliers\\nsample_weight_last_ten[15:] *= 5\\nsample_weight_last_ten[9] *= 15\\n\\n# Fit the models.\\n\\n# This model does not take into account sample weights.\\nclf_no_weights = svm.SVC(gamma=1)\\nclf_no_weights.fit(X, y)\\n\\n# This other model takes into account some dedicated sample weights.\\nclf_weights = svm.SVC(gamma=1)\\nclf_weights.fit(X, y, sample_weight=sample_weight_last_ten)\\n\\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\\nplot_decision_function(\\n    clf_no_weights, sample_weight_constant, axes[0], \"Constant weights\"\\n)\\nplot_decision_function(clf_weights, sample_weight_last_ten, axes[1], \"Modified weights\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_custom_kernel.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def my_kernel(X, Y):\\n    \"\"\"\\n    We create a custom kernel:\\n\\n                 (2  0)\\n    k(X, Y) = X  (    ) Y.T\\n                 (0  1)\\n    \"\"\"\\n    M = np.array([[2, 0], [0, 1.0]])\\n    return np.dot(np.dot(X, M), Y.T)'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_custom_kernel.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================\\nSVM with custom kernel\\n======================\\n\\nSimple usage of Support Vector Machines to classify a sample. It will\\nplot the decision surface and the support vectors.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets, svm\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\n# import some data to play with\\niris = datasets.load_iris()\\nX = iris.data[:, :2]  # we only take the first two features. We could\\n# avoid this ugly slicing by using a two-dim dataset\\nY = iris.target\\n\\n\\n# Code for: def my_kernel(X, Y):\\n\\n\\nh = 0.02  # step size in the mesh\\n\\n# we create an instance of SVM and fit out data.\\nclf = svm.SVC(kernel=my_kernel)\\nclf.fit(X, Y)\\n\\nax = plt.gca()\\nDecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    cmap=plt.cm.Paired,\\n    ax=ax,\\n    response_method=\"predict\",\\n    plot_method=\"pcolormesh\",\\n    shading=\"auto\",\\n)\\n\\n# Plot also the training points\\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolors=\"k\")\\nplt.title(\"3-Class classification using Support Vector Machine with custom kernel\")\\nplt.axis(\"tight\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_linearsvc_support_vectors.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================================\\nPlot the support vectors in LinearSVC\\n=====================================\\n\\nUnlike SVC (based on LIBSVM), LinearSVC (based on LIBLINEAR) does not provide\\nthe support vectors. This example demonstrates how to obtain the support\\nvectors in LinearSVC.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.svm import LinearSVC\\n\\nX, y = make_blobs(n_samples=40, centers=2, random_state=0)\\n\\nplt.figure(figsize=(10, 5))\\nfor i, C in enumerate([1, 100]):\\n    # \"hinge\" is the standard SVM loss\\n    clf = LinearSVC(C=C, loss=\"hinge\", random_state=42).fit(X, y)\\n    # obtain the support vectors through the decision function\\n    decision_function = clf.decision_function(X)\\n    # we can also calculate the decision function manually\\n    # decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_[0]\\n    # The support vectors are the samples that lie within the margin\\n    # boundaries, whose size is conventionally constrained to 1\\n    support_vector_indices = np.where(np.abs(decision_function) <= 1 + 1e-15)[0]\\n    support_vectors = X[support_vector_indices]\\n\\n    plt.subplot(1, 2, i + 1)\\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\\n    ax = plt.gca()\\n    DecisionBoundaryDisplay.from_estimator(\\n        clf,\\n        X,\\n        ax=ax,\\n        grid_resolution=50,\\n        plot_method=\"contour\",\\n        colors=\"k\",\\n        levels=[-1, 0, 1],\\n        alpha=0.5,\\n        linestyles=[\"--\", \"-\", \"--\"],\\n    )\\n    plt.scatter(\\n        support_vectors[:, 0],\\n        support_vectors[:, 1],\\n        s=100,\\n        linewidth=1,\\n        facecolors=\"none\",\\n        edgecolors=\"k\",\\n    )\\n    plt.title(\"C=\" + str(C))\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_rbf_parameters.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class MidpointNormalize(Normalize):\\n    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\\n        self.midpoint = midpoint\\n        Normalize.__init__(self, vmin, vmax, clip)\\n\\n    def __call__(self, value, clip=None):\\n        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\\n        return np.ma.masked_array(np.interp(value, x, y))'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_rbf_parameters.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================\\nRBF SVM parameters\\n==================\\n\\nThis example illustrates the effect of the parameters ``gamma`` and ``C`` of\\nthe Radial Basis Function (RBF) kernel SVM.\\n\\nIntuitively, the ``gamma`` parameter defines how far the influence of a single\\ntraining example reaches, with low values meaning \\'far\\' and high values meaning\\n\\'close\\'. The ``gamma`` parameters can be seen as the inverse of the radius of\\ninfluence of samples selected by the model as support vectors.\\n\\nThe ``C`` parameter trades off correct classification of training examples\\nagainst maximization of the decision function\\'s margin. For larger values of\\n``C``, a smaller margin will be accepted if the decision function is better at\\nclassifying all training points correctly. A lower ``C`` will encourage a\\nlarger margin, therefore a simpler decision function, at the cost of training\\naccuracy. In other words ``C`` behaves as a regularization parameter in the\\nSVM.\\n\\nThe first plot is a visualization of the decision function for a variety of\\nparameter values on a simplified classification problem involving only 2 input\\nfeatures and 2 possible target classes (binary classification). Note that this\\nkind of plot is not possible to do for problems with more features or target\\nclasses.\\n\\nThe second plot is a heatmap of the classifier\\'s cross-validation accuracy as a\\nfunction of ``C`` and ``gamma``. For this example we explore a relatively large\\ngrid for illustration purposes. In practice, a logarithmic grid from\\n:math:`10^{-3}` to :math:`10^3` is usually sufficient. If the best parameters\\nlie on the boundaries of the grid, it can be extended in that direction in a\\nsubsequent search.\\n\\nNote that the heat map plot has a special colorbar with a midpoint value close\\nto the score values of the best performing models so as to make it easy to tell\\nthem apart in the blink of an eye.\\n\\nThe behavior of the model is very sensitive to the ``gamma`` parameter. If\\n``gamma`` is too large, the radius of the area of influence of the support\\nvectors only includes the support vector itself and no amount of\\nregularization with ``C`` will be able to prevent overfitting.\\n\\nWhen ``gamma`` is very small, the model is too constrained and cannot capture\\nthe complexity or \"shape\" of the data. The region of influence of any selected\\nsupport vector would include the whole training set. The resulting model will\\nbehave similarly to a linear model with a set of hyperplanes that separate the\\ncenters of high density of any pair of two classes.\\n\\nFor intermediate values, we can see on the second plot that good models can\\nbe found on a diagonal of ``C`` and ``gamma``. Smooth models (lower ``gamma``\\nvalues) can be made more complex by increasing the importance of classifying\\neach point correctly (larger ``C`` values) hence the diagonal of good\\nperforming models.'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_rbf_parameters.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='Finally, one can also observe that for some intermediate values of ``gamma`` we\\nget equally performing models when ``C`` becomes very large. This suggests that\\nthe set of support vectors does not change anymore. The radius of the RBF\\nkernel alone acts as a good structural regularizer. Increasing ``C`` further\\ndoesn\\'t help, likely because there are no more training points in violation\\n(inside the margin or wrongly classified), or at least no better solution can\\nbe found. Scores being equal, it may make sense to use the smaller ``C``\\nvalues, since very high ``C`` values typically increase fitting time.\\n\\nOn the other hand, lower ``C`` values generally lead to more support vectors,\\nwhich may increase prediction time. Therefore, lowering the value of ``C``\\ninvolves a trade-off between fitting time and prediction time.\\n\\nWe should also note that small differences in scores results from the random\\nsplits of the cross-validation procedure. Those spurious variations can be\\nsmoothed out by increasing the number of CV iterations ``n_splits`` at the\\nexpense of compute time. Increasing the value number of ``C_range`` and\\n``gamma_range`` steps will increase the resolution of the hyper-parameter heat\\nmap.\\n\\n\"\"\"\\n\\n# %%\\n# Utility class to move the midpoint of a colormap to be around\\n# the values of interest.\\n\\nimport numpy as np\\nfrom matplotlib.colors import Normalize\\n\\n\\n# Code for: class MidpointNormalize(Normalize):\\n\\n\\n# %%\\n# Load and prepare data set\\n# -------------------------\\n#\\n# dataset for grid search\\n\\nfrom sklearn.datasets import load_iris\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\n\\n# %%\\n# Dataset for decision function visualization: we only keep the first two\\n# features in X and sub-sample the dataset to keep only 2 classes and\\n# make it a binary classification problem.\\n\\nX_2d = X[:, :2]\\nX_2d = X_2d[y > 0]\\ny_2d = y[y > 0]\\ny_2d -= 1\\n\\n# %%\\n# It is usually a good idea to scale the data for SVM training.\\n# We are cheating a bit in this example in scaling all of the data,\\n# instead of fitting the transformation on the training set and\\n# just applying it on the test set.\\n\\nfrom sklearn.preprocessing import StandardScaler\\n\\nscaler = StandardScaler()\\nX = scaler.fit_transform(X)\\nX_2d = scaler.fit_transform(X_2d)\\n\\n# %%\\n# Train classifiers\\n# -----------------\\n#\\n# For an initial search, a logarithmic grid with basis\\n# 10 is often helpful. Using a basis of 2, a finer\\n# tuning can be achieved but at a much higher cost.\\n\\nfrom sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\\nfrom sklearn.svm import SVC\\n\\nC_range = np.logspace(-2, 10, 13)\\ngamma_range = np.logspace(-9, 3, 13)\\nparam_grid = dict(gamma=gamma_range, C=C_range)\\ncv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\\ngrid.fit(X, y)\\n\\nprint(\\n    \"The best parameters are %s with a score of %0.2f\"\\n    % (grid.best_params_, grid.best_score_)\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_rbf_parameters.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(\\n    \"The best parameters are %s with a score of %0.2f\"\\n    % (grid.best_params_, grid.best_score_)\\n)\\n\\n# %%\\n# Now we need to fit a classifier for all parameters in the 2d version\\n# (we use a smaller set of parameters here because it takes a while to train)\\n\\nC_2d_range = [1e-2, 1, 1e2]\\ngamma_2d_range = [1e-1, 1, 1e1]\\nclassifiers = []\\nfor C in C_2d_range:\\n    for gamma in gamma_2d_range:\\n        clf = SVC(C=C, gamma=gamma)\\n        clf.fit(X_2d, y_2d)\\n        classifiers.append((C, gamma, clf))\\n\\n# %%\\n# Visualization\\n# -------------\\n#\\n# draw visualization of parameter effects\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.figure(figsize=(8, 6))\\nxx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\\nfor k, (C, gamma, clf) in enumerate(classifiers):\\n    # evaluate decision function in a grid\\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    # visualize decision function for these parameters\\n    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\\n    plt.title(\"gamma=10^%d, C=10^%d\" % (np.log10(gamma), np.log10(C)), size=\"medium\")\\n\\n    # visualize parameter\\'s effect on decision function\\n    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\\n    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r, edgecolors=\"k\")\\n    plt.xticks(())\\n    plt.yticks(())\\n    plt.axis(\"tight\")\\n\\nscores = grid.cv_results_[\"mean_test_score\"].reshape(len(C_range), len(gamma_range))\\n\\n# %%\\n# Draw heatmap of the validation accuracy as a function of gamma and C\\n#\\n# The score are encoded as colors with the hot colormap which varies from dark\\n# red to bright yellow. As the most interesting scores are all located in the\\n# 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so\\n# as to make it easier to visualize the small variations of score values in the\\n# interesting range while not brutally collapsing all the low score values to\\n# the same color.\\n\\nplt.figure(figsize=(8, 6))\\nplt.subplots_adjust(left=0.2, right=0.95, bottom=0.15, top=0.95)\\nplt.imshow(\\n    scores,\\n    interpolation=\"nearest\",\\n    cmap=plt.cm.hot,\\n    norm=MidpointNormalize(vmin=0.2, midpoint=0.92),\\n)\\nplt.xlabel(\"gamma\")\\nplt.ylabel(\"C\")\\nplt.colorbar()\\nplt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\\nplt.yticks(np.arange(len(C_range)), C_range)\\nplt.title(\"Validation accuracy\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================================================\\nSupport Vector Regression (SVR) using linear and non-linear kernels\\n===================================================================\\n\\nToy example of 1D regression using linear, polynomial and RBF kernels.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.svm import SVR\\n\\n# %%\\n# Generate sample data\\n# --------------------\\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\\ny = np.sin(X).ravel()\\n\\n# add noise to targets\\ny[::5] += 3 * (0.5 - np.random.rand(8))\\n\\n# %%\\n# Fit regression model\\n# --------------------\\nsvr_rbf = SVR(kernel=\"rbf\", C=100, gamma=0.1, epsilon=0.1)\\nsvr_lin = SVR(kernel=\"linear\", C=100, gamma=\"auto\")\\nsvr_poly = SVR(kernel=\"poly\", C=100, gamma=\"auto\", degree=3, epsilon=0.1, coef0=1)\\n\\n# %%\\n# Look at the results\\n# -------------------\\nlw = 2\\n\\nsvrs = [svr_rbf, svr_lin, svr_poly]\\nkernel_label = [\"RBF\", \"Linear\", \"Polynomial\"]\\nmodel_color = [\"m\", \"c\", \"g\"]\\n\\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)\\nfor ix, svr in enumerate(svrs):\\n    axes[ix].plot(\\n        X,\\n        svr.fit(X, y).predict(X),\\n        color=model_color[ix],\\n        lw=lw,\\n        label=\"{} model\".format(kernel_label[ix]),\\n    )\\n    axes[ix].scatter(\\n        X[svr.support_],\\n        y[svr.support_],\\n        facecolor=\"none\",\\n        edgecolor=model_color[ix],\\n        s=50,\\n        label=\"{} support vectors\".format(kernel_label[ix]),\\n    )\\n    axes[ix].scatter(\\n        X[np.setdiff1d(np.arange(len(X)), svr.support_)],\\n        y[np.setdiff1d(np.arange(len(X)), svr.support_)],\\n        facecolor=\"none\",\\n        edgecolor=\"k\",\\n        s=50,\\n        label=\"other training data\",\\n    )\\n    axes[ix].legend(\\n        loc=\"upper center\",\\n        bbox_to_anchor=(0.5, 1.1),\\n        ncol=1,\\n        fancybox=True,\\n        shadow=True,\\n    )\\n\\nfig.text(0.5, 0.04, \"data\", ha=\"center\", va=\"center\")\\nfig.text(0.06, 0.5, \"target\", ha=\"center\", va=\"center\", rotation=\"vertical\")\\nfig.suptitle(\"Support Vector Regression\", fontsize=14)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_kernels.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_training_data_with_decision_boundary(\\n    kernel, ax=None, long_title=True, support_vectors=True\\n):\\n    # Train the SVC\\n    clf = svm.SVC(kernel=kernel, gamma=2).fit(X, y)\\n\\n    # Settings for plotting\\n    if ax is None:\\n        _, ax = plt.subplots(figsize=(4, 3))\\n    x_min, x_max, y_min, y_max = -3, 3, -3, 3\\n    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\\n\\n    # Plot decision boundary and margins\\n    common_params = {\"estimator\": clf, \"X\": X, \"ax\": ax}\\n    DecisionBoundaryDisplay.from_estimator(\\n        **common_params,\\n        response_method=\"predict\",\\n        plot_method=\"pcolormesh\",\\n        alpha=0.3,\\n    )\\n    DecisionBoundaryDisplay.from_estimator(\\n        **common_params,\\n        response_method=\"decision_function\",\\n        plot_method=\"contour\",\\n        levels=[-1, 0, 1],\\n        colors=[\"k\", \"k\", \"k\"],\\n        linestyles=[\"--\", \"-\", \"--\"],\\n    )\\n\\n    if support_vectors:\\n        # Plot bigger circles around samples that serve as support vectors\\n        ax.scatter(\\n            clf.support_vectors_[:, 0],\\n            clf.support_vectors_[:, 1],\\n            s=150,\\n            facecolors=\"none\",\\n            edgecolors=\"k\",\\n        )\\n\\n    # Plot samples by color and add legend\\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolors=\"k\")\\n    ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\\n    if long_title:\\n        ax.set_title(f\" Decision boundaries of {kernel} kernel in SVC\")\\n    else:\\n        ax.set_title(kernel)\\n\\n    if ax is None:\\n        plt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_kernels.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nPlot classification boundaries with different SVM Kernels\\n=========================================================\\nThis example shows how different kernels in a :class:`~sklearn.svm.SVC` (Support Vector\\nClassifier) influence the classification boundaries in a binary, two-dimensional\\nclassification problem.\\n\\nSVCs aim to find a hyperplane that effectively separates the classes in their training\\ndata by maximizing the margin between the outermost data points of each class. This is\\nachieved by finding the best weight vector :math:`w` that defines the decision boundary\\nhyperplane and minimizes the sum of hinge losses for misclassified samples, as measured\\nby the :func:`~sklearn.metrics.hinge_loss` function. By default, regularization is\\napplied with the parameter `C=1`, which allows for a certain degree of misclassification\\ntolerance.\\n\\nIf the data is not linearly separable in the original feature space, a non-linear kernel\\nparameter can be set. Depending on the kernel, the process involves adding new features\\nor transforming existing features to enrich and potentially add meaning to the data.\\nWhen a kernel other than `\"linear\"` is set, the SVC applies the `kernel trick\\n<https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick>`__, which\\ncomputes the similarity between pairs of data points using the kernel function without\\nexplicitly transforming the entire dataset. The kernel trick surpasses the otherwise\\nnecessary matrix transformation of the whole dataset by only considering the relations\\nbetween all pairs of data points. The kernel function maps two vectors (each pair of\\nobservations) to their similarity using their dot product.\\n\\nThe hyperplane can then be calculated using the kernel function as if the dataset were\\nrepresented in a higher-dimensional space. Using a kernel function instead of an\\nexplicit matrix transformation improves performance, as the kernel function has a time\\ncomplexity of :math:`O({n}^2)`, whereas matrix transformation scales according to the\\nspecific transformation being applied.\\n\\nIn this example, we compare the most common kernel types of Support Vector Machines: the\\nlinear kernel (`\"linear\"`), the polynomial kernel (`\"poly\"`), the radial basis function\\nkernel (`\"rbf\"`) and the sigmoid kernel (`\"sigmoid\"`).\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Creating a dataset\\n# ------------------\\n# We create a two-dimensional classification dataset with 16 samples and two classes. We\\n# plot the samples with the colors matching their respective targets.\\nimport matplotlib.pyplot as plt\\nimport numpy as np'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_kernels.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Creating a dataset\\n# ------------------\\n# We create a two-dimensional classification dataset with 16 samples and two classes. We\\n# plot the samples with the colors matching their respective targets.\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nX = np.array(\\n    [\\n        [0.4, -0.7],\\n        [-1.5, -1.0],\\n        [-1.4, -0.9],\\n        [-1.3, -1.2],\\n        [-1.1, -0.2],\\n        [-1.2, -0.4],\\n        [-0.5, 1.2],\\n        [-1.5, 2.1],\\n        [1.0, 1.0],\\n        [1.3, 0.8],\\n        [1.2, 0.5],\\n        [0.2, -2.0],\\n        [0.5, -2.4],\\n        [0.2, -2.3],\\n        [0.0, -2.7],\\n        [1.3, 2.1],\\n    ]\\n)\\n\\ny = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\\n\\n# Plotting settings\\nfig, ax = plt.subplots(figsize=(4, 3))\\nx_min, x_max, y_min, y_max = -3, 3, -3, 3\\nax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\\n\\n# Plot samples by color and add legend\\nscatter = ax.scatter(X[:, 0], X[:, 1], s=150, c=y, label=y, edgecolors=\"k\")\\nax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\\nax.set_title(\"Samples in two-dimensional feature space\")\\n_ = plt.show()\\n\\n# %%\\n# We can see that the samples are not clearly separable by a straight line.\\n#\\n# Training SVC model and plotting decision boundaries\\n# ---------------------------------------------------\\n# We define a function that fits a :class:`~sklearn.svm.SVC` classifier,\\n# allowing the `kernel` parameter as an input, and then plots the decision\\n# boundaries learned by the model using\\n# :class:`~sklearn.inspection.DecisionBoundaryDisplay`.\\n#\\n# Notice that for the sake of simplicity, the `C` parameter is set to its\\n# default value (`C=1`) in this example and the `gamma` parameter is set to\\n# `gamma=2` across all kernels, although it is automatically ignored for the\\n# linear kernel. In a real classification task, where performance matters,\\n# parameter tuning (by using :class:`~sklearn.model_selection.GridSearchCV` for\\n# instance) is highly recommended to capture different structures within the\\n# data.\\n#\\n# Setting `response_method=\"predict\"` in\\n# :class:`~sklearn.inspection.DecisionBoundaryDisplay` colors the areas based\\n# on their predicted class. Using `response_method=\"decision_function\"` allows\\n# us to also plot the decision boundary and the margins to both sides of it.\\n# Finally the support vectors used during training (which always lay on the\\n# margins) are identified by means of the `support_vectors_` attribute of\\n# the trained SVCs, and plotted as well.\\nfrom sklearn import svm\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\n\\n# Code for: def plot_training_data_with_decision_boundary('), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_kernels.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def plot_training_data_with_decision_boundary(\\n\\n\\n# %%\\n# Linear kernel\\n# *************\\n# Linear kernel is the dot product of the input samples:\\n#\\n# .. math:: K(\\\\mathbf{x}_1, \\\\mathbf{x}_2) = \\\\mathbf{x}_1^\\\\top \\\\mathbf{x}_2\\n#\\n# It is then applied to any combination of two data points (samples) in the\\n# dataset. The dot product of the two points determines the\\n# :func:`~sklearn.metrics.pairwise.cosine_similarity` between both points. The\\n# higher the value, the more similar the points are.\\nplot_training_data_with_decision_boundary(\"linear\")\\n\\n# %%\\n# Training a :class:`~sklearn.svm.SVC` on a linear kernel results in an\\n# untransformed feature space, where the hyperplane and the margins are\\n# straight lines. Due to the lack of expressivity of the linear kernel, the\\n# trained classes do not perfectly capture the training data.\\n#\\n# Polynomial kernel\\n# *****************\\n# The polynomial kernel changes the notion of similarity. The kernel function\\n# is defined as:\\n#\\n# .. math::\\n#   K(\\\\mathbf{x}_1, \\\\mathbf{x}_2) = (\\\\gamma \\\\cdot \\\\\\n#       \\\\mathbf{x}_1^\\\\top\\\\mathbf{x}_2 + r)^d\\n#\\n# where :math:`{d}` is the degree (`degree`) of the polynomial, :math:`{\\\\gamma}`\\n# (`gamma`) controls the influence of each individual training sample on the\\n# decision boundary and :math:`{r}` is the bias term (`coef0`) that shifts the\\n# data up or down. Here, we use the default value for the degree of the\\n# polynomial in the kernel function (`degree=3`). When `coef0=0` (the default),\\n# the data is only transformed, but no additional dimension is added. Using a\\n# polynomial kernel is equivalent to creating\\n# :class:`~sklearn.preprocessing.PolynomialFeatures` and then fitting a\\n# :class:`~sklearn.svm.SVC` with a linear kernel on the transformed data,\\n# although this alternative approach would be computationally expensive for most\\n# datasets.\\nplot_training_data_with_decision_boundary(\"poly\")\\n\\n# %%\\n# The polynomial kernel with `gamma=2`` adapts well to the training data,\\n# causing the margins on both sides of the hyperplane to bend accordingly.\\n#\\n# RBF kernel\\n# **********\\n# The radial basis function (RBF) kernel, also known as the Gaussian kernel, is\\n# the default kernel for Support Vector Machines in scikit-learn. It measures\\n# similarity between two data points in infinite dimensions and then approaches\\n# classification by majority vote. The kernel function is defined as:\\n#\\n# .. math::\\n#   K(\\\\mathbf{x}_1, \\\\mathbf{x}_2) = \\\\exp\\\\left(-\\\\gamma \\\\cdot\\n#       {\\\\|\\\\mathbf{x}_1 - \\\\mathbf{x}_2\\\\|^2}\\\\right)\\n#\\n# where :math:`{\\\\gamma}` (`gamma`) controls the influence of each individual\\n# training sample on the decision boundary.\\n#\\n# The larger the euclidean distance between two points\\n# :math:`\\\\|\\\\mathbf{x}_1 - \\\\mathbf{x}_2\\\\|^2`\\n# the closer the kernel function is to zero. This means that two points far away\\n# are more likely to be dissimilar.\\nplot_training_data_with_decision_boundary(\"rbf\")'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_kernels.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# In the plot we can see how the decision boundaries tend to contract around\\n# data points that are close to each other.\\n#\\n# Sigmoid kernel\\n# **************\\n# The sigmoid kernel function is defined as:\\n#\\n# .. math::\\n#   K(\\\\mathbf{x}_1, \\\\mathbf{x}_2) = \\\\tanh(\\\\gamma \\\\cdot\\n#       \\\\mathbf{x}_1^\\\\top\\\\mathbf{x}_2 + r)\\n#\\n# where the kernel coefficient :math:`{\\\\gamma}` (`gamma`) controls the influence\\n# of each individual training sample on the decision boundary and :math:`{r}` is\\n# the bias term (`coef0`) that shifts the data up or down.\\n#\\n# In the sigmoid kernel, the similarity between two data points is computed\\n# using the hyperbolic tangent function (:math:`\\\\tanh`). The kernel function\\n# scales and possibly shifts the dot product of the two points\\n# (:math:`\\\\mathbf{x}_1` and :math:`\\\\mathbf{x}_2`).\\nplot_training_data_with_decision_boundary(\"sigmoid\")\\n\\n# %%\\n# We can see that the decision boundaries obtained with the sigmoid kernel\\n# appear curved and irregular. The decision boundary tries to separate the\\n# classes by fitting a sigmoid-shaped curve, resulting in a complex boundary\\n# that may not generalize well to unseen data. From this example it becomes\\n# obvious, that the sigmoid kernel has very specific use cases, when dealing\\n# with data that exhibits a sigmoidal shape. In this example, careful fine\\n# tuning might find more generalizable decision boundaries. Because of it\\'s\\n# specificity, the sigmoid kernel is less commonly used in practice compared to\\n# other kernels.\\n#\\n# Conclusion\\n# ----------\\n# In this example, we have visualized the decision boundaries trained with the\\n# provided dataset. The plots serve as an intuitive demonstration of how\\n# different kernels utilize the training data to determine the classification\\n# boundaries.\\n#\\n# The hyperplanes and margins, although computed indirectly, can be imagined as\\n# planes in the transformed feature space. However, in the plots, they are\\n# represented relative to the original feature space, resulting in curved\\n# decision boundaries for the polynomial, RBF, and sigmoid kernels.\\n#\\n# Please note that the plots do not evaluate the individual kernel\\'s accuracy or\\n# quality. They are intended to provide a visual understanding of how the\\n# different kernels use the training data.\\n#\\n# For a comprehensive evaluation, fine-tuning of :class:`~sklearn.svm.SVC`\\n# parameters using techniques such as\\n# :class:`~sklearn.model_selection.GridSearchCV` is recommended to capture the\\n# underlying structures within the data.\\n\\n# %%\\n# XOR dataset\\n# -----------\\n# A classical example of a dataset which is not linearly separable is the XOR\\n# pattern. HEre we demonstrate how different kernels work on such a dataset.\\n\\nxx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))\\nnp.random.seed(0)\\nX = np.random.randn(300, 2)\\ny = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_kernels.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))\\nnp.random.seed(0)\\nX = np.random.randn(300, 2)\\ny = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\\n\\n_, ax = plt.subplots(2, 2, figsize=(8, 8))\\nargs = dict(long_title=False, support_vectors=False)\\nplot_training_data_with_decision_boundary(\"linear\", ax[0, 0], **args)\\nplot_training_data_with_decision_boundary(\"poly\", ax[0, 1], **args)\\nplot_training_data_with_decision_boundary(\"rbf\", ax[1, 0], **args)\\nplot_training_data_with_decision_boundary(\"sigmoid\", ax[1, 1], **args)\\nplt.show()\\n\\n# %%\\n# As you can see from the plots above, only the `rbf` kernel can find a\\n# reasonable decision boundary for the above dataset.'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_margin.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nSVM Margins Example\\n=========================================================\\nThe plots below illustrate the effect the parameter `C` has\\non the separation line. A large value of `C` basically tells\\nour model that we do not have that much faith in our data\\'s\\ndistribution, and will only consider points close to line\\nof separation.\\n\\nA small value of `C` includes more/all the observations, allowing\\nthe margins to be calculated using all the data in the area.\\n\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# Modified for documentation by Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import svm\\n\\n# we create 40 separable points\\nnp.random.seed(0)\\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\\nY = [0] * 20 + [1] * 20\\n\\n# figure number\\nfignum = 1\\n\\n# fit the model\\nfor name, penalty in ((\"unreg\", 1), (\"reg\", 0.05)):\\n    clf = svm.SVC(kernel=\"linear\", C=penalty)\\n    clf.fit(X, Y)\\n\\n    # get the separating hyperplane\\n    w = clf.coef_[0]\\n    a = -w[0] / w[1]\\n    xx = np.linspace(-5, 5)\\n    yy = a * xx - (clf.intercept_[0]) / w[1]\\n\\n    # plot the parallels to the separating hyperplane that pass through the\\n    # support vectors (margin away from hyperplane in direction\\n    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\\n    # 2-d.\\n    margin = 1 / np.sqrt(np.sum(clf.coef_**2))\\n    yy_down = yy - np.sqrt(1 + a**2) * margin\\n    yy_up = yy + np.sqrt(1 + a**2) * margin\\n\\n    # plot the line, the points, and the nearest vectors to the plane\\n    plt.figure(fignum, figsize=(4, 3))\\n    plt.clf()\\n    plt.plot(xx, yy, \"k-\")\\n    plt.plot(xx, yy_down, \"k--\")\\n    plt.plot(xx, yy_up, \"k--\")\\n\\n    plt.scatter(\\n        clf.support_vectors_[:, 0],\\n        clf.support_vectors_[:, 1],\\n        s=80,\\n        facecolors=\"none\",\\n        zorder=10,\\n        edgecolors=\"k\",\\n    )\\n    plt.scatter(\\n        X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.get_cmap(\"RdBu\"), edgecolors=\"k\"\\n    )\\n\\n    plt.axis(\"tight\")\\n    x_min = -4.8\\n    x_max = 4.2\\n    y_min = -6\\n    y_max = 6\\n\\n    YY, XX = np.meshgrid(yy, xx)\\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\\n    Z = clf.decision_function(xy).reshape(XX.shape)\\n\\n    # Put the result into a contour plot\\n    plt.contourf(XX, YY, Z, cmap=plt.get_cmap(\"RdBu\"), alpha=0.5, linestyles=[\"-\"])\\n\\n    plt.xlim(x_min, x_max)\\n    plt.ylim(y_min, y_max)\\n\\n    plt.xticks(())\\n    plt.yticks(())\\n    fignum = fignum + 1\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/svm/plot_svm_tie_breaking.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nSVM Tie Breaking Example\\n=========================================================\\nTie breaking is costly if ``decision_function_shape=\\'ovr\\'``, and therefore it\\nis not enabled by default. This example illustrates the effect of the\\n``break_ties`` parameter for a multiclass classification problem and\\n``decision_function_shape=\\'ovr\\'``.\\n\\nThe two plots differ only in the area in the middle where the classes are\\ntied. If ``break_ties=False``, all input in that area would be classified as\\none class, whereas if ``break_ties=True``, the tie-breaking mechanism will\\ncreate a non-convex decision boundary in that area.\\n\\n\"\"\"\\n\\n# Code source: Andreas Mueller, Adrin Jalali\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.svm import SVC\\n\\nX, y = make_blobs(random_state=27)\\n\\nfig, sub = plt.subplots(2, 1, figsize=(5, 8))\\ntitles = (\"break_ties = False\", \"break_ties = True\")\\n\\nfor break_ties, title, ax in zip((False, True), titles, sub.flatten()):\\n    svm = SVC(\\n        kernel=\"linear\", C=1, break_ties=break_ties, decision_function_shape=\"ovr\"\\n    ).fit(X, y)\\n\\n    xlim = [X[:, 0].min(), X[:, 0].max()]\\n    ylim = [X[:, 1].min(), X[:, 1].max()]\\n\\n    xs = np.linspace(xlim[0], xlim[1], 1000)\\n    ys = np.linspace(ylim[0], ylim[1], 1000)\\n    xx, yy = np.meshgrid(xs, ys)\\n\\n    pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])\\n\\n    colors = [plt.cm.Accent(i) for i in [0, 4, 7]]\\n\\n    points = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"Accent\")\\n    classes = [(0, 1), (0, 2), (1, 2)]\\n    line = np.linspace(X[:, 1].min() - 5, X[:, 1].max() + 5)\\n    ax.imshow(\\n        -pred.reshape(xx.shape),\\n        cmap=\"Accent\",\\n        alpha=0.2,\\n        extent=(xlim[0], xlim[1], ylim[1], ylim[0]),\\n    )\\n\\n    for coef, intercept, col in zip(svm.coef_, svm.intercept_, classes):\\n        line2 = -(line * coef[1] + intercept) / coef[0]\\n        ax.plot(line2, line, \"-\", c=colors[col[0]])\\n        ax.plot(line2, line, \"--\", c=colors[col[1]])\\n    ax.set_xlim(xlim)\\n    ax.set_ylim(ylim)\\n    ax.set_title(title)\\n    ax.set_aspect(\"equal\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_adaboost_multiclass.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def misclassification_error(y_true, y_pred):\\n    return 1 - accuracy_score(y_true, y_pred)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_adaboost_multiclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================================\\nMulti-class AdaBoosted Decision Trees\\n=====================================\\n\\nThis example shows how boosting can improve the prediction accuracy on a\\nmulti-label classification problem. It reproduces a similar experiment as\\ndepicted by Figure 1 in Zhu et al [1]_.\\n\\nThe core principle of AdaBoost (Adaptive Boosting) is to fit a sequence of weak\\nlearners (e.g. Decision Trees) on repeatedly re-sampled versions of the data.\\nEach sample carries a weight that is adjusted after each training step, such\\nthat misclassified samples will be assigned higher weights. The re-sampling\\nprocess with replacement takes into account the weights assigned to each sample.\\nSamples with higher weights have a greater chance of being selected multiple\\ntimes in the new data set, while samples with lower weights are less likely to\\nbe selected. This ensures that subsequent iterations of the algorithm focus on\\nthe difficult-to-classify samples.\\n\\n.. rubric:: References\\n\\n.. [1] :doi:`J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class adaboost.\"\\n    Statistics and its Interface 2.3 (2009): 349-360.\\n    <10.4310/SII.2009.v2.n3.a8>`\\n\\n\"\"\"\\n\\n# Noel Dawe <noel.dawe@gmail.com>\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Creating the dataset\\n# --------------------\\n# The classification dataset is constructed by taking a ten-dimensional standard\\n# normal distribution (:math:`x` in :math:`R^{10}`) and defining three classes\\n# separated by nested concentric ten-dimensional spheres such that roughly equal\\n# numbers of samples are in each class (quantiles of the :math:`\\\\chi^2`\\n# distribution).\\nfrom sklearn.datasets import make_gaussian_quantiles\\n\\nX, y = make_gaussian_quantiles(\\n    n_samples=2_000, n_features=10, n_classes=3, random_state=1\\n)\\n\\n# %%\\n# We split the dataset into 2 sets: 70 percent of the samples are used for\\n# training and the remaining 30 percent for testing.\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, train_size=0.7, random_state=42\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_adaboost_multiclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We split the dataset into 2 sets: 70 percent of the samples are used for\\n# training and the remaining 30 percent for testing.\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, train_size=0.7, random_state=42\\n)\\n\\n# %%\\n# Training the `AdaBoostClassifier`\\n# ---------------------------------\\n# We train the :class:`~sklearn.ensemble.AdaBoostClassifier`. The estimator\\n# utilizes boosting to improve the classification accuracy. Boosting is a method\\n# designed to train weak learners (i.e. `estimator`) that learn from their\\n# predecessor\\'s mistakes.\\n#\\n# Here, we define the weak learner as a\\n# :class:`~sklearn.tree.DecisionTreeClassifier` and set the maximum number of\\n# leaves to 8. In a real setting, this parameter should be tuned. We set it to a\\n# rather low value to limit the runtime of the example.\\n#\\n# The `SAMME` algorithm build into the\\n# :class:`~sklearn.ensemble.AdaBoostClassifier` then uses the correct or\\n# incorrect predictions made be the current weak learner to update the sample\\n# weights used for training the consecutive weak learners. Also, the weight of\\n# the weak learner itself is calculated based on its accuracy in classifying the\\n# training examples. The weight of the weak learner determines its influence on\\n# the final ensemble prediction.\\nfrom sklearn.ensemble import AdaBoostClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\nweak_learner = DecisionTreeClassifier(max_leaf_nodes=8)\\nn_estimators = 300\\n\\nadaboost_clf = AdaBoostClassifier(\\n    estimator=weak_learner,\\n    n_estimators=n_estimators,\\n    algorithm=\"SAMME\",\\n    random_state=42,\\n).fit(X_train, y_train)\\n\\n# %%\\n# Analysis\\n# --------\\n# Convergence of the `AdaBoostClassifier`\\n# ***************************************\\n# To demonstrate the effectiveness of boosting in improving accuracy, we\\n# evaluate the misclassification error of the boosted trees in comparison to two\\n# baseline scores. The first baseline score is the `misclassification_error`\\n# obtained from a single weak-learner (i.e.\\n# :class:`~sklearn.tree.DecisionTreeClassifier`), which serves as a reference\\n# point. The second baseline score is obtained from the\\n# :class:`~sklearn.dummy.DummyClassifier`, which predicts the most prevalent\\n# class in a dataset.\\nfrom sklearn.dummy import DummyClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\ndummy_clf = DummyClassifier()\\n\\n\\n# Code for: def misclassification_error(y_true, y_pred):\\n\\n\\nweak_learners_misclassification_error = misclassification_error(\\n    y_test, weak_learner.fit(X_train, y_train).predict(X_test)\\n)\\n\\ndummy_classifiers_misclassification_error = misclassification_error(\\n    y_test, dummy_clf.fit(X_train, y_train).predict(X_test)\\n)\\n\\nprint(\\n    \"DecisionTreeClassifier\\'s misclassification_error: \"\\n    f\"{weak_learners_misclassification_error:.3f}\"\\n)\\nprint(\\n    \"DummyClassifier\\'s misclassification_error: \"\\n    f\"{dummy_classifiers_misclassification_error:.3f}\"\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_adaboost_multiclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(\\n    \"DecisionTreeClassifier\\'s misclassification_error: \"\\n    f\"{weak_learners_misclassification_error:.3f}\"\\n)\\nprint(\\n    \"DummyClassifier\\'s misclassification_error: \"\\n    f\"{dummy_classifiers_misclassification_error:.3f}\"\\n)\\n\\n# %%\\n# After training the :class:`~sklearn.tree.DecisionTreeClassifier` model, the\\n# achieved error surpasses the expected value that would have been obtained by\\n# guessing the most frequent class label, as the\\n# :class:`~sklearn.dummy.DummyClassifier` does.\\n#\\n# Now, we calculate the `misclassification_error`, i.e. `1 - accuracy`, of the\\n# additive model (:class:`~sklearn.tree.DecisionTreeClassifier`) at each\\n# boosting iteration on the test set to assess its performance.\\n#\\n# We use :meth:`~sklearn.ensemble.AdaBoostClassifier.staged_predict` that makes\\n# as many iterations as the number of fitted estimator (i.e. corresponding to\\n# `n_estimators`). At iteration `n`, the predictions of AdaBoost only use the\\n# `n` first weak learners. We compare these predictions with the true\\n# predictions `y_test` and we, therefore, conclude on the benefit (or not) of adding a\\n# new weak learner into the chain.\\n#\\n# We plot the misclassification error for the different stages:\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\nboosting_errors = pd.DataFrame(\\n    {\\n        \"Number of trees\": range(1, n_estimators + 1),\\n        \"AdaBoost\": [\\n            misclassification_error(y_test, y_pred)\\n            for y_pred in adaboost_clf.staged_predict(X_test)\\n        ],\\n    }\\n).set_index(\"Number of trees\")\\nax = boosting_errors.plot()\\nax.set_ylabel(\"Misclassification error on test set\")\\nax.set_title(\"Convergence of AdaBoost algorithm\")\\n\\nplt.plot(\\n    [boosting_errors.index.min(), boosting_errors.index.max()],\\n    [weak_learners_misclassification_error, weak_learners_misclassification_error],\\n    color=\"tab:orange\",\\n    linestyle=\"dashed\",\\n)\\nplt.plot(\\n    [boosting_errors.index.min(), boosting_errors.index.max()],\\n    [\\n        dummy_classifiers_misclassification_error,\\n        dummy_classifiers_misclassification_error,\\n    ],\\n    color=\"c\",\\n    linestyle=\"dotted\",\\n)\\nplt.legend([\"AdaBoost\", \"DecisionTreeClassifier\", \"DummyClassifier\"], loc=1)\\nplt.show()\\n\\n# %%\\n# The plot shows the missclassification error on the test set after each\\n# boosting iteration. We see that the error of the boosted trees converges to an\\n# error of around 0.3 after 50 iterations, indicating a significantly higher\\n# accuracy compared to a single tree, as illustrated by the dashed line in the\\n# plot.\\n#\\n# The misclassification error jitters because the `SAMME` algorithm uses the\\n# discrete outputs of the weak learners to train the boosted model.\\n#\\n# The convergence of :class:`~sklearn.ensemble.AdaBoostClassifier` is mainly\\n# influenced by the learning rate (i.e. `learning_rate`), the number of weak\\n# learners used (`n_estimators`), and the expressivity of the weak learners\\n# (e.g. `max_leaf_nodes`).'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_adaboost_multiclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Errors and weights of the Weak Learners\\n# ***************************************\\n# As previously mentioned, AdaBoost is a forward stagewise additive model. We\\n# now focus on understanding the relationship between the attributed weights of\\n# the weak learners and their statistical performance.\\n#\\n# We use the fitted :class:`~sklearn.ensemble.AdaBoostClassifier`\\'s attributes\\n# `estimator_errors_` and `estimator_weights_` to investigate this link.\\nweak_learners_info = pd.DataFrame(\\n    {\\n        \"Number of trees\": range(1, n_estimators + 1),\\n        \"Errors\": adaboost_clf.estimator_errors_,\\n        \"Weights\": adaboost_clf.estimator_weights_,\\n    }\\n).set_index(\"Number of trees\")\\n\\naxs = weak_learners_info.plot(\\n    subplots=True, layout=(1, 2), figsize=(10, 4), legend=False, color=\"tab:blue\"\\n)\\naxs[0, 0].set_ylabel(\"Train error\")\\naxs[0, 0].set_title(\"Weak learner\\'s training error\")\\naxs[0, 1].set_ylabel(\"Weight\")\\naxs[0, 1].set_title(\"Weak learner\\'s weight\")\\nfig = axs[0, 0].get_figure()\\nfig.suptitle(\"Weak learner\\'s errors and weights for the AdaBoostClassifier\")\\nfig.tight_layout()\\n\\n# %%\\n# On the left plot, we show the weighted error of each weak learner on the\\n# reweighted training set at each boosting iteration. On the right plot, we show\\n# the weights associated with each weak learner later used to make the\\n# predictions of the final additive model.\\n#\\n# We see that the error of the weak learner is the inverse of the weights. It\\n# means that our additive model will trust more a weak learner that makes\\n# smaller errors (on the training set) by increasing its impact on the final\\n# decision. Indeed, this exactly is the formulation of updating the base\\n# estimators\\' weights after each iteration in AdaBoost.\\n#\\n# .. dropdown:: Mathematical details\\n#\\n#    The weight associated with a weak learner trained at the stage :math:`m` is\\n#    inversely associated with its misclassification error such that:\\n#\\n#    .. math:: \\\\alpha^{(m)} = \\\\log \\\\frac{1 - err^{(m)}}{err^{(m)}} + \\\\log (K - 1),\\n#\\n#    where :math:`\\\\alpha^{(m)}` and :math:`err^{(m)}` are the weight and the error\\n#    of the :math:`m` th weak learner, respectively, and :math:`K` is the number of\\n#    classes in our classification problem.\\n#\\n# Another interesting observation boils down to the fact that the first weak\\n# learners of the model make fewer errors than later weak learners of the\\n# boosting chain.\\n#\\n# The intuition behind this observation is the following: due to the sample\\n# reweighting, later classifiers are forced to try to classify more difficult or\\n# noisy samples and to ignore already well classified samples. Therefore, the\\n# overall error on the training set will increase. That\\'s why the weak learner\\'s\\n# weights are built to counter-balance the worse performing weak learners.'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_bias_variance.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def f(x):\\n    x = x.ravel()\\n\\n    return np.exp(-(x**2)) + 1.5 * np.exp(-((x - 2) ** 2))'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_bias_variance.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def generate(n_samples, noise, n_repeat=1):\\n    X = np.random.rand(n_samples) * 10 - 5\\n    X = np.sort(X)\\n\\n    if n_repeat == 1:\\n        y = f(X) + np.random.normal(0.0, noise, n_samples)\\n    else:\\n        y = np.zeros((n_samples, n_repeat))\\n\\n        for i in range(n_repeat):\\n            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\\n\\n    X = X.reshape((n_samples, 1))\\n\\n    return X, y'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_bias_variance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================================================\\nSingle estimator versus bagging: bias-variance decomposition\\n============================================================\\n\\nThis example illustrates and compares the bias-variance decomposition of the\\nexpected mean squared error of a single estimator against a bagging ensemble.\\n\\nIn regression, the expected mean squared error of an estimator can be\\ndecomposed in terms of bias, variance and noise. On average over datasets of\\nthe regression problem, the bias term measures the average amount by which the\\npredictions of the estimator differ from the predictions of the best possible\\nestimator for the problem (i.e., the Bayes model). The variance term measures\\nthe variability of the predictions of the estimator when fit over different\\nrandom instances of the same problem. Each problem instance is noted \"LS\", for\\n\"Learning Sample\", in the following. Finally, the noise measures the irreducible part\\nof the error which is due the variability in the data.\\n\\nThe upper left figure illustrates the predictions (in dark red) of a single\\ndecision tree trained over a random dataset LS (the blue dots) of a toy 1d\\nregression problem. It also illustrates the predictions (in light red) of other\\nsingle decision trees trained over other (and different) randomly drawn\\ninstances LS of the problem. Intuitively, the variance term here corresponds to\\nthe width of the beam of predictions (in light red) of the individual\\nestimators. The larger the variance, the more sensitive are the predictions for\\n`x` to small changes in the training set. The bias term corresponds to the\\ndifference between the average prediction of the estimator (in cyan) and the\\nbest possible model (in dark blue). On this problem, we can thus observe that\\nthe bias is quite low (both the cyan and the blue curves are close to each\\nother) while the variance is large (the red beam is rather wide).\\n\\nThe lower left figure plots the pointwise decomposition of the expected mean\\nsquared error of a single decision tree. It confirms that the bias term (in\\nblue) is low while the variance is large (in green). It also illustrates the\\nnoise part of the error which, as expected, appears to be constant and around\\n`0.01`.'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_bias_variance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='The right figures correspond to the same plots but using instead a bagging\\nensemble of decision trees. In both figures, we can observe that the bias term\\nis larger than in the previous case. In the upper right figure, the difference\\nbetween the average prediction (in cyan) and the best possible model is larger\\n(e.g., notice the offset around `x=2`). In the lower right figure, the bias\\ncurve is also slightly higher than in the lower left figure. In terms of\\nvariance however, the beam of predictions is narrower, which suggests that the\\nvariance is lower. Indeed, as the lower right figure confirms, the variance\\nterm (in green) is lower than for single decision trees. Overall, the bias-\\nvariance decomposition is therefore no longer the same. The tradeoff is better\\nfor bagging: averaging several decision trees fit on bootstrap copies of the\\ndataset slightly increases the bias term but allows for a larger reduction of\\nthe variance, which results in a lower overall mean squared error (compare the\\nred curves int the lower figures). The script output also confirms this\\nintuition. The total error of the bagging ensemble is lower than the total\\nerror of a single decision tree, and this difference indeed mainly stems from a\\nreduced variance.\\n\\nFor further details on bias-variance decomposition, see section 7.3 of [1]_.\\n\\nReferences\\n----------\\n\\n.. [1] T. Hastie, R. Tibshirani and J. Friedman,\\n       \"Elements of Statistical Learning\", Springer, 2009.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.ensemble import BaggingRegressor\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\n# Settings\\nn_repeat = 50  # Number of iterations for computing expectations\\nn_train = 50  # Size of the training set\\nn_test = 1000  # Size of the test set\\nnoise = 0.1  # Standard deviation of the noise\\nnp.random.seed(0)\\n\\n# Change this for exploring the bias-variance decomposition of other\\n# estimators. This should work well for estimators with high variance (e.g.,\\n# decision trees or KNN), but poorly for estimators with low variance (e.g.,\\n# linear models).\\nestimators = [\\n    (\"Tree\", DecisionTreeRegressor()),\\n    (\"Bagging(Tree)\", BaggingRegressor(DecisionTreeRegressor())),\\n]\\n\\nn_estimators = len(estimators)\\n\\n\\n# Generate data\\n# Code for: def f(x):\\n\\n\\n# Code for: def generate(n_samples, noise, n_repeat=1):\\n\\n\\nX_train = []\\ny_train = []\\n\\nfor i in range(n_repeat):\\n    X, y = generate(n_samples=n_train, noise=noise)\\n    X_train.append(X)\\n    y_train.append(y)\\n\\nX_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\\n\\nplt.figure(figsize=(10, 8))\\n\\n# Loop over estimators to compare\\nfor n, (name, estimator) in enumerate(estimators):\\n    # Compute predictions\\n    y_predict = np.zeros((n_test, n_repeat))\\n\\n    for i in range(n_repeat):\\n        estimator.fit(X_train[i], y_train[i])\\n        y_predict[:, i] = estimator.predict(X_test)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_bias_variance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Loop over estimators to compare\\nfor n, (name, estimator) in enumerate(estimators):\\n    # Compute predictions\\n    y_predict = np.zeros((n_test, n_repeat))\\n\\n    for i in range(n_repeat):\\n        estimator.fit(X_train[i], y_train[i])\\n        y_predict[:, i] = estimator.predict(X_test)\\n\\n    # Bias^2 + Variance + Noise decomposition of the mean squared error\\n    y_error = np.zeros(n_test)\\n\\n    for i in range(n_repeat):\\n        for j in range(n_repeat):\\n            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\\n\\n    y_error /= n_repeat * n_repeat\\n\\n    y_noise = np.var(y_test, axis=1)\\n    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\\n    y_var = np.var(y_predict, axis=1)\\n\\n    print(\\n        \"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \"\\n        \" + {3:.4f} (var) + {4:.4f} (noise)\".format(\\n            name, np.mean(y_error), np.mean(y_bias), np.mean(y_var), np.mean(y_noise)\\n        )\\n    )\\n\\n    # Plot figures\\n    plt.subplot(2, n_estimators, n + 1)\\n    plt.plot(X_test, f(X_test), \"b\", label=\"$f(x)$\")\\n    plt.plot(X_train[0], y_train[0], \".b\", label=\"LS ~ $y = f(x)+noise$\")\\n\\n    for i in range(n_repeat):\\n        if i == 0:\\n            plt.plot(X_test, y_predict[:, i], \"r\", label=r\"$\\\\^y(x)$\")\\n        else:\\n            plt.plot(X_test, y_predict[:, i], \"r\", alpha=0.05)\\n\\n    plt.plot(X_test, np.mean(y_predict, axis=1), \"c\", label=r\"$\\\\mathbb{E}_{LS} \\\\^y(x)$\")\\n\\n    plt.xlim([-5, 5])\\n    plt.title(name)\\n\\n    if n == n_estimators - 1:\\n        plt.legend(loc=(1.1, 0.5))\\n\\n    plt.subplot(2, n_estimators, n_estimators + n + 1)\\n    plt.plot(X_test, y_error, \"r\", label=\"$error(x)$\")\\n    plt.plot(X_test, y_bias, \"b\", label=\"$bias^2(x)$\"),\\n    plt.plot(X_test, y_var, \"g\", label=\"$variance(x)$\"),\\n    plt.plot(X_test, y_noise, \"c\", label=\"$noise(x)$\")\\n\\n    plt.xlim([-5, 5])\\n    plt.ylim([0, 0.1])\\n\\n    if n == n_estimators - 1:\\n        plt.legend(loc=(1.1, 0.5))\\n\\nplt.subplots_adjust(right=0.75)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_monotonic_constraints.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================\\nMonotonic Constraints\\n=====================\\n\\nThis example illustrates the effect of monotonic constraints on a gradient\\nboosting estimator.\\n\\nWe build an artificial dataset where the target value is in general\\npositively correlated with the first feature (with some random and\\nnon-random variations), and in general negatively correlated with the second\\nfeature.\\n\\nBy imposing a monotonic increase or a monotonic decrease constraint, respectively,\\non the features during the learning process, the estimator is able to properly follow\\nthe general trend instead of being subject to the variations.\\n\\nThis example was inspired by the `XGBoost documentation\\n<https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html>`_.\\n\\n\"\"\"\\n\\n# %%\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\nfrom sklearn.inspection import PartialDependenceDisplay\\n\\nrng = np.random.RandomState(0)\\n\\nn_samples = 1000\\nf_0 = rng.rand(n_samples)\\nf_1 = rng.rand(n_samples)\\nX = np.c_[f_0, f_1]\\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\\n\\n# y is positively correlated with f_0, and negatively correlated with f_1\\ny = 5 * f_0 + np.sin(10 * np.pi * f_0) - 5 * f_1 - np.cos(10 * np.pi * f_1) + noise\\n\\n\\n# %%\\n# Fit a first model on this dataset without any constraints.\\ngbdt_no_cst = HistGradientBoostingRegressor()\\ngbdt_no_cst.fit(X, y)\\n\\n# %%\\n# Fit a second model on this dataset with monotonic increase (1)\\n# and a monotonic decrease (-1) constraints, respectively.\\ngbdt_with_monotonic_cst = HistGradientBoostingRegressor(monotonic_cst=[1, -1])\\ngbdt_with_monotonic_cst.fit(X, y)\\n\\n\\n# %%\\n# Let\\'s display the partial dependence of the predictions on the two features.\\nfig, ax = plt.subplots()\\ndisp = PartialDependenceDisplay.from_estimator(\\n    gbdt_no_cst,\\n    X,\\n    features=[0, 1],\\n    feature_names=(\\n        \"First feature\",\\n        \"Second feature\",\\n    ),\\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\\n    ax=ax,\\n)\\nPartialDependenceDisplay.from_estimator(\\n    gbdt_with_monotonic_cst,\\n    X,\\n    features=[0, 1],\\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\\n    ax=disp.axes_,\\n)\\n\\nfor f_idx in (0, 1):\\n    disp.axes_[0, f_idx].plot(\\n        X[:, f_idx], y, \"o\", alpha=0.3, zorder=-1, color=\"tab:green\"\\n    )\\n    disp.axes_[0, f_idx].set_ylim(-6, 6)\\n\\nplt.legend()\\nfig.suptitle(\"Monotonic constraints effect on partial dependences\")\\nplt.show()\\n\\n# %%\\n# We can see that the predictions of the unconstrained model capture the\\n# oscillations of the data while the constrained model follows the general\\n# trend and ignores the local variations.\\n\\n# %%\\n# .. _monotonic_cst_features_names:\\n#\\n# Using feature names to specify monotonic constraints\\n# ----------------------------------------------------\\n#\\n# Note that if the training data has feature names, it\\'s possible to specify the\\n# monotonic constraints by passing a dictionary:\\nimport pandas as pd'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_monotonic_constraints.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X_df = pd.DataFrame(X, columns=[\"f_0\", \"f_1\"])\\n\\ngbdt_with_monotonic_cst_df = HistGradientBoostingRegressor(\\n    monotonic_cst={\"f_0\": 1, \"f_1\": -1}\\n).fit(X_df, y)\\n\\nnp.allclose(\\n    gbdt_with_monotonic_cst_df.predict(X_df), gbdt_with_monotonic_cst.predict(X)\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_iris.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================================================\\nPlot the decision surfaces of ensembles of trees on the iris dataset\\n====================================================================\\n\\nPlot the decision surfaces of forests of randomized trees trained on pairs of\\nfeatures of the iris dataset.\\n\\nThis plot compares the decision surfaces learned by a decision tree classifier\\n(first column), by a random forest classifier (second column), by an extra-\\ntrees classifier (third column) and by an AdaBoost classifier (fourth column).\\n\\nIn the first row, the classifiers are built using the sepal width and\\nthe sepal length features only, on the second row using the petal length and\\nsepal length only, and on the third row using the petal width and the\\npetal length only.\\n\\nIn descending order of quality, when trained (outside of this example) on all\\n4 features using 30 estimators and scored using 10 fold cross validation,\\nwe see::\\n\\n    ExtraTreesClassifier()  # 0.95 score\\n    RandomForestClassifier()  # 0.94 score\\n    AdaBoost(DecisionTree(max_depth=3))  # 0.94 score\\n    DecisionTree(max_depth=None)  # 0.94 score\\n\\nIncreasing `max_depth` for AdaBoost lowers the standard deviation of\\nthe scores (but the average score does not improve).\\n\\nSee the console\\'s output for further details about each model.\\n\\nIn this example you might try to:\\n\\n1) vary the ``max_depth`` for the ``DecisionTreeClassifier`` and\\n   ``AdaBoostClassifier``, perhaps try ``max_depth=3`` for the\\n   ``DecisionTreeClassifier`` or ``max_depth=None`` for ``AdaBoostClassifier``\\n2) vary ``n_estimators``\\n\\nIt is worth noting that RandomForests and ExtraTrees can be fitted in parallel\\non many cores as each tree is built independently of the others. AdaBoost\\'s\\nsamples are built sequentially and so do not use multiple cores.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib.colors import ListedColormap\\n\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    ExtraTreesClassifier,\\n    RandomForestClassifier,\\n)\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Parameters\\nn_classes = 3\\nn_estimators = 30\\ncmap = plt.cm.RdYlBu\\nplot_step = 0.02  # fine step width for decision surface contours\\nplot_step_coarser = 0.5  # step widths for coarse classifier guesses\\nRANDOM_SEED = 13  # fix the seed on each iteration\\n\\n# Load data\\niris = load_iris()\\n\\nplot_idx = 1\\n\\nmodels = [\\n    DecisionTreeClassifier(max_depth=None),\\n    RandomForestClassifier(n_estimators=n_estimators),\\n    ExtraTreesClassifier(n_estimators=n_estimators),\\n    AdaBoostClassifier(\\n        DecisionTreeClassifier(max_depth=3),\\n        n_estimators=n_estimators,\\n        algorithm=\"SAMME\",\\n    ),\\n]\\n\\nfor pair in ([0, 1], [0, 2], [2, 3]):\\n    for model in models:\\n        # We only take the two corresponding features\\n        X = iris.data[:, pair]\\n        y = iris.target'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_iris.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for pair in ([0, 1], [0, 2], [2, 3]):\\n    for model in models:\\n        # We only take the two corresponding features\\n        X = iris.data[:, pair]\\n        y = iris.target\\n\\n        # Shuffle\\n        idx = np.arange(X.shape[0])\\n        np.random.seed(RANDOM_SEED)\\n        np.random.shuffle(idx)\\n        X = X[idx]\\n        y = y[idx]\\n\\n        # Standardize\\n        mean = X.mean(axis=0)\\n        std = X.std(axis=0)\\n        X = (X - mean) / std\\n\\n        # Train\\n        model.fit(X, y)\\n\\n        scores = model.score(X, y)\\n        # Create a title for each column and the console by using str() and\\n        # slicing away useless parts of the string\\n        model_title = str(type(model)).split(\".\")[-1][:-2][: -len(\"Classifier\")]\\n\\n        model_details = model_title\\n        if hasattr(model, \"estimators_\"):\\n            model_details += \" with {} estimators\".format(len(model.estimators_))\\n        print(model_details + \" with features\", pair, \"has a score of\", scores)\\n\\n        plt.subplot(3, 4, plot_idx)\\n        if plot_idx <= len(models):\\n            # Add a title at the top of each column\\n            plt.title(model_title, fontsize=9)\\n\\n        # Now plot the decision boundary using a fine mesh as input to a\\n        # filled contour plot\\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\\n        xx, yy = np.meshgrid(\\n            np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\\n        )\\n\\n        # Plot either a single DecisionTreeClassifier or alpha blend the\\n        # decision surfaces of the ensemble of classifiers\\n        if isinstance(model, DecisionTreeClassifier):\\n            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\\n            Z = Z.reshape(xx.shape)\\n            cs = plt.contourf(xx, yy, Z, cmap=cmap)\\n        else:\\n            # Choose alpha blend level with respect to the number\\n            # of estimators\\n            # that are in use (noting that AdaBoost can use fewer estimators\\n            # than its maximum if it achieves a good enough fit early on)\\n            estimator_alpha = 1.0 / len(model.estimators_)\\n            for tree in model.estimators_:\\n                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\\n                Z = Z.reshape(xx.shape)\\n                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_iris.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Build a coarser grid to plot a set of ensemble classifications\\n        # to show how these are different to what we see in the decision\\n        # surfaces. These points are regularly space and do not have a\\n        # black outline\\n        xx_coarser, yy_coarser = np.meshgrid(\\n            np.arange(x_min, x_max, plot_step_coarser),\\n            np.arange(y_min, y_max, plot_step_coarser),\\n        )\\n        Z_points_coarser = model.predict(\\n            np.c_[xx_coarser.ravel(), yy_coarser.ravel()]\\n        ).reshape(xx_coarser.shape)\\n        cs_points = plt.scatter(\\n            xx_coarser,\\n            yy_coarser,\\n            s=15,\\n            c=Z_points_coarser,\\n            cmap=cmap,\\n            edgecolors=\"none\",\\n        )\\n\\n        # Plot the training points, these are clustered together and have a\\n        # black outline\\n        plt.scatter(\\n            X[:, 0],\\n            X[:, 1],\\n            c=y,\\n            cmap=ListedColormap([\"r\", \"y\", \"b\"]),\\n            edgecolor=\"k\",\\n            s=20,\\n        )\\n        plot_idx += 1  # move on to the next plot in sequence\\n\\nplt.suptitle(\"Classifiers on feature subsets of the Iris dataset\", fontsize=12)\\nplt.axis(\"tight\")\\nplt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_feature_transformation.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def rf_apply(X, model):\\n    return model.apply(X)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_feature_transformation.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def gbdt_apply(X, model):\\n    return model.apply(X)[:, :, 0]'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_feature_transformation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================================\\nFeature transformations with ensembles of trees\\n===============================================\\n\\nTransform your features into a higher dimensional, sparse space. Then train a\\nlinear model on these features.\\n\\nFirst fit an ensemble of trees (totally random trees, a random forest, or\\ngradient boosted trees) on the training set. Then each leaf of each tree in the\\nensemble is assigned a fixed arbitrary feature index in a new feature space.\\nThese leaf indices are then encoded in a one-hot fashion.\\n\\nEach sample goes through the decisions of each tree of the ensemble and ends up\\nin one leaf per tree. The sample is encoded by setting feature values for these\\nleaves to 1 and the other feature values to 0.\\n\\nThe resulting transformer has then learned a supervised, sparse,\\nhigh-dimensional categorical embedding of the data.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# First, we will create a large dataset and split it into three sets:\\n#\\n# - a set to train the ensemble methods which are later used to as a feature\\n#   engineering transformer;\\n# - a set to train the linear model;\\n# - a set to test the linear model.\\n#\\n# It is important to split the data in such way to avoid overfitting by leaking\\n# data.\\n\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = make_classification(n_samples=80_000, random_state=10)\\n\\nX_full_train, X_test, y_full_train, y_test = train_test_split(\\n    X, y, test_size=0.5, random_state=10\\n)\\nX_train_ensemble, X_train_linear, y_train_ensemble, y_train_linear = train_test_split(\\n    X_full_train, y_full_train, test_size=0.5, random_state=10\\n)\\n\\n# %%\\n# For each of the ensemble methods, we will use 10 estimators and a maximum\\n# depth of 3 levels.\\n\\nn_estimators = 10\\nmax_depth = 3\\n\\n# %%\\n# First, we will start by training the random forest and gradient boosting on\\n# the separated training set\\n\\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\\n\\nrandom_forest = RandomForestClassifier(\\n    n_estimators=n_estimators, max_depth=max_depth, random_state=10\\n)\\nrandom_forest.fit(X_train_ensemble, y_train_ensemble)\\n\\ngradient_boosting = GradientBoostingClassifier(\\n    n_estimators=n_estimators, max_depth=max_depth, random_state=10\\n)\\n_ = gradient_boosting.fit(X_train_ensemble, y_train_ensemble)\\n\\n# %%\\n# Notice that :class:`~sklearn.ensemble.HistGradientBoostingClassifier` is much\\n# faster than :class:`~sklearn.ensemble.GradientBoostingClassifier` starting\\n# with intermediate datasets (`n_samples >= 10_000`), which is not the case of\\n# the present example.\\n#\\n# The :class:`~sklearn.ensemble.RandomTreesEmbedding` is an unsupervised method\\n# and thus does not required to be trained independently.\\n\\nfrom sklearn.ensemble import RandomTreesEmbedding\\n\\nrandom_tree_embedding = RandomTreesEmbedding(\\n    n_estimators=n_estimators, max_depth=max_depth, random_state=0\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_feature_transformation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.ensemble import RandomTreesEmbedding\\n\\nrandom_tree_embedding = RandomTreesEmbedding(\\n    n_estimators=n_estimators, max_depth=max_depth, random_state=0\\n)\\n\\n# %%\\n# Now, we will create three pipelines that will use the above embedding as\\n# a preprocessing stage.\\n#\\n# The random trees embedding can be directly pipelined with the logistic\\n# regression because it is a standard scikit-learn transformer.\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import make_pipeline\\n\\nrt_model = make_pipeline(random_tree_embedding, LogisticRegression(max_iter=1000))\\nrt_model.fit(X_train_linear, y_train_linear)\\n\\n# %%\\n# Then, we can pipeline random forest or gradient boosting with a logistic\\n# regression. However, the feature transformation will happen by calling the\\n# method `apply`. The pipeline in scikit-learn expects a call to `transform`.\\n# Therefore, we wrapped the call to `apply` within a `FunctionTransformer`.\\n\\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\\n\\n\\n# Code for: def rf_apply(X, model):\\n\\n\\nrf_leaves_yielder = FunctionTransformer(rf_apply, kw_args={\"model\": random_forest})\\n\\nrf_model = make_pipeline(\\n    rf_leaves_yielder,\\n    OneHotEncoder(handle_unknown=\"ignore\"),\\n    LogisticRegression(max_iter=1000),\\n)\\nrf_model.fit(X_train_linear, y_train_linear)\\n\\n\\n# %%\\n# Code for: def gbdt_apply(X, model):\\n\\n\\ngbdt_leaves_yielder = FunctionTransformer(\\n    gbdt_apply, kw_args={\"model\": gradient_boosting}\\n)\\n\\ngbdt_model = make_pipeline(\\n    gbdt_leaves_yielder,\\n    OneHotEncoder(handle_unknown=\"ignore\"),\\n    LogisticRegression(max_iter=1000),\\n)\\ngbdt_model.fit(X_train_linear, y_train_linear)\\n\\n# %%\\n# We can finally show the different ROC curves for all the models.\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.metrics import RocCurveDisplay\\n\\n_, ax = plt.subplots()\\n\\nmodels = [\\n    (\"RT embedding -> LR\", rt_model),\\n    (\"RF\", random_forest),\\n    (\"RF embedding -> LR\", rf_model),\\n    (\"GBDT\", gradient_boosting),\\n    (\"GBDT embedding -> LR\", gbdt_model),\\n]\\n\\nmodel_displays = {}\\nfor name, pipeline in models:\\n    model_displays[name] = RocCurveDisplay.from_estimator(\\n        pipeline, X_test, y_test, ax=ax, name=name\\n    )\\n_ = ax.set_title(\"ROC curve\")\\n\\n# %%\\n_, ax = plt.subplots()\\nfor name, pipeline in models:\\n    model_displays[name].plot(ax=ax)\\n\\nax.set_xlim(0, 0.2)\\nax.set_ylim(0.8, 1)\\n_ = ax.set_title(\"ROC curve (zoomed in at top left)\")'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_random_forest_regression_multioutput.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================================================\\nComparing random forests and the multi-output meta estimator\\n============================================================\\n\\nAn example to compare multi-output regression with random forest and\\nthe :ref:`multioutput.MultiOutputRegressor <multiclass>` meta-estimator.\\n\\nThis example illustrates the use of the\\n:ref:`multioutput.MultiOutputRegressor <multiclass>` meta-estimator\\nto perform multi-output regression. A random forest regressor is used,\\nwhich supports multi-output regression natively, so the results can be\\ncompared.\\n\\nThe random forest regressor will only ever predict values within the\\nrange of observations or closer to zero for each of the targets. As a\\nresult the predictions are biased towards the centre of the circle.\\n\\nUsing a single underlying feature the model learns both the\\nx and y coordinate as output.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.multioutput import MultiOutputRegressor\\n\\n# Create a random dataset\\nrng = np.random.RandomState(1)\\nX = np.sort(200 * rng.rand(600, 1) - 100, axis=0)\\ny = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T\\ny += 0.5 - rng.rand(*y.shape)\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, train_size=400, test_size=200, random_state=4\\n)\\n\\nmax_depth = 30\\nregr_multirf = MultiOutputRegressor(\\n    RandomForestRegressor(n_estimators=100, max_depth=max_depth, random_state=0)\\n)\\nregr_multirf.fit(X_train, y_train)\\n\\nregr_rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth, random_state=2)\\nregr_rf.fit(X_train, y_train)\\n\\n# Predict on new data\\ny_multirf = regr_multirf.predict(X_test)\\ny_rf = regr_rf.predict(X_test)\\n\\n# Plot the results\\nplt.figure()\\ns = 50\\na = 0.4\\nplt.scatter(\\n    y_test[:, 0],\\n    y_test[:, 1],\\n    edgecolor=\"k\",\\n    c=\"navy\",\\n    s=s,\\n    marker=\"s\",\\n    alpha=a,\\n    label=\"Data\",\\n)\\nplt.scatter(\\n    y_multirf[:, 0],\\n    y_multirf[:, 1],\\n    edgecolor=\"k\",\\n    c=\"cornflowerblue\",\\n    s=s,\\n    alpha=a,\\n    label=\"Multi RF score=%.2f\" % regr_multirf.score(X_test, y_test),\\n)\\nplt.scatter(\\n    y_rf[:, 0],\\n    y_rf[:, 1],\\n    edgecolor=\"k\",\\n    c=\"c\",\\n    s=s,\\n    marker=\"^\",\\n    alpha=a,\\n    label=\"RF score=%.2f\" % regr_rf.score(X_test, y_test),\\n)\\nplt.xlim([-6, 6])\\nplt.ylim([-6, 6])\\nplt.xlabel(\"target 1\")\\nplt.ylabel(\"target 2\")\\nplt.title(\"Comparing random forests and the multi-output meta estimator\")\\nplt.legend()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_voting_decision_regions.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================================\\nPlot the decision boundaries of a VotingClassifier\\n==================================================\\n\\n.. currentmodule:: sklearn\\n\\nPlot the decision boundaries of a :class:`~ensemble.VotingClassifier` for two\\nfeatures of the Iris dataset.\\n\\nPlot the class probabilities of the first sample in a toy dataset predicted by\\nthree different classifiers and averaged by the\\n:class:`~ensemble.VotingClassifier`.\\n\\nFirst, three exemplary classifiers are initialized\\n(:class:`~tree.DecisionTreeClassifier`,\\n:class:`~neighbors.KNeighborsClassifier`, and :class:`~svm.SVC`) and used to\\ninitialize a soft-voting :class:`~ensemble.VotingClassifier` with weights `[2,\\n1, 2]`, which means that the predicted probabilities of the\\n:class:`~tree.DecisionTreeClassifier` and :class:`~svm.SVC` each count 2 times\\nas much as the weights of the :class:`~neighbors.KNeighborsClassifier`\\nclassifier when the averaged probability is calculated.\\n\\n\"\"\"\\n\\nfrom itertools import product\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import datasets\\nfrom sklearn.ensemble import VotingClassifier\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Loading some example data\\niris = datasets.load_iris()\\nX = iris.data[:, [0, 2]]\\ny = iris.target\\n\\n# Training classifiers\\nclf1 = DecisionTreeClassifier(max_depth=4)\\nclf2 = KNeighborsClassifier(n_neighbors=7)\\nclf3 = SVC(gamma=0.1, kernel=\"rbf\", probability=True)\\neclf = VotingClassifier(\\n    estimators=[(\"dt\", clf1), (\"knn\", clf2), (\"svc\", clf3)],\\n    voting=\"soft\",\\n    weights=[2, 1, 2],\\n)\\n\\nclf1.fit(X, y)\\nclf2.fit(X, y)\\nclf3.fit(X, y)\\neclf.fit(X, y)\\n\\n# Plotting decision regions\\nf, axarr = plt.subplots(2, 2, sharex=\"col\", sharey=\"row\", figsize=(10, 8))\\nfor idx, clf, tt in zip(\\n    product([0, 1], [0, 1]),\\n    [clf1, clf2, clf3, eclf],\\n    [\"Decision Tree (depth=4)\", \"KNN (k=7)\", \"Kernel SVM\", \"Soft Voting\"],\\n):\\n    DecisionBoundaryDisplay.from_estimator(\\n        clf, X, alpha=0.4, ax=axarr[idx[0], idx[1]], response_method=\"predict\"\\n    )\\n    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\\n    axarr[idx[0], idx[1]].set_title(tt)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_adaboost_twoclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================\\nTwo-class AdaBoost\\n==================\\n\\nThis example fits an AdaBoosted decision stump on a non-linearly separable\\nclassification dataset composed of two \"Gaussian quantiles\" clusters\\n(see :func:`sklearn.datasets.make_gaussian_quantiles`) and plots the decision\\nboundary and decision scores. The distributions of decision scores are shown\\nseparately for samples of class A and B. The predicted class label for each\\nsample is determined by the sign of the decision score. Samples with decision\\nscores greater than zero are classified as B, and are otherwise classified\\nas A. The magnitude of a decision score determines the degree of likeness with\\nthe predicted class label. Additionally, a new dataset could be constructed\\ncontaining a desired purity of class B, for example, by only selecting samples\\nwith a decision score above some value.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_gaussian_quantiles\\nfrom sklearn.ensemble import AdaBoostClassifier\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Construct dataset\\nX1, y1 = make_gaussian_quantiles(\\n    cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=1\\n)\\nX2, y2 = make_gaussian_quantiles(\\n    mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=1\\n)\\nX = np.concatenate((X1, X2))\\ny = np.concatenate((y1, -y2 + 1))\\n\\n# Create and fit an AdaBoosted decision tree\\nbdt = AdaBoostClassifier(\\n    DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=200\\n)\\n\\nbdt.fit(X, y)\\n\\nplot_colors = \"br\"\\nplot_step = 0.02\\nclass_names = \"AB\"\\n\\nplt.figure(figsize=(10, 5))\\n\\n# Plot the decision boundaries\\nax = plt.subplot(121)\\ndisp = DecisionBoundaryDisplay.from_estimator(\\n    bdt,\\n    X,\\n    cmap=plt.cm.Paired,\\n    response_method=\"predict\",\\n    ax=ax,\\n    xlabel=\"x\",\\n    ylabel=\"y\",\\n)\\nx_min, x_max = disp.xx0.min(), disp.xx0.max()\\ny_min, y_max = disp.xx1.min(), disp.xx1.max()\\nplt.axis(\"tight\")\\n\\n# Plot the training points\\nfor i, n, c in zip(range(2), class_names, plot_colors):\\n    idx = np.where(y == i)\\n    plt.scatter(\\n        X[idx, 0],\\n        X[idx, 1],\\n        c=c,\\n        s=20,\\n        edgecolor=\"k\",\\n        label=\"Class %s\" % n,\\n    )\\nplt.xlim(x_min, x_max)\\nplt.ylim(y_min, y_max)\\nplt.legend(loc=\"upper right\")\\n\\nplt.title(\"Decision Boundary\")'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_adaboost_twoclass.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.title(\"Decision Boundary\")\\n\\n# Plot the two-class decision scores\\ntwoclass_output = bdt.decision_function(X)\\nplot_range = (twoclass_output.min(), twoclass_output.max())\\nplt.subplot(122)\\nfor i, n, c in zip(range(2), class_names, plot_colors):\\n    plt.hist(\\n        twoclass_output[y == i],\\n        bins=10,\\n        range=plot_range,\\n        facecolor=c,\\n        label=\"Class %s\" % n,\\n        alpha=0.5,\\n        edgecolor=\"k\",\\n    )\\nx1, x2, y1, y2 = plt.axis()\\nplt.axis((x1, x2, y1, y2 * 1.2))\\nplt.legend(loc=\"upper right\")\\nplt.ylabel(\"Samples\")\\nplt.xlabel(\"Score\")\\nplt.title(\"Decision Scores\")\\n\\nplt.tight_layout()\\nplt.subplots_adjust(wspace=0.35)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_random_forest_embedding.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nHashing feature transformation using Totally Random Trees\\n=========================================================\\n\\nRandomTreesEmbedding provides a way to map data to a\\nvery high-dimensional, sparse representation, which might\\nbe beneficial for classification.\\nThe mapping is completely unsupervised and very efficient.\\n\\nThis example visualizes the partitions given by several\\ntrees and shows how the transformation can also be used for\\nnon-linear dimensionality reduction or non-linear classification.\\n\\nPoints that are neighboring often share the same leaf of a tree and therefore\\nshare large parts of their hashed representation. This allows to\\nseparate two concentric circles simply based on the principal components\\nof the transformed data with truncated SVD.\\n\\nIn high-dimensional spaces, linear classifiers often achieve\\nexcellent accuracy. For sparse binary data, BernoulliNB\\nis particularly well-suited. The bottom row compares the\\ndecision boundary obtained by BernoulliNB in the transformed\\nspace with an ExtraTreesClassifier forests learned on the\\noriginal data.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_circles\\nfrom sklearn.decomposition import TruncatedSVD\\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomTreesEmbedding\\nfrom sklearn.naive_bayes import BernoulliNB\\n\\n# make a synthetic dataset\\nX, y = make_circles(factor=0.5, random_state=0, noise=0.05)\\n\\n# use RandomTreesEmbedding to transform data\\nhasher = RandomTreesEmbedding(n_estimators=10, random_state=0, max_depth=3)\\nX_transformed = hasher.fit_transform(X)\\n\\n# Visualize result after dimensionality reduction using truncated SVD\\nsvd = TruncatedSVD(n_components=2)\\nX_reduced = svd.fit_transform(X_transformed)\\n\\n# Learn a Naive Bayes classifier on the transformed data\\nnb = BernoulliNB()\\nnb.fit(X_transformed, y)\\n\\n\\n# Learn an ExtraTreesClassifier for comparison\\ntrees = ExtraTreesClassifier(max_depth=3, n_estimators=10, random_state=0)\\ntrees.fit(X, y)\\n\\n\\n# scatter plot of original and reduced data\\nfig = plt.figure(figsize=(9, 8))\\n\\nax = plt.subplot(221)\\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\"k\")\\nax.set_title(\"Original Data (2d)\")\\nax.set_xticks(())\\nax.set_yticks(())\\n\\nax = plt.subplot(222)\\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50, edgecolor=\"k\")\\nax.set_title(\\n    \"Truncated SVD reduction (2d) of transformed data (%dd)\" % X_transformed.shape[1]\\n)\\nax.set_xticks(())\\nax.set_yticks(())\\n\\n# Plot the decision in original space. For that, we will assign a color\\n# to each point in the mesh [x_min, x_max]x[y_min, y_max].\\nh = 0.01\\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n\\n# transform grid using RandomTreesEmbedding\\ntransformed_grid = hasher.transform(np.c_[xx.ravel(), yy.ravel()])\\ny_grid_pred = nb.predict_proba(transformed_grid)[:, 1]'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_random_forest_embedding.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# transform grid using RandomTreesEmbedding\\ntransformed_grid = hasher.transform(np.c_[xx.ravel(), yy.ravel()])\\ny_grid_pred = nb.predict_proba(transformed_grid)[:, 1]\\n\\nax = plt.subplot(223)\\nax.set_title(\"Naive Bayes on Transformed data\")\\nax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\"k\")\\nax.set_ylim(-1.4, 1.4)\\nax.set_xlim(-1.4, 1.4)\\nax.set_xticks(())\\nax.set_yticks(())\\n\\n# transform grid using ExtraTreesClassifier\\ny_grid_pred = trees.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\\n\\nax = plt.subplot(224)\\nax.set_title(\"ExtraTrees predictions\")\\nax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\"k\")\\nax.set_ylim(-1.4, 1.4)\\nax.set_xlim(-1.4, 1.4)\\nax.set_xticks(())\\nax.set_yticks(())\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_hgbt_regression.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def generate_missing_values(X, missing_fraction):\\n    total_cells = X.shape[0] * X.shape[1]\\n    num_missing_cells = int(total_cells * missing_fraction)\\n    row_indices = rng.choice(X.shape[0], num_missing_cells, replace=True)\\n    col_indices = rng.choice(X.shape[1], num_missing_cells, replace=True)\\n    X_missing = X.copy()\\n    X_missing.iloc[row_indices, col_indices] = np.nan\\n    return X_missing'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_hgbt_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================\\nFeatures in Histogram Gradient Boosting Trees\\n==============================================\\n\\n:ref:`histogram_based_gradient_boosting` (HGBT) models may be one of the most\\nuseful supervised learning models in scikit-learn. They are based on a modern\\ngradient boosting implementation comparable to LightGBM and XGBoost. As such,\\nHGBT models are more feature rich than and often outperform alternative models\\nlike random forests, especially when the number of samples is larger than some\\nten thousands (see\\n:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`).\\n\\nThe top usability features of HGBT models are:\\n\\n1. Several available loss functions for mean and quantile regression tasks, see\\n   :ref:`Quantile loss <quantile_support_hgbdt>`.\\n2. :ref:`categorical_support_gbdt`, see\\n   :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py`.\\n3. Early stopping.\\n4. :ref:`nan_support_hgbt`, which avoids the need for an imputer.\\n5. :ref:`monotonic_cst_gbdt`.\\n6. :ref:`interaction_cst_hgbt`.\\n\\nThis example aims at showcasing all points except 2 and 6 in a real life\\nsetting.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Preparing the data\\n# ==================\\n# The `electricity dataset <http://www.openml.org/d/151>`_ consists of data\\n# collected from the Australian New South Wales Electricity Market. In this\\n# market, prices are not fixed and are affected by supply and demand. They are\\n# set every five minutes. Electricity transfers to/from the neighboring state of\\n# Victoria were done to alleviate fluctuations.\\n#\\n# The dataset, originally named ELEC2, contains 45,312 instances dated from 7\\n# May 1996 to 5 December 1998. Each sample of the dataset refers to a period of\\n# 30 minutes, i.e. there are 48 instances for each time period of one day. Each\\n# sample on the dataset has 7 columns:\\n#\\n# - date: between 7 May 1996 to 5 December 1998. Normalized between 0 and 1;\\n# - day: day of week (1-7);\\n# - period: half hour intervals over 24 hours. Normalized between 0 and 1;\\n# - nswprice/nswdemand: electricity price/demand of New South Wales;\\n# - vicprice/vicdemand: electricity price/demand of Victoria.\\n#\\n# Originally, it is a classification task, but here we use it for the regression\\n# task to predict the scheduled electricity transfer between states.\\n\\nfrom sklearn.datasets import fetch_openml\\n\\nelectricity = fetch_openml(\\n    name=\"electricity\", version=1, as_frame=True, parser=\"pandas\"\\n)\\ndf = electricity.frame\\n\\n# %%\\n# This particular dataset has a stepwise constant target for the first 17,760\\n# samples:\\n\\ndf[\"transfer\"][:17_760].unique()\\n\\n# %%\\n# Let us drop those entries and explore the hourly electricity transfer over\\n# different days of the week:\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ndf = electricity.frame.iloc[17_760:]\\nX = df.drop(columns=[\"transfer\", \"class\"])\\ny = df[\"transfer\"]'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_hgbt_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Let us drop those entries and explore the hourly electricity transfer over\\n# different days of the week:\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ndf = electricity.frame.iloc[17_760:]\\nX = df.drop(columns=[\"transfer\", \"class\"])\\ny = df[\"transfer\"]\\n\\nfig, ax = plt.subplots(figsize=(15, 10))\\npointplot = sns.lineplot(x=df[\"period\"], y=df[\"transfer\"], hue=df[\"day\"], ax=ax)\\nhandles, lables = ax.get_legend_handles_labels()\\nax.set(\\n    title=\"Hourly energy transfer for different days of the week\",\\n    xlabel=\"Normalized time of the day\",\\n    ylabel=\"Normalized energy transfer\",\\n)\\n_ = ax.legend(handles, [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"])\\n\\n# %%\\n# Notice that energy transfer increases systematically during weekends.\\n#\\n# Effect of number of trees and early stopping\\n# ============================================\\n# For the sake of illustrating the effect of the (maximum) number of trees, we\\n# train a :class:`~sklearn.ensemble.HistGradientBoostingRegressor` over the\\n# daily electricity transfer using the whole dataset. Then we visualize its\\n# predictions depending on the `max_iter` parameter. Here we don\\'t try to\\n# evaluate the performance of the model and its capacity to generalize but\\n# rather its capability to learn from the training data.\\n\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, shuffle=False)\\n\\nprint(f\"Training sample size: {X_train.shape[0]}\")\\nprint(f\"Test sample size: {X_test.shape[0]}\")\\nprint(f\"Number of features: {X_train.shape[1]}\")\\n\\n# %%\\nmax_iter_list = [5, 50]\\naverage_week_demand = (\\n    df.loc[X_test.index].groupby([\"day\", \"period\"], observed=False)[\"transfer\"].mean()\\n)\\ncolors = sns.color_palette(\"colorblind\")\\nfig, ax = plt.subplots(figsize=(10, 5))\\naverage_week_demand.plot(color=colors[0], label=\"recorded average\", linewidth=2, ax=ax)\\n\\nfor idx, max_iter in enumerate(max_iter_list):\\n    hgbt = HistGradientBoostingRegressor(\\n        max_iter=max_iter, categorical_features=None, random_state=42\\n    )\\n    hgbt.fit(X_train, y_train)\\n\\n    y_pred = hgbt.predict(X_test)\\n    prediction_df = df.loc[X_test.index].copy()\\n    prediction_df[\"y_pred\"] = y_pred\\n    average_pred = prediction_df.groupby([\"day\", \"period\"], observed=False)[\\n        \"y_pred\"\\n    ].mean()\\n    average_pred.plot(\\n        color=colors[idx + 1], label=f\"max_iter={max_iter}\", linewidth=2, ax=ax\\n    )\\n\\nax.set(\\n    title=\"Predicted average energy transfer during the week\",\\n    xticks=[(i + 0.2) * 48 for i in range(7)],\\n    xticklabels=[\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"],\\n    xlabel=\"Time of the week\",\\n    ylabel=\"Normalized energy transfer\",\\n)\\n_ = ax.legend()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_hgbt_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='ax.set(\\n    title=\"Predicted average energy transfer during the week\",\\n    xticks=[(i + 0.2) * 48 for i in range(7)],\\n    xticklabels=[\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"],\\n    xlabel=\"Time of the week\",\\n    ylabel=\"Normalized energy transfer\",\\n)\\n_ = ax.legend()\\n\\n# %%\\n# With just a few iterations, HGBT models can achieve convergence (see\\n# :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`),\\n# meaning that adding more trees does not improve the model anymore. In the\\n# figure above, 5 iterations are not enough to get good predictions. With 50\\n# iterations, we are already able to do a good job.\\n#\\n# Setting `max_iter` too high might degrade the prediction quality and cost a lot of\\n# avoidable computing resources. Therefore, the HGBT implementation in scikit-learn\\n# provides an automatic **early stopping** strategy. With it, the model\\n# uses a fraction of the training data as internal validation set\\n# (`validation_fraction`) and stops training if the validation score does not\\n# improve (or degrades) after `n_iter_no_change` iterations up to a certain\\n# tolerance (`tol`).\\n#\\n# Notice that there is a trade-off between `learning_rate` and `max_iter`:\\n# Generally, smaller learning rates are preferable but require more iterations\\n# to converge to the minimum loss, while larger learning rates converge faster\\n# (less iterations/trees needed) but at the cost of a larger minimum loss.\\n#\\n# Because of this high correlation between the learning rate the number of iterations,\\n# a good practice is to tune the learning rate along with all (important) other\\n# hyperparameters, fit the HBGT on the training set with a large enough value\\n# for `max_iter` and determine the best `max_iter` via early stopping and some\\n# explicit `validation_fraction`.\\n\\ncommon_params = {\\n    \"max_iter\": 1_000,\\n    \"learning_rate\": 0.3,\\n    \"validation_fraction\": 0.2,\\n    \"random_state\": 42,\\n    \"categorical_features\": None,\\n    \"scoring\": \"neg_root_mean_squared_error\",\\n}\\n\\nhgbt = HistGradientBoostingRegressor(early_stopping=True, **common_params)\\nhgbt.fit(X_train, y_train)\\n\\n_, ax = plt.subplots()\\nplt.plot(-hgbt.validation_score_)\\n_ = ax.set(\\n    xlabel=\"number of iterations\",\\n    ylabel=\"root mean squared error\",\\n    title=f\"Loss of hgbt with early stopping (n_iter={hgbt.n_iter_})\",\\n)\\n\\n# %%\\n# We can then overwrite the value for `max_iter` to a reasonable value and avoid\\n# the extra computational cost of the inner validation. Rounding up the number\\n# of iterations may account for variability of the training set:\\n\\nimport math\\n\\ncommon_params[\"max_iter\"] = math.ceil(hgbt.n_iter_ / 100) * 100\\ncommon_params[\"early_stopping\"] = False\\nhgbt = HistGradientBoostingRegressor(**common_params)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_hgbt_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import math\\n\\ncommon_params[\"max_iter\"] = math.ceil(hgbt.n_iter_ / 100) * 100\\ncommon_params[\"early_stopping\"] = False\\nhgbt = HistGradientBoostingRegressor(**common_params)\\n\\n# %%\\n# .. note:: The inner validation done during early stopping is not optimal for\\n#    time series.\\n#\\n# Support for missing values\\n# ==========================\\n# HGBT models have native support of missing values. During training, the tree\\n# grower decides where samples with missing values should go (left or right\\n# child) at each split, based on the potential gain. When predicting, these\\n# samples are sent to the learnt child accordingly. If a feature had no missing\\n# values during training, then for prediction, samples with missing values for that\\n# feature are sent to the child with the most samples (as seen during fit).\\n#\\n# The present example shows how HGBT regressions deal with values missing\\n# completely at random (MCAR), i.e. the missingness does not depend on the\\n# observed data or the unobserved data. We can simulate such scenario by\\n# randomly replacing values from randomly selected features with `nan` values.\\n\\nimport numpy as np\\n\\nfrom sklearn.metrics import root_mean_squared_error\\n\\nrng = np.random.RandomState(42)\\nfirst_week = slice(0, 336)  # first week in the test set as 7 * 48 = 336\\nmissing_fraction_list = [0, 0.01, 0.03]\\n\\n\\n# Code for: def generate_missing_values(X, missing_fraction):\\n\\n\\nfig, ax = plt.subplots(figsize=(12, 6))\\nax.plot(y_test.values[first_week], label=\"Actual transfer\")\\n\\nfor missing_fraction in missing_fraction_list:\\n    X_train_missing = generate_missing_values(X_train, missing_fraction)\\n    X_test_missing = generate_missing_values(X_test, missing_fraction)\\n    hgbt.fit(X_train_missing, y_train)\\n    y_pred = hgbt.predict(X_test_missing[first_week])\\n    rmse = root_mean_squared_error(y_test[first_week], y_pred)\\n    ax.plot(\\n        y_pred[first_week],\\n        label=f\"missing_fraction={missing_fraction}, RMSE={rmse:.3f}\",\\n        alpha=0.5,\\n    )\\nax.set(\\n    title=\"Daily energy transfer predictions on data with MCAR values\",\\n    xticks=[(i + 0.2) * 48 for i in range(7)],\\n    xticklabels=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\\n    xlabel=\"Time of the week\",\\n    ylabel=\"Normalized energy transfer\",\\n)\\n_ = ax.legend(loc=\"lower right\")\\n\\n# %%\\n# As expected, the model degrades as the proportion of missing values increases.\\n#\\n# Support for quantile loss\\n# =========================\\n#\\n# The quantile loss in regression enables a view of the variability or\\n# uncertainty of the target variable. For instance, predicting the 5th and 95th\\n# percentiles can provide a 90% prediction interval, i.e. the range within which\\n# we expect a new observed value to fall with 90% probability.\\n\\nfrom sklearn.metrics import mean_pinball_loss\\n\\nquantiles = [0.95, 0.05]\\npredictions = []\\n\\nfig, ax = plt.subplots(figsize=(12, 6))\\nax.plot(y_test.values[first_week], label=\"Actual transfer\")'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_hgbt_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.metrics import mean_pinball_loss\\n\\nquantiles = [0.95, 0.05]\\npredictions = []\\n\\nfig, ax = plt.subplots(figsize=(12, 6))\\nax.plot(y_test.values[first_week], label=\"Actual transfer\")\\n\\nfor quantile in quantiles:\\n    hgbt_quantile = HistGradientBoostingRegressor(\\n        loss=\"quantile\", quantile=quantile, **common_params\\n    )\\n    hgbt_quantile.fit(X_train, y_train)\\n    y_pred = hgbt_quantile.predict(X_test[first_week])\\n\\n    predictions.append(y_pred)\\n    score = mean_pinball_loss(y_test[first_week], y_pred)\\n    ax.plot(\\n        y_pred[first_week],\\n        label=f\"quantile={quantile}, pinball loss={score:.2f}\",\\n        alpha=0.5,\\n    )\\n\\nax.fill_between(\\n    range(len(predictions[0][first_week])),\\n    predictions[0][first_week],\\n    predictions[1][first_week],\\n    color=colors[0],\\n    alpha=0.1,\\n)\\nax.set(\\n    title=\"Daily energy transfer predictions with quantile loss\",\\n    xticks=[(i + 0.2) * 48 for i in range(7)],\\n    xticklabels=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\\n    xlabel=\"Time of the week\",\\n    ylabel=\"Normalized energy transfer\",\\n)\\n_ = ax.legend(loc=\"lower right\")'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_hgbt_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We observe a tendence to over-estimate the energy transfer. This could be be\\n# quantitatively confirmed by computing empirical coverage numbers as done in\\n# the :ref:`calibration of confidence intervals section <calibration-section>`.\\n# Keep in mind that those predicted percentiles are just estimations from a\\n# model. One can still improve the quality of such estimations by:\\n#\\n# - collecting more data-points;\\n# - better tuning of the model hyperparameters, see\\n#   :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`;\\n# - engineering more predictive features from the same data, see\\n#   :ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`.\\n#\\n# Monotonic constraints\\n# =====================\\n#\\n# Given specific domain knowledge that requires the relationship between a\\n# feature and the target to be monotonically increasing or decreasing, one can\\n# enforce such behaviour in the predictions of a HGBT model using monotonic\\n# constraints. This makes the model more interpretable and can reduce its\\n# variance (and potentially mitigate overfitting) at the risk of increasing\\n# bias. Monotonic constraints can also be used to enforce specific regulatory\\n# requirements, ensure compliance and align with ethical considerations.\\n#\\n# In the present example, the policy of transferring energy from Victoria to New\\n# South Wales is meant to alleviate price fluctuations, meaning that the model\\n# predictions have to enforce such goal, i.e. transfer should increase with\\n# price and demand in New South Wales, but also decrease with price and demand\\n# in Victoria, in order to benefit both populations.\\n#\\n# If the training data has feature names, it‚Äôs possible to specify the monotonic\\n# constraints by passing a dictionary with the convention:\\n#\\n# - 1: monotonic increase\\n# - 0: no constraint\\n# - -1: monotonic decrease\\n#\\n# Alternatively, one can pass an array-like object encoding the above convention by\\n# position.\\n\\nfrom sklearn.inspection import PartialDependenceDisplay\\n\\nmonotonic_cst = {\\n    \"date\": 0,\\n    \"day\": 0,\\n    \"period\": 0,\\n    \"nswdemand\": 1,\\n    \"nswprice\": 1,\\n    \"vicdemand\": -1,\\n    \"vicprice\": -1,\\n}\\nhgbt_no_cst = HistGradientBoostingRegressor(\\n    categorical_features=None, random_state=42\\n).fit(X, y)\\nhgbt_cst = HistGradientBoostingRegressor(\\n    monotonic_cst=monotonic_cst, categorical_features=None, random_state=42\\n).fit(X, y)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_hgbt_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig, ax = plt.subplots(nrows=2, figsize=(15, 10))\\ndisp = PartialDependenceDisplay.from_estimator(\\n    hgbt_no_cst,\\n    X,\\n    features=[\"nswdemand\", \"nswprice\"],\\n    line_kw={\"linewidth\": 2, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\\n    ax=ax[0],\\n)\\nPartialDependenceDisplay.from_estimator(\\n    hgbt_cst,\\n    X,\\n    features=[\"nswdemand\", \"nswprice\"],\\n    line_kw={\"linewidth\": 2, \"label\": \"constrained\", \"color\": \"tab:orange\"},\\n    ax=disp.axes_,\\n)\\ndisp = PartialDependenceDisplay.from_estimator(\\n    hgbt_no_cst,\\n    X,\\n    features=[\"vicdemand\", \"vicprice\"],\\n    line_kw={\"linewidth\": 2, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\\n    ax=ax[1],\\n)\\nPartialDependenceDisplay.from_estimator(\\n    hgbt_cst,\\n    X,\\n    features=[\"vicdemand\", \"vicprice\"],\\n    line_kw={\"linewidth\": 2, \"label\": \"constrained\", \"color\": \"tab:orange\"},\\n    ax=disp.axes_,\\n)\\n_ = plt.legend()\\n\\n# %%\\n# Observe that `nswdemand` and `vicdemand` seem already monotonic without constraint.\\n# This is a good example to show that the model with monotonicity constraints is\\n# \"overconstraining\".\\n#\\n# Additionally, we can verify that the predictive quality of the model is not\\n# significantly degraded by introducing the monotonic constraints. For such\\n# purpose we use :class:`~sklearn.model_selection.TimeSeriesSplit`\\n# cross-validation to estimate the variance of the test score. By doing so we\\n# guarantee that the training data does not succeed the testing data, which is\\n# crucial when dealing with data that have a temporal relationship.\\n\\nfrom sklearn.metrics import make_scorer, root_mean_squared_error\\nfrom sklearn.model_selection import TimeSeriesSplit, cross_validate\\n\\nts_cv = TimeSeriesSplit(n_splits=5, gap=48, test_size=336)  # a week has 336 samples\\nscorer = make_scorer(root_mean_squared_error)\\n\\ncv_results = cross_validate(hgbt_no_cst, X, y, cv=ts_cv, scoring=scorer)\\nrmse = cv_results[\"test_score\"]\\nprint(f\"RMSE without constraints = {rmse.mean():.3f} +/- {rmse.std():.3f}\")\\n\\ncv_results = cross_validate(hgbt_cst, X, y, cv=ts_cv, scoring=scorer)\\nrmse = cv_results[\"test_score\"]\\nprint(f\"RMSE with constraints    = {rmse.mean():.3f} +/- {rmse.std():.3f}\")\\n\\n# %%\\n# That being said, notice the comparison is between two different models that\\n# may be optimized by a different combination of hyperparameters. That is the\\n# reason why we do no use the `common_params` in this section as done before.'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_regularization.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================\\nGradient Boosting regularization\\n================================\\n\\nIllustration of the effect of different regularization strategies\\nfor Gradient Boosting. The example is taken from Hastie et al 2009 [1]_.\\n\\nThe loss function used is binomial deviance. Regularization via\\nshrinkage (``learning_rate < 1.0``) improves performance considerably.\\nIn combination with shrinkage, stochastic gradient boosting\\n(``subsample < 1.0``) can produce more accurate models by reducing the\\nvariance via bagging.\\nSubsampling without shrinkage usually does poorly.\\nAnother strategy to reduce the variance is by subsampling the features\\nanalogous to the random splits in Random Forests\\n(via the ``max_features`` parameter).\\n\\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\\n    Learning Ed. 2\", Springer, 2009.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets, ensemble\\nfrom sklearn.metrics import log_loss\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = datasets.make_hastie_10_2(n_samples=4000, random_state=1)\\n\\n# map labels from {-1, 1} to {0, 1}\\nlabels, y = np.unique(y, return_inverse=True)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)\\n\\noriginal_params = {\\n    \"n_estimators\": 400,\\n    \"max_leaf_nodes\": 4,\\n    \"max_depth\": None,\\n    \"random_state\": 2,\\n    \"min_samples_split\": 5,\\n}\\n\\nplt.figure()\\n\\nfor label, color, setting in [\\n    (\"No shrinkage\", \"orange\", {\"learning_rate\": 1.0, \"subsample\": 1.0}),\\n    (\"learning_rate=0.2\", \"turquoise\", {\"learning_rate\": 0.2, \"subsample\": 1.0}),\\n    (\"subsample=0.5\", \"blue\", {\"learning_rate\": 1.0, \"subsample\": 0.5}),\\n    (\\n        \"learning_rate=0.2, subsample=0.5\",\\n        \"gray\",\\n        {\"learning_rate\": 0.2, \"subsample\": 0.5},\\n    ),\\n    (\\n        \"learning_rate=0.2, max_features=2\",\\n        \"magenta\",\\n        {\"learning_rate\": 0.2, \"max_features\": 2},\\n    ),\\n]:\\n    params = dict(original_params)\\n    params.update(setting)\\n\\n    clf = ensemble.GradientBoostingClassifier(**params)\\n    clf.fit(X_train, y_train)\\n\\n    # compute test set deviance\\n    test_deviance = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\\n\\n    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\\n        test_deviance[i] = 2 * log_loss(y_test, y_proba[:, 1])\\n\\n    plt.plot(\\n        (np.arange(test_deviance.shape[0]) + 1)[::5],\\n        test_deviance[::5],\\n        \"-\",\\n        color=color,\\n        label=label,\\n    )\\n\\nplt.legend(loc=\"upper right\")\\nplt.xlabel(\"Boosting Iterations\")\\nplt.ylabel(\"Test Set Deviance\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_voting_regressor.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================\\nPlot individual and voting regression predictions\\n=================================================\\n\\n.. currentmodule:: sklearn\\n\\nA voting regressor is an ensemble meta-estimator that fits several base\\nregressors, each on the whole dataset. Then it averages the individual\\npredictions to form a final prediction.\\nWe will use three different regressors to predict the data:\\n:class:`~ensemble.GradientBoostingRegressor`,\\n:class:`~ensemble.RandomForestRegressor`, and\\n:class:`~linear_model.LinearRegression`).\\nThen the above 3 regressors will be used for the\\n:class:`~ensemble.VotingRegressor`.\\n\\nFinally, we will plot the predictions made by all models for comparison.\\n\\nWe will work with the diabetes dataset which consists of 10 features\\ncollected from a cohort of diabetes patients. The target is a quantitative\\nmeasure of disease progression one year after baseline.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.ensemble import (\\n    GradientBoostingRegressor,\\n    RandomForestRegressor,\\n    VotingRegressor,\\n)\\nfrom sklearn.linear_model import LinearRegression\\n\\n# %%\\n# Training classifiers\\n# --------------------------------\\n#\\n# First, we will load the diabetes dataset and initiate a gradient boosting\\n# regressor, a random forest regressor and a linear regression. Next, we will\\n# use the 3 regressors to build the voting regressor:\\n\\nX, y = load_diabetes(return_X_y=True)\\n\\n# Train classifiers\\nreg1 = GradientBoostingRegressor(random_state=1)\\nreg2 = RandomForestRegressor(random_state=1)\\nreg3 = LinearRegression()\\n\\nreg1.fit(X, y)\\nreg2.fit(X, y)\\nreg3.fit(X, y)\\n\\nereg = VotingRegressor([(\"gb\", reg1), (\"rf\", reg2), (\"lr\", reg3)])\\nereg.fit(X, y)\\n\\n# %%\\n# Making predictions\\n# --------------------------------\\n#\\n# Now we will use each of the regressors to make the 20 first predictions.\\n\\nxt = X[:20]\\n\\npred1 = reg1.predict(xt)\\npred2 = reg2.predict(xt)\\npred3 = reg3.predict(xt)\\npred4 = ereg.predict(xt)\\n\\n# %%\\n# Plot the results\\n# --------------------------------\\n#\\n# Finally, we will visualize the 20 predictions. The red stars show the average\\n# prediction made by :class:`~ensemble.VotingRegressor`.\\n\\nplt.figure()\\nplt.plot(pred1, \"gd\", label=\"GradientBoostingRegressor\")\\nplt.plot(pred2, \"b^\", label=\"RandomForestRegressor\")\\nplt.plot(pred3, \"ys\", label=\"LinearRegression\")\\nplt.plot(pred4, \"r*\", ms=10, label=\"VotingRegressor\")\\n\\nplt.tick_params(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\\nplt.ylabel(\"predicted\")\\nplt.xlabel(\"training samples\")\\nplt.legend(loc=\"best\")\\nplt.title(\"Regressor predictions and their average\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_early_stopping.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================\\nEarly stopping in Gradient Boosting\\n===================================\\n\\nGradient Boosting is an ensemble technique that combines multiple weak\\nlearners, typically decision trees, to create a robust and powerful\\npredictive model. It does so in an iterative fashion, where each new stage\\n(tree) corrects the errors of the previous ones.\\n\\nEarly stopping is a technique in Gradient Boosting that allows us to find\\nthe optimal number of iterations required to build a model that generalizes\\nwell to unseen data and avoids overfitting. The concept is simple: we set\\naside a portion of our dataset as a validation set (specified using\\n`validation_fraction`) to assess the model\\'s performance during training.\\nAs the model is iteratively built with additional stages (trees), its\\nperformance on the validation set is monitored as a function of the\\nnumber of steps.\\n\\nEarly stopping becomes effective when the model\\'s performance on the\\nvalidation set plateaus or worsens (within deviations specified by `tol`)\\nover a certain number of consecutive stages (specified by `n_iter_no_change`).\\nThis signals that the model has reached a point where further iterations may\\nlead to overfitting, and it\\'s time to stop training.\\n\\nThe number of estimators (trees) in the final model, when early stopping is\\napplied, can be accessed using the `n_estimators_` attribute. Overall, early\\nstopping is a valuable tool to strike a balance between model performance and\\nefficiency in gradient boosting.\\n\\nLicense: BSD 3 clause\\n\\n\"\"\"\\n\\n# %%\\n# Data Preparation\\n# ----------------\\n# First we load and prepares the California Housing Prices dataset for\\n# training and evaluation. It subsets the dataset, splits it into training\\n# and validation sets.\\n\\nimport time\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\n\\ndata = fetch_california_housing()\\nX, y = data.data[:600], data.target[:600]\\n\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# %%\\n# Model Training and Comparison\\n# -----------------------------\\n# Two :class:`~sklearn.ensemble.GradientBoostingRegressor` models are trained:\\n# one with and another without early stopping. The purpose is to compare their\\n# performance. It also calculates the training time and the `n_estimators_`\\n# used by both models.\\n\\nparams = dict(n_estimators=1000, max_depth=5, learning_rate=0.1, random_state=42)\\n\\ngbm_full = GradientBoostingRegressor(**params)\\ngbm_early_stopping = GradientBoostingRegressor(\\n    **params,\\n    validation_fraction=0.1,\\n    n_iter_no_change=10,\\n)\\n\\nstart_time = time.time()\\ngbm_full.fit(X_train, y_train)\\ntraining_time_full = time.time() - start_time\\nn_estimators_full = gbm_full.n_estimators_'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_early_stopping.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='start_time = time.time()\\ngbm_full.fit(X_train, y_train)\\ntraining_time_full = time.time() - start_time\\nn_estimators_full = gbm_full.n_estimators_\\n\\nstart_time = time.time()\\ngbm_early_stopping.fit(X_train, y_train)\\ntraining_time_early_stopping = time.time() - start_time\\nestimators_early_stopping = gbm_early_stopping.n_estimators_\\n\\n# %%\\n# Error Calculation\\n# -----------------\\n# The code calculates the :func:`~sklearn.metrics.mean_squared_error` for both\\n# training and validation datasets for the models trained in the previous\\n# section. It computes the errors for each boosting iteration. The purpose is\\n# to assess the performance and convergence of the models.\\n\\ntrain_errors_without = []\\nval_errors_without = []\\n\\ntrain_errors_with = []\\nval_errors_with = []\\n\\nfor i, (train_pred, val_pred) in enumerate(\\n    zip(\\n        gbm_full.staged_predict(X_train),\\n        gbm_full.staged_predict(X_val),\\n    )\\n):\\n    train_errors_without.append(mean_squared_error(y_train, train_pred))\\n    val_errors_without.append(mean_squared_error(y_val, val_pred))\\n\\nfor i, (train_pred, val_pred) in enumerate(\\n    zip(\\n        gbm_early_stopping.staged_predict(X_train),\\n        gbm_early_stopping.staged_predict(X_val),\\n    )\\n):\\n    train_errors_with.append(mean_squared_error(y_train, train_pred))\\n    val_errors_with.append(mean_squared_error(y_val, val_pred))\\n\\n# %%\\n# Visualize Comparison\\n# --------------------\\n# It includes three subplots:\\n#\\n# 1. Plotting training errors of both models over boosting iterations.\\n# 2. Plotting validation errors of both models over boosting iterations.\\n# 3. Creating a bar chart to compare the training times and the estimator used\\n#    of the models with and without early stopping.\\n#\\n\\nfig, axes = plt.subplots(ncols=3, figsize=(12, 4))\\n\\naxes[0].plot(train_errors_without, label=\"gbm_full\")\\naxes[0].plot(train_errors_with, label=\"gbm_early_stopping\")\\naxes[0].set_xlabel(\"Boosting Iterations\")\\naxes[0].set_ylabel(\"MSE (Training)\")\\naxes[0].set_yscale(\"log\")\\naxes[0].legend()\\naxes[0].set_title(\"Training Error\")\\n\\naxes[1].plot(val_errors_without, label=\"gbm_full\")\\naxes[1].plot(val_errors_with, label=\"gbm_early_stopping\")\\naxes[1].set_xlabel(\"Boosting Iterations\")\\naxes[1].set_ylabel(\"MSE (Validation)\")\\naxes[1].set_yscale(\"log\")\\naxes[1].legend()\\naxes[1].set_title(\"Validation Error\")\\n\\ntraining_times = [training_time_full, training_time_early_stopping]\\nlabels = [\"gbm_full\", \"gbm_early_stopping\"]\\nbars = axes[2].bar(labels, training_times)\\naxes[2].set_ylabel(\"Training Time (s)\")\\n\\nfor bar, n_estimators in zip(bars, [n_estimators_full, estimators_early_stopping]):\\n    height = bar.get_height()\\n    axes[2].text(\\n        bar.get_x() + bar.get_width() / 2,\\n        height + 0.001,\\n        f\"Estimators: {n_estimators}\",\\n        ha=\"center\",\\n        va=\"bottom\",\\n    )\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_early_stopping.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.tight_layout()\\nplt.show()\\n\\n# %%\\n# The difference in training error between the `gbm_full` and the\\n# `gbm_early_stopping` stems from the fact that `gbm_early_stopping` sets\\n# aside `validation_fraction` of the training data as internal validation set.\\n# Early stopping is decided based on this internal validation score.\\n\\n# %%\\n# Summary\\n# -------\\n# In our example with the :class:`~sklearn.ensemble.GradientBoostingRegressor`\\n# model on the California Housing Prices dataset, we have demonstrated the\\n# practical benefits of early stopping:\\n#\\n# - **Preventing Overfitting:** We showed how the validation error stabilizes\\n#   or starts to increase after a certain point, indicating that the model\\n#   generalizes better to unseen data. This is achieved by stopping the training\\n#   process before overfitting occurs.\\n# - **Improving Training Efficiency:** We compared training times between\\n#   models with and without early stopping. The model with early stopping\\n#   achieved comparable accuracy while requiring significantly fewer\\n#   estimators, resulting in faster training.'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================\\nGradient Boosting regression\\n============================\\n\\nThis example demonstrates Gradient Boosting to produce a predictive\\nmodel from an ensemble of weak predictive models. Gradient boosting can be used\\nfor regression and classification problems. Here, we will train a model to\\ntackle a diabetes regression task. We will obtain the results from\\n:class:`~sklearn.ensemble.GradientBoostingRegressor` with least squares loss\\nand 500 regression trees of depth 4.\\n\\nNote: For larger datasets (n_samples >= 10000), please refer to\\n:class:`~sklearn.ensemble.HistGradientBoostingRegressor`. See\\n:ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an example\\nshowcasing some other advantages of\\n:class:`~ensemble.HistGradientBoostingRegressor`.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets, ensemble\\nfrom sklearn.inspection import permutation_importance\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils.fixes import parse_version\\n\\n# %%\\n# Load the data\\n# -------------------------------------\\n#\\n# First we need to load the data.\\n\\ndiabetes = datasets.load_diabetes()\\nX, y = diabetes.data, diabetes.target\\n\\n# %%\\n# Data preprocessing\\n# -------------------------------------\\n#\\n# Next, we will split our dataset to use 90% for training and leave the rest\\n# for testing. We will also set the regression model parameters. You can play\\n# with these parameters to see how the results change.\\n#\\n# `n_estimators` : the number of boosting stages that will be performed.\\n# Later, we will plot deviance against boosting iterations.\\n#\\n# `max_depth` : limits the number of nodes in the tree.\\n# The best value depends on the interaction of the input variables.\\n#\\n# `min_samples_split` : the minimum number of samples required to split an\\n# internal node.\\n#\\n# `learning_rate` : how much the contribution of each tree will shrink.\\n#\\n# `loss` : loss function to optimize. The least squares function is  used in\\n# this case however, there are many other options (see\\n# :class:`~sklearn.ensemble.GradientBoostingRegressor` ).\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.1, random_state=13\\n)\\n\\nparams = {\\n    \"n_estimators\": 500,\\n    \"max_depth\": 4,\\n    \"min_samples_split\": 5,\\n    \"learning_rate\": 0.01,\\n    \"loss\": \"squared_error\",\\n}\\n\\n# %%\\n# Fit regression model\\n# --------------------\\n#\\n# Now we will initiate the gradient boosting regressors and fit it with our\\n# training data. Let\\'s also look and the mean squared error on the test data.\\n\\nreg = ensemble.GradientBoostingRegressor(**params)\\nreg.fit(X_train, y_train)\\n\\nmse = mean_squared_error(y_test, reg.predict(X_test))\\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='reg = ensemble.GradientBoostingRegressor(**params)\\nreg.fit(X_train, y_train)\\n\\nmse = mean_squared_error(y_test, reg.predict(X_test))\\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\\n\\n# %%\\n# Plot training deviance\\n# ----------------------\\n#\\n# Finally, we will visualize the results. To do that we will first compute the\\n# test set deviance and then plot it against boosting iterations.\\n\\ntest_score = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\\nfor i, y_pred in enumerate(reg.staged_predict(X_test)):\\n    test_score[i] = mean_squared_error(y_test, y_pred)\\n\\nfig = plt.figure(figsize=(6, 6))\\nplt.subplot(1, 1, 1)\\nplt.title(\"Deviance\")\\nplt.plot(\\n    np.arange(params[\"n_estimators\"]) + 1,\\n    reg.train_score_,\\n    \"b-\",\\n    label=\"Training Set Deviance\",\\n)\\nplt.plot(\\n    np.arange(params[\"n_estimators\"]) + 1, test_score, \"r-\", label=\"Test Set Deviance\"\\n)\\nplt.legend(loc=\"upper right\")\\nplt.xlabel(\"Boosting Iterations\")\\nplt.ylabel(\"Deviance\")\\nfig.tight_layout()\\nplt.show()\\n\\n# %%\\n# Plot feature importance\\n# -----------------------\\n#\\n# .. warning::\\n#    Careful, impurity-based feature importances can be misleading for\\n#    **high cardinality** features (many unique values). As an alternative,\\n#    the permutation importances of ``reg`` can be computed on a\\n#    held out test set. See :ref:`permutation_importance` for more details.\\n#\\n# For this example, the impurity-based and permutation methods identify the\\n# same 2 strongly predictive features but not in the same order. The third most\\n# predictive feature, \"bp\", is also the same for the 2 methods. The remaining\\n# features are less predictive and the error bars of the permutation plot\\n# show that they overlap with 0.\\n\\nfeature_importance = reg.feature_importances_\\nsorted_idx = np.argsort(feature_importance)\\npos = np.arange(sorted_idx.shape[0]) + 0.5\\nfig = plt.figure(figsize=(12, 6))\\nplt.subplot(1, 2, 1)\\nplt.barh(pos, feature_importance[sorted_idx], align=\"center\")\\nplt.yticks(pos, np.array(diabetes.feature_names)[sorted_idx])\\nplt.title(\"Feature Importance (MDI)\")\\n\\nresult = permutation_importance(\\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\\n)\\nsorted_idx = result.importances_mean.argsort()\\nplt.subplot(1, 2, 2)\\n\\n# `labels` argument in boxplot is deprecated in matplotlib 3.9 and has been\\n# renamed to `tick_labels`. The following code handles this, but as a\\n# scikit-learn user you probably can write simpler code by using `labels=...`\\n# (matplotlib < 3.9) or `tick_labels=...` (matplotlib >= 3.9).\\ntick_labels_parameter_name = (\\n    \"tick_labels\"\\n    if parse_version(matplotlib.__version__) >= parse_version(\"3.9\")\\n    else \"labels\"\\n)\\ntick_labels_dict = {\\n    tick_labels_parameter_name: np.array(diabetes.feature_names)[sorted_idx]\\n}\\nplt.boxplot(result.importances[sorted_idx].T, vert=False, **tick_labels_dict)\\nplt.title(\"Permutation Importance (test set)\")\\nfig.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_ensemble_oob.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================\\nOOB Errors for Random Forests\\n=============================\\n\\nThe ``RandomForestClassifier`` is trained using *bootstrap aggregation*, where\\neach new tree is fit from a bootstrap sample of the training observations\\n:math:`z_i = (x_i, y_i)`. The *out-of-bag* (OOB) error is the average error for\\neach :math:`z_i` calculated using predictions from the trees that do not\\ncontain :math:`z_i` in their respective bootstrap sample. This allows the\\n``RandomForestClassifier`` to be fit and validated whilst being trained [1]_.\\n\\nThe example below demonstrates how the OOB error can be measured at the\\naddition of each new tree during training. The resulting plot allows a\\npractitioner to approximate a suitable value of ``n_estimators`` at which the\\nerror stabilizes.\\n\\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\\n       Learning Ed. 2\", p592-593, Springer, 2009.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom collections import OrderedDict\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nRANDOM_STATE = 123\\n\\n# Generate a binary classification dataset.\\nX, y = make_classification(\\n    n_samples=500,\\n    n_features=25,\\n    n_clusters_per_class=1,\\n    n_informative=15,\\n    random_state=RANDOM_STATE,\\n)\\n\\n# NOTE: Setting the `warm_start` construction parameter to `True` disables\\n# support for parallelized ensembles but is necessary for tracking the OOB\\n# error trajectory during training.\\nensemble_clfs = [\\n    (\\n        \"RandomForestClassifier, max_features=\\'sqrt\\'\",\\n        RandomForestClassifier(\\n            warm_start=True,\\n            oob_score=True,\\n            max_features=\"sqrt\",\\n            random_state=RANDOM_STATE,\\n        ),\\n    ),\\n    (\\n        \"RandomForestClassifier, max_features=\\'log2\\'\",\\n        RandomForestClassifier(\\n            warm_start=True,\\n            max_features=\"log2\",\\n            oob_score=True,\\n            random_state=RANDOM_STATE,\\n        ),\\n    ),\\n    (\\n        \"RandomForestClassifier, max_features=None\",\\n        RandomForestClassifier(\\n            warm_start=True,\\n            max_features=None,\\n            oob_score=True,\\n            random_state=RANDOM_STATE,\\n        ),\\n    ),\\n]\\n\\n# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\\nerror_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\\n\\n# Range of `n_estimators` values to explore.\\nmin_estimators = 15\\nmax_estimators = 150\\n\\nfor label, clf in ensemble_clfs:\\n    for i in range(min_estimators, max_estimators + 1, 5):\\n        clf.set_params(n_estimators=i)\\n        clf.fit(X, y)\\n\\n        # Record the OOB error for each `n_estimators=i` setting.\\n        oob_error = 1 - clf.oob_score_\\n        error_rate[label].append((i, oob_error))'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_ensemble_oob.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Record the OOB error for each `n_estimators=i` setting.\\n        oob_error = 1 - clf.oob_score_\\n        error_rate[label].append((i, oob_error))\\n\\n# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\\nfor label, clf_err in error_rate.items():\\n    xs, ys = zip(*clf_err)\\n    plt.plot(xs, ys, label=label)\\n\\nplt.xlim(min_estimators, max_estimators)\\nplt.xlabel(\"n_estimators\")\\nplt.ylabel(\"OOB error rate\")\\nplt.legend(loc=\"upper right\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_isolation_forest.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=======================\\nIsolationForest example\\n=======================\\n\\nAn example using :class:`~sklearn.ensemble.IsolationForest` for anomaly\\ndetection.\\n\\nThe :ref:`isolation_forest` is an ensemble of \"Isolation Trees\" that \"isolate\"\\nobservations by recursive random partitioning, which can be represented by a\\ntree structure. The number of splittings required to isolate a sample is lower\\nfor outliers and higher for inliers.\\n\\nIn the present example we demo two ways to visualize the decision boundary of an\\nIsolation Forest trained on a toy dataset.\\n\\n\"\"\"\\n\\n# %%\\n# Data generation\\n# ---------------\\n#\\n# We generate two clusters (each one containing `n_samples`) by randomly\\n# sampling the standard normal distribution as returned by\\n# :func:`numpy.random.randn`. One of them is spherical and the other one is\\n# slightly deformed.\\n#\\n# For consistency with the :class:`~sklearn.ensemble.IsolationForest` notation,\\n# the inliers (i.e. the gaussian clusters) are assigned a ground truth label `1`\\n# whereas the outliers (created with :func:`numpy.random.uniform`) are assigned\\n# the label `-1`.\\n\\nimport numpy as np\\n\\nfrom sklearn.model_selection import train_test_split\\n\\nn_samples, n_outliers = 120, 40\\nrng = np.random.RandomState(0)\\ncovariance = np.array([[0.5, -0.1], [0.7, 0.4]])\\ncluster_1 = 0.4 * rng.randn(n_samples, 2) @ covariance + np.array([2, 2])  # general\\ncluster_2 = 0.3 * rng.randn(n_samples, 2) + np.array([-2, -2])  # spherical\\noutliers = rng.uniform(low=-4, high=4, size=(n_outliers, 2))\\n\\nX = np.concatenate([cluster_1, cluster_2, outliers])\\ny = np.concatenate(\\n    [np.ones((2 * n_samples), dtype=int), -np.ones((n_outliers), dtype=int)]\\n)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\\n\\n# %%\\n# We can visualize the resulting clusters:\\n\\nimport matplotlib.pyplot as plt\\n\\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\\nhandles, labels = scatter.legend_elements()\\nplt.axis(\"square\")\\nplt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\\nplt.title(\"Gaussian inliers with \\\\nuniformly distributed outliers\")\\nplt.show()\\n\\n# %%\\n# Training of the model\\n# ---------------------\\n\\nfrom sklearn.ensemble import IsolationForest\\n\\nclf = IsolationForest(max_samples=100, random_state=0)\\nclf.fit(X_train)\\n\\n# %%\\n# Plot discrete decision boundary\\n# -------------------------------\\n#\\n# We use the class :class:`~sklearn.inspection.DecisionBoundaryDisplay` to\\n# visualize a discrete decision boundary. The background color represents\\n# whether a sample in that given area is predicted to be an outlier\\n# or not. The scatter plot displays the true labels.\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.inspection import DecisionBoundaryDisplay'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_isolation_forest.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import matplotlib.pyplot as plt\\n\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\ndisp = DecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    response_method=\"predict\",\\n    alpha=0.5,\\n)\\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\\ndisp.ax_.set_title(\"Binary decision boundary \\\\nof IsolationForest\")\\nplt.axis(\"square\")\\nplt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\\nplt.show()\\n\\n# %%\\n# Plot path length decision boundary\\n# ----------------------------------\\n#\\n# By setting the `response_method=\"decision_function\"`, the background of the\\n# :class:`~sklearn.inspection.DecisionBoundaryDisplay` represents the measure of\\n# normality of an observation. Such score is given by the path length averaged\\n# over a forest of random trees, which itself is given by the depth of the leaf\\n# (or equivalently the number of splits) required to isolate a given sample.\\n#\\n# When a forest of random trees collectively produce short path lengths for\\n# isolating some particular samples, they are highly likely to be anomalies and\\n# the measure of normality is close to `0`. Similarly, large paths correspond to\\n# values close to `1` and are more likely to be inliers.\\n\\ndisp = DecisionBoundaryDisplay.from_estimator(\\n    clf,\\n    X,\\n    response_method=\"decision_function\",\\n    alpha=0.5,\\n)\\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\\ndisp.ax_.set_title(\"Path length decision boundary \\\\nof IsolationForest\")\\nplt.axis(\"square\")\\nplt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\\nplt.colorbar(disp.ax_.collections[1])\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_hist_grad_boosting_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================================================\\nComparing Random Forests and Histogram Gradient Boosting models\\n===============================================================\\n\\nIn this example we compare the performance of Random Forest (RF) and Histogram\\nGradient Boosting (HGBT) models in terms of score and computation time for a\\nregression dataset, though **all the concepts here presented apply to\\nclassification as well**.\\n\\nThe comparison is made by varying the parameters that control the number of\\ntrees according to each estimator:\\n\\n- `n_estimators` controls the number of trees in the forest. It\\'s a fixed number.\\n- `max_iter` is the maximum number of iterations in a gradient boosting\\n  based model. The number of iterations corresponds to the number of trees for\\n  regression and binary classification problems. Furthermore, the actual number\\n  of trees required by the model depends on the stopping criteria.\\n\\nHGBT uses gradient boosting to iteratively improve the model\\'s performance by\\nfitting each tree to the negative gradient of the loss function with respect to\\nthe predicted value. RFs, on the other hand, are based on bagging and use a\\nmajority vote to predict the outcome.\\n\\nSee the :ref:`User Guide <ensemble>` for more information on ensemble models or\\nsee :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an\\nexample showcasing some other features of HGBT models.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Load dataset\\n# ------------\\n\\nfrom sklearn.datasets import fetch_california_housing\\n\\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\\nn_samples, n_features = X.shape\\n\\n# %%\\n# HGBT uses a histogram-based algorithm on binned feature values that can\\n# efficiently handle large datasets (tens of thousands of samples or more) with\\n# a high number of features (see :ref:`Why_it\\'s_faster`). The scikit-learn\\n# implementation of RF does not use binning and relies on exact splitting, which\\n# can be computationally expensive.\\n\\nprint(f\"The dataset consists of {n_samples} samples and {n_features} features\")\\n\\n# %%\\n# Compute score and computation times\\n# -----------------------------------\\n#\\n# Notice that many parts of the implementation of\\n# :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor` are parallelized by\\n# default.\\n#\\n# The implementation of :class:`~sklearn.ensemble.RandomForestRegressor` and\\n# :class:`~sklearn.ensemble.RandomForestClassifier` can also be run on multiple\\n# cores by using the `n_jobs` parameter, here set to match the number of\\n# physical cores on the host machine. See :ref:`parallelism` for more\\n# information.\\n\\nimport joblib\\n\\nN_CORES = joblib.cpu_count(only_physical_cores=True)\\nprint(f\"Number of physical cores: {N_CORES}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_hist_grad_boosting_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import joblib\\n\\nN_CORES = joblib.cpu_count(only_physical_cores=True)\\nprint(f\"Number of physical cores: {N_CORES}\")\\n\\n# %%\\n# Unlike RF, HGBT models offer an early-stopping option (see\\n# :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`)\\n# to avoid adding new unnecessary trees. Internally, the algorithm uses an\\n# out-of-sample set to compute the generalization performance of the model at\\n# each addition of a tree. Thus, if the generalization performance is not\\n# improving for more than `n_iter_no_change` iterations, it stops adding trees.\\n#\\n# The other parameters of both models were tuned but the procedure is not shown\\n# here to keep the example simple.\\n\\nimport pandas as pd\\n\\nfrom sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\\nfrom sklearn.model_selection import GridSearchCV, KFold\\n\\nmodels = {\\n    \"Random Forest\": RandomForestRegressor(\\n        min_samples_leaf=5, random_state=0, n_jobs=N_CORES\\n    ),\\n    \"Hist Gradient Boosting\": HistGradientBoostingRegressor(\\n        max_leaf_nodes=15, random_state=0, early_stopping=False\\n    ),\\n}\\nparam_grids = {\\n    \"Random Forest\": {\"n_estimators\": [10, 20, 50, 100]},\\n    \"Hist Gradient Boosting\": {\"max_iter\": [10, 20, 50, 100, 300, 500]},\\n}\\ncv = KFold(n_splits=4, shuffle=True, random_state=0)\\n\\nresults = []\\nfor name, model in models.items():\\n    grid_search = GridSearchCV(\\n        estimator=model,\\n        param_grid=param_grids[name],\\n        return_train_score=True,\\n        cv=cv,\\n    ).fit(X, y)\\n    result = {\"model\": name, \"cv_results\": pd.DataFrame(grid_search.cv_results_)}\\n    results.append(result)\\n\\n# %%\\n# .. Note::\\n#  Tuning the `n_estimators` for RF generally results in a waste of computer\\n#  power. In practice one just needs to ensure that it is large enough so that\\n#  doubling its value does not lead to a significant improvement of the testing\\n#  score.\\n#\\n# Plot results\\n# ------------\\n# We can use a `plotly.express.scatter\\n# <https://plotly.com/python-api-reference/generated/plotly.express.scatter.html>`_\\n# to visualize the trade-off between elapsed computing time and mean test score.\\n# Passing the cursor over a given point displays the corresponding parameters.\\n# Error bars correspond to one standard deviation as computed in the different\\n# folds of the cross-validation.\\n\\nimport plotly.colors as colors\\nimport plotly.express as px\\nfrom plotly.subplots import make_subplots\\n\\nfig = make_subplots(\\n    rows=1,\\n    cols=2,\\n    shared_yaxes=True,\\n    subplot_titles=[\"Train time vs score\", \"Predict time vs score\"],\\n)\\nmodel_names = [result[\"model\"] for result in results]\\ncolors_list = colors.qualitative.Plotly * (\\n    len(model_names) // len(colors.qualitative.Plotly) + 1\\n)\\n\\nfor idx, result in enumerate(results):\\n    cv_results = result[\"cv_results\"].round(3)\\n    model_name = result[\"model\"]\\n    param_name = list(param_grids[model_name].keys())[0]\\n    cv_results[param_name] = cv_results[\"param_\" + param_name]\\n    cv_results[\"model\"] = model_name'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_hist_grad_boosting_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for idx, result in enumerate(results):\\n    cv_results = result[\"cv_results\"].round(3)\\n    model_name = result[\"model\"]\\n    param_name = list(param_grids[model_name].keys())[0]\\n    cv_results[param_name] = cv_results[\"param_\" + param_name]\\n    cv_results[\"model\"] = model_name\\n\\n    scatter_fig = px.scatter(\\n        cv_results,\\n        x=\"mean_fit_time\",\\n        y=\"mean_test_score\",\\n        error_x=\"std_fit_time\",\\n        error_y=\"std_test_score\",\\n        hover_data=param_name,\\n        color=\"model\",\\n    )\\n    line_fig = px.line(\\n        cv_results,\\n        x=\"mean_fit_time\",\\n        y=\"mean_test_score\",\\n    )\\n\\n    scatter_trace = scatter_fig[\"data\"][0]\\n    line_trace = line_fig[\"data\"][0]\\n    scatter_trace.update(marker=dict(color=colors_list[idx]))\\n    line_trace.update(line=dict(color=colors_list[idx]))\\n    fig.add_trace(scatter_trace, row=1, col=1)\\n    fig.add_trace(line_trace, row=1, col=1)\\n\\n    scatter_fig = px.scatter(\\n        cv_results,\\n        x=\"mean_score_time\",\\n        y=\"mean_test_score\",\\n        error_x=\"std_score_time\",\\n        error_y=\"std_test_score\",\\n        hover_data=param_name,\\n    )\\n    line_fig = px.line(\\n        cv_results,\\n        x=\"mean_score_time\",\\n        y=\"mean_test_score\",\\n    )\\n\\n    scatter_trace = scatter_fig[\"data\"][0]\\n    line_trace = line_fig[\"data\"][0]\\n    scatter_trace.update(marker=dict(color=colors_list[idx]))\\n    line_trace.update(line=dict(color=colors_list[idx]))\\n    fig.add_trace(scatter_trace, row=1, col=2)\\n    fig.add_trace(line_trace, row=1, col=2)\\n\\nfig.update_layout(\\n    xaxis=dict(title=\"Train time (s) - lower is better\"),\\n    yaxis=dict(title=\"Test R2 score - higher is better\"),\\n    xaxis2=dict(title=\"Predict time (s) - lower is better\"),\\n    legend=dict(x=0.72, y=0.05, traceorder=\"normal\", borderwidth=1),\\n    title=dict(x=0.5, text=\"Speed-score trade-off of tree-based ensembles\"),\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_hist_grad_boosting_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Both HGBT and RF models improve when increasing the number of trees in the\\n# ensemble. However, the scores reach a plateau where adding new trees just\\n# makes fitting and scoring slower. The RF model reaches such plateau earlier\\n# and can never reach the test score of the largest HGBDT model.\\n#\\n# Note that the results shown on the above plot can change slightly across runs\\n# and even more significantly when running on other machines: try to run this\\n# example on your own local machine.\\n#\\n# Overall, one should often observe that the Histogram-based gradient boosting\\n# models uniformly dominate the Random Forest models in the \"test score vs\\n# training speed trade-off\" (the HGBDT curve should be on the top left of the RF\\n# curve, without ever crossing). The \"test score vs prediction speed\" trade-off\\n# can also be more disputed, but it\\'s most often favorable to HGBDT. It\\'s always\\n# a good idea to check both kinds of model (with hyper-parameter tuning) and\\n# compare their performance on your specific problem to determine which model is\\n# the best fit but **HGBT almost always offers a more favorable speed-accuracy\\n# trade-off than RF**, either with the default hyper-parameters or including the\\n# hyper-parameter tuning cost.\\n#\\n# There is one exception to this rule of thumb though: when training a\\n# multiclass classification model with a large number of possible classes, HGBDT\\n# fits internally one-tree per class at each boosting iteration while the trees\\n# used by the RF models are naturally multiclass which should improve the speed\\n# accuracy trade-off of the RF models in this case.'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_quantile.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def f(x):\\n    \"\"\"The function to predict.\"\"\"\\n    return x * np.sin(x)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_quantile.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def highlight_min(x):\\n    x_min = x.min()\\n    return [\"font-weight: bold\" if v == x_min else \"\" for v in x]'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_quantile.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def coverage_fraction(y, y_low, y_high):\\n    return np.mean(np.logical_and(y >= y_low, y <= y_high))'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_quantile.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================================================\\nPrediction Intervals for Gradient Boosting Regression\\n=====================================================\\n\\nThis example shows how quantile regression can be used to create prediction\\nintervals. See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`\\nfor an example showcasing some other features of\\n:class:`~ensemble.HistGradientBoostingRegressor`.\\n\\n\"\"\"\\n\\n# %%\\n# Generate some data for a synthetic regression problem by applying the\\n# function f to uniformly sampled random inputs.\\nimport numpy as np\\n\\nfrom sklearn.model_selection import train_test_split\\n\\n\\n# Code for: def f(x):\\n\\n\\nrng = np.random.RandomState(42)\\nX = np.atleast_2d(rng.uniform(0, 10.0, size=1000)).T\\nexpected_y = f(X).ravel()\\n\\n# %%\\n# To make the problem interesting, we generate observations of the target y as\\n# the sum of a deterministic term computed by the function f and a random noise\\n# term that follows a centered `log-normal\\n# <https://en.wikipedia.org/wiki/Log-normal_distribution>`_. To make this even\\n# more interesting we consider the case where the amplitude of the noise\\n# depends on the input variable x (heteroscedastic noise).\\n#\\n# The lognormal distribution is non-symmetric and long tailed: observing large\\n# outliers is likely but it is impossible to observe small outliers.\\nsigma = 0.5 + X.ravel() / 10\\nnoise = rng.lognormal(sigma=sigma) - np.exp(sigma**2 / 2)\\ny = expected_y + noise\\n\\n# %%\\n# Split into train, test datasets:\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\n# %%\\n# Fitting non-linear quantile and least squares regressors\\n# --------------------------------------------------------\\n#\\n# Fit gradient boosting models trained with the quantile loss and\\n# alpha=0.05, 0.5, 0.95.\\n#\\n# The models obtained for alpha=0.05 and alpha=0.95 produce a 90% confidence\\n# interval (95% - 5% = 90%).\\n#\\n# The model trained with alpha=0.5 produces a regression of the median: on\\n# average, there should be the same number of target observations above and\\n# below the predicted values.\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.metrics import mean_pinball_loss, mean_squared_error\\n\\nall_models = {}\\ncommon_params = dict(\\n    learning_rate=0.05,\\n    n_estimators=200,\\n    max_depth=2,\\n    min_samples_leaf=9,\\n    min_samples_split=9,\\n)\\nfor alpha in [0.05, 0.5, 0.95]:\\n    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\\n    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_quantile.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Notice that :class:`~sklearn.ensemble.HistGradientBoostingRegressor` is much\\n# faster than :class:`~sklearn.ensemble.GradientBoostingRegressor` starting with\\n# intermediate datasets (`n_samples >= 10_000`), which is not the case of the\\n# present example.\\n#\\n# For the sake of comparison, we also fit a baseline model trained with the\\n# usual (mean) squared error (MSE).\\ngbr_ls = GradientBoostingRegressor(loss=\"squared_error\", **common_params)\\nall_models[\"mse\"] = gbr_ls.fit(X_train, y_train)\\n\\n# %%\\n# Create an evenly spaced evaluation set of input values spanning the [0, 10]\\n# range.\\nxx = np.atleast_2d(np.linspace(0, 10, 1000)).T\\n\\n# %%\\n# Plot the true conditional mean function f, the predictions of the conditional\\n# mean (loss equals squared error), the conditional median and the conditional\\n# 90% interval (from 5th to 95th conditional percentiles).\\nimport matplotlib.pyplot as plt\\n\\ny_pred = all_models[\"mse\"].predict(xx)\\ny_lower = all_models[\"q 0.05\"].predict(xx)\\ny_upper = all_models[\"q 0.95\"].predict(xx)\\ny_med = all_models[\"q 0.50\"].predict(xx)\\n\\nfig = plt.figure(figsize=(10, 10))\\nplt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\\\,\\\\sin(x)$\")\\nplt.plot(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\\nplt.plot(xx, y_med, \"r-\", label=\"Predicted median\")\\nplt.plot(xx, y_pred, \"r-\", label=\"Predicted mean\")\\nplt.plot(xx, y_upper, \"k-\")\\nplt.plot(xx, y_lower, \"k-\")\\nplt.fill_between(\\n    xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\\n)\\nplt.xlabel(\"$x$\")\\nplt.ylabel(\"$f(x)$\")\\nplt.ylim(-10, 25)\\nplt.legend(loc=\"upper left\")\\nplt.show()\\n\\n# %%\\n# Comparing the predicted median with the predicted mean, we note that the\\n# median is on average below the mean as the noise is skewed towards high\\n# values (large outliers). The median estimate also seems to be smoother\\n# because of its natural robustness to outliers.\\n#\\n# Also observe that the inductive bias of gradient boosting trees is\\n# unfortunately preventing our 0.05 quantile to fully capture the sinoisoidal\\n# shape of the signal, in particular around x=8. Tuning hyper-parameters can\\n# reduce this effect as shown in the last part of this notebook.\\n#\\n# Analysis of the error metrics\\n# -----------------------------\\n#\\n# Measure the models with :func:`~sklearn.metrics.mean_squared_error` and\\n# :func:`~sklearn.metrics.mean_pinball_loss` metrics on the training dataset.\\nimport pandas as pd\\n\\n\\n# Code for: def highlight_min(x):\\n\\n\\nresults = []\\nfor name, gbr in sorted(all_models.items()):\\n    metrics = {\"model\": name}\\n    y_pred = gbr.predict(X_train)\\n    for alpha in [0.05, 0.5, 0.95]:\\n        metrics[\"pbl=%1.2f\" % alpha] = mean_pinball_loss(y_train, y_pred, alpha=alpha)\\n    metrics[\"MSE\"] = mean_squared_error(y_train, y_pred)\\n    results.append(metrics)\\n\\npd.DataFrame(results).set_index(\"model\").style.apply(highlight_min)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_quantile.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='pd.DataFrame(results).set_index(\"model\").style.apply(highlight_min)\\n\\n# %%\\n# One column shows all models evaluated by the same metric. The minimum number\\n# on a column should be obtained when the model is trained and measured with\\n# the same metric. This should be always the case on the training set if the\\n# training converged.\\n#\\n# Note that because the target distribution is asymmetric, the expected\\n# conditional mean and conditional median are significantly different and\\n# therefore one could not use the squared error model get a good estimation of\\n# the conditional median nor the converse.\\n#\\n# If the target distribution were symmetric and had no outliers (e.g. with a\\n# Gaussian noise), then median estimator and the least squares estimator would\\n# have yielded similar predictions.\\n#\\n# We then do the same on the test set.\\nresults = []\\nfor name, gbr in sorted(all_models.items()):\\n    metrics = {\"model\": name}\\n    y_pred = gbr.predict(X_test)\\n    for alpha in [0.05, 0.5, 0.95]:\\n        metrics[\"pbl=%1.2f\" % alpha] = mean_pinball_loss(y_test, y_pred, alpha=alpha)\\n    metrics[\"MSE\"] = mean_squared_error(y_test, y_pred)\\n    results.append(metrics)\\n\\npd.DataFrame(results).set_index(\"model\").style.apply(highlight_min)\\n\\n\\n# %%\\n# Errors are higher meaning the models slightly overfitted the data. It still\\n# shows that the best test metric is obtained when the model is trained by\\n# minimizing this same metric.\\n#\\n# Note that the conditional median estimator is competitive with the squared\\n# error estimator in terms of MSE on the test set: this can be explained by\\n# the fact the squared error estimator is very sensitive to large outliers\\n# which can cause significant overfitting. This can be seen on the right hand\\n# side of the previous plot. The conditional median estimator is biased\\n# (underestimation for this asymmetric noise) but is also naturally robust to\\n# outliers and overfits less.\\n#\\n# .. _calibration-section:\\n#\\n# Calibration of the confidence interval\\n# --------------------------------------\\n#\\n# We can also evaluate the ability of the two extreme quantile estimators at\\n# producing a well-calibrated conditional 90%-confidence interval.\\n#\\n# To do this we can compute the fraction of observations that fall between the\\n# predictions:\\n# Code for: def coverage_fraction(y, y_low, y_high):\\n\\n\\ncoverage_fraction(\\n    y_train,\\n    all_models[\"q 0.05\"].predict(X_train),\\n    all_models[\"q 0.95\"].predict(X_train),\\n)\\n\\n# %%\\n# On the training set the calibration is very close to the expected coverage\\n# value for a 90% confidence interval.\\ncoverage_fraction(\\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_quantile.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# On the training set the calibration is very close to the expected coverage\\n# value for a 90% confidence interval.\\ncoverage_fraction(\\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\\n)\\n\\n\\n# %%\\n# On the test set, the estimated confidence interval is slightly too narrow.\\n# Note, however, that we would need to wrap those metrics in a cross-validation\\n# loop to assess their variability under data resampling.\\n#\\n# Tuning the hyper-parameters of the quantile regressors\\n# ------------------------------------------------------\\n#\\n# In the plot above, we observed that the 5th percentile regressor seems to\\n# underfit and could not adapt to sinusoidal shape of the signal.\\n#\\n# The hyper-parameters of the model were approximately hand-tuned for the\\n# median regressor and there is no reason that the same hyper-parameters are\\n# suitable for the 5th percentile regressor.\\n#\\n# To confirm this hypothesis, we tune the hyper-parameters of a new regressor\\n# of the 5th percentile by selecting the best model parameters by\\n# cross-validation on the pinball loss with alpha=0.05:\\n\\n# %%\\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\\nfrom sklearn.model_selection import HalvingRandomSearchCV\\nfrom sklearn.metrics import make_scorer\\nfrom pprint import pprint\\n\\nparam_grid = dict(\\n    learning_rate=[0.05, 0.1, 0.2],\\n    max_depth=[2, 5, 10],\\n    min_samples_leaf=[1, 5, 10, 20],\\n    min_samples_split=[5, 10, 20, 30, 50],\\n)\\nalpha = 0.05\\nneg_mean_pinball_loss_05p_scorer = make_scorer(\\n    mean_pinball_loss,\\n    alpha=alpha,\\n    greater_is_better=False,  # maximize the negative loss\\n)\\ngbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, random_state=0)\\nsearch_05p = HalvingRandomSearchCV(\\n    gbr,\\n    param_grid,\\n    resource=\"n_estimators\",\\n    max_resources=250,\\n    min_resources=50,\\n    scoring=neg_mean_pinball_loss_05p_scorer,\\n    n_jobs=2,\\n    random_state=0,\\n).fit(X_train, y_train)\\npprint(search_05p.best_params_)\\n\\n# %%\\n# We observe that the hyper-parameters that were hand-tuned for the median\\n# regressor are in the same range as the hyper-parameters suitable for the 5th\\n# percentile regressor.\\n#\\n# Let\\'s now tune the hyper-parameters for the 95th percentile regressor. We\\n# need to redefine the `scoring` metric used to select the best model, along\\n# with adjusting the alpha parameter of the inner gradient boosting estimator\\n# itself:\\nfrom sklearn.base import clone\\n\\nalpha = 0.95\\nneg_mean_pinball_loss_95p_scorer = make_scorer(\\n    mean_pinball_loss,\\n    alpha=alpha,\\n    greater_is_better=False,  # maximize the negative loss\\n)\\nsearch_95p = clone(search_05p).set_params(\\n    estimator__alpha=alpha,\\n    scoring=neg_mean_pinball_loss_95p_scorer,\\n)\\nsearch_95p.fit(X_train, y_train)\\npprint(search_95p.best_params_)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_quantile.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The result shows that the hyper-parameters for the 95th percentile regressor\\n# identified by the search procedure are roughly in the same range as the hand-\\n# tuned hyper-parameters for the median regressor and the hyper-parameters\\n# identified by the search procedure for the 5th percentile regressor. However,\\n# the hyper-parameter searches did lead to an improved 90% confidence interval\\n# that is comprised by the predictions of those two tuned quantile regressors.\\n# Note that the prediction of the upper 95th percentile has a much coarser shape\\n# than the prediction of the lower 5th percentile because of the outliers:\\ny_lower = search_05p.predict(xx)\\ny_upper = search_95p.predict(xx)\\n\\nfig = plt.figure(figsize=(10, 10))\\nplt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\\\,\\\\sin(x)$\")\\nplt.plot(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\\nplt.plot(xx, y_upper, \"k-\")\\nplt.plot(xx, y_lower, \"k-\")\\nplt.fill_between(\\n    xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\\n)\\nplt.xlabel(\"$x$\")\\nplt.ylabel(\"$f(x)$\")\\nplt.ylim(-10, 25)\\nplt.legend(loc=\"upper left\")\\nplt.title(\"Prediction with tuned hyper-parameters\")\\nplt.show()\\n\\n# %%\\n# The plot looks qualitatively better than for the untuned models, especially\\n# for the shape of the of lower quantile.\\n#\\n# We now quantitatively evaluate the joint-calibration of the pair of\\n# estimators:\\ncoverage_fraction(y_train, search_05p.predict(X_train), search_95p.predict(X_train))\\n# %%\\ncoverage_fraction(y_test, search_05p.predict(X_test), search_95p.predict(X_test))\\n# %%\\n# The calibration of the tuned pair is sadly not better on the test set: the\\n# width of the estimated confidence interval is still too narrow.\\n#\\n# Again, we would need to wrap this study in a cross-validation loop to\\n# better assess the variability of those estimates.'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_stack_predictors.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def load_ames_housing():\\n    df = fetch_openml(name=\"house_prices\", as_frame=True)\\n    X = df.data\\n    y = df.target\\n\\n    features = [\\n        \"YrSold\",\\n        \"HeatingQC\",\\n        \"Street\",\\n        \"YearRemodAdd\",\\n        \"Heating\",\\n        \"MasVnrType\",\\n        \"BsmtUnfSF\",\\n        \"Foundation\",\\n        \"MasVnrArea\",\\n        \"MSSubClass\",\\n        \"ExterQual\",\\n        \"Condition2\",\\n        \"GarageCars\",\\n        \"GarageType\",\\n        \"OverallQual\",\\n        \"TotalBsmtSF\",\\n        \"BsmtFinSF1\",\\n        \"HouseStyle\",\\n        \"MiscFeature\",\\n        \"MoSold\",\\n    ]\\n\\n    X = X.loc[:, features]\\n    X, y = shuffle(X, y, random_state=0)\\n\\n    X = X.iloc[:600]\\n    y = y.iloc[:600]\\n    return X, np.log(y)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_stack_predictors.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================\\nCombine predictors using stacking\\n=================================\\n\\n.. currentmodule:: sklearn\\n\\nStacking refers to a method to blend estimators. In this strategy, some\\nestimators are individually fitted on some training data while a final\\nestimator is trained using the stacked predictions of these base estimators.\\n\\nIn this example, we illustrate the use case in which different regressors are\\nstacked together and a final linear penalized regressor is used to output the\\nprediction. We compare the performance of each individual regressor with the\\nstacking strategy. Stacking slightly improves the overall performance.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Download the dataset\\n######################\\n#\\n# We will use the `Ames Housing`_ dataset which was first compiled by Dean De Cock\\n# and became better known after it was used in Kaggle challenge. It is a set\\n# of 1460 residential homes in Ames, Iowa, each described by 80 features. We\\n# will use it to predict the final logarithmic price of the houses. In this\\n# example we will use only 20 most interesting features chosen using\\n# GradientBoostingRegressor() and limit number of entries (here we won\\'t go\\n# into the details on how to select the most interesting features).\\n#\\n# The Ames housing dataset is not shipped with scikit-learn and therefore we\\n# will fetch it from `OpenML`_.\\n#\\n# .. _`Ames Housing`: http://jse.amstat.org/v19n3/decock.pdf\\n# .. _`OpenML`: https://www.openml.org/d/42165\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.utils import shuffle\\n\\n\\n# Code for: def load_ames_housing():\\n\\n\\nX, y = load_ames_housing()\\n\\n# %%\\n# Make pipeline to preprocess the data\\n######################################\\n#\\n# Before we can use Ames dataset we still need to do some preprocessing.\\n# First, we will select the categorical and numerical columns of the dataset to\\n# construct the first step of the pipeline.\\n\\nfrom sklearn.compose import make_column_selector\\n\\ncat_selector = make_column_selector(dtype_include=object)\\nnum_selector = make_column_selector(dtype_include=np.number)\\ncat_selector(X)\\n\\n# %%\\nnum_selector(X)\\n\\n# %%\\n# Then, we will need to design preprocessing pipelines which depends on the\\n# ending regressor. If the ending regressor is a linear model, one needs to\\n# one-hot encode the categories. If the ending regressor is a tree-based model\\n# an ordinal encoder will be sufficient. Besides, numerical values need to be\\n# standardized for a linear model while the raw numerical data can be treated\\n# as is by a tree-based model. However, both models need an imputer to\\n# handle missing values.\\n#\\n# We will first design the pipeline required for the tree-based models.\\n\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import OrdinalEncoder'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_stack_predictors.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.compose import make_column_transformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import OrdinalEncoder\\n\\ncat_tree_processor = OrdinalEncoder(\\n    handle_unknown=\"use_encoded_value\",\\n    unknown_value=-1,\\n    encoded_missing_value=-2,\\n)\\nnum_tree_processor = SimpleImputer(strategy=\"mean\", add_indicator=True)\\n\\ntree_preprocessor = make_column_transformer(\\n    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector)\\n)\\ntree_preprocessor\\n\\n# %%\\n# Then, we will now define the preprocessor used when the ending regressor\\n# is a linear model.\\n\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\ncat_linear_processor = OneHotEncoder(handle_unknown=\"ignore\")\\nnum_linear_processor = make_pipeline(\\n    StandardScaler(), SimpleImputer(strategy=\"mean\", add_indicator=True)\\n)\\n\\nlinear_preprocessor = make_column_transformer(\\n    (num_linear_processor, num_selector), (cat_linear_processor, cat_selector)\\n)\\nlinear_preprocessor\\n\\n# %%\\n# Stack of predictors on a single data set\\n##########################################\\n#\\n# It is sometimes tedious to find the model which will best perform on a given\\n# dataset. Stacking provide an alternative by combining the outputs of several\\n# learners, without the need to choose a model specifically. The performance of\\n# stacking is usually close to the best model and sometimes it can outperform\\n# the prediction performance of each individual model.\\n#\\n# Here, we combine 3 learners (linear and non-linear) and use a ridge regressor\\n# to combine their outputs together.\\n#\\n# .. note::\\n#    Although we will make new pipelines with the processors which we wrote in\\n#    the previous section for the 3 learners, the final estimator\\n#    :class:`~sklearn.linear_model.RidgeCV()` does not need preprocessing of\\n#    the data as it will be fed with the already preprocessed output from the 3\\n#    learners.\\n\\nfrom sklearn.linear_model import LassoCV\\n\\nlasso_pipeline = make_pipeline(linear_preprocessor, LassoCV())\\nlasso_pipeline\\n\\n# %%\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\nrf_pipeline = make_pipeline(tree_preprocessor, RandomForestRegressor(random_state=42))\\nrf_pipeline\\n\\n# %%\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\n\\ngbdt_pipeline = make_pipeline(\\n    tree_preprocessor, HistGradientBoostingRegressor(random_state=0)\\n)\\ngbdt_pipeline\\n\\n# %%\\nfrom sklearn.ensemble import StackingRegressor\\nfrom sklearn.linear_model import RidgeCV\\n\\nestimators = [\\n    (\"Random Forest\", rf_pipeline),\\n    (\"Lasso\", lasso_pipeline),\\n    (\"Gradient Boosting\", gbdt_pipeline),\\n]\\n\\nstacking_regressor = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\\nstacking_regressor\\n\\n# %%\\n# Measure and plot the results\\n##############################\\n#\\n# Now we can use Ames Housing dataset to make the predictions. We check the\\n# performance of each individual predictor as well as of the stack of the\\n# regressors.\\n\\n\\nimport time'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_stack_predictors.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Measure and plot the results\\n##############################\\n#\\n# Now we can use Ames Housing dataset to make the predictions. We check the\\n# performance of each individual predictor as well as of the stack of the\\n# regressors.\\n\\n\\nimport time\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.metrics import PredictionErrorDisplay\\nfrom sklearn.model_selection import cross_val_predict, cross_validate\\n\\nfig, axs = plt.subplots(2, 2, figsize=(9, 7))\\naxs = np.ravel(axs)\\n\\nfor ax, (name, est) in zip(\\n    axs, estimators + [(\"Stacking Regressor\", stacking_regressor)]\\n):\\n    scorers = {\"R2\": \"r2\", \"MAE\": \"neg_mean_absolute_error\"}\\n\\n    start_time = time.time()\\n    scores = cross_validate(\\n        est, X, y, scoring=list(scorers.values()), n_jobs=-1, verbose=0\\n    )\\n    elapsed_time = time.time() - start_time\\n\\n    y_pred = cross_val_predict(est, X, y, n_jobs=-1, verbose=0)\\n    scores = {\\n        key: (\\n            f\"{np.abs(np.mean(scores[f\\'test_{value}\\'])):.2f} +- \"\\n            f\"{np.std(scores[f\\'test_{value}\\']):.2f}\"\\n        )\\n        for key, value in scorers.items()\\n    }\\n\\n    display = PredictionErrorDisplay.from_predictions(\\n        y_true=y,\\n        y_pred=y_pred,\\n        kind=\"actual_vs_predicted\",\\n        ax=ax,\\n        scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\\n        line_kwargs={\"color\": \"tab:red\"},\\n    )\\n    ax.set_title(f\"{name}\\\\nEvaluation in {elapsed_time:.2f} seconds\")\\n\\n    for name, score in scores.items():\\n        ax.plot([], [], \" \", label=f\"{name}: {score}\")\\n    ax.legend(loc=\"upper left\")\\n\\nplt.suptitle(\"Single predictors versus stacked predictors\")\\nplt.tight_layout()\\nplt.subplots_adjust(top=0.9)\\nplt.show()\\n\\n# %%\\n# The stacked regressor will combine the strengths of the different regressors.\\n# However, we also see that training the stacked regressor is much more\\n# computationally expensive.'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_importances.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================================\\nFeature importances with a forest of trees\\n==========================================\\n\\nThis example shows the use of a forest of trees to evaluate the importance of\\nfeatures on an artificial classification task. The blue bars are the feature\\nimportances of the forest, along with their inter-trees variability represented\\nby the error bars.\\n\\nAs expected, the plot suggests that 3 features are informative, while the\\nremaining are not.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\n\\n# %%\\n# Data generation and model fitting\\n# ---------------------------------\\n# We generate a synthetic dataset with only 3 informative features. We will\\n# explicitly not shuffle the dataset to ensure that the informative features\\n# will correspond to the three first columns of X. In addition, we will split\\n# our dataset into training and testing subsets.\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = make_classification(\\n    n_samples=1000,\\n    n_features=10,\\n    n_informative=3,\\n    n_redundant=0,\\n    n_repeated=0,\\n    n_classes=2,\\n    random_state=0,\\n    shuffle=False,\\n)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\\n\\n# %%\\n# A random forest classifier will be fitted to compute the feature importances.\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nfeature_names = [f\"feature {i}\" for i in range(X.shape[1])]\\nforest = RandomForestClassifier(random_state=0)\\nforest.fit(X_train, y_train)\\n\\n# %%\\n# Feature importance based on mean decrease in impurity\\n# -----------------------------------------------------\\n# Feature importances are provided by the fitted attribute\\n# `feature_importances_` and they are computed as the mean and standard\\n# deviation of accumulation of the impurity decrease within each tree.\\n#\\n# .. warning::\\n#     Impurity-based feature importances can be misleading for **high\\n#     cardinality** features (many unique values). See\\n#     :ref:`permutation_importance` as an alternative below.\\nimport time\\n\\nimport numpy as np\\n\\nstart_time = time.time()\\nimportances = forest.feature_importances_\\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\\nelapsed_time = time.time() - start_time\\n\\nprint(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\\n\\n# %%\\n# Let\\'s plot the impurity-based importance.\\nimport pandas as pd\\n\\nforest_importances = pd.Series(importances, index=feature_names)\\n\\nfig, ax = plt.subplots()\\nforest_importances.plot.bar(yerr=std, ax=ax)\\nax.set_title(\"Feature importances using MDI\")\\nax.set_ylabel(\"Mean decrease in impurity\")\\nfig.tight_layout()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_importances.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='forest_importances = pd.Series(importances, index=feature_names)\\n\\nfig, ax = plt.subplots()\\nforest_importances.plot.bar(yerr=std, ax=ax)\\nax.set_title(\"Feature importances using MDI\")\\nax.set_ylabel(\"Mean decrease in impurity\")\\nfig.tight_layout()\\n\\n# %%\\n# We observe that, as expected, the three first features are found important.\\n#\\n# Feature importance based on feature permutation\\n# -----------------------------------------------\\n# Permutation feature importance overcomes limitations of the impurity-based\\n# feature importance: they do not have a bias toward high-cardinality features\\n# and can be computed on a left-out test set.\\nfrom sklearn.inspection import permutation_importance\\n\\nstart_time = time.time()\\nresult = permutation_importance(\\n    forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\\n)\\nelapsed_time = time.time() - start_time\\nprint(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\\n\\nforest_importances = pd.Series(result.importances_mean, index=feature_names)\\n\\n# %%\\n# The computation for full permutation importance is more costly. Features are\\n# shuffled n times and the model refitted to estimate the importance of it.\\n# Please see :ref:`permutation_importance` for more details. We can now plot\\n# the importance ranking.\\n\\nfig, ax = plt.subplots()\\nforest_importances.plot.bar(yerr=result.importances_std, ax=ax)\\nax.set_title(\"Feature importances using permutation on full model\")\\nax.set_ylabel(\"Mean accuracy decrease\")\\nfig.tight_layout()\\nplt.show()\\n\\n# %%\\n# The same features are detected as most important using both methods. Although\\n# the relative importances vary. As seen on the plots, MDI is less likely than\\n# permutation importance to fully omit a feature.'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_categorical.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_results(figure_title):\\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\\n\\n    plot_info = [\\n        (\"fit_time\", \"Fit times (s)\", ax1, None),\\n        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\\n    ]\\n\\n    x, width = np.arange(4), 0.9\\n    for key, title, ax, y_limit in plot_info:\\n        items = [\\n            dropped_result[key],\\n            one_hot_result[key],\\n            ordinal_result[key],\\n            native_result[key],\\n        ]\\n\\n        mape_cv_mean = [np.mean(np.abs(item)) for item in items]\\n        mape_cv_std = [np.std(item) for item in items]\\n\\n        ax.bar(\\n            x=x,\\n            height=mape_cv_mean,\\n            width=width,\\n            yerr=mape_cv_std,\\n            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\\n        )\\n        ax.set(\\n            xlabel=\"Model\",\\n            title=title,\\n            xticks=x,\\n            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\\n            ylim=y_limit,\\n        )\\n    fig.suptitle(figure_title)'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_categorical.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================================\\nCategorical Feature Support in Gradient Boosting\\n================================================\\n\\n.. currentmodule:: sklearn\\n\\nIn this example, we will compare the training times and prediction\\nperformances of :class:`~ensemble.HistGradientBoostingRegressor` with\\ndifferent encoding strategies for categorical features. In\\nparticular, we will evaluate:\\n\\n- dropping the categorical features\\n- using a :class:`~preprocessing.OneHotEncoder`\\n- using an :class:`~preprocessing.OrdinalEncoder` and treat categories as\\n  ordered, equidistant quantities\\n- using an :class:`~preprocessing.OrdinalEncoder` and rely on the :ref:`native\\n  category support <categorical_support_gbdt>` of the\\n  :class:`~ensemble.HistGradientBoostingRegressor` estimator.\\n\\nWe will work with the Ames Iowa Housing dataset which consists of numerical\\nand categorical features, where the houses\\' sales prices is the target.\\n\\nSee :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an\\nexample showcasing some other features of\\n:class:`~ensemble.HistGradientBoostingRegressor`.\\n\\n\"\"\"\\n\\n# %%\\n# Load Ames Housing dataset\\n# -------------------------\\n# First, we load the Ames Housing data as a pandas dataframe. The features\\n# are either categorical or numerical:\\nfrom sklearn.datasets import fetch_openml\\n\\nX, y = fetch_openml(data_id=42165, as_frame=True, return_X_y=True)\\n\\n# Select only a subset of features of X to make the example faster to run\\ncategorical_columns_subset = [\\n    \"BldgType\",\\n    \"GarageFinish\",\\n    \"LotConfig\",\\n    \"Functional\",\\n    \"MasVnrType\",\\n    \"HouseStyle\",\\n    \"FireplaceQu\",\\n    \"ExterCond\",\\n    \"ExterQual\",\\n    \"PoolQC\",\\n]\\n\\nnumerical_columns_subset = [\\n    \"3SsnPorch\",\\n    \"Fireplaces\",\\n    \"BsmtHalfBath\",\\n    \"HalfBath\",\\n    \"GarageCars\",\\n    \"TotRmsAbvGrd\",\\n    \"BsmtFinSF1\",\\n    \"BsmtFinSF2\",\\n    \"GrLivArea\",\\n    \"ScreenPorch\",\\n]\\n\\nX = X[categorical_columns_subset + numerical_columns_subset]\\nX[categorical_columns_subset] = X[categorical_columns_subset].astype(\"category\")\\n\\ncategorical_columns = X.select_dtypes(include=\"category\").columns\\nn_categorical_features = len(categorical_columns)\\nn_numerical_features = X.select_dtypes(include=\"number\").shape[1]\\n\\nprint(f\"Number of samples: {X.shape[0]}\")\\nprint(f\"Number of features: {X.shape[1]}\")\\nprint(f\"Number of categorical features: {n_categorical_features}\")\\nprint(f\"Number of numerical features: {n_numerical_features}\")\\n\\n# %%\\n# Gradient boosting estimator with dropped categorical features\\n# -------------------------------------------------------------\\n# As a baseline, we create an estimator where the categorical features are\\n# dropped:\\n\\nfrom sklearn.compose import make_column_selector, make_column_transformer\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\nfrom sklearn.pipeline import make_pipeline'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_categorical.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.compose import make_column_selector, make_column_transformer\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\nfrom sklearn.pipeline import make_pipeline\\n\\ndropper = make_column_transformer(\\n    (\"drop\", make_column_selector(dtype_include=\"category\")), remainder=\"passthrough\"\\n)\\nhist_dropped = make_pipeline(dropper, HistGradientBoostingRegressor(random_state=42))\\n\\n# %%\\n# Gradient boosting estimator with one-hot encoding\\n# -------------------------------------------------\\n# Next, we create a pipeline that will one-hot encode the categorical features\\n# and let the rest of the numerical data to passthrough:\\n\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\none_hot_encoder = make_column_transformer(\\n    (\\n        OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\\n        make_column_selector(dtype_include=\"category\"),\\n    ),\\n    remainder=\"passthrough\",\\n)\\n\\nhist_one_hot = make_pipeline(\\n    one_hot_encoder, HistGradientBoostingRegressor(random_state=42)\\n)\\n\\n# %%\\n# Gradient boosting estimator with ordinal encoding\\n# -------------------------------------------------\\n# Next, we create a pipeline that will treat categorical features as if they\\n# were ordered quantities, i.e. the categories will be encoded as 0, 1, 2,\\n# etc., and treated as continuous features.\\n\\nimport numpy as np\\n\\nfrom sklearn.preprocessing import OrdinalEncoder\\n\\nordinal_encoder = make_column_transformer(\\n    (\\n        OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan),\\n        make_column_selector(dtype_include=\"category\"),\\n    ),\\n    remainder=\"passthrough\",\\n    # Use short feature names to make it easier to specify the categorical\\n    # variables in the HistGradientBoostingRegressor in the next step\\n    # of the pipeline.\\n    verbose_feature_names_out=False,\\n)\\n\\nhist_ordinal = make_pipeline(\\n    ordinal_encoder, HistGradientBoostingRegressor(random_state=42)\\n)\\n\\n# %%\\n# Gradient boosting estimator with native categorical support\\n# -----------------------------------------------------------\\n# We now create a :class:`~ensemble.HistGradientBoostingRegressor` estimator\\n# that will natively handle categorical features. This estimator will not treat\\n# categorical features as ordered quantities. We set\\n# `categorical_features=\"from_dtype\"` such that features with categorical dtype\\n# are considered categorical features.\\n#\\n# The main difference between this estimator and the previous one is that in\\n# this one, we let the :class:`~ensemble.HistGradientBoostingRegressor` detect\\n# which features are categorical from the DataFrame columns\\' dtypes.\\n\\nhist_native = HistGradientBoostingRegressor(\\n    random_state=42, categorical_features=\"from_dtype\"\\n)\\n\\n# %%\\n# Model comparison\\n# ----------------\\n# Finally, we evaluate the models using cross validation. Here we compare the\\n# models performance in terms of\\n# :func:`~metrics.mean_absolute_percentage_error` and fit times.\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.model_selection import cross_validate'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_categorical.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import matplotlib.pyplot as plt\\n\\nfrom sklearn.model_selection import cross_validate\\n\\nscoring = \"neg_mean_absolute_percentage_error\"\\nn_cv_folds = 3\\n\\ndropped_result = cross_validate(hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\\none_hot_result = cross_validate(hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\\nordinal_result = cross_validate(hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\\nnative_result = cross_validate(hist_native, X, y, cv=n_cv_folds, scoring=scoring)\\n\\n\\n# Code for: def plot_results(figure_title):\\n\\n\\nplot_results(\"Gradient Boosting on Ames Housing\")\\n\\n# %%\\n# We see that the model with one-hot-encoded data is by far the slowest. This\\n# is to be expected, since one-hot-encoding creates one additional feature per\\n# category value (for each categorical feature), and thus more split points\\n# need to be considered during fitting. In theory, we expect the native\\n# handling of categorical features to be slightly slower than treating\\n# categories as ordered quantities (\\'Ordinal\\'), since native handling requires\\n# :ref:`sorting categories <categorical_support_gbdt>`. Fitting times should\\n# however be close when the number of categories is small, and this may not\\n# always be reflected in practice.\\n#\\n# In terms of prediction performance, dropping the categorical features leads\\n# to poorer performance. The three models that use categorical features have\\n# comparable error rates, with a slight edge for the native handling.\\n\\n# %%\\n# Limiting the number of splits\\n# -----------------------------\\n# In general, one can expect poorer predictions from one-hot-encoded data,\\n# especially when the tree depths or the number of nodes are limited: with\\n# one-hot-encoded data, one needs more split points, i.e. more depth, in order\\n# to recover an equivalent split that could be obtained in one single split\\n# point with native handling.\\n#\\n# This is also true when categories are treated as ordinal quantities: if\\n# categories are `A..F` and the best split is `ACF - BDE` the one-hot-encoder\\n# model will need 3 split points (one per category in the left node), and the\\n# ordinal non-native model will need 4 splits: 1 split to isolate `A`, 1 split\\n# to isolate `F`, and 2 splits to isolate `C` from `BCDE`.\\n#\\n# How strongly the models\\' performances differ in practice will depend on the\\n# dataset and on the flexibility of the trees.\\n#\\n# To see this, let us re-run the same analysis with under-fitting models where\\n# we artificially limit the total number of splits by both limiting the number\\n# of trees and the depth of each tree.\\n\\nfor pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\\n    if pipe is hist_native:\\n        # The native model does not use a pipeline so, we can set the parameters\\n        # directly.\\n        pipe.set_params(max_depth=3, max_iter=15)\\n    else:\\n        pipe.set_params(\\n            histgradientboostingregressor__max_depth=3,\\n            histgradientboostingregressor__max_iter=15,\\n        )'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_categorical.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='dropped_result = cross_validate(hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\\none_hot_result = cross_validate(hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\\nordinal_result = cross_validate(hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\\nnative_result = cross_validate(hist_native, X, y, cv=n_cv_folds, scoring=scoring)\\n\\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\\n\\nplt.show()\\n\\n# %%\\n# The results for these under-fitting models confirm our previous intuition:\\n# the native category handling strategy performs the best when the splitting\\n# budget is constrained. The two other strategies (one-hot encoding and\\n# treating categories as ordinal values) lead to error values comparable\\n# to the baseline model that just dropped the categorical features altogether.'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_importances_faces.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================\\nPixel importances with a parallel forest of trees\\n=================================================\\n\\nThis example shows the use of a forest of trees to evaluate the impurity\\nbased importance of the pixels in an image classification task on the faces\\ndataset. The hotter the pixel, the more important it is.\\n\\nThe code below also illustrates how the construction and the computation\\nof the predictions can be parallelized within multiple jobs.\\n\\n\"\"\"\\n\\n# %%\\n# Loading the data and model fitting\\n# ----------------------------------\\n# First, we load the olivetti faces dataset and limit the dataset to contain\\n# only the first five classes. Then we train a random forest on the dataset\\n# and evaluate the impurity-based feature importance. One drawback of this\\n# method is that it cannot be evaluated on a separate test set. For this\\n# example, we are interested in representing the information learned from\\n# the full dataset. Also, we\\'ll set the number of cores to use for the tasks.\\nfrom sklearn.datasets import fetch_olivetti_faces\\n\\n# %%\\n# We select the number of cores to use to perform parallel fitting of\\n# the forest model. `-1` means use all available cores.\\nn_jobs = -1\\n\\n# %%\\n# Load the faces dataset\\ndata = fetch_olivetti_faces()\\nX, y = data.data, data.target\\n\\n# %%\\n# Limit the dataset to 5 classes.\\nmask = y < 5\\nX = X[mask]\\ny = y[mask]\\n\\n# %%\\n# A random forest classifier will be fitted to compute the feature importances.\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nforest = RandomForestClassifier(n_estimators=750, n_jobs=n_jobs, random_state=42)\\n\\nforest.fit(X, y)\\n\\n# %%\\n# Feature importance based on mean decrease in impurity (MDI)\\n# -----------------------------------------------------------\\n# Feature importances are provided by the fitted attribute\\n# `feature_importances_` and they are computed as the mean and standard\\n# deviation of accumulation of the impurity decrease within each tree.\\n#\\n# .. warning::\\n#     Impurity-based feature importances can be misleading for **high\\n#     cardinality** features (many unique values). See\\n#     :ref:`permutation_importance` as an alternative.\\nimport time\\n\\nimport matplotlib.pyplot as plt\\n\\nstart_time = time.time()\\nimg_shape = data.images[0].shape\\nimportances = forest.feature_importances_\\nelapsed_time = time.time() - start_time\\n\\nprint(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\\nimp_reshaped = importances.reshape(img_shape)\\nplt.matshow(imp_reshaped, cmap=plt.cm.hot)\\nplt.title(\"Pixel importances using impurity values\")\\nplt.colorbar()\\nplt.show()\\n\\n# %%\\n# Can you still recognize a face?'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_forest_importances_faces.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\\nimp_reshaped = importances.reshape(img_shape)\\nplt.matshow(imp_reshaped, cmap=plt.cm.hot)\\nplt.title(\"Pixel importances using impurity values\")\\nplt.colorbar()\\nplt.show()\\n\\n# %%\\n# Can you still recognize a face?\\n\\n# %%\\n# The limitations of MDI is not a problem for this dataset because:\\n#\\n#  1. All features are (ordered) numeric and will thus not suffer the\\n#     cardinality bias\\n#  2. We are only interested to represent knowledge of the forest acquired\\n#     on the training set.\\n#\\n# If these two conditions are not met, it is recommended to instead use\\n# the :func:`~sklearn.inspection.permutation_importance`.'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_voting_probas.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================================\\nPlot class probabilities calculated by the VotingClassifier\\n===========================================================\\n\\n.. currentmodule:: sklearn\\n\\nPlot the class probabilities of the first sample in a toy dataset predicted by\\nthree different classifiers and averaged by the\\n:class:`~ensemble.VotingClassifier`.\\n\\nFirst, three exemplary classifiers are initialized\\n(:class:`~linear_model.LogisticRegression`, :class:`~naive_bayes.GaussianNB`,\\nand :class:`~ensemble.RandomForestClassifier`) and used to initialize a\\nsoft-voting :class:`~ensemble.VotingClassifier` with weights `[1, 1, 5]`, which\\nmeans that the predicted probabilities of the\\n:class:`~ensemble.RandomForestClassifier` count 5 times as much as the weights\\nof the other classifiers when the averaged probability is calculated.\\n\\nTo visualize the probability weighting, we fit each classifier on the training\\nset and plot the predicted class probabilities for the first sample in this\\nexample dataset.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.naive_bayes import GaussianNB\\n\\nclf1 = LogisticRegression(max_iter=1000, random_state=123)\\nclf2 = RandomForestClassifier(n_estimators=100, random_state=123)\\nclf3 = GaussianNB()\\nX = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\\ny = np.array([1, 1, 2, 2])\\n\\neclf = VotingClassifier(\\n    estimators=[(\"lr\", clf1), (\"rf\", clf2), (\"gnb\", clf3)],\\n    voting=\"soft\",\\n    weights=[1, 1, 5],\\n)\\n\\n# predict class probabilities for all classifiers\\nprobas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\\n\\n# get class probabilities for the first sample in the dataset\\nclass1_1 = [pr[0, 0] for pr in probas]\\nclass2_1 = [pr[0, 1] for pr in probas]\\n\\n\\n# plotting\\n\\nN = 4  # number of groups\\nind = np.arange(N)  # group positions\\nwidth = 0.35  # bar width\\n\\nfig, ax = plt.subplots()\\n\\n# bars for classifier 1-3\\np1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color=\"green\", edgecolor=\"k\")\\np2 = ax.bar(\\n    ind + width,\\n    np.hstack(([class2_1[:-1], [0]])),\\n    width,\\n    color=\"lightgreen\",\\n    edgecolor=\"k\",\\n)\\n\\n# bars for VotingClassifier\\np3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width, color=\"blue\", edgecolor=\"k\")\\np4 = ax.bar(\\n    ind + width, [0, 0, 0, class2_1[-1]], width, color=\"steelblue\", edgecolor=\"k\"\\n)\\n\\n# plot annotations\\nplt.axvline(2.8, color=\"k\", linestyle=\"dashed\")\\nax.set_xticks(ind + width)\\nax.set_xticklabels(\\n    [\\n        \"LogisticRegression\\\\nweight 1\",\\n        \"GaussianNB\\\\nweight 1\",\\n        \"RandomForestClassifier\\\\nweight 5\",\\n        \"VotingClassifier\\\\n(average probabilities)\",\\n    ],\\n    rotation=40,\\n    ha=\"right\",\\n)\\nplt.ylim([0, 1])\\nplt.title(\"Class probabilities for sample 1 by different classifiers\")\\nplt.legend([p1[0], p2[0]], [\"class 1\", \"class 2\"], loc=\"upper left\")\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_oob.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def heldout_score(clf, X_test, y_test):\\n    \"\"\"compute deviance scores on ``X_test`` and ``y_test``.\"\"\"\\n    score = np.zeros((n_estimators,), dtype=np.float64)\\n    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\\n        score[i] = 2 * log_loss(y_test, y_proba[:, 1])\\n    return score'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_oob.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def cv_estimate(n_splits=None):\\n    cv = KFold(n_splits=n_splits)\\n    cv_clf = ensemble.GradientBoostingClassifier(**params)\\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\\n    for train, test in cv.split(X_train, y_train):\\n        cv_clf.fit(X_train[train], y_train[train])\\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\\n    val_scores /= n_splits\\n    return val_scores'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_oob.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================\\nGradient Boosting Out-of-Bag estimates\\n======================================\\nOut-of-bag (OOB) estimates can be a useful heuristic to estimate\\nthe \"optimal\" number of boosting iterations.\\nOOB estimates are almost identical to cross-validation estimates but\\nthey can be computed on-the-fly without the need for repeated model\\nfitting.\\nOOB estimates are only available for Stochastic Gradient Boosting\\n(i.e. ``subsample < 1.0``), the estimates are derived from the improvement\\nin loss based on the examples not included in the bootstrap sample\\n(the so-called out-of-bag examples).\\nThe OOB estimator is a pessimistic estimator of the true\\ntest loss, but remains a fairly good approximation for a small number of trees.\\nThe figure shows the cumulative sum of the negative OOB improvements\\nas a function of the boosting iteration. As you can see, it tracks the test\\nloss for the first hundred iterations but then diverges in a\\npessimistic way.\\nThe figure also shows the performance of 3-fold cross validation which\\nusually gives a better estimate of the test loss\\nbut is computationally more demanding.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom scipy.special import expit\\n\\nfrom sklearn import ensemble\\nfrom sklearn.metrics import log_loss\\nfrom sklearn.model_selection import KFold, train_test_split\\n\\n# Generate data (adapted from G. Ridgeway\\'s gbm example)\\nn_samples = 1000\\nrandom_state = np.random.RandomState(13)\\nx1 = random_state.uniform(size=n_samples)\\nx2 = random_state.uniform(size=n_samples)\\nx3 = random_state.randint(0, 4, size=n_samples)\\n\\np = expit(np.sin(3 * x1) - 4 * x2 + x3)\\ny = random_state.binomial(1, p, size=n_samples)\\n\\nX = np.c_[x1, x2, x3]\\n\\nX = X.astype(np.float32)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=9)\\n\\n# Fit classifier with out-of-bag estimates\\nparams = {\\n    \"n_estimators\": 1200,\\n    \"max_depth\": 3,\\n    \"subsample\": 0.5,\\n    \"learning_rate\": 0.01,\\n    \"min_samples_leaf\": 1,\\n    \"random_state\": 3,\\n}\\nclf = ensemble.GradientBoostingClassifier(**params)\\n\\nclf.fit(X_train, y_train)\\nacc = clf.score(X_test, y_test)\\nprint(\"Accuracy: {:.4f}\".format(acc))\\n\\nn_estimators = params[\"n_estimators\"]\\nx = np.arange(n_estimators) + 1\\n\\n\\n# Code for: def heldout_score(clf, X_test, y_test):\\n\\n\\n# Code for: def cv_estimate(n_splits=None):\\n\\n\\n# Estimate best n_estimator using cross-validation\\ncv_score = cv_estimate(3)\\n\\n# Compute best n_estimator for test data\\ntest_score = heldout_score(clf, X_test, y_test)\\n\\n# negative cumulative sum of oob improvements\\ncumsum = -np.cumsum(clf.oob_improvement_)\\n\\n# min loss according to OOB\\noob_best_iter = x[np.argmin(cumsum)]\\n\\n# min loss according to test (normalize such that first loss is 0)\\ntest_score -= test_score[0]\\ntest_best_iter = x[np.argmin(test_score)]'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_gradient_boosting_oob.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# negative cumulative sum of oob improvements\\ncumsum = -np.cumsum(clf.oob_improvement_)\\n\\n# min loss according to OOB\\noob_best_iter = x[np.argmin(cumsum)]\\n\\n# min loss according to test (normalize such that first loss is 0)\\ntest_score -= test_score[0]\\ntest_best_iter = x[np.argmin(test_score)]\\n\\n# min loss according to cv (normalize such that first loss is 0)\\ncv_score -= cv_score[0]\\ncv_best_iter = x[np.argmin(cv_score)]\\n\\n# color brew for the three curves\\noob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))\\ntest_color = list(map(lambda x: x / 256.0, (127, 201, 127)))\\ncv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))\\n\\n# line type for the three curves\\noob_line = \"dashed\"\\ntest_line = \"solid\"\\ncv_line = \"dashdot\"\\n\\n# plot curves and vertical lines for best iterations\\nplt.figure(figsize=(8, 4.8))\\nplt.plot(x, cumsum, label=\"OOB loss\", color=oob_color, linestyle=oob_line)\\nplt.plot(x, test_score, label=\"Test loss\", color=test_color, linestyle=test_line)\\nplt.plot(x, cv_score, label=\"CV loss\", color=cv_color, linestyle=cv_line)\\nplt.axvline(x=oob_best_iter, color=oob_color, linestyle=oob_line)\\nplt.axvline(x=test_best_iter, color=test_color, linestyle=test_line)\\nplt.axvline(x=cv_best_iter, color=cv_color, linestyle=cv_line)\\n\\n# add three vertical lines to xticks\\nxticks = plt.xticks()\\nxticks_pos = np.array(\\n    xticks[0].tolist() + [oob_best_iter, cv_best_iter, test_best_iter]\\n)\\nxticks_label = np.array(list(map(lambda t: int(t), xticks[0])) + [\"OOB\", \"CV\", \"Test\"])\\nind = np.argsort(xticks_pos)\\nxticks_pos = xticks_pos[ind]\\nxticks_label = xticks_label[ind]\\nplt.xticks(xticks_pos, xticks_label, rotation=90)\\n\\nplt.legend(loc=\"upper center\")\\nplt.ylabel(\"normalized loss\")\\nplt.xlabel(\"number of iterations\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/ensemble/plot_adaboost_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================\\nDecision Tree Regression with AdaBoost\\n======================================\\n\\nA decision tree is boosted using the AdaBoost.R2 [1]_ algorithm on a 1D\\nsinusoidal dataset with a small amount of Gaussian noise.\\n299 boosts (300 decision trees) is compared with a single decision tree\\nregressor. As the number of boosts is increased the regressor can fit more\\ndetail.\\n\\nSee :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an\\nexample showcasing the benefits of using more efficient regression models such\\nas :class:`~ensemble.HistGradientBoostingRegressor`.\\n\\n.. [1] `H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\\n        <https://citeseerx.ist.psu.edu/doc_view/pid/8d49e2dedb817f2c3330e74b63c5fc86d2399ce3>`_\\n\\n\"\"\"\\n\\n# %%\\n# Preparing the data\\n# ------------------\\n# First, we prepare dummy data with a sinusoidal relationship and some gaussian noise.\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport numpy as np\\n\\nrng = np.random.RandomState(1)\\nX = np.linspace(0, 6, 100)[:, np.newaxis]\\ny = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\\n\\n# %%\\n# Training and prediction with DecisionTree and AdaBoost Regressors\\n# -----------------------------------------------------------------\\n# Now, we define the classifiers and fit them to the data.\\n# Then we predict on that same data to see how well they could fit it.\\n# The first regressor is a `DecisionTreeRegressor` with `max_depth=4`.\\n# The second regressor is an `AdaBoostRegressor` with a `DecisionTreeRegressor`\\n# of `max_depth=4` as base learner and will be built with `n_estimators=300`\\n# of those base learners.\\n\\nfrom sklearn.ensemble import AdaBoostRegressor\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\nregr_1 = DecisionTreeRegressor(max_depth=4)\\n\\nregr_2 = AdaBoostRegressor(\\n    DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=rng\\n)\\n\\nregr_1.fit(X, y)\\nregr_2.fit(X, y)\\n\\ny_1 = regr_1.predict(X)\\ny_2 = regr_2.predict(X)\\n\\n# %%\\n# Plotting the results\\n# --------------------\\n# Finally, we plot how well our two regressors,\\n# single decision tree regressor and AdaBoost regressor, could fit the data.\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ncolors = sns.color_palette(\"colorblind\")\\n\\nplt.figure()\\nplt.scatter(X, y, color=colors[0], label=\"training samples\")\\nplt.plot(X, y_1, color=colors[1], label=\"n_estimators=1\", linewidth=2)\\nplt.plot(X, y_2, color=colors[2], label=\"n_estimators=300\", linewidth=2)\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\nplt.title(\"Boosted Decision Tree Regression\")\\nplt.legend()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/tree/plot_cost_complexity_pruning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n========================================================\\nPost pruning decision trees with cost complexity pruning\\n========================================================\\n\\n.. currentmodule:: sklearn.tree\\n\\nThe :class:`DecisionTreeClassifier` provides parameters such as\\n``min_samples_leaf`` and ``max_depth`` to prevent a tree from overfiting. Cost\\ncomplexity pruning provides another option to control the size of a tree. In\\n:class:`DecisionTreeClassifier`, this pruning technique is parameterized by the\\ncost complexity parameter, ``ccp_alpha``. Greater values of ``ccp_alpha``\\nincrease the number of nodes pruned. Here we only show the effect of\\n``ccp_alpha`` on regularizing the trees and how to choose a ``ccp_alpha``\\nbased on validation scores.\\n\\nSee also :ref:`minimal_cost_complexity_pruning` for details on pruning.\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# %%\\n# Total impurity of leaves vs effective alphas of pruned tree\\n# ---------------------------------------------------------------\\n# Minimal cost complexity pruning recursively finds the node with the \"weakest\\n# link\". The weakest link is characterized by an effective alpha, where the\\n# nodes with the smallest effective alpha are pruned first. To get an idea of\\n# what values of ``ccp_alpha`` could be appropriate, scikit-learn provides\\n# :func:`DecisionTreeClassifier.cost_complexity_pruning_path` that returns the\\n# effective alphas and the corresponding total leaf impurities at each step of\\n# the pruning process. As alpha increases, more of the tree is pruned, which\\n# increases the total impurity of its leaves.\\nX, y = load_breast_cancer(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\nclf = DecisionTreeClassifier(random_state=0)\\npath = clf.cost_complexity_pruning_path(X_train, y_train)\\nccp_alphas, impurities = path.ccp_alphas, path.impurities\\n\\n# %%\\n# In the following plot, the maximum effective alpha value is removed, because\\n# it is the trivial tree with only one node.\\nfig, ax = plt.subplots()\\nax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\\nax.set_xlabel(\"effective alpha\")\\nax.set_ylabel(\"total impurity of leaves\")\\nax.set_title(\"Total Impurity vs effective alpha for training set\")\\n\\n# %%\\n# Next, we train a decision tree using the effective alphas. The last value\\n# in ``ccp_alphas`` is the alpha value that prunes the whole tree,\\n# leaving the tree, ``clfs[-1]``, with one node.\\nclfs = []\\nfor ccp_alpha in ccp_alphas:\\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\\n    clf.fit(X_train, y_train)\\n    clfs.append(clf)\\nprint(\\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\\n    )\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/tree/plot_cost_complexity_pruning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# For the remainder of this example, we remove the last element in\\n# ``clfs`` and ``ccp_alphas``, because it is the trivial tree with only one\\n# node. Here we show that the number of nodes and tree depth decreases as alpha\\n# increases.\\nclfs = clfs[:-1]\\nccp_alphas = ccp_alphas[:-1]\\n\\nnode_counts = [clf.tree_.node_count for clf in clfs]\\ndepth = [clf.tree_.max_depth for clf in clfs]\\nfig, ax = plt.subplots(2, 1)\\nax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\\nax[0].set_xlabel(\"alpha\")\\nax[0].set_ylabel(\"number of nodes\")\\nax[0].set_title(\"Number of nodes vs alpha\")\\nax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\\nax[1].set_xlabel(\"alpha\")\\nax[1].set_ylabel(\"depth of tree\")\\nax[1].set_title(\"Depth vs alpha\")\\nfig.tight_layout()\\n\\n# %%\\n# Accuracy vs alpha for training and testing sets\\n# ----------------------------------------------------\\n# When ``ccp_alpha`` is set to zero and keeping the other default parameters\\n# of :class:`DecisionTreeClassifier`, the tree overfits, leading to\\n# a 100% training accuracy and 88% testing accuracy. As alpha increases, more\\n# of the tree is pruned, thus creating a decision tree that generalizes better.\\n# In this example, setting ``ccp_alpha=0.015`` maximizes the testing accuracy.\\ntrain_scores = [clf.score(X_train, y_train) for clf in clfs]\\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\\n\\nfig, ax = plt.subplots()\\nax.set_xlabel(\"alpha\")\\nax.set_ylabel(\"accuracy\")\\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\\nax.legend()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/tree/plot_unveil_tree_structure.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================\\nUnderstanding the decision tree structure\\n=========================================\\n\\nThe decision tree structure can be analysed to gain further insight on the\\nrelation between the features and the target to predict. In this example, we\\nshow how to retrieve:\\n\\n- the binary tree structure;\\n- the depth of each node and whether or not it\\'s a leaf;\\n- the nodes that were reached by a sample using the ``decision_path`` method;\\n- the leaf that was reached by a sample using the apply method;\\n- the rules that were used to predict a sample;\\n- the decision path shared by a group of samples.\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn import tree\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n##############################################################################\\n# Train tree classifier\\n# ---------------------\\n# First, we fit a :class:`~sklearn.tree.DecisionTreeClassifier` using the\\n# :func:`~sklearn.datasets.load_iris` dataset.\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\nclf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\\nclf.fit(X_train, y_train)'), Document(metadata={'source': '/content/local_copy_repo/examples/tree/plot_unveil_tree_structure.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"iris = load_iris()\\nX = iris.data\\ny = iris.target\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\nclf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\\nclf.fit(X_train, y_train)\\n\\n##############################################################################\\n# Tree structure\\n# --------------\\n#\\n# The decision classifier has an attribute called ``tree_`` which allows access\\n# to low level attributes such as ``node_count``, the total number of nodes,\\n# and ``max_depth``, the maximal depth of the tree. The\\n# ``tree_.compute_node_depths()`` method computes the depth of each node in the\\n# tree. `tree_` also stores the entire binary tree structure, represented as a\\n# number of parallel arrays. The i-th element of each array holds information\\n# about the node ``i``. Node 0 is the tree's root. Some of the arrays only\\n# apply to either leaves or split nodes. In this case the values of the nodes\\n# of the other type is arbitrary. For example, the arrays ``feature`` and\\n# ``threshold`` only apply to split nodes. The values for leaf nodes in these\\n# arrays are therefore arbitrary.\\n#\\n# Among these arrays, we have:\\n#\\n#   - ``children_left[i]``: id of the left child of node ``i`` or -1 if leaf\\n#     node\\n#   - ``children_right[i]``: id of the right child of node ``i`` or -1 if leaf\\n#     node\\n#   - ``feature[i]``: feature used for splitting node ``i``\\n#   - ``threshold[i]``: threshold value at node ``i``\\n#   - ``n_node_samples[i]``: the number of training samples reaching node\\n#     ``i``\\n#   - ``impurity[i]``: the impurity at node ``i``\\n#   - ``weighted_n_node_samples[i]``: the weighted number of training samples\\n#     reaching node ``i``\\n#   - ``value[i, j, k]``: the summary of the training samples that reached node i for\\n#     output j and class k (for regression tree, class is set to 1). See below\\n#     for more information about ``value``.\\n#\\n# Using the arrays, we can traverse the tree structure to compute various\\n# properties. Below, we will compute the depth of each node and whether or not\\n# it is a leaf.\\n\\nn_nodes = clf.tree_.node_count\\nchildren_left = clf.tree_.children_left\\nchildren_right = clf.tree_.children_right\\nfeature = clf.tree_.feature\\nthreshold = clf.tree_.threshold\\nvalues = clf.tree_.value\\n\\nnode_depth = np.zeros(shape=n_nodes, dtype=np.int64)\\nis_leaves = np.zeros(shape=n_nodes, dtype=bool)\\nstack = [(0, 0)]  # start with the root node id (0) and its depth (0)\\nwhile len(stack) > 0:\\n    # `pop` ensures each node is only visited once\\n    node_id, depth = stack.pop()\\n    node_depth[node_id] = depth\"), Document(metadata={'source': '/content/local_copy_repo/examples/tree/plot_unveil_tree_structure.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# If the left and right child of a node is not the same we have a split\\n    # node\\n    is_split_node = children_left[node_id] != children_right[node_id]\\n    # If a split node, append left and right children and depth to `stack`\\n    # so we can loop through them\\n    if is_split_node:\\n        stack.append((children_left[node_id], depth + 1))\\n        stack.append((children_right[node_id], depth + 1))\\n    else:\\n        is_leaves[node_id] = True\\n\\nprint(\\n    \"The binary tree structure has {n} nodes and has \"\\n    \"the following tree structure:\\\\n\".format(n=n_nodes)\\n)\\nfor i in range(n_nodes):\\n    if is_leaves[i]:\\n        print(\\n            \"{space}node={node} is a leaf node with value={value}.\".format(\\n                space=node_depth[i] * \"\\\\t\", node=i, value=np.around(values[i], 3)\\n            )\\n        )\\n    else:\\n        print(\\n            \"{space}node={node} is a split node with value={value}: \"\\n            \"go to node {left} if X[:, {feature}] <= {threshold} \"\\n            \"else to node {right}.\".format(\\n                space=node_depth[i] * \"\\\\t\",\\n                node=i,\\n                left=children_left[i],\\n                feature=feature[i],\\n                threshold=threshold[i],\\n                right=children_right[i],\\n                value=np.around(values[i], 3),\\n            )\\n        )'), Document(metadata={'source': '/content/local_copy_repo/examples/tree/plot_unveil_tree_structure.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# What is the values array used here?\\n# -----------------------------------\\n# The `tree_.value` array is a 3D array of shape\\n# [``n_nodes``, ``n_classes``, ``n_outputs``] which provides the proportion of samples\\n# reaching a node for each class and for each output.\\n# Each node has a ``value`` array which is the proportion of weighted samples reaching\\n# this node for each output and class with respect to the parent node.\\n#\\n# One could convert this to the absolute weighted number of samples reaching a node,\\n# by multiplying this number by `tree_.weighted_n_node_samples[node_idx]` for the\\n# given node. Note sample weights are not used in this example, so the weighted\\n# number of samples is the number of samples reaching the node because each sample\\n# has a weight of 1 by default.\\n#\\n# For example, in the above tree built on the iris dataset, the root node has\\n# ``value = [0.33, 0.304, 0.366]`` indicating there are 33% of class 0 samples,\\n# 30.4% of class 1 samples, and 36.6% of class 2 samples at the root node. One can\\n# convert this to the absolute number of samples by multiplying by the number of\\n# samples reaching the root node, which is `tree_.weighted_n_node_samples[0]`.\\n# Then the root node has ``value = [37, 34, 41]``, indicating there are 37 samples\\n# of class 0, 34 samples of class 1, and 41 samples of class 2 at the root node.\\n#\\n# Traversing the tree, the samples are split and as a result, the ``value`` array\\n# reaching each node changes. The left child of the root node has ``value = [1., 0, 0]``\\n# (or ``value = [37, 0, 0]`` when converted to the absolute number of samples)\\n# because all 37 samples in the left child node are from class 0.\\n#\\n# Note: In this example, `n_outputs=1`, but the tree classifier can also handle\\n# multi-output problems. The `value` array at each node would just be a 2D\\n# array instead.\\n\\n##############################################################################\\n# We can compare the above output to the plot of the decision tree.\\n# Here, we show the proportions of samples of each class that reach each\\n# node corresponding to the actual elements of `tree_.value` array.\\n\\ntree.plot_tree(clf, proportion=True)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/tree/plot_unveil_tree_structure.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='tree.plot_tree(clf, proportion=True)\\nplt.show()\\n\\n##############################################################################\\n# Decision path\\n# -------------\\n#\\n# We can also retrieve the decision path of samples of interest. The\\n# ``decision_path`` method outputs an indicator matrix that allows us to\\n# retrieve the nodes the samples of interest traverse through. A non zero\\n# element in the indicator matrix at position ``(i, j)`` indicates that\\n# the sample ``i`` goes through the node ``j``. Or, for one sample ``i``, the\\n# positions of the non zero elements in row ``i`` of the indicator matrix\\n# designate the ids of the nodes that sample goes through.\\n#\\n# The leaf ids reached by samples of interest can be obtained with the\\n# ``apply`` method. This returns an array of the node ids of the leaves\\n# reached by each sample of interest. Using the leaf ids and the\\n# ``decision_path`` we can obtain the splitting conditions that were used to\\n# predict a sample or a group of samples. First, let\\'s do it for one sample.\\n# Note that ``node_index`` is a sparse matrix.\\n\\nnode_indicator = clf.decision_path(X_test)\\nleaf_id = clf.apply(X_test)\\n\\nsample_id = 0\\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\\nnode_index = node_indicator.indices[\\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\\n]\\n\\nprint(\"Rules used to predict sample {id}:\\\\n\".format(id=sample_id))\\nfor node_id in node_index:\\n    # continue to the next node if it is a leaf node\\n    if leaf_id[sample_id] == node_id:\\n        continue\\n\\n    # check if value of the split feature for sample 0 is below threshold\\n    if X_test[sample_id, feature[node_id]] <= threshold[node_id]:\\n        threshold_sign = \"<=\"\\n    else:\\n        threshold_sign = \">\"\\n\\n    print(\\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\\n        \"{inequality} {threshold})\".format(\\n            node=node_id,\\n            sample=sample_id,\\n            feature=feature[node_id],\\n            value=X_test[sample_id, feature[node_id]],\\n            inequality=threshold_sign,\\n            threshold=threshold[node_id],\\n        )\\n    )\\n\\n##############################################################################\\n# For a group of samples, we can determine the common nodes the samples go\\n# through.\\n\\nsample_ids = [0, 1]\\n# boolean array indicating the nodes both samples go through\\ncommon_nodes = node_indicator.toarray()[sample_ids].sum(axis=0) == len(sample_ids)\\n# obtain node ids using position in array\\ncommon_node_id = np.arange(n_nodes)[common_nodes]\\n\\nprint(\\n    \"\\\\nThe following samples {samples} share the node(s) {nodes} in the tree.\".format(\\n        samples=sample_ids, nodes=common_node_id\\n    )\\n)\\nprint(\"This is {prop}% of all nodes.\".format(prop=100 * len(common_node_id) / n_nodes))'), Document(metadata={'source': '/content/local_copy_repo/examples/tree/plot_tree_regression_multioutput.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================================================\\nMulti-output Decision Tree Regression\\n===================================================================\\n\\nAn example to illustrate multi-output regression with decision tree.\\n\\nThe :ref:`decision trees <tree>`\\nis used to predict simultaneously the noisy x and y observations of a circle\\ngiven a single underlying feature. As a result, it learns local linear\\nregressions approximating the circle.\\n\\nWe can see that if the maximum depth of the tree (controlled by the\\n`max_depth` parameter) is set too high, the decision trees learn too fine\\ndetails of the training data and learn from the noise, i.e. they overfit.\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\n# Create a random dataset\\nrng = np.random.RandomState(1)\\nX = np.sort(200 * rng.rand(100, 1) - 100, axis=0)\\ny = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T\\ny[::5, :] += 0.5 - rng.rand(20, 2)\\n\\n# Fit regression model\\nregr_1 = DecisionTreeRegressor(max_depth=2)\\nregr_2 = DecisionTreeRegressor(max_depth=5)\\nregr_3 = DecisionTreeRegressor(max_depth=8)\\nregr_1.fit(X, y)\\nregr_2.fit(X, y)\\nregr_3.fit(X, y)\\n\\n# Predict\\nX_test = np.arange(-100.0, 100.0, 0.01)[:, np.newaxis]\\ny_1 = regr_1.predict(X_test)\\ny_2 = regr_2.predict(X_test)\\ny_3 = regr_3.predict(X_test)\\n\\n# Plot the results\\nplt.figure()\\ns = 25\\nplt.scatter(y[:, 0], y[:, 1], c=\"navy\", s=s, edgecolor=\"black\", label=\"data\")\\nplt.scatter(\\n    y_1[:, 0],\\n    y_1[:, 1],\\n    c=\"cornflowerblue\",\\n    s=s,\\n    edgecolor=\"black\",\\n    label=\"max_depth=2\",\\n)\\nplt.scatter(y_2[:, 0], y_2[:, 1], c=\"red\", s=s, edgecolor=\"black\", label=\"max_depth=5\")\\nplt.scatter(\\n    y_3[:, 0], y_3[:, 1], c=\"orange\", s=s, edgecolor=\"black\", label=\"max_depth=8\"\\n)\\nplt.xlim([-6, 6])\\nplt.ylim([-6, 6])\\nplt.xlabel(\"target 1\")\\nplt.ylabel(\"target 2\")\\nplt.title(\"Multi-output Decision Tree Regression\")\\nplt.legend(loc=\"best\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/tree/plot_iris_dtc.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=======================================================================\\nPlot the decision surface of decision trees trained on the iris dataset\\n=======================================================================\\n\\nPlot the decision surface of a decision tree trained on pairs\\nof features of the iris dataset.\\n\\nSee :ref:`decision tree <tree>` for more information on the estimator.\\n\\nFor each pair of iris features, the decision tree learns decision\\nboundaries made of combinations of simple thresholding rules inferred from\\nthe training samples.\\n\\nWe also show the tree structure of a model built on all of the features.\\n\"\"\"\\n\\n# %%\\n# First load the copy of the Iris dataset shipped with scikit-learn:\\nfrom sklearn.datasets import load_iris\\n\\niris = load_iris()\\n\\n\\n# %%\\n# Display the decision functions of trees trained on all pairs of features.\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Parameters\\nn_classes = 3\\nplot_colors = \"ryb\"\\nplot_step = 0.02\\n\\n\\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\\n    # We only take the two corresponding features\\n    X = iris.data[:, pair]\\n    y = iris.target\\n\\n    # Train\\n    clf = DecisionTreeClassifier().fit(X, y)\\n\\n    # Plot the decision boundary\\n    ax = plt.subplot(2, 3, pairidx + 1)\\n    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\\n    DecisionBoundaryDisplay.from_estimator(\\n        clf,\\n        X,\\n        cmap=plt.cm.RdYlBu,\\n        response_method=\"predict\",\\n        ax=ax,\\n        xlabel=iris.feature_names[pair[0]],\\n        ylabel=iris.feature_names[pair[1]],\\n    )\\n\\n    # Plot the training points\\n    for i, color in zip(range(n_classes), plot_colors):\\n        idx = np.where(y == i)\\n        plt.scatter(\\n            X[idx, 0],\\n            X[idx, 1],\\n            c=color,\\n            label=iris.target_names[i],\\n            edgecolor=\"black\",\\n            s=15,\\n        )\\n\\nplt.suptitle(\"Decision surface of decision trees trained on pairs of features\")\\nplt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\\n_ = plt.axis(\"tight\")\\n\\n# %%\\n# Display the structure of a single decision tree trained on all the features\\n# together.\\nfrom sklearn.tree import plot_tree\\n\\nplt.figure()\\nclf = DecisionTreeClassifier().fit(iris.data, iris.target)\\nplot_tree(clf, filled=True)\\nplt.title(\"Decision tree trained on all the iris features\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/tree/plot_tree_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================================================\\nDecision Tree Regression\\n===================================================================\\n\\nA 1D regression with decision tree.\\n\\nThe :ref:`decision trees <tree>` is\\nused to fit a sine curve with addition noisy observation. As a result, it\\nlearns local linear regressions approximating the sine curve.\\n\\nWe can see that if the maximum depth of the tree (controlled by the\\n`max_depth` parameter) is set too high, the decision trees learn too fine\\ndetails of the training data and learn from the noise, i.e. they overfit.\\n\"\"\"\\n\\n# Import the necessary modules and libraries\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\n# Create a random dataset\\nrng = np.random.RandomState(1)\\nX = np.sort(5 * rng.rand(80, 1), axis=0)\\ny = np.sin(X).ravel()\\ny[::5] += 3 * (0.5 - rng.rand(16))\\n\\n# Fit regression model\\nregr_1 = DecisionTreeRegressor(max_depth=2)\\nregr_2 = DecisionTreeRegressor(max_depth=5)\\nregr_1.fit(X, y)\\nregr_2.fit(X, y)\\n\\n# Predict\\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\\ny_1 = regr_1.predict(X_test)\\ny_2 = regr_2.predict(X_test)\\n\\n# Plot the results\\nplt.figure()\\nplt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\\nplt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\nplt.title(\"Decision Tree Regression\")\\nplt.legend()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/kernel_approximation/plot_scalable_poly_kernels.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================================\\nScalable learning with polynomial kernel approximation\\n======================================================\\n\\n.. currentmodule:: sklearn.kernel_approximation\\n\\nThis example illustrates the use of :class:`PolynomialCountSketch` to\\nefficiently generate polynomial kernel feature-space approximations.\\nThis is used to train linear classifiers that approximate the accuracy\\nof kernelized ones.\\n\\nWe use the Covtype dataset [2], trying to reproduce the experiments on the\\noriginal paper of Tensor Sketch [1], i.e. the algorithm implemented by\\n:class:`PolynomialCountSketch`.\\n\\nFirst, we compute the accuracy of a linear classifier on the original\\nfeatures. Then, we train linear classifiers on different numbers of\\nfeatures (`n_components`) generated by :class:`PolynomialCountSketch`,\\napproximating the accuracy of a kernelized classifier in a scalable manner.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Preparing the data\\n# ------------------\\n#\\n# Load the Covtype dataset, which contains 581,012 samples\\n# with 54 features each, distributed among 6 classes. The goal of this dataset\\n# is to predict forest cover type from cartographic variables only\\n# (no remotely sensed data). After loading, we transform it into a binary\\n# classification problem to match the version of the dataset in the\\n# LIBSVM webpage [2], which was the one used in [1].\\n\\nfrom sklearn.datasets import fetch_covtype\\n\\nX, y = fetch_covtype(return_X_y=True)\\n\\ny[y != 2] = 0\\ny[y == 2] = 1  # We will try to separate class 2 from the other 6 classes.\\n\\n# %%\\n# Partitioning the data\\n# ---------------------\\n#\\n# Here we select 5,000 samples for training and 10,000 for testing.\\n# To actually reproduce the results in the original Tensor Sketch paper,\\n# select 100,000 for training.\\n\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, train_size=5_000, test_size=10_000, random_state=42\\n)\\n\\n# %%\\n# Feature normalization\\n# ---------------------\\n#\\n# Now scale features to the range [0, 1] to match the format of the dataset in\\n# the LIBSVM webpage, and then normalize to unit length as done in the\\n# original Tensor Sketch paper [1].\\n\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer\\n\\nmm = make_pipeline(MinMaxScaler(), Normalizer())\\nX_train = mm.fit_transform(X_train)\\nX_test = mm.transform(X_test)\\n\\n# %%\\n# Establishing a baseline model\\n# -----------------------------\\n#\\n# As a baseline, train a linear SVM on the original features and print the\\n# accuracy. We also measure and store accuracies and training times to\\n# plot them later.\\n\\nimport time\\n\\nfrom sklearn.svm import LinearSVC\\n\\nresults = {}\\n\\nlsvm = LinearSVC()\\nstart = time.time()\\nlsvm.fit(X_train, y_train)\\nlsvm_time = time.time() - start\\nlsvm_score = 100 * lsvm.score(X_test, y_test)'), Document(metadata={'source': '/content/local_copy_repo/examples/kernel_approximation/plot_scalable_poly_kernels.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import time\\n\\nfrom sklearn.svm import LinearSVC\\n\\nresults = {}\\n\\nlsvm = LinearSVC()\\nstart = time.time()\\nlsvm.fit(X_train, y_train)\\nlsvm_time = time.time() - start\\nlsvm_score = 100 * lsvm.score(X_test, y_test)\\n\\nresults[\"LSVM\"] = {\"time\": lsvm_time, \"score\": lsvm_score}\\nprint(f\"Linear SVM score on raw features: {lsvm_score:.2f}%\")\\n\\n# %%\\n# Establishing the kernel approximation model\\n# -------------------------------------------\\n#\\n# Then we train linear SVMs on the features generated by\\n# :class:`PolynomialCountSketch` with different values for `n_components`,\\n# showing that these kernel feature approximations improve the accuracy\\n# of linear classification. In typical application scenarios, `n_components`\\n# should be larger than the number of features in the input representation\\n# in order to achieve an improvement with respect to linear classification.\\n# As a rule of thumb, the optimum of evaluation score / run time cost is\\n# typically achieved at around `n_components` = 10 * `n_features`, though this\\n# might depend on the specific dataset being handled. Note that, since the\\n# original samples have 54 features, the explicit feature map of the\\n# polynomial kernel of degree four would have approximately 8.5 million\\n# features (precisely, 54^4). Thanks to :class:`PolynomialCountSketch`, we can\\n# condense most of the discriminative information of that feature space into a\\n# much more compact representation. While we run the experiment only a single time\\n# (`n_runs` = 1) in this example, in practice one should repeat the experiment several\\n# times to compensate for the stochastic nature of :class:`PolynomialCountSketch`.\\n\\nfrom sklearn.kernel_approximation import PolynomialCountSketch\\n\\nn_runs = 1\\nN_COMPONENTS = [250, 500, 1000, 2000]\\n\\nfor n_components in N_COMPONENTS:\\n    ps_lsvm_time = 0\\n    ps_lsvm_score = 0\\n    for _ in range(n_runs):\\n        pipeline = make_pipeline(\\n            PolynomialCountSketch(n_components=n_components, degree=4),\\n            LinearSVC(),\\n        )\\n\\n        start = time.time()\\n        pipeline.fit(X_train, y_train)\\n        ps_lsvm_time += time.time() - start\\n        ps_lsvm_score += 100 * pipeline.score(X_test, y_test)\\n\\n    ps_lsvm_time /= n_runs\\n    ps_lsvm_score /= n_runs\\n\\n    results[f\"LSVM + PS({n_components})\"] = {\\n        \"time\": ps_lsvm_time,\\n        \"score\": ps_lsvm_score,\\n    }\\n    print(\\n        f\"Linear SVM score on {n_components} PolynomialCountSketch \"\\n        + f\"features: {ps_lsvm_score:.2f}%\"\\n    )\\n\\n# %%\\n# Establishing the kernelized SVM model\\n# -------------------------------------\\n#\\n# Train a kernelized SVM to see how well :class:`PolynomialCountSketch`\\n# is approximating the performance of the kernel. This, of course, may take\\n# some time, as the SVC class has a relatively poor scalability. This is the\\n# reason why kernel approximators are so useful:\\n\\nfrom sklearn.svm import SVC\\n\\nksvm = SVC(C=500.0, kernel=\"poly\", degree=4, coef0=0, gamma=1.0)'), Document(metadata={'source': '/content/local_copy_repo/examples/kernel_approximation/plot_scalable_poly_kernels.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.svm import SVC\\n\\nksvm = SVC(C=500.0, kernel=\"poly\", degree=4, coef0=0, gamma=1.0)\\n\\nstart = time.time()\\nksvm.fit(X_train, y_train)\\nksvm_time = time.time() - start\\nksvm_score = 100 * ksvm.score(X_test, y_test)\\n\\nresults[\"KSVM\"] = {\"time\": ksvm_time, \"score\": ksvm_score}\\nprint(f\"Kernel-SVM score on raw features: {ksvm_score:.2f}%\")\\n\\n# %%\\n# Comparing the results\\n# ---------------------\\n#\\n# Finally, plot the results of the different methods against their training\\n# times. As we can see, the kernelized SVM achieves a higher accuracy,\\n# but its training time is much larger and, most importantly, will grow\\n# much faster if the number of training samples increases.\\n\\nimport matplotlib.pyplot as plt\\n\\nfig, ax = plt.subplots(figsize=(7, 7))\\nax.scatter(\\n    [\\n        results[\"LSVM\"][\"time\"],\\n    ],\\n    [\\n        results[\"LSVM\"][\"score\"],\\n    ],\\n    label=\"Linear SVM\",\\n    c=\"green\",\\n    marker=\"^\",\\n)\\n\\nax.scatter(\\n    [\\n        results[\"LSVM + PS(250)\"][\"time\"],\\n    ],\\n    [\\n        results[\"LSVM + PS(250)\"][\"score\"],\\n    ],\\n    label=\"Linear SVM + PolynomialCountSketch\",\\n    c=\"blue\",\\n)\\n\\nfor n_components in N_COMPONENTS:\\n    ax.scatter(\\n        [\\n            results[f\"LSVM + PS({n_components})\"][\"time\"],\\n        ],\\n        [\\n            results[f\"LSVM + PS({n_components})\"][\"score\"],\\n        ],\\n        c=\"blue\",\\n    )\\n    ax.annotate(\\n        f\"n_comp.={n_components}\",\\n        (\\n            results[f\"LSVM + PS({n_components})\"][\"time\"],\\n            results[f\"LSVM + PS({n_components})\"][\"score\"],\\n        ),\\n        xytext=(-30, 10),\\n        textcoords=\"offset pixels\",\\n    )\\n\\nax.scatter(\\n    [\\n        results[\"KSVM\"][\"time\"],\\n    ],\\n    [\\n        results[\"KSVM\"][\"score\"],\\n    ],\\n    label=\"Kernel SVM\",\\n    c=\"red\",\\n    marker=\"x\",\\n)\\n\\nax.set_xlabel(\"Training time (s)\")\\nax.set_ylabel(\"Accuracy (%)\")\\nax.legend()\\nplt.show()\\n\\n# %%\\n# References\\n# ==========\\n#\\n# [1] Pham, Ninh and Rasmus Pagh. \"Fast and scalable polynomial kernels via\\n# explicit feature maps.\" KDD \\'13 (2013).\\n# https://doi.org/10.1145/2487575.2487591\\n#\\n# [2] LIBSVM binary datasets repository\\n# https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html'), Document(metadata={'source': '/content/local_copy_repo/examples/cross_decomposition/plot_compare_cross_decomposition.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================\\nCompare cross decomposition methods\\n===================================\\n\\nSimple usage of various cross decomposition algorithms:\\n\\n- PLSCanonical\\n- PLSRegression, with multivariate response, a.k.a. PLS2\\n- PLSRegression, with univariate response, a.k.a. PLS1\\n- CCA\\n\\nGiven 2 multivariate covarying two-dimensional datasets, X, and Y,\\nPLS extracts the \\'directions of covariance\\', i.e. the components of each\\ndatasets that explain the most shared variance between both datasets.\\nThis is apparent on the **scatterplot matrix** display: components 1 in\\ndataset X and dataset Y are maximally correlated (points lie around the\\nfirst diagonal). This is also true for components 2 in both dataset,\\nhowever, the correlation across datasets for different components is\\nweak: the point cloud is very spherical.\\n\\n\"\"\"\\n\\n# %%\\n# Dataset based latent variables model\\n# ------------------------------------\\n\\nimport numpy as np\\n\\nn = 500\\n# 2 latents vars:\\nl1 = np.random.normal(size=n)\\nl2 = np.random.normal(size=n)\\n\\nlatents = np.array([l1, l1, l2, l2]).T\\nX = latents + np.random.normal(size=4 * n).reshape((n, 4))\\nY = latents + np.random.normal(size=4 * n).reshape((n, 4))\\n\\nX_train = X[: n // 2]\\nY_train = Y[: n // 2]\\nX_test = X[n // 2 :]\\nY_test = Y[n // 2 :]\\n\\nprint(\"Corr(X)\")\\nprint(np.round(np.corrcoef(X.T), 2))\\nprint(\"Corr(Y)\")\\nprint(np.round(np.corrcoef(Y.T), 2))\\n\\n# %%\\n# Canonical (symmetric) PLS\\n# -------------------------\\n#\\n# Transform data\\n# ~~~~~~~~~~~~~~\\n\\nfrom sklearn.cross_decomposition import PLSCanonical\\n\\nplsca = PLSCanonical(n_components=2)\\nplsca.fit(X_train, Y_train)\\nX_train_r, Y_train_r = plsca.transform(X_train, Y_train)\\nX_test_r, Y_test_r = plsca.transform(X_test, Y_test)\\n\\n# %%\\n# Scatter plot of scores\\n# ~~~~~~~~~~~~~~~~~~~~~~\\n\\nimport matplotlib.pyplot as plt\\n\\n# On diagonal plot X vs Y scores on each components\\nplt.figure(figsize=(12, 8))\\nplt.subplot(221)\\nplt.scatter(X_train_r[:, 0], Y_train_r[:, 0], label=\"train\", marker=\"o\", s=25)\\nplt.scatter(X_test_r[:, 0], Y_test_r[:, 0], label=\"test\", marker=\"o\", s=25)\\nplt.xlabel(\"x scores\")\\nplt.ylabel(\"y scores\")\\nplt.title(\\n    \"Comp. 1: X vs Y (test corr = %.2f)\"\\n    % np.corrcoef(X_test_r[:, 0], Y_test_r[:, 0])[0, 1]\\n)\\nplt.xticks(())\\nplt.yticks(())\\nplt.legend(loc=\"best\")\\n\\nplt.subplot(224)\\nplt.scatter(X_train_r[:, 1], Y_train_r[:, 1], label=\"train\", marker=\"o\", s=25)\\nplt.scatter(X_test_r[:, 1], Y_test_r[:, 1], label=\"test\", marker=\"o\", s=25)\\nplt.xlabel(\"x scores\")\\nplt.ylabel(\"y scores\")\\nplt.title(\\n    \"Comp. 2: X vs Y (test corr = %.2f)\"\\n    % np.corrcoef(X_test_r[:, 1], Y_test_r[:, 1])[0, 1]\\n)\\nplt.xticks(())\\nplt.yticks(())\\nplt.legend(loc=\"best\")'), Document(metadata={'source': '/content/local_copy_repo/examples/cross_decomposition/plot_compare_cross_decomposition.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Off diagonal plot components 1 vs 2 for X and Y\\nplt.subplot(222)\\nplt.scatter(X_train_r[:, 0], X_train_r[:, 1], label=\"train\", marker=\"*\", s=50)\\nplt.scatter(X_test_r[:, 0], X_test_r[:, 1], label=\"test\", marker=\"*\", s=50)\\nplt.xlabel(\"X comp. 1\")\\nplt.ylabel(\"X comp. 2\")\\nplt.title(\\n    \"X comp. 1 vs X comp. 2 (test corr = %.2f)\"\\n    % np.corrcoef(X_test_r[:, 0], X_test_r[:, 1])[0, 1]\\n)\\nplt.legend(loc=\"best\")\\nplt.xticks(())\\nplt.yticks(())\\n\\nplt.subplot(223)\\nplt.scatter(Y_train_r[:, 0], Y_train_r[:, 1], label=\"train\", marker=\"*\", s=50)\\nplt.scatter(Y_test_r[:, 0], Y_test_r[:, 1], label=\"test\", marker=\"*\", s=50)\\nplt.xlabel(\"Y comp. 1\")\\nplt.ylabel(\"Y comp. 2\")\\nplt.title(\\n    \"Y comp. 1 vs Y comp. 2 , (test corr = %.2f)\"\\n    % np.corrcoef(Y_test_r[:, 0], Y_test_r[:, 1])[0, 1]\\n)\\nplt.legend(loc=\"best\")\\nplt.xticks(())\\nplt.yticks(())\\nplt.show()\\n\\n# %%\\n# PLS regression, with multivariate response, a.k.a. PLS2\\n# -------------------------------------------------------\\n\\nfrom sklearn.cross_decomposition import PLSRegression\\n\\nn = 1000\\nq = 3\\np = 10\\nX = np.random.normal(size=n * p).reshape((n, p))\\nB = np.array([[1, 2] + [0] * (p - 2)] * q).T\\n# each Yj = 1*X1 + 2*X2 + noize\\nY = np.dot(X, B) + np.random.normal(size=n * q).reshape((n, q)) + 5\\n\\npls2 = PLSRegression(n_components=3)\\npls2.fit(X, Y)\\nprint(\"True B (such that: Y = XB + Err)\")\\nprint(B)\\n# compare pls2.coef_ with B\\nprint(\"Estimated B\")\\nprint(np.round(pls2.coef_, 1))\\npls2.predict(X)\\n\\n# %%\\n# PLS regression, with univariate response, a.k.a. PLS1\\n# -----------------------------------------------------\\n\\nn = 1000\\np = 10\\nX = np.random.normal(size=n * p).reshape((n, p))\\ny = X[:, 0] + 2 * X[:, 1] + np.random.normal(size=n * 1) + 5\\npls1 = PLSRegression(n_components=3)\\npls1.fit(X, y)\\n# note that the number of components exceeds 1 (the dimension of y)\\nprint(\"Estimated betas\")\\nprint(np.round(pls1.coef_, 1))\\n\\n# %%\\n# CCA (PLS mode B with symmetric deflation)\\n# -----------------------------------------\\n\\nfrom sklearn.cross_decomposition import CCA\\n\\ncca = CCA(n_components=2)\\ncca.fit(X_train, Y_train)\\nX_train_r, Y_train_r = cca.transform(X_train, Y_train)\\nX_test_r, Y_test_r = cca.transform(X_test, Y_test)'), Document(metadata={'source': '/content/local_copy_repo/examples/cross_decomposition/plot_pcr_vs_pls.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================================================\\nPrincipal Component Regression vs Partial Least Squares Regression\\n==================================================================\\n\\nThis example compares `Principal Component Regression\\n<https://en.wikipedia.org/wiki/Principal_component_regression>`_ (PCR) and\\n`Partial Least Squares Regression\\n<https://en.wikipedia.org/wiki/Partial_least_squares_regression>`_ (PLS) on a\\ntoy dataset. Our goal is to illustrate how PLS can outperform PCR when the\\ntarget is strongly correlated with some directions in the data that have a\\nlow variance.\\n\\nPCR is a regressor composed of two steps: first,\\n:class:`~sklearn.decomposition.PCA` is applied to the training data, possibly\\nperforming dimensionality reduction; then, a regressor (e.g. a linear\\nregressor) is trained on the transformed samples. In\\n:class:`~sklearn.decomposition.PCA`, the transformation is purely\\nunsupervised, meaning that no information about the targets is used. As a\\nresult, PCR may perform poorly in some datasets where the target is strongly\\ncorrelated with *directions* that have low variance. Indeed, the\\ndimensionality reduction of PCA projects the data into a lower dimensional\\nspace where the variance of the projected data is greedily maximized along\\neach axis. Despite them having the most predictive power on the target, the\\ndirections with a lower variance will be dropped, and the final regressor\\nwill not be able to leverage them.\\n\\nPLS is both a transformer and a regressor, and it is quite similar to PCR: it\\nalso applies a dimensionality reduction to the samples before applying a\\nlinear regressor to the transformed data. The main difference with PCR is\\nthat the PLS transformation is supervised. Therefore, as we will see in this\\nexample, it does not suffer from the issue we just mentioned.\\n\\n\"\"\"\\n\\n# %%\\n# The data\\n# --------\\n#\\n# We start by creating a simple dataset with two features. Before we even dive\\n# into PCR and PLS, we fit a PCA estimator to display the two principal\\n# components of this dataset, i.e. the two directions that explain the most\\n# variance in the data.\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.decomposition import PCA\\n\\nrng = np.random.RandomState(0)\\nn_samples = 500\\ncov = [[3, 3], [3, 4]]\\nX = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)\\npca = PCA(n_components=2).fit(X)\\n\\n\\nplt.scatter(X[:, 0], X[:, 1], alpha=0.3, label=\"samples\")\\nfor i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\\n    comp = comp * var  # scale component by its variance explanation power\\n    plt.plot(\\n        [0, comp[0]],\\n        [0, comp[1]],\\n        label=f\"Component {i}\",\\n        linewidth=5,\\n        color=f\"C{i + 2}\",\\n    )\\nplt.gca().set(\\n    aspect=\"equal\",\\n    title=\"2-dimensional dataset with principal components\",\\n    xlabel=\"first feature\",\\n    ylabel=\"second feature\",\\n)\\nplt.legend()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cross_decomposition/plot_pcr_vs_pls.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# For the purpose of this example, we now define the target `y` such that it is\\n# strongly correlated with a direction that has a small variance. To this end,\\n# we will project `X` onto the second component, and add some noise to it.\\n\\ny = X.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\\n\\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\\n\\naxes[0].scatter(X.dot(pca.components_[0]), y, alpha=0.3)\\naxes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\\naxes[1].scatter(X.dot(pca.components_[1]), y, alpha=0.3)\\naxes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\\nplt.tight_layout()\\nplt.show()\\n\\n# %%\\n# Projection on one component and predictive power\\n# ------------------------------------------------\\n#\\n# We now create two regressors: PCR and PLS, and for our illustration purposes\\n# we set the number of components to 1. Before feeding the data to the PCA step\\n# of PCR, we first standardize it, as recommended by good practice. The PLS\\n# estimator has built-in scaling capabilities.\\n#\\n# For both models, we plot the projected data onto the first component against\\n# the target. In both cases, this projected data is what the regressors will\\n# use as training data.\\nfrom sklearn.cross_decomposition import PLSRegression\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\\n\\npcr = make_pipeline(StandardScaler(), PCA(n_components=1), LinearRegression())\\npcr.fit(X_train, y_train)\\npca = pcr.named_steps[\"pca\"]  # retrieve the PCA step of the pipeline\\n\\npls = PLSRegression(n_components=1)\\npls.fit(X_train, y_train)\\n\\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\\naxes[0].scatter(pca.transform(X_test), y_test, alpha=0.3, label=\"ground truth\")\\naxes[0].scatter(\\n    pca.transform(X_test), pcr.predict(X_test), alpha=0.3, label=\"predictions\"\\n)\\naxes[0].set(\\n    xlabel=\"Projected data onto first PCA component\", ylabel=\"y\", title=\"PCR / PCA\"\\n)\\naxes[0].legend()\\naxes[1].scatter(pls.transform(X_test), y_test, alpha=0.3, label=\"ground truth\")\\naxes[1].scatter(\\n    pls.transform(X_test), pls.predict(X_test), alpha=0.3, label=\"predictions\"\\n)\\naxes[1].set(xlabel=\"Projected data onto first PLS component\", ylabel=\"y\", title=\"PLS\")\\naxes[1].legend()\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/cross_decomposition/plot_pcr_vs_pls.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# As expected, the unsupervised PCA transformation of PCR has dropped the\\n# second component, i.e. the direction with the lowest variance, despite\\n# it being the most predictive direction. This is because PCA is a completely\\n# unsupervised transformation, and results in the projected data having a low\\n# predictive power on the target.\\n#\\n# On the other hand, the PLS regressor manages to capture the effect of the\\n# direction with the lowest variance, thanks to its use of target information\\n# during the transformation: it can recognize that this direction is actually\\n# the most predictive. We note that the first PLS component is negatively\\n# correlated with the target, which comes from the fact that the signs of\\n# eigenvectors are arbitrary.\\n#\\n# We also print the R-squared scores of both estimators, which further confirms\\n# that PLS is a better alternative than PCR in this case. A negative R-squared\\n# indicates that PCR performs worse than a regressor that would simply predict\\n# the mean of the target.\\n\\nprint(f\"PCR r-squared {pcr.score(X_test, y_test):.3f}\")\\nprint(f\"PLS r-squared {pls.score(X_test, y_test):.3f}\")\\n\\n# %%\\n# As a final remark, we note that PCR with 2 components performs as well as\\n# PLS: this is because in this case, PCR was able to leverage the second\\n# component which has the most preditive power on the target.\\n\\npca_2 = make_pipeline(PCA(n_components=2), LinearRegression())\\npca_2.fit(X_train, y_train)\\nprint(f\"PCR r-squared with 2 components {pca_2.score(X_test, y_test):.3f}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/multioutput/plot_classifier_chain_yeast.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================================\\nMultilabel classification using a classifier chain\\n==================================================\\nThis example shows how to use :class:`~sklearn.multioutput.ClassifierChain` to solve\\na multilabel classification problem.\\n\\nThe most naive strategy to solve such a task is to independently train a binary\\nclassifier on each label (i.e. each column of the target variable). At prediction\\ntime, the ensemble of binary classifiers is used to assemble multitask prediction.\\n\\nThis strategy does not allow to model relationship between different tasks. The\\n:class:`~sklearn.multioutput.ClassifierChain` is the meta-estimator (i.e. an estimator\\ntaking an inner estimator) that implements a more advanced strategy. The ensemble\\nof binary classifiers are used as a chain where the prediction of a classifier in the\\nchain is used as a feature for training the next classifier on a new label. Therefore,\\nthese additional features allow each chain to exploit correlations among labels.\\n\\nThe :ref:`Jaccard similarity <jaccard_similarity_score>` score for chain tends to be\\ngreater than that of the set independent base models.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Loading a dataset\\n# -----------------\\n# For this example, we use the `yeast\\n# <https://www.openml.org/d/40597>`_ dataset which contains\\n# 2,417 datapoints each with 103 features and 14 possible labels. Each\\n# data point has at least one label. As a baseline we first train a logistic\\n# regression classifier for each of the 14 labels. To evaluate the performance of\\n# these classifiers we predict on a held-out test set and calculate the\\n# Jaccard similarity for each sample.\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load a multi-label dataset from https://www.openml.org/d/40597\\nX, Y = fetch_openml(\"yeast\", version=4, return_X_y=True)\\nY = Y == \"TRUE\"\\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\\n\\n# %%\\n# Fit models\\n# ----------\\n# We fit :class:`~sklearn.linear_model.LogisticRegression` wrapped by\\n# :class:`~sklearn.multiclass.OneVsRestClassifier` and ensemble of multiple\\n# :class:`~sklearn.multioutput.ClassifierChain`.\\n#\\n# LogisticRegression wrapped by OneVsRestClassifier\\n# **************************************************\\n# Since by default :class:`~sklearn.linear_model.LogisticRegression` can\\'t\\n# handle data with multiple targets, we need to use\\n# :class:`~sklearn.multiclass.OneVsRestClassifier`.\\n# After fitting the model we calculate Jaccard similarity.\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import jaccard_score\\nfrom sklearn.multiclass import OneVsRestClassifier'), Document(metadata={'source': '/content/local_copy_repo/examples/multioutput/plot_classifier_chain_yeast.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import jaccard_score\\nfrom sklearn.multiclass import OneVsRestClassifier\\n\\nbase_lr = LogisticRegression()\\novr = OneVsRestClassifier(base_lr)\\novr.fit(X_train, Y_train)\\nY_pred_ovr = ovr.predict(X_test)\\novr_jaccard_score = jaccard_score(Y_test, Y_pred_ovr, average=\"samples\")\\n\\n# %%\\n# Chain of binary classifiers\\n# ***************************\\n# Because the models in each chain are arranged randomly there is significant\\n# variation in performance among the chains. Presumably there is an optimal\\n# ordering of the classes in a chain that will yield the best performance.\\n# However, we do not know that ordering a priori. Instead, we can build a\\n# voting ensemble of classifier chains by averaging the binary predictions of\\n# the chains and apply a threshold of 0.5. The Jaccard similarity score of the\\n# ensemble is greater than that of the independent models and tends to exceed\\n# the score of each chain in the ensemble (although this is not guaranteed\\n# with randomly ordered chains).\\n\\nfrom sklearn.multioutput import ClassifierChain\\n\\nchains = [ClassifierChain(base_lr, order=\"random\", random_state=i) for i in range(10)]\\nfor chain in chains:\\n    chain.fit(X_train, Y_train)\\n\\nY_pred_chains = np.array([chain.predict_proba(X_test) for chain in chains])\\nchain_jaccard_scores = [\\n    jaccard_score(Y_test, Y_pred_chain >= 0.5, average=\"samples\")\\n    for Y_pred_chain in Y_pred_chains\\n]\\n\\nY_pred_ensemble = Y_pred_chains.mean(axis=0)\\nensemble_jaccard_score = jaccard_score(\\n    Y_test, Y_pred_ensemble >= 0.5, average=\"samples\"\\n)\\n\\n# %%\\n# Plot results\\n# ------------\\n# Plot the Jaccard similarity scores for the independent model, each of the\\n# chains, and the ensemble (note that the vertical axis on this plot does\\n# not begin at 0).\\n\\nmodel_scores = [ovr_jaccard_score] + chain_jaccard_scores + [ensemble_jaccard_score]\\n\\nmodel_names = (\\n    \"Independent\",\\n    \"Chain 1\",\\n    \"Chain 2\",\\n    \"Chain 3\",\\n    \"Chain 4\",\\n    \"Chain 5\",\\n    \"Chain 6\",\\n    \"Chain 7\",\\n    \"Chain 8\",\\n    \"Chain 9\",\\n    \"Chain 10\",\\n    \"Ensemble\",\\n)\\n\\nx_pos = np.arange(len(model_names))\\n\\nfig, ax = plt.subplots(figsize=(7, 4))\\nax.grid(True)\\nax.set_title(\"Classifier Chain Ensemble Performance Comparison\")\\nax.set_xticks(x_pos)\\nax.set_xticklabels(model_names, rotation=\"vertical\")\\nax.set_ylabel(\"Jaccard Similarity Score\")\\nax.set_ylim([min(model_scores) * 0.9, max(model_scores) * 1.1])\\ncolors = [\"r\"] + [\"b\"] * len(chain_jaccard_scores) + [\"g\"]\\nax.bar(x_pos, model_scores, alpha=0.5, color=colors)\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/multioutput/plot_classifier_chain_yeast.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"# %%\\n# Results interpretation\\n# ----------------------\\n# There are three main takeaways from this plot:\\n#\\n# - Independent model wrapped by :class:`~sklearn.multiclass.OneVsRestClassifier`\\n#   performs worse than the ensemble of classifier chains and some of individual chains.\\n#   This is caused by the fact that the logistic regression doesn't model relationship\\n#   between the labels.\\n# - :class:`~sklearn.multioutput.ClassifierChain` takes advantage of correlation\\n#   among labels but due to random nature of labels ordering, it could yield worse\\n#   result than an independent model.\\n# - An ensemble of chains performs better because it not only captures relationship\\n#   between labels but also does not make strong assumptions about their correct order.\"), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_classification_probability.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================\\nPlot classification probability\\n===============================\\n\\nPlot the classification probability for different classifiers. We use a 3 class\\ndataset, and we classify it with a Support Vector classifier, L1 and L2\\npenalized logistic regression (multinomial multiclass), a One-Vs-Rest version with\\nlogistic regression, and Gaussian process classification.\\n\\nLinear SVC is not a probabilistic classifier by default but it has a built-in\\ncalibration option enabled in this example (`probability=True`).\\n\\nThe logistic regression with One-Vs-Rest is not a multiclass classifier out of\\nthe box. As a result it has more trouble in separating class 2 and 3 than the\\nother estimators.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib import cm\\n\\nfrom sklearn import datasets\\nfrom sklearn.gaussian_process import GaussianProcessClassifier\\nfrom sklearn.gaussian_process.kernels import RBF\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.multiclass import OneVsRestClassifier\\nfrom sklearn.svm import SVC\\n\\niris = datasets.load_iris()\\nX = iris.data[:, 0:2]  # we only take the first two features for visualization\\ny = iris.target\\n\\nn_features = X.shape[1]\\n\\nC = 10\\nkernel = 1.0 * RBF([1.0, 1.0])  # for GPC\\n\\n# Create different classifiers.\\nclassifiers = {\\n    \"L1 logistic\": LogisticRegression(C=C, penalty=\"l1\", solver=\"saga\", max_iter=10000),\\n    \"L2 logistic (Multinomial)\": LogisticRegression(\\n        C=C, penalty=\"l2\", solver=\"saga\", max_iter=10000\\n    ),\\n    \"L2 logistic (OvR)\": OneVsRestClassifier(\\n        LogisticRegression(C=C, penalty=\"l2\", solver=\"saga\", max_iter=10000)\\n    ),\\n    \"Linear SVC\": SVC(kernel=\"linear\", C=C, probability=True, random_state=0),\\n    \"GPC\": GaussianProcessClassifier(kernel),\\n}\\n\\nn_classifiers = len(classifiers)'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_classification_probability.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='n_classifiers = len(classifiers)\\n\\nfig, axes = plt.subplots(\\n    nrows=n_classifiers,\\n    ncols=len(iris.target_names),\\n    figsize=(3 * 2, n_classifiers * 2),\\n)\\nfor classifier_idx, (name, classifier) in enumerate(classifiers.items()):\\n    y_pred = classifier.fit(X, y).predict(X)\\n    accuracy = accuracy_score(y, y_pred)\\n    print(f\"Accuracy (train) for {name}: {accuracy:0.1%}\")\\n    for label in np.unique(y):\\n        # plot the probability estimate provided by the classifier\\n        disp = DecisionBoundaryDisplay.from_estimator(\\n            classifier,\\n            X,\\n            response_method=\"predict_proba\",\\n            class_of_interest=label,\\n            ax=axes[classifier_idx, label],\\n            vmin=0,\\n            vmax=1,\\n        )\\n        axes[classifier_idx, label].set_title(f\"Class {label}\")\\n        # plot data predicted to belong to given class\\n        mask_y_pred = y_pred == label\\n        axes[classifier_idx, label].scatter(\\n            X[mask_y_pred, 0], X[mask_y_pred, 1], marker=\"o\", c=\"w\", edgecolor=\"k\"\\n        )\\n        axes[classifier_idx, label].set(xticks=(), yticks=())\\n    axes[classifier_idx, 0].set_ylabel(name)\\n\\nax = plt.axes([0.15, 0.04, 0.7, 0.02])\\nplt.title(\"Probability\")\\n_ = plt.colorbar(\\n    cm.ScalarMappable(norm=None, cmap=\"viridis\"), cax=ax, orientation=\"horizontal\"\\n)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_lda.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def generate_data(n_samples, n_features):\\n    \"\"\"Generate random blob-ish data with noisy features.\\n\\n    This returns an array of input data with shape `(n_samples, n_features)`\\n    and an array of `n_samples` target labels.\\n\\n    Only one feature contains discriminative information, the other features\\n    contain only noise.\\n    \"\"\"\\n    X, y = make_blobs(n_samples=n_samples, n_features=1, centers=[[-2], [2]])\\n\\n    # add non-discriminative features\\n    if n_features > 1:\\n        X = np.hstack([X, np.random.randn(n_samples, n_features - 1)])\\n    return X, y'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_lda.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================================================\\nNormal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification\\n===========================================================================\\n\\nThis example illustrates how the Ledoit-Wolf and Oracle Approximating\\nShrinkage (OAS) estimators of covariance can improve classification.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.covariance import OAS\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\n\\nn_train = 20  # samples for training\\nn_test = 200  # samples for testing\\nn_averages = 50  # how often to repeat classification\\nn_features_max = 75  # maximum number of features\\nstep = 4  # step size for the calculation\\n\\n\\n# Code for: def generate_data(n_samples, n_features):\\n\\n\\nacc_clf1, acc_clf2, acc_clf3 = [], [], []\\nn_features_range = range(1, n_features_max + 1, step)\\nfor n_features in n_features_range:\\n    score_clf1, score_clf2, score_clf3 = 0, 0, 0\\n    for _ in range(n_averages):\\n        X, y = generate_data(n_train, n_features)\\n\\n        clf1 = LinearDiscriminantAnalysis(solver=\"lsqr\", shrinkage=None).fit(X, y)\\n        clf2 = LinearDiscriminantAnalysis(solver=\"lsqr\", shrinkage=\"auto\").fit(X, y)\\n        oa = OAS(store_precision=False, assume_centered=False)\\n        clf3 = LinearDiscriminantAnalysis(solver=\"lsqr\", covariance_estimator=oa).fit(\\n            X, y\\n        )\\n\\n        X, y = generate_data(n_test, n_features)\\n        score_clf1 += clf1.score(X, y)\\n        score_clf2 += clf2.score(X, y)\\n        score_clf3 += clf3.score(X, y)\\n\\n    acc_clf1.append(score_clf1 / n_averages)\\n    acc_clf2.append(score_clf2 / n_averages)\\n    acc_clf3.append(score_clf3 / n_averages)\\n\\nfeatures_samples_ratio = np.array(n_features_range) / n_train\\n\\nplt.plot(\\n    features_samples_ratio,\\n    acc_clf1,\\n    linewidth=2,\\n    label=\"LDA\",\\n    color=\"gold\",\\n    linestyle=\"solid\",\\n)\\nplt.plot(\\n    features_samples_ratio,\\n    acc_clf2,\\n    linewidth=2,\\n    label=\"LDA with Ledoit Wolf\",\\n    color=\"navy\",\\n    linestyle=\"dashed\",\\n)\\nplt.plot(\\n    features_samples_ratio,\\n    acc_clf3,\\n    linewidth=2,\\n    label=\"LDA with OAS\",\\n    color=\"red\",\\n    linestyle=\"dotted\",\\n)\\n\\nplt.xlabel(\"n_features / n_samples\")\\nplt.ylabel(\"Classification accuracy\")\\n\\nplt.legend(loc=\"lower left\")\\nplt.ylim((0.65, 1.0))\\nplt.suptitle(\\n    \"LDA (Linear Discriminant Analysis) vs. \"\\n    + \"\\\\n\"\\n    + \"LDA with Ledoit Wolf vs. \"\\n    + \"\\\\n\"\\n    + \"LDA with OAS (1 discriminative feature)\"\\n)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_digits_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================\\nRecognizing hand-written digits\\n================================\\n\\nThis example shows how scikit-learn can be used to recognize images of\\nhand-written digits, from 0-9.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# Standard scientific Python imports\\nimport matplotlib.pyplot as plt\\n\\n# Import datasets, classifiers and performance metrics\\nfrom sklearn import datasets, metrics, svm\\nfrom sklearn.model_selection import train_test_split\\n\\n###############################################################################\\n# Digits dataset\\n# --------------\\n#\\n# The digits dataset consists of 8x8\\n# pixel images of digits. The ``images`` attribute of the dataset stores\\n# 8x8 arrays of grayscale values for each image. We will use these arrays to\\n# visualize the first 4 images. The ``target`` attribute of the dataset stores\\n# the digit each image represents and this is included in the title of the 4\\n# plots below.\\n#\\n# Note: if we were working from image files (e.g., \\'png\\' files), we would load\\n# them using :func:`matplotlib.pyplot.imread`.\\n\\ndigits = datasets.load_digits()\\n\\n_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\\nfor ax, image, label in zip(axes, digits.images, digits.target):\\n    ax.set_axis_off()\\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\\n    ax.set_title(\"Training: %i\" % label)\\n\\n###############################################################################\\n# Classification\\n# --------------\\n#\\n# To apply a classifier on this data, we need to flatten the images, turning\\n# each 2-D array of grayscale values from shape ``(8, 8)`` into shape\\n# ``(64,)``. Subsequently, the entire dataset will be of shape\\n# ``(n_samples, n_features)``, where ``n_samples`` is the number of images and\\n# ``n_features`` is the total number of pixels in each image.\\n#\\n# We can then split the data into train and test subsets and fit a support\\n# vector classifier on the train samples. The fitted classifier can\\n# subsequently be used to predict the value of the digit for the samples\\n# in the test subset.\\n\\n# flatten the images\\nn_samples = len(digits.images)\\ndata = digits.images.reshape((n_samples, -1))\\n\\n# Create a classifier: a support vector classifier\\nclf = svm.SVC(gamma=0.001)\\n\\n# Split data into 50% train and 50% test subsets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    data, digits.target, test_size=0.5, shuffle=False\\n)\\n\\n# Learn the digits on the train subset\\nclf.fit(X_train, y_train)\\n\\n# Predict the value of the digit on the test subset\\npredicted = clf.predict(X_test)\\n\\n###############################################################################\\n# Below we visualize the first 4 test samples and show their predicted\\n# digit value in the title.'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_digits_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Predict the value of the digit on the test subset\\npredicted = clf.predict(X_test)\\n\\n###############################################################################\\n# Below we visualize the first 4 test samples and show their predicted\\n# digit value in the title.\\n\\n_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\\nfor ax, image, prediction in zip(axes, X_test, predicted):\\n    ax.set_axis_off()\\n    image = image.reshape(8, 8)\\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\\n    ax.set_title(f\"Prediction: {prediction}\")\\n\\n###############################################################################\\n# :func:`~sklearn.metrics.classification_report` builds a text report showing\\n# the main classification metrics.\\n\\nprint(\\n    f\"Classification report for classifier {clf}:\\\\n\"\\n    f\"{metrics.classification_report(y_test, predicted)}\\\\n\"\\n)\\n\\n###############################################################################\\n# We can also plot a :ref:`confusion matrix <confusion_matrix>` of the\\n# true digit values and the predicted digit values.\\n\\ndisp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted)\\ndisp.figure_.suptitle(\"Confusion Matrix\")\\nprint(f\"Confusion matrix:\\\\n{disp.confusion_matrix}\")\\n\\nplt.show()\\n\\n###############################################################################\\n# If the results from evaluating a classifier are stored in the form of a\\n# :ref:`confusion matrix <confusion_matrix>` and not in terms of `y_true` and\\n# `y_pred`, one can still build a :func:`~sklearn.metrics.classification_report`\\n# as follows:\\n\\n\\n# The ground truth and predicted lists\\ny_true = []\\ny_pred = []\\ncm = disp.confusion_matrix\\n\\n# For each cell in the confusion matrix, add the corresponding ground truths\\n# and predictions to the lists\\nfor gt in range(len(cm)):\\n    for pred in range(len(cm)):\\n        y_true += [gt] * cm[gt][pred]\\n        y_pred += [pred] * cm[gt][pred]\\n\\nprint(\\n    \"Classification report rebuilt from confusion matrix:\\\\n\"\\n    f\"{metrics.classification_report(y_true, y_pred)}\\\\n\"\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_lda_qda.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def make_data(n_samples, n_features, cov_class_1, cov_class_2, seed=0):\\n    rng = np.random.RandomState(seed)\\n    X = np.concatenate(\\n        [\\n            rng.randn(n_samples, n_features) @ cov_class_1,\\n            rng.randn(n_samples, n_features) @ cov_class_2 + np.array([1, 1]),\\n        ]\\n    )\\n    y = np.concatenate([np.zeros(n_samples), np.ones(n_samples)])\\n    return X, y'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_lda_qda.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_ellipse(mean, cov, color, ax):\\n    v, w = np.linalg.eigh(cov)\\n    u = w[0] / np.linalg.norm(w[0])\\n    angle = np.arctan(u[1] / u[0])\\n    angle = 180 * angle / np.pi  # convert to degrees\\n    # filled Gaussian at 2 standard deviation\\n    ell = mpl.patches.Ellipse(\\n        mean,\\n        2 * v[0] ** 0.5,\\n        2 * v[1] ** 0.5,\\n        angle=180 + angle,\\n        facecolor=color,\\n        edgecolor=\"black\",\\n        linewidth=2,\\n    )\\n    ell.set_clip_box(ax.bbox)\\n    ell.set_alpha(0.4)\\n    ax.add_artist(ell)'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_lda_qda.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_result(estimator, X, y, ax):\\n    cmap = colors.ListedColormap([\"tab:red\", \"tab:blue\"])\\n    DecisionBoundaryDisplay.from_estimator(\\n        estimator,\\n        X,\\n        response_method=\"predict_proba\",\\n        plot_method=\"pcolormesh\",\\n        ax=ax,\\n        cmap=\"RdBu\",\\n        alpha=0.3,\\n    )\\n    DecisionBoundaryDisplay.from_estimator(\\n        estimator,\\n        X,\\n        response_method=\"predict_proba\",\\n        plot_method=\"contour\",\\n        ax=ax,\\n        alpha=1.0,\\n        levels=[0.5],\\n    )\\n    y_pred = estimator.predict(X)\\n    X_right, y_right = X[y == y_pred], y[y == y_pred]\\n    X_wrong, y_wrong = X[y != y_pred], y[y != y_pred]\\n    ax.scatter(X_right[:, 0], X_right[:, 1], c=y_right, s=20, cmap=cmap, alpha=0.5)\\n    ax.scatter(\\n        X_wrong[:, 0],\\n        X_wrong[:, 1],\\n        c=y_wrong,\\n        s=30,\\n        cmap=cmap,\\n        alpha=0.9,\\n        marker=\"x\",\\n    )\\n    ax.scatter(\\n        estimator.means_[:, 0],\\n        estimator.means_[:, 1],\\n        c=\"yellow\",\\n        s=200,\\n        marker=\"*\",\\n        edgecolor=\"black\",\\n    )\\n\\n    if isinstance(estimator, LinearDiscriminantAnalysis):\\n        covariance = [estimator.covariance_] * 2\\n    else:\\n        covariance = estimator.covariance_\\n    plot_ellipse(estimator.means_[0], covariance[0], \"tab:red\", ax)\\n    plot_ellipse(estimator.means_[1], covariance[1], \"tab:blue\", ax)\\n\\n    ax.set_box_aspect(1)\\n    ax.spines[\"top\"].set_visible(False)\\n    ax.spines[\"bottom\"].set_visible(False)\\n    ax.spines[\"left\"].set_visible(False)\\n    ax.spines[\"right\"].set_visible(False)\\n    ax.set(xticks=[], yticks=[])'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_lda_qda.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================================================\\nLinear and Quadratic Discriminant Analysis with covariance ellipsoid\\n====================================================================\\n\\nThis example plots the covariance ellipsoids of each class and the decision boundary\\nlearned by :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis` (LDA) and\\n:class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis` (QDA). The\\nellipsoids display the double standard deviation for each class. With LDA, the standard\\ndeviation is the same for all the classes, while each class has its own standard\\ndeviation with QDA.\\n\"\"\"\\n\\n# %%\\n# Data generation\\n# ---------------\\n#\\n# First, we define a function to generate synthetic data. It creates two blobs centered\\n# at `(0, 0)` and `(1, 1)`. Each blob is assigned a specific class. The dispersion of\\n# the blob is controlled by the parameters `cov_class_1` and `cov_class_2`, that are the\\n# covariance matrices used when generating the samples from the Gaussian distributions.\\nimport numpy as np\\n\\n\\n# Code for: def make_data(n_samples, n_features, cov_class_1, cov_class_2, seed=0):\\n\\n\\n# %%\\n# We generate three datasets. In the first dataset, the two classes share the same\\n# covariance matrix, and this covariance matrix has the specificity of being spherical\\n# (isotropic). The second dataset is similar to the first one but does not enforce the\\n# covariance to be spherical. Finally, the third dataset has a non-spherical covariance\\n# matrix for each class.\\ncovariance = np.array([[1, 0], [0, 1]])\\nX_isotropic_covariance, y_isotropic_covariance = make_data(\\n    n_samples=1_000,\\n    n_features=2,\\n    cov_class_1=covariance,\\n    cov_class_2=covariance,\\n    seed=0,\\n)\\ncovariance = np.array([[0.0, -0.23], [0.83, 0.23]])\\nX_shared_covariance, y_shared_covariance = make_data(\\n    n_samples=300,\\n    n_features=2,\\n    cov_class_1=covariance,\\n    cov_class_2=covariance,\\n    seed=0,\\n)\\ncov_class_1 = np.array([[0.0, -1.0], [2.5, 0.7]]) * 2.0\\ncov_class_2 = cov_class_1.T\\nX_different_covariance, y_different_covariance = make_data(\\n    n_samples=300,\\n    n_features=2,\\n    cov_class_1=cov_class_1,\\n    cov_class_2=cov_class_2,\\n    seed=0,\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_lda_qda.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Plotting Functions\\n# ------------------\\n#\\n# The code below is used to plot several pieces of information from the estimators used,\\n# i.e., :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis` (LDA) and\\n# :class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis` (QDA). The\\n# displayed information includes:\\n#\\n# - the decision boundary based on the probability estimate of the estimator;\\n# - a scatter plot with circles representing the well-classified samples;\\n# - a scatter plot with crosses representing the misclassified samples;\\n# - the mean of each class, estimated by the estimator, marked with a star;\\n# - the estimated covariance represented by an ellipse at 2 standard deviations from the\\n#   mean.\\nimport matplotlib as mpl\\nfrom matplotlib import colors\\n\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\n\\n# Code for: def plot_ellipse(mean, cov, color, ax):\\n\\n\\n# Code for: def plot_result(estimator, X, y, ax):\\n\\n\\n# %%\\n# Comparison of LDA and QDA\\n# -------------------------\\n#\\n# We compare the two estimators LDA and QDA on all three datasets.\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.discriminant_analysis import (\\n    LinearDiscriminantAnalysis,\\n    QuadraticDiscriminantAnalysis,\\n)\\n\\nfig, axs = plt.subplots(nrows=3, ncols=2, sharex=\"row\", sharey=\"row\", figsize=(8, 12))\\n\\nlda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\\nqda = QuadraticDiscriminantAnalysis(store_covariance=True)\\n\\nfor ax_row, X, y in zip(\\n    axs,\\n    (X_isotropic_covariance, X_shared_covariance, X_different_covariance),\\n    (y_isotropic_covariance, y_shared_covariance, y_different_covariance),\\n):\\n    lda.fit(X, y)\\n    plot_result(lda, X, y, ax_row[0])\\n    qda.fit(X, y)\\n    plot_result(qda, X, y, ax_row[1])\\n\\naxs[0, 0].set_title(\"Linear Discriminant Analysis\")\\naxs[0, 0].set_ylabel(\"Data with fixed and spherical covariance\")\\naxs[1, 0].set_ylabel(\"Data with fixed covariance\")\\naxs[0, 1].set_title(\"Quadratic Discriminant Analysis\")\\naxs[2, 0].set_ylabel(\"Data with varying covariances\")\\nfig.suptitle(\\n    \"Linear Discriminant Analysis vs Quadratic Discriminant Analysis\",\\n    y=0.94,\\n    fontsize=15,\\n)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_lda_qda.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The first important thing to notice is that LDA and QDA are equivalent for the\\n# first and second datasets. Indeed, the major difference is that LDA assumes\\n# that the covariance matrix of each class is equal, while QDA estimates a\\n# covariance matrix per class. Since in these cases the data generative process\\n# has the same covariance matrix for both classes, QDA estimates two covariance\\n# matrices that are (almost) equal and therefore equivalent to the covariance\\n# matrix estimated by LDA.\\n#\\n# In the first dataset the covariance matrix used to generate the dataset is\\n# spherical, which results in a discriminant boundary that aligns with the\\n# perpendicular bisector between the two means. This is no longer the case for\\n# the second dataset. The discriminant boundary only passes through the middle\\n# of the two means.\\n#\\n# Finally, in the third dataset, we observe the real difference between LDA and\\n# QDA. QDA fits two covariance matrices and provides a non-linear discriminant\\n# boundary, whereas LDA underfits since it assumes that both classes share a\\n# single covariance matrix.'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_classifier_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================\\nClassifier comparison\\n=====================\\n\\nA comparison of several classifiers in scikit-learn on synthetic datasets.\\nThe point of this example is to illustrate the nature of decision boundaries\\nof different classifiers.\\nThis should be taken with a grain of salt, as the intuition conveyed by\\nthese examples does not necessarily carry over to real datasets.\\n\\nParticularly in high-dimensional spaces, data can more easily be separated\\nlinearly and the simplicity of classifiers such as naive Bayes and linear SVMs\\nmight lead to better generalization than is achieved by other classifiers.\\n\\nThe plots show training points in solid colors and testing points\\nsemi-transparent. The lower right shows the classification accuracy on the test\\nset.\\n\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n#              Andreas M√ºller\\n# Modified for documentation by Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib.colors import ListedColormap\\n\\nfrom sklearn.datasets import make_circles, make_classification, make_moons\\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\\nfrom sklearn.gaussian_process import GaussianProcessClassifier\\nfrom sklearn.gaussian_process.kernels import RBF\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\nnames = [\\n    \"Nearest Neighbors\",\\n    \"Linear SVM\",\\n    \"RBF SVM\",\\n    \"Gaussian Process\",\\n    \"Decision Tree\",\\n    \"Random Forest\",\\n    \"Neural Net\",\\n    \"AdaBoost\",\\n    \"Naive Bayes\",\\n    \"QDA\",\\n]\\n\\nclassifiers = [\\n    KNeighborsClassifier(3),\\n    SVC(kernel=\"linear\", C=0.025, random_state=42),\\n    SVC(gamma=2, C=1, random_state=42),\\n    GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\\n    DecisionTreeClassifier(max_depth=5, random_state=42),\\n    RandomForestClassifier(\\n        max_depth=5, n_estimators=10, max_features=1, random_state=42\\n    ),\\n    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\\n    AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\\n    GaussianNB(),\\n    QuadraticDiscriminantAnalysis(),\\n]\\n\\nX, y = make_classification(\\n    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\\n)\\nrng = np.random.RandomState(2)\\nX += 2 * rng.uniform(size=X.shape)\\nlinearly_separable = (X, y)\\n\\ndatasets = [\\n    make_moons(noise=0.3, random_state=0),\\n    make_circles(noise=0.2, factor=0.5, random_state=1),\\n    linearly_separable,\\n]'), Document(metadata={'source': '/content/local_copy_repo/examples/classification/plot_classifier_comparison.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='datasets = [\\n    make_moons(noise=0.3, random_state=0),\\n    make_circles(noise=0.2, factor=0.5, random_state=1),\\n    linearly_separable,\\n]\\n\\nfigure = plt.figure(figsize=(27, 9))\\ni = 1\\n# iterate over datasets\\nfor ds_cnt, ds in enumerate(datasets):\\n    # preprocess dataset, split into training and test part\\n    X, y = ds\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=0.4, random_state=42\\n    )\\n\\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n\\n    # just plot the dataset first\\n    cm = plt.cm.RdBu\\n    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\\n    if ds_cnt == 0:\\n        ax.set_title(\"Input data\")\\n    # Plot the training points\\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\\n    # Plot the testing points\\n    ax.scatter(\\n        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\\n    )\\n    ax.set_xlim(x_min, x_max)\\n    ax.set_ylim(y_min, y_max)\\n    ax.set_xticks(())\\n    ax.set_yticks(())\\n    i += 1\\n\\n    # iterate over classifiers\\n    for name, clf in zip(names, classifiers):\\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\\n\\n        clf = make_pipeline(StandardScaler(), clf)\\n        clf.fit(X_train, y_train)\\n        score = clf.score(X_test, y_test)\\n        DecisionBoundaryDisplay.from_estimator(\\n            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\\n        )\\n\\n        # Plot the training points\\n        ax.scatter(\\n            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\\n        )\\n        # Plot the testing points\\n        ax.scatter(\\n            X_test[:, 0],\\n            X_test[:, 1],\\n            c=y_test,\\n            cmap=cm_bright,\\n            edgecolors=\"k\",\\n            alpha=0.6,\\n        )\\n\\n        ax.set_xlim(x_min, x_max)\\n        ax.set_ylim(y_min, y_max)\\n        ax.set_xticks(())\\n        ax.set_yticks(())\\n        if ds_cnt == 0:\\n            ax.set_title(name)\\n        ax.text(\\n            x_max - 0.3,\\n            y_min + 0.3,\\n            (\"%.2f\" % score).lstrip(\"0\"),\\n            size=15,\\n            horizontalalignment=\"right\",\\n        )\\n        i += 1\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_robust_vs_empirical_covariance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='r\"\"\"\\n=======================================\\nRobust vs Empirical covariance estimate\\n=======================================\\n\\nThe usual covariance maximum likelihood estimate is very sensitive to the\\npresence of outliers in the data set. In such a case, it would be better to\\nuse a robust estimator of covariance to guarantee that the estimation is\\nresistant to \"erroneous\" observations in the data set. [1]_, [2]_\\n\\nMinimum Covariance Determinant Estimator\\n----------------------------------------\\nThe Minimum Covariance Determinant estimator is a robust, high-breakdown point\\n(i.e. it can be used to estimate the covariance matrix of highly contaminated\\ndatasets, up to\\n:math:`\\\\frac{n_\\\\text{samples} - n_\\\\text{features}-1}{2}` outliers) estimator of\\ncovariance. The idea is to find\\n:math:`\\\\frac{n_\\\\text{samples} + n_\\\\text{features}+1}{2}`\\nobservations whose empirical covariance has the smallest determinant, yielding\\na \"pure\" subset of observations from which to compute standards estimates of\\nlocation and covariance. After a correction step aiming at compensating the\\nfact that the estimates were learned from only a portion of the initial data,\\nwe end up with robust estimates of the data set location and covariance.\\n\\nThe Minimum Covariance Determinant estimator (MCD) has been introduced by\\nP.J.Rousseuw in [3]_.\\n\\nEvaluation\\n----------\\nIn this example, we compare the estimation errors that are made when using\\nvarious types of location and covariance estimates on contaminated Gaussian\\ndistributed data sets:\\n\\n- The mean and the empirical covariance of the full dataset, which break\\n  down as soon as there are outliers in the data set\\n- The robust MCD, that has a low error provided\\n  :math:`n_\\\\text{samples} > 5n_\\\\text{features}`\\n- The mean and the empirical covariance of the observations that are known\\n  to be good ones. This can be considered as a \"perfect\" MCD estimation,\\n  so one can trust our implementation by comparing to this case.\\n\\n\\nReferences\\n----------\\n.. [1] Johanna Hardin, David M Rocke. The distribution of robust distances.\\n    Journal of Computational and Graphical Statistics. December 1, 2005,\\n    14(4): 928-946.\\n.. [2] Zoubir A., Koivunen V., Chakhchoukh Y. and Muma M. (2012). Robust\\n    estimation in signal processing: A tutorial-style treatment of\\n    fundamental concepts. IEEE Signal Processing Magazine 29(4), 61-80.\\n.. [3] P. J. Rousseeuw. Least median of squares regression. Journal of American\\n    Statistical Ass., 79:871, 1984.\\n\\n\"\"\"\\n\\nimport matplotlib.font_manager\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\\n\\n# example settings\\nn_samples = 80\\nn_features = 5\\nrepeat = 10\\n\\nrange_n_outliers = np.concatenate(\\n    (\\n        np.linspace(0, n_samples / 8, 5),\\n        np.linspace(n_samples / 8, n_samples / 2, 5)[1:-1],\\n    )\\n).astype(int)'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_robust_vs_empirical_covariance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.covariance import EmpiricalCovariance, MinCovDet\\n\\n# example settings\\nn_samples = 80\\nn_features = 5\\nrepeat = 10\\n\\nrange_n_outliers = np.concatenate(\\n    (\\n        np.linspace(0, n_samples / 8, 5),\\n        np.linspace(n_samples / 8, n_samples / 2, 5)[1:-1],\\n    )\\n).astype(int)\\n\\n# definition of arrays to store results\\nerr_loc_mcd = np.zeros((range_n_outliers.size, repeat))\\nerr_cov_mcd = np.zeros((range_n_outliers.size, repeat))\\nerr_loc_emp_full = np.zeros((range_n_outliers.size, repeat))\\nerr_cov_emp_full = np.zeros((range_n_outliers.size, repeat))\\nerr_loc_emp_pure = np.zeros((range_n_outliers.size, repeat))\\nerr_cov_emp_pure = np.zeros((range_n_outliers.size, repeat))\\n\\n# computation\\nfor i, n_outliers in enumerate(range_n_outliers):\\n    for j in range(repeat):\\n        rng = np.random.RandomState(i * j)\\n\\n        # generate data\\n        X = rng.randn(n_samples, n_features)\\n        # add some outliers\\n        outliers_index = rng.permutation(n_samples)[:n_outliers]\\n        outliers_offset = 10.0 * (\\n            np.random.randint(2, size=(n_outliers, n_features)) - 0.5\\n        )\\n        X[outliers_index] += outliers_offset\\n        inliers_mask = np.ones(n_samples).astype(bool)\\n        inliers_mask[outliers_index] = False\\n\\n        # fit a Minimum Covariance Determinant (MCD) robust estimator to data\\n        mcd = MinCovDet().fit(X)\\n        # compare raw robust estimates with the true location and covariance\\n        err_loc_mcd[i, j] = np.sum(mcd.location_**2)\\n        err_cov_mcd[i, j] = mcd.error_norm(np.eye(n_features))\\n\\n        # compare estimators learned from the full data set with true\\n        # parameters\\n        err_loc_emp_full[i, j] = np.sum(X.mean(0) ** 2)\\n        err_cov_emp_full[i, j] = (\\n            EmpiricalCovariance().fit(X).error_norm(np.eye(n_features))\\n        )\\n\\n        # compare with an empirical covariance learned from a pure data set\\n        # (i.e. \"perfect\" mcd)\\n        pure_X = X[inliers_mask]\\n        pure_location = pure_X.mean(0)\\n        pure_emp_cov = EmpiricalCovariance().fit(pure_X)\\n        err_loc_emp_pure[i, j] = np.sum(pure_location**2)\\n        err_cov_emp_pure[i, j] = pure_emp_cov.error_norm(np.eye(n_features))\\n\\n# Display results\\nfont_prop = matplotlib.font_manager.FontProperties(size=11)\\nplt.subplot(2, 1, 1)\\nlw = 2\\nplt.errorbar(\\n    range_n_outliers,\\n    err_loc_mcd.mean(1),\\n    yerr=err_loc_mcd.std(1) / np.sqrt(repeat),\\n    label=\"Robust location\",\\n    lw=lw,\\n    color=\"m\",\\n)\\nplt.errorbar(\\n    range_n_outliers,\\n    err_loc_emp_full.mean(1),\\n    yerr=err_loc_emp_full.std(1) / np.sqrt(repeat),\\n    label=\"Full data set mean\",\\n    lw=lw,\\n    color=\"green\",\\n)\\nplt.errorbar(\\n    range_n_outliers,\\n    err_loc_emp_pure.mean(1),\\n    yerr=err_loc_emp_pure.std(1) / np.sqrt(repeat),\\n    label=\"Pure data set mean\",\\n    lw=lw,\\n    color=\"black\",\\n)\\nplt.title(\"Influence of outliers on the location estimation\")\\nplt.ylabel(r\"Error ($||\\\\mu - \\\\hat{\\\\mu}||_2^2$)\")\\nplt.legend(loc=\"upper left\", prop=font_prop)'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_robust_vs_empirical_covariance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.subplot(2, 1, 2)\\nx_size = range_n_outliers.size\\nplt.errorbar(\\n    range_n_outliers,\\n    err_cov_mcd.mean(1),\\n    yerr=err_cov_mcd.std(1),\\n    label=\"Robust covariance (mcd)\",\\n    color=\"m\",\\n)\\nplt.errorbar(\\n    range_n_outliers[: (x_size // 5 + 1)],\\n    err_cov_emp_full.mean(1)[: (x_size // 5 + 1)],\\n    yerr=err_cov_emp_full.std(1)[: (x_size // 5 + 1)],\\n    label=\"Full data set empirical covariance\",\\n    color=\"green\",\\n)\\nplt.plot(\\n    range_n_outliers[(x_size // 5) : (x_size // 2 - 1)],\\n    err_cov_emp_full.mean(1)[(x_size // 5) : (x_size // 2 - 1)],\\n    color=\"green\",\\n    ls=\"--\",\\n)\\nplt.errorbar(\\n    range_n_outliers,\\n    err_cov_emp_pure.mean(1),\\n    yerr=err_cov_emp_pure.std(1),\\n    label=\"Pure data set empirical covariance\",\\n    color=\"black\",\\n)\\nplt.title(\"Influence of outliers on the covariance estimation\")\\nplt.xlabel(\"Amount of contamination (%)\")\\nplt.ylabel(\"RMSE\")\\nplt.legend(loc=\"upper center\", prop=font_prop)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_sparse_cov.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================\\nSparse inverse covariance estimation\\n======================================\\n\\nUsing the GraphicalLasso estimator to learn a covariance and sparse precision\\nfrom a small number of samples.\\n\\nTo estimate a probabilistic model (e.g. a Gaussian model), estimating the\\nprecision matrix, that is the inverse covariance matrix, is as important\\nas estimating the covariance matrix. Indeed a Gaussian model is\\nparametrized by the precision matrix.\\n\\nTo be in favorable recovery conditions, we sample the data from a model\\nwith a sparse inverse covariance matrix. In addition, we ensure that the\\ndata is not too much correlated (limiting the largest coefficient of the\\nprecision matrix) and that there a no small coefficients in the\\nprecision matrix that cannot be recovered. In addition, with a small\\nnumber of observations, it is easier to recover a correlation matrix\\nrather than a covariance, thus we scale the time series.\\n\\nHere, the number of samples is slightly larger than the number of\\ndimensions, thus the empirical covariance is still invertible. However,\\nas the observations are strongly correlated, the empirical covariance\\nmatrix is ill-conditioned and as a result its inverse --the empirical\\nprecision matrix-- is very far from the ground truth.\\n\\nIf we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number\\nof samples is small, we need to shrink a lot. As a result, the\\nLedoit-Wolf precision is fairly close to the ground truth precision, that\\nis not far from being diagonal, but the off-diagonal structure is lost.\\n\\nThe l1-penalized estimator can recover part of this off-diagonal\\nstructure. It learns a sparse precision. It is not able to\\nrecover the exact sparsity pattern: it detects too many non-zero\\ncoefficients. However, the highest non-zero coefficients of the l1\\nestimated correspond to the non-zero coefficients in the ground truth.\\nFinally, the coefficients of the l1 precision estimate are biased toward\\nzero: because of the penalty, they are all smaller than the corresponding\\nground truth value, as can be seen on the figure.\\n\\nNote that, the color range of the precision matrices is tweaked to\\nimprove readability of the figure. The full range of values of the\\nempirical precision is not displayed.\\n\\nThe alpha parameter of the GraphicalLasso setting the sparsity of the model is\\nset by internal cross-validation in the GraphicalLassoCV. As can be\\nseen on figure 2, the grid to compute the cross-validation score is\\niteratively refined in the neighborhood of the maximum.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate the data\\n# -----------------\\nimport numpy as np\\nfrom scipy import linalg\\n\\nfrom sklearn.datasets import make_sparse_spd_matrix\\n\\nn_samples = 60\\nn_features = 20'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_sparse_cov.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate the data\\n# -----------------\\nimport numpy as np\\nfrom scipy import linalg\\n\\nfrom sklearn.datasets import make_sparse_spd_matrix\\n\\nn_samples = 60\\nn_features = 20\\n\\nprng = np.random.RandomState(1)\\nprec = make_sparse_spd_matrix(\\n    n_features, alpha=0.98, smallest_coef=0.4, largest_coef=0.7, random_state=prng\\n)\\ncov = linalg.inv(prec)\\nd = np.sqrt(np.diag(cov))\\ncov /= d\\ncov /= d[:, np.newaxis]\\nprec *= d\\nprec *= d[:, np.newaxis]\\nX = prng.multivariate_normal(np.zeros(n_features), cov, size=n_samples)\\nX -= X.mean(axis=0)\\nX /= X.std(axis=0)\\n\\n# %%\\n# Estimate the covariance\\n# -----------------------\\nfrom sklearn.covariance import GraphicalLassoCV, ledoit_wolf\\n\\nemp_cov = np.dot(X.T, X) / n_samples\\n\\nmodel = GraphicalLassoCV()\\nmodel.fit(X)\\ncov_ = model.covariance_\\nprec_ = model.precision_\\n\\nlw_cov_, _ = ledoit_wolf(X)\\nlw_prec_ = linalg.inv(lw_cov_)\\n\\n# %%\\n# Plot the results\\n# ----------------\\nimport matplotlib.pyplot as plt\\n\\nplt.figure(figsize=(10, 6))\\nplt.subplots_adjust(left=0.02, right=0.98)\\n\\n# plot the covariances\\ncovs = [\\n    (\"Empirical\", emp_cov),\\n    (\"Ledoit-Wolf\", lw_cov_),\\n    (\"GraphicalLassoCV\", cov_),\\n    (\"True\", cov),\\n]\\nvmax = cov_.max()\\nfor i, (name, this_cov) in enumerate(covs):\\n    plt.subplot(2, 4, i + 1)\\n    plt.imshow(\\n        this_cov, interpolation=\"nearest\", vmin=-vmax, vmax=vmax, cmap=plt.cm.RdBu_r\\n    )\\n    plt.xticks(())\\n    plt.yticks(())\\n    plt.title(\"%s covariance\" % name)\\n\\n\\n# plot the precisions\\nprecs = [\\n    (\"Empirical\", linalg.inv(emp_cov)),\\n    (\"Ledoit-Wolf\", lw_prec_),\\n    (\"GraphicalLasso\", prec_),\\n    (\"True\", prec),\\n]\\nvmax = 0.9 * prec_.max()\\nfor i, (name, this_prec) in enumerate(precs):\\n    ax = plt.subplot(2, 4, i + 5)\\n    plt.imshow(\\n        np.ma.masked_equal(this_prec, 0),\\n        interpolation=\"nearest\",\\n        vmin=-vmax,\\n        vmax=vmax,\\n        cmap=plt.cm.RdBu_r,\\n    )\\n    plt.xticks(())\\n    plt.yticks(())\\n    plt.title(\"%s precision\" % name)\\n    if hasattr(ax, \"set_facecolor\"):\\n        ax.set_facecolor(\".7\")\\n    else:\\n        ax.set_axis_bgcolor(\".7\")\\n\\n# %%\\n\\n# plot the model selection metric\\nplt.figure(figsize=(4, 3))\\nplt.axes([0.2, 0.15, 0.75, 0.7])\\nplt.plot(model.cv_results_[\"alphas\"], model.cv_results_[\"mean_test_score\"], \"o-\")\\nplt.axvline(model.alpha_, color=\".5\")\\nplt.title(\"Model selection\")\\nplt.ylabel(\"Cross-validation score\")\\nplt.xlabel(\"alpha\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_mahalanobis_distances.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='r\"\"\"\\n================================================================\\nRobust covariance estimation and Mahalanobis distances relevance\\n================================================================\\n\\nThis example shows covariance estimation with Mahalanobis\\ndistances on Gaussian distributed data.\\n\\nFor Gaussian distributed data, the distance of an observation\\n:math:`x_i` to the mode of the distribution can be computed using its\\nMahalanobis distance:\\n\\n.. math::\\n\\n    d_{(\\\\mu,\\\\Sigma)}(x_i)^2 = (x_i - \\\\mu)^T\\\\Sigma^{-1}(x_i - \\\\mu)\\n\\nwhere :math:`\\\\mu` and :math:`\\\\Sigma` are the location and the covariance of\\nthe underlying Gaussian distributions.\\n\\nIn practice, :math:`\\\\mu` and :math:`\\\\Sigma` are replaced by some\\nestimates. The standard covariance maximum likelihood estimate (MLE) is very\\nsensitive to the presence of outliers in the data set and therefore,\\nthe downstream Mahalanobis distances also are. It would be better to\\nuse a robust estimator of covariance to guarantee that the estimation is\\nresistant to \"erroneous\" observations in the dataset and that the\\ncalculated Mahalanobis distances accurately reflect the true\\norganization of the observations.\\n\\nThe Minimum Covariance Determinant estimator (MCD) is a robust,\\nhigh-breakdown point (i.e. it can be used to estimate the covariance\\nmatrix of highly contaminated datasets, up to\\n:math:`\\\\frac{n_\\\\text{samples}-n_\\\\text{features}-1}{2}` outliers)\\nestimator of covariance. The idea behind the MCD is to find\\n:math:`\\\\frac{n_\\\\text{samples}+n_\\\\text{features}+1}{2}`\\nobservations whose empirical covariance has the smallest determinant,\\nyielding a \"pure\" subset of observations from which to compute\\nstandards estimates of location and covariance. The MCD was introduced by\\nP.J.Rousseuw in [1]_.\\n\\nThis example illustrates how the Mahalanobis distances are affected by\\noutlying data. Observations drawn from a contaminating distribution\\nare not distinguishable from the observations coming from the real,\\nGaussian distribution when using standard covariance MLE based Mahalanobis\\ndistances. Using MCD-based\\nMahalanobis distances, the two populations become\\ndistinguishable. Associated applications include outlier detection,\\nobservation ranking and clustering.\\n\\n.. note::\\n\\n    See also :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py`\\n\\n.. rubric:: References\\n\\n.. [1] P. J. Rousseeuw. `Least median of squares regression\\n    <http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/LeastMedianOfSquares.pdf>`_. J. Am\\n    Stat Ass, 79:871, 1984.\\n.. [2] Wilson, E. B., & Hilferty, M. M. (1931). `The distribution of chi-square.\\n    <https://water.usgs.gov/osw/bulletin17b/Wilson_Hilferty_1931.pdf>`_\\n    Proceedings of the National Academy of Sciences of the United States\\n    of America, 17, 684-688.\\n\\n\"\"\"  # noqa: E501'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_mahalanobis_distances.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"  # noqa: E501\\n\\n# %%\\n# Generate data\\n# --------------\\n#\\n# First, we generate a dataset of 125 samples and 2 features. Both features\\n# are Gaussian distributed with mean of 0 but feature 1 has a standard\\n# deviation equal to 2 and feature 2 has a standard deviation equal to 1. Next,\\n# 25 samples are replaced with Gaussian outlier samples where feature 1 has\\n# a standard deviation equal to 1 and feature 2 has a standard deviation equal\\n# to 7.\\n\\nimport numpy as np\\n\\n# for consistent results\\nnp.random.seed(7)\\n\\nn_samples = 125\\nn_outliers = 25\\nn_features = 2\\n\\n# generate Gaussian data of shape (125, 2)\\ngen_cov = np.eye(n_features)\\ngen_cov[0, 0] = 2.0\\nX = np.dot(np.random.randn(n_samples, n_features), gen_cov)\\n# add some outliers\\noutliers_cov = np.eye(n_features)\\noutliers_cov[np.arange(1, n_features), np.arange(1, n_features)] = 7.0\\nX[-n_outliers:] = np.dot(np.random.randn(n_outliers, n_features), outliers_cov)\\n\\n# %%\\n# Comparison of results\\n# ---------------------\\n#\\n# Below, we fit MCD and MLE based covariance estimators to our data and print\\n# the estimated covariance matrices. Note that the estimated variance of\\n# feature 2 is much higher with the MLE based estimator (7.5) than\\n# that of the MCD robust estimator (1.2). This shows that the MCD based\\n# robust estimator is much more resistant to the outlier samples, which were\\n# designed to have a much larger variance in feature 2.\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\\n\\n# fit a MCD robust estimator to data\\nrobust_cov = MinCovDet().fit(X)\\n# fit a MLE estimator to data\\nemp_cov = EmpiricalCovariance().fit(X)\\nprint(\\n    \"Estimated covariance matrix:\\\\nMCD (Robust):\\\\n{}\\\\nMLE:\\\\n{}\".format(\\n        robust_cov.covariance_, emp_cov.covariance_\\n    )\\n)\\n\\n# %%\\n# To better visualize the difference, we plot contours of the\\n# Mahalanobis distances calculated by both methods. Notice that the robust\\n# MCD based Mahalanobis distances fit the inlier black points much better,\\n# whereas the MLE based distances are more influenced by the outlier\\n# red points.\\nimport matplotlib.lines as mlines\\n\\nfig, ax = plt.subplots(figsize=(10, 5))\\n# Plot data set\\ninlier_plot = ax.scatter(X[:, 0], X[:, 1], color=\"black\", label=\"inliers\")\\noutlier_plot = ax.scatter(\\n    X[:, 0][-n_outliers:], X[:, 1][-n_outliers:], color=\"red\", label=\"outliers\"\\n)\\nax.set_xlim(ax.get_xlim()[0], 10.0)\\nax.set_title(\"Mahalanobis distances of a contaminated data set\")'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_mahalanobis_distances.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Create meshgrid of feature 1 and feature 2 values\\nxx, yy = np.meshgrid(\\n    np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\\n    np.linspace(plt.ylim()[0], plt.ylim()[1], 100),\\n)\\nzz = np.c_[xx.ravel(), yy.ravel()]\\n# Calculate the MLE based Mahalanobis distances of the meshgrid\\nmahal_emp_cov = emp_cov.mahalanobis(zz)\\nmahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\\nemp_cov_contour = plt.contour(\\n    xx, yy, np.sqrt(mahal_emp_cov), cmap=plt.cm.PuBu_r, linestyles=\"dashed\"\\n)\\n# Calculate the MCD based Mahalanobis distances\\nmahal_robust_cov = robust_cov.mahalanobis(zz)\\nmahal_robust_cov = mahal_robust_cov.reshape(xx.shape)\\nrobust_contour = ax.contour(\\n    xx, yy, np.sqrt(mahal_robust_cov), cmap=plt.cm.YlOrBr_r, linestyles=\"dotted\"\\n)\\n\\n# Add legend\\nax.legend(\\n    [\\n        mlines.Line2D([], [], color=\"tab:blue\", linestyle=\"dashed\"),\\n        mlines.Line2D([], [], color=\"tab:orange\", linestyle=\"dotted\"),\\n        inlier_plot,\\n        outlier_plot,\\n    ],\\n    [\"MLE dist\", \"MCD dist\", \"inliers\", \"outliers\"],\\n    loc=\"upper right\",\\n    borderaxespad=0,\\n)\\n\\nplt.show()\\n\\n# %%\\n# Finally, we highlight the ability of MCD based Mahalanobis distances to\\n# distinguish outliers. We take the cubic root of the Mahalanobis distances,\\n# yielding approximately normal distributions (as suggested by Wilson and\\n# Hilferty [2]_), then plot the values of inlier and outlier samples with\\n# boxplots. The distribution of outlier samples is more separated from the\\n# distribution of inlier samples for robust MCD based Mahalanobis distances.\\n\\nfig, (ax1, ax2) = plt.subplots(1, 2)\\nplt.subplots_adjust(wspace=0.6)\\n\\n# Calculate cubic root of MLE Mahalanobis distances for samples\\nemp_mahal = emp_cov.mahalanobis(X - np.mean(X, 0)) ** (0.33)\\n# Plot boxplots\\nax1.boxplot([emp_mahal[:-n_outliers], emp_mahal[-n_outliers:]], widths=0.25)\\n# Plot individual samples\\nax1.plot(\\n    np.full(n_samples - n_outliers, 1.26),\\n    emp_mahal[:-n_outliers],\\n    \"+k\",\\n    markeredgewidth=1,\\n)\\nax1.plot(np.full(n_outliers, 2.26), emp_mahal[-n_outliers:], \"+k\", markeredgewidth=1)\\nax1.axes.set_xticklabels((\"inliers\", \"outliers\"), size=15)\\nax1.set_ylabel(r\"$\\\\sqrt[3]{\\\\rm{(Mahal. dist.)}}$\", size=16)\\nax1.set_title(\"Using non-robust estimates\\\\n(Maximum Likelihood)\")\\n\\n# Calculate cubic root of MCD Mahalanobis distances for samples\\nrobust_mahal = robust_cov.mahalanobis(X - robust_cov.location_) ** (0.33)\\n# Plot boxplots\\nax2.boxplot([robust_mahal[:-n_outliers], robust_mahal[-n_outliers:]], widths=0.25)\\n# Plot individual samples\\nax2.plot(\\n    np.full(n_samples - n_outliers, 1.26),\\n    robust_mahal[:-n_outliers],\\n    \"+k\",\\n    markeredgewidth=1,\\n)\\nax2.plot(np.full(n_outliers, 2.26), robust_mahal[-n_outliers:], \"+k\", markeredgewidth=1)\\nax2.axes.set_xticklabels((\"inliers\", \"outliers\"), size=15)\\nax2.set_ylabel(r\"$\\\\sqrt[3]{\\\\rm{(Mahal. dist.)}}$\", size=16)\\nax2.set_title(\"Using robust estimates\\\\n(Minimum Covariance Determinant)\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_lw_vs_oas.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================\\nLedoit-Wolf vs OAS estimation\\n=============================\\n\\nThe usual covariance maximum likelihood estimate can be regularized\\nusing shrinkage. Ledoit and Wolf proposed a close formula to compute\\nthe asymptotically optimal shrinkage parameter (minimizing a MSE\\ncriterion), yielding the Ledoit-Wolf covariance estimate.\\n\\nChen et al. proposed an improvement of the Ledoit-Wolf shrinkage\\nparameter, the OAS coefficient, whose convergence is significantly\\nbetter under the assumption that the data are Gaussian.\\n\\nThis example, inspired from Chen\\'s publication [1], shows a comparison\\nof the estimated MSE of the LW and OAS methods, using Gaussian\\ndistributed data.\\n\\n[1] \"Shrinkage Algorithms for MMSE Covariance Estimation\"\\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom scipy.linalg import cholesky, toeplitz\\n\\nfrom sklearn.covariance import OAS, LedoitWolf\\n\\nnp.random.seed(0)\\n# %%\\nn_features = 100\\n# simulation covariance matrix (AR(1) process)\\nr = 0.1\\nreal_cov = toeplitz(r ** np.arange(n_features))\\ncoloring_matrix = cholesky(real_cov)\\n\\nn_samples_range = np.arange(6, 31, 1)\\nrepeat = 100\\nlw_mse = np.zeros((n_samples_range.size, repeat))\\noa_mse = np.zeros((n_samples_range.size, repeat))\\nlw_shrinkage = np.zeros((n_samples_range.size, repeat))\\noa_shrinkage = np.zeros((n_samples_range.size, repeat))\\nfor i, n_samples in enumerate(n_samples_range):\\n    for j in range(repeat):\\n        X = np.dot(np.random.normal(size=(n_samples, n_features)), coloring_matrix.T)\\n\\n        lw = LedoitWolf(store_precision=False, assume_centered=True)\\n        lw.fit(X)\\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\\n        lw_shrinkage[i, j] = lw.shrinkage_\\n\\n        oa = OAS(store_precision=False, assume_centered=True)\\n        oa.fit(X)\\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\\n        oa_shrinkage[i, j] = oa.shrinkage_\\n\\n# plot MSE\\nplt.subplot(2, 1, 1)\\nplt.errorbar(\\n    n_samples_range,\\n    lw_mse.mean(1),\\n    yerr=lw_mse.std(1),\\n    label=\"Ledoit-Wolf\",\\n    color=\"navy\",\\n    lw=2,\\n)\\nplt.errorbar(\\n    n_samples_range,\\n    oa_mse.mean(1),\\n    yerr=oa_mse.std(1),\\n    label=\"OAS\",\\n    color=\"darkorange\",\\n    lw=2,\\n)\\nplt.ylabel(\"Squared error\")\\nplt.legend(loc=\"upper right\")\\nplt.title(\"Comparison of covariance estimators\")\\nplt.xlim(5, 31)\\n\\n# plot shrinkage coefficient\\nplt.subplot(2, 1, 2)\\nplt.errorbar(\\n    n_samples_range,\\n    lw_shrinkage.mean(1),\\n    yerr=lw_shrinkage.std(1),\\n    label=\"Ledoit-Wolf\",\\n    color=\"navy\",\\n    lw=2,\\n)\\nplt.errorbar(\\n    n_samples_range,\\n    oa_shrinkage.mean(1),\\n    yerr=oa_shrinkage.std(1),\\n    label=\"OAS\",\\n    color=\"darkorange\",\\n    lw=2,\\n)\\nplt.xlabel(\"n_samples\")\\nplt.ylabel(\"Shrinkage\")\\nplt.legend(loc=\"lower right\")\\nplt.ylim(plt.ylim()[0], 1.0 + (plt.ylim()[1] - plt.ylim()[0]) / 10.0)\\nplt.xlim(5, 31)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_covariance_estimation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=======================================================================\\nShrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood\\n=======================================================================\\n\\nWhen working with covariance estimation, the usual approach is to use\\na maximum likelihood estimator, such as the\\n:class:`~sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it\\nconverges to the true (population) covariance when given many\\nobservations. However, it can also be beneficial to regularize it, in\\norder to reduce its variance; this, in turn, introduces some bias. This\\nexample illustrates the simple regularization used in\\n:ref:`shrunk_covariance` estimators. In particular, it focuses on how to\\nset the amount of regularization, i.e. how to choose the bias-variance\\ntrade-off.\\n\"\"\"\\n\\n# %%\\n# Generate sample data\\n# --------------------\\n\\nimport numpy as np\\n\\nn_features, n_samples = 40, 20\\nnp.random.seed(42)\\nbase_X_train = np.random.normal(size=(n_samples, n_features))\\nbase_X_test = np.random.normal(size=(n_samples, n_features))\\n\\n# Color samples\\ncoloring_matrix = np.random.normal(size=(n_features, n_features))\\nX_train = np.dot(base_X_train, coloring_matrix)\\nX_test = np.dot(base_X_test, coloring_matrix)\\n\\n\\n# %%\\n# Compute the likelihood on test data\\n# -----------------------------------\\n\\nfrom scipy import linalg\\n\\nfrom sklearn.covariance import ShrunkCovariance, empirical_covariance, log_likelihood\\n\\n# spanning a range of possible shrinkage coefficient values\\nshrinkages = np.logspace(-2, 0, 30)\\nnegative_logliks = [\\n    -ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test) for s in shrinkages\\n]\\n\\n# under the ground-truth model, which we would not have access to in real\\n# settings\\nreal_cov = np.dot(coloring_matrix.T, coloring_matrix)\\nemp_cov = empirical_covariance(X_train)\\nloglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))\\n\\n\\n# %%\\n# Compare different approaches to setting the regularization parameter\\n# --------------------------------------------------------------------\\n#\\n# Here we compare 3 approaches:\\n#\\n# * Setting the parameter by cross-validating the likelihood on three folds\\n#   according to a grid of potential shrinkage parameters.\\n#\\n# * A close formula proposed by Ledoit and Wolf to compute\\n#   the asymptotically optimal regularization parameter (minimizing a MSE\\n#   criterion), yielding the :class:`~sklearn.covariance.LedoitWolf`\\n#   covariance estimate.\\n#\\n# * An improvement of the Ledoit-Wolf shrinkage, the\\n#   :class:`~sklearn.covariance.OAS`, proposed by Chen et al. Its\\n#   convergence is significantly better under the assumption that the data\\n#   are Gaussian, in particular for small samples.\\n\\n\\nfrom sklearn.covariance import OAS, LedoitWolf\\nfrom sklearn.model_selection import GridSearchCV\\n\\n# GridSearch for an optimal shrinkage coefficient\\ntuned_parameters = [{\"shrinkage\": shrinkages}]\\ncv = GridSearchCV(ShrunkCovariance(), tuned_parameters)\\ncv.fit(X_train)'), Document(metadata={'source': '/content/local_copy_repo/examples/covariance/plot_covariance_estimation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.covariance import OAS, LedoitWolf\\nfrom sklearn.model_selection import GridSearchCV\\n\\n# GridSearch for an optimal shrinkage coefficient\\ntuned_parameters = [{\"shrinkage\": shrinkages}]\\ncv = GridSearchCV(ShrunkCovariance(), tuned_parameters)\\ncv.fit(X_train)\\n\\n# Ledoit-Wolf optimal shrinkage coefficient estimate\\nlw = LedoitWolf()\\nloglik_lw = lw.fit(X_train).score(X_test)\\n\\n# OAS coefficient estimate\\noa = OAS()\\nloglik_oa = oa.fit(X_train).score(X_test)\\n\\n# %%\\n# Plot results\\n# ------------\\n#\\n#\\n# To quantify estimation error, we plot the likelihood of unseen data for\\n# different values of the shrinkage parameter. We also show the choices by\\n# cross-validation, or with the LedoitWolf and OAS estimates.\\n\\nimport matplotlib.pyplot as plt\\n\\nfig = plt.figure()\\nplt.title(\"Regularized covariance: likelihood and shrinkage coefficient\")\\nplt.xlabel(\"Regularization parameter: shrinkage coefficient\")\\nplt.ylabel(\"Error: negative log-likelihood on test data\")\\n# range shrinkage curve\\nplt.loglog(shrinkages, negative_logliks, label=\"Negative log-likelihood\")\\n\\nplt.plot(plt.xlim(), 2 * [loglik_real], \"--r\", label=\"Real covariance likelihood\")\\n\\n# adjust view\\nlik_max = np.amax(negative_logliks)\\nlik_min = np.amin(negative_logliks)\\nymin = lik_min - 6.0 * np.log((plt.ylim()[1] - plt.ylim()[0]))\\nymax = lik_max + 10.0 * np.log(lik_max - lik_min)\\nxmin = shrinkages[0]\\nxmax = shrinkages[-1]\\n# LW likelihood\\nplt.vlines(\\n    lw.shrinkage_,\\n    ymin,\\n    -loglik_lw,\\n    color=\"magenta\",\\n    linewidth=3,\\n    label=\"Ledoit-Wolf estimate\",\\n)\\n# OAS likelihood\\nplt.vlines(\\n    oa.shrinkage_, ymin, -loglik_oa, color=\"purple\", linewidth=3, label=\"OAS estimate\"\\n)\\n# best CV estimator likelihood\\nplt.vlines(\\n    cv.best_estimator_.shrinkage,\\n    ymin,\\n    -cv.best_estimator_.score(X_test),\\n    color=\"cyan\",\\n    linewidth=3,\\n    label=\"Cross-validation best estimate\",\\n)\\n\\nplt.ylim(ymin, ymax)\\nplt.xlim(xmin, xmax)\\nplt.legend()\\n\\nplt.show()\\n\\n# %%\\n# .. note::\\n#\\n#    The maximum likelihood estimate corresponds to no shrinkage,\\n#    and thus performs poorly. The Ledoit-Wolf estimate performs really well,\\n#    as it is close to the optimal and is not computationally costly. In this\\n#    example, the OAS estimate is a bit further away. Interestingly, both\\n#    approaches outperform cross-validation, which is significantly most\\n#    computationally costly.'), Document(metadata={'source': '/content/local_copy_repo/examples/exercises/plot_iris_exercise.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================\\nSVM Exercise\\n================================\\n\\nA tutorial exercise for using different SVM kernels.\\n\\nThis exercise is used in the :ref:`using_kernels_tut` part of the\\n:ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets, svm\\n\\niris = datasets.load_iris()\\nX = iris.data\\ny = iris.target\\n\\nX = X[y != 0, :2]\\ny = y[y != 0]\\n\\nn_sample = len(X)\\n\\nnp.random.seed(0)\\norder = np.random.permutation(n_sample)\\nX = X[order]\\ny = y[order].astype(float)\\n\\nX_train = X[: int(0.9 * n_sample)]\\ny_train = y[: int(0.9 * n_sample)]\\nX_test = X[int(0.9 * n_sample) :]\\ny_test = y[int(0.9 * n_sample) :]\\n\\n# fit the model\\nfor kernel in (\"linear\", \"rbf\", \"poly\"):\\n    clf = svm.SVC(kernel=kernel, gamma=10)\\n    clf.fit(X_train, y_train)\\n\\n    plt.figure()\\n    plt.clf()\\n    plt.scatter(\\n        X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolor=\"k\", s=20\\n    )\\n\\n    # Circle out the test data\\n    plt.scatter(\\n        X_test[:, 0], X_test[:, 1], s=80, facecolors=\"none\", zorder=10, edgecolor=\"k\"\\n    )\\n\\n    plt.axis(\"tight\")\\n    x_min = X[:, 0].min()\\n    x_max = X[:, 0].max()\\n    y_min = X[:, 1].min()\\n    y_max = X[:, 1].max()\\n\\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\\n\\n    # Put the result into a color plot\\n    Z = Z.reshape(XX.shape)\\n    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\\n    plt.contour(\\n        XX,\\n        YY,\\n        Z,\\n        colors=[\"k\", \"k\", \"k\"],\\n        linestyles=[\"--\", \"-\", \"--\"],\\n        levels=[-0.5, 0, 0.5],\\n    )\\n\\n    plt.title(kernel)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/exercises/plot_digits_classification_exercise.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================\\nDigits Classification Exercise\\n================================\\n\\nA tutorial exercise regarding the use of classification techniques on\\nthe Digits dataset.\\n\\nThis exercise is used in the :ref:`clf_tut` part of the\\n:ref:`supervised_learning_tut` section of the\\n:ref:`stat_learn_tut_index`.\\n\\n\"\"\"\\n\\nfrom sklearn import datasets, linear_model, neighbors\\n\\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\\nX_digits = X_digits / X_digits.max()\\n\\nn_samples = len(X_digits)\\n\\nX_train = X_digits[: int(0.9 * n_samples)]\\ny_train = y_digits[: int(0.9 * n_samples)]\\nX_test = X_digits[int(0.9 * n_samples) :]\\ny_test = y_digits[int(0.9 * n_samples) :]\\n\\nknn = neighbors.KNeighborsClassifier()\\nlogistic = linear_model.LogisticRegression(max_iter=1000)\\n\\nprint(\"KNN score: %f\" % knn.fit(X_train, y_train).score(X_test, y_test))\\nprint(\\n    \"LogisticRegression score: %f\"\\n    % logistic.fit(X_train, y_train).score(X_test, y_test)\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/exercises/plot_cv_diabetes.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================================\\nCross-validation on diabetes Dataset Exercise\\n===============================================\\n\\nA tutorial exercise which uses cross-validation with linear models.\\n\\nThis exercise is used in the :ref:`cv_estimators_tut` part of the\\n:ref:`model_selection_tut` section of the :ref:`stat_learn_tut_index`.\\n\\n\"\"\"\\n\\n# %%\\n# Load dataset and apply GridSearchCV\\n# -----------------------------------\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.linear_model import Lasso\\nfrom sklearn.model_selection import GridSearchCV\\n\\nX, y = datasets.load_diabetes(return_X_y=True)\\nX = X[:150]\\ny = y[:150]\\n\\nlasso = Lasso(random_state=0, max_iter=10000)\\nalphas = np.logspace(-4, -0.5, 30)\\n\\ntuned_parameters = [{\"alpha\": alphas}]\\nn_folds = 5\\n\\nclf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=False)\\nclf.fit(X, y)\\nscores = clf.cv_results_[\"mean_test_score\"]\\nscores_std = clf.cv_results_[\"std_test_score\"]\\n\\n# %%\\n# Plot error lines showing +/- std. errors of the scores\\n# ------------------------------------------------------\\n\\nplt.figure().set_size_inches(8, 6)\\nplt.semilogx(alphas, scores)\\n\\nstd_error = scores_std / np.sqrt(n_folds)\\n\\nplt.semilogx(alphas, scores + std_error, \"b--\")\\nplt.semilogx(alphas, scores - std_error, \"b--\")\\n\\n# alpha=0.2 controls the translucency of the fill color\\nplt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\\n\\nplt.ylabel(\"CV score +/- std error\")\\nplt.xlabel(\"alpha\")\\nplt.axhline(np.max(scores), linestyle=\"--\", color=\".5\")\\nplt.xlim([alphas[0], alphas[-1]])\\n\\n# %%\\n# Bonus: how much can you trust the selection of alpha?\\n# -----------------------------------------------------\\n\\n# To answer this question we use the LassoCV object that sets its alpha\\n# parameter automatically from the data by internal cross-validation (i.e. it\\n# performs cross-validation on the training data it receives).\\n# We use external cross-validation to see how much the automatically obtained\\n# alphas differ across different cross-validation folds.\\n\\nfrom sklearn.linear_model import LassoCV\\nfrom sklearn.model_selection import KFold\\n\\nlasso_cv = LassoCV(alphas=alphas, random_state=0, max_iter=10000)\\nk_fold = KFold(3)\\n\\nprint(\"Answer to the bonus question:\", \"how much can you trust the selection of alpha?\")\\nprint()\\nprint(\"Alpha parameters maximising the generalization score on different\")\\nprint(\"subsets of the data:\")\\nfor k, (train, test) in enumerate(k_fold.split(X, y)):\\n    lasso_cv.fit(X[train], y[train])\\n    print(\\n        \"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".format(\\n            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])\\n        )\\n    )\\nprint()\\nprint(\"Answer: Not very much since we obtained different alphas for different\")\\nprint(\"subsets of the data and moreover, the scores for these alphas differ\")\\nprint(\"quite substantially.\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpc.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================================================\\nProbabilistic predictions with Gaussian process classification (GPC)\\n====================================================================\\n\\nThis example illustrates the predicted probability of GPC for an RBF kernel\\nwith different choices of the hyperparameters. The first figure shows the\\npredicted probability of GPC with arbitrarily chosen hyperparameters and with\\nthe hyperparameters corresponding to the maximum log-marginal-likelihood (LML).\\n\\nWhile the hyperparameters chosen by optimizing LML have a considerable larger\\nLML, they perform slightly worse according to the log-loss on test data. The\\nfigure shows that this is because they exhibit a steep change of the class\\nprobabilities at the class boundaries (which is good) but have predicted\\nprobabilities close to 0.5 far away from the class boundaries (which is bad)\\nThis undesirable effect is caused by the Laplace approximation used\\ninternally by GPC.\\n\\nThe second figure shows the log-marginal-likelihood for different choices of\\nthe kernel\\'s hyperparameters, highlighting the two choices of the\\nhyperparameters used in the first figure by black dots.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn.gaussian_process import GaussianProcessClassifier\\nfrom sklearn.gaussian_process.kernels import RBF\\nfrom sklearn.metrics import accuracy_score, log_loss\\n\\n# Generate data\\ntrain_size = 50\\nrng = np.random.RandomState(0)\\nX = rng.uniform(0, 5, 100)[:, np.newaxis]\\ny = np.array(X[:, 0] > 2.5, dtype=int)\\n\\n# Specify Gaussian Processes with fixed and optimized hyperparameters\\ngp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0), optimizer=None)\\ngp_fix.fit(X[:train_size], y[:train_size])\\n\\ngp_opt = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\\ngp_opt.fit(X[:train_size], y[:train_size])\\n\\nprint(\\n    \"Log Marginal Likelihood (initial): %.3f\"\\n    % gp_fix.log_marginal_likelihood(gp_fix.kernel_.theta)\\n)\\nprint(\\n    \"Log Marginal Likelihood (optimized): %.3f\"\\n    % gp_opt.log_marginal_likelihood(gp_opt.kernel_.theta)\\n)\\n\\nprint(\\n    \"Accuracy: %.3f (initial) %.3f (optimized)\"\\n    % (\\n        accuracy_score(y[:train_size], gp_fix.predict(X[:train_size])),\\n        accuracy_score(y[:train_size], gp_opt.predict(X[:train_size])),\\n    )\\n)\\nprint(\\n    \"Log-loss: %.3f (initial) %.3f (optimized)\"\\n    % (\\n        log_loss(y[:train_size], gp_fix.predict_proba(X[:train_size])[:, 1]),\\n        log_loss(y[:train_size], gp_opt.predict_proba(X[:train_size])[:, 1]),\\n    )\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpc.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Plot posteriors\\nplt.figure()\\nplt.scatter(\\n    X[:train_size, 0], y[:train_size], c=\"k\", label=\"Train data\", edgecolors=(0, 0, 0)\\n)\\nplt.scatter(\\n    X[train_size:, 0], y[train_size:], c=\"g\", label=\"Test data\", edgecolors=(0, 0, 0)\\n)\\nX_ = np.linspace(0, 5, 100)\\nplt.plot(\\n    X_,\\n    gp_fix.predict_proba(X_[:, np.newaxis])[:, 1],\\n    \"r\",\\n    label=\"Initial kernel: %s\" % gp_fix.kernel_,\\n)\\nplt.plot(\\n    X_,\\n    gp_opt.predict_proba(X_[:, np.newaxis])[:, 1],\\n    \"b\",\\n    label=\"Optimized kernel: %s\" % gp_opt.kernel_,\\n)\\nplt.xlabel(\"Feature\")\\nplt.ylabel(\"Class 1 probability\")\\nplt.xlim(0, 5)\\nplt.ylim(-0.25, 1.5)\\nplt.legend(loc=\"best\")\\n\\n# Plot LML landscape\\nplt.figure()\\ntheta0 = np.logspace(0, 8, 30)\\ntheta1 = np.logspace(-1, 1, 29)\\nTheta0, Theta1 = np.meshgrid(theta0, theta1)\\nLML = [\\n    [\\n        gp_opt.log_marginal_likelihood(np.log([Theta0[i, j], Theta1[i, j]]))\\n        for i in range(Theta0.shape[0])\\n    ]\\n    for j in range(Theta0.shape[1])\\n]\\nLML = np.array(LML).T\\nplt.plot(\\n    np.exp(gp_fix.kernel_.theta)[0], np.exp(gp_fix.kernel_.theta)[1], \"ko\", zorder=10\\n)\\nplt.plot(\\n    np.exp(gp_opt.kernel_.theta)[0], np.exp(gp_opt.kernel_.theta)[1], \"ko\", zorder=10\\n)\\nplt.pcolor(Theta0, Theta1, LML)\\nplt.xscale(\"log\")\\nplt.yscale(\"log\")\\nplt.colorbar()\\nplt.xlabel(\"Magnitude\")\\nplt.ylabel(\"Length-scale\")\\nplt.title(\"Log-marginal-likelihood\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpc_iris.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================================================\\nGaussian process classification (GPC) on iris dataset\\n=====================================================\\n\\nThis example illustrates the predicted probability of GPC for an isotropic\\nand anisotropic RBF kernel on a two-dimensional version for the iris-dataset.\\nThe anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by\\nassigning different length-scales to the two feature dimensions.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.gaussian_process import GaussianProcessClassifier\\nfrom sklearn.gaussian_process.kernels import RBF\\n\\n# import some data to play with\\niris = datasets.load_iris()\\nX = iris.data[:, :2]  # we only take the first two features.\\ny = np.array(iris.target, dtype=int)\\n\\nh = 0.02  # step size in the mesh\\n\\nkernel = 1.0 * RBF([1.0])\\ngpc_rbf_isotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)\\nkernel = 1.0 * RBF([1.0, 1.0])\\ngpc_rbf_anisotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)\\n\\n# create a mesh to plot in\\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n\\ntitles = [\"Isotropic RBF\", \"Anisotropic RBF\"]\\nplt.figure(figsize=(10, 5))\\nfor i, clf in enumerate((gpc_rbf_isotropic, gpc_rbf_anisotropic)):\\n    # Plot the predicted probabilities. For that, we will assign a color to\\n    # each point in the mesh [x_min, m_max]x[y_min, y_max].\\n    plt.subplot(1, 2, i + 1)\\n\\n    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\\n\\n    # Put the result into a color plot\\n    Z = Z.reshape((xx.shape[0], xx.shape[1], 3))\\n    plt.imshow(Z, extent=(x_min, x_max, y_min, y_max), origin=\"lower\")\\n\\n    # Plot also the training points\\n    plt.scatter(X[:, 0], X[:, 1], c=np.array([\"r\", \"g\", \"b\"])[y], edgecolors=(0, 0, 0))\\n    plt.xlabel(\"Sepal length\")\\n    plt.ylabel(\"Sepal width\")\\n    plt.xlim(xx.min(), xx.max())\\n    plt.ylim(yy.min(), yy.max())\\n    plt.xticks(())\\n    plt.yticks(())\\n    plt.title(\\n        \"%s, LML: %.3f\" % (titles[i], clf.log_marginal_likelihood(clf.kernel_.theta))\\n    )\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_on_structured_data.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class SequenceKernel(GenericKernelMixin, Kernel):\\n    \"\"\"\\n    A minimal (but valid) convolutional kernel for sequences of variable\\n    lengths.\"\"\"\\n\\n    def __init__(self, baseline_similarity=0.5, baseline_similarity_bounds=(1e-5, 1)):\\n        self.baseline_similarity = baseline_similarity\\n        self.baseline_similarity_bounds = baseline_similarity_bounds\\n\\n    @property\\n    def hyperparameter_baseline_similarity(self):\\n        return Hyperparameter(\\n            \"baseline_similarity\", \"numeric\", self.baseline_similarity_bounds\\n        )\\n\\n    def _f(self, s1, s2):\\n        \"\"\"\\n        kernel value between a pair of sequences\\n        \"\"\"\\n        return sum(\\n            [1.0 if c1 == c2 else self.baseline_similarity for c1 in s1 for c2 in s2]\\n        )\\n\\n    def _g(self, s1, s2):\\n        \"\"\"\\n        kernel derivative between a pair of sequences\\n        \"\"\"\\n        return sum([0.0 if c1 == c2 else 1.0 for c1 in s1 for c2 in s2])\\n\\n    def __call__(self, X, Y=None, eval_gradient=False):\\n        if Y is None:\\n            Y = X\\n\\n        if eval_gradient:\\n            return (\\n                np.array([[self._f(x, y) for y in Y] for x in X]),\\n                np.array([[[self._g(x, y)] for y in Y] for x in X]),\\n            )\\n        else:\\n            return np.array([[self._f(x, y) for y in Y] for x in X])\\n\\n    def diag(self, X):\\n        return np.array([self._f(x, x) for x in X])\\n\\n    def is_stationary(self):\\n        return False\\n\\n    def clone_with_theta(self, theta):\\n        cloned = clone(self)\\n        cloned.theta = theta\\n        return cloned'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_on_structured_data.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================================================================\\nGaussian processes on discrete data structures\\n==========================================================================\\n\\nThis example illustrates the use of Gaussian processes for regression and\\nclassification tasks on data that are not in fixed-length feature vector form.\\nThis is achieved through the use of kernel functions that operates directly\\non discrete structures such as variable-length sequences, trees, and graphs.\\n\\nSpecifically, here the input variables are some gene sequences stored as\\nvariable-length strings consisting of letters \\'A\\', \\'T\\', \\'C\\', and \\'G\\',\\nwhile the output variables are floating point numbers and True/False labels\\nin the regression and classification tasks, respectively.\\n\\nA kernel between the gene sequences is defined using R-convolution [1]_ by\\nintegrating a binary letter-wise kernel over all pairs of letters among a pair\\nof strings.\\n\\nThis example will generate three figures.\\n\\nIn the first figure, we visualize the value of the kernel, i.e. the similarity\\nof the sequences, using a colormap. Brighter color here indicates higher\\nsimilarity.\\n\\nIn the second figure, we show some regression result on a dataset of 6\\nsequences. Here we use the 1st, 2nd, 4th, and 5th sequences as the training set\\nto make predictions on the 3rd and 6th sequences.\\n\\nIn the third figure, we demonstrate a classification model by training on 6\\nsequences and make predictions on another 5 sequences. The ground truth here is\\nsimply  whether there is at least one \\'A\\' in the sequence. Here the model makes\\nfour correct classifications and fails on one.\\n\\n.. [1] Haussler, D. (1999). Convolution kernels on discrete structures\\n       (Vol. 646). Technical report, Department of Computer Science, University\\n       of California at Santa Cruz.\\n\\n\"\"\"\\n\\n# %%\\nimport numpy as np\\n\\nfrom sklearn.base import clone\\nfrom sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\\nfrom sklearn.gaussian_process.kernels import GenericKernelMixin, Hyperparameter, Kernel\\n\\n\\n# Code for: class SequenceKernel(GenericKernelMixin, Kernel):\\n\\n\\nkernel = SequenceKernel()\\n\\n# %%\\n# Sequence similarity matrix under the kernel\\n# ===========================================\\n\\nimport matplotlib.pyplot as plt\\n\\nX = np.array([\"AGCT\", \"AGC\", \"AACT\", \"TAA\", \"AAA\", \"GAACA\"])\\n\\nK = kernel(X)\\nD = kernel.diag(X)\\n\\nplt.figure(figsize=(8, 5))\\nplt.imshow(np.diag(D**-0.5).dot(K).dot(np.diag(D**-0.5)))\\nplt.xticks(np.arange(len(X)), X)\\nplt.yticks(np.arange(len(X)), X)\\nplt.title(\"Sequence similarity under the kernel\")\\nplt.show()\\n\\n# %%\\n# Regression\\n# ==========\\n\\nX = np.array([\"AGCT\", \"AGC\", \"AACT\", \"TAA\", \"AAA\", \"GAACA\"])\\nY = np.array([1.0, 1.0, 2.0, 2.0, 3.0, 3.0])\\n\\ntraining_idx = [0, 1, 3, 4]\\ngp = GaussianProcessRegressor(kernel=kernel)\\ngp.fit(X[training_idx], Y[training_idx])'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_on_structured_data.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Regression\\n# ==========\\n\\nX = np.array([\"AGCT\", \"AGC\", \"AACT\", \"TAA\", \"AAA\", \"GAACA\"])\\nY = np.array([1.0, 1.0, 2.0, 2.0, 3.0, 3.0])\\n\\ntraining_idx = [0, 1, 3, 4]\\ngp = GaussianProcessRegressor(kernel=kernel)\\ngp.fit(X[training_idx], Y[training_idx])\\n\\nplt.figure(figsize=(8, 5))\\nplt.bar(np.arange(len(X)), gp.predict(X), color=\"b\", label=\"prediction\")\\nplt.bar(training_idx, Y[training_idx], width=0.2, color=\"r\", alpha=1, label=\"training\")\\nplt.xticks(np.arange(len(X)), X)\\nplt.title(\"Regression on sequences\")\\nplt.legend()\\nplt.show()\\n\\n# %%\\n# Classification\\n# ==============\\n\\nX_train = np.array([\"AGCT\", \"CGA\", \"TAAC\", \"TCG\", \"CTTT\", \"TGCT\"])\\n# whether there are \\'A\\'s in the sequence\\nY_train = np.array([True, True, True, False, False, False])\\n\\ngp = GaussianProcessClassifier(kernel)\\ngp.fit(X_train, Y_train)\\n\\nX_test = [\"AAA\", \"ATAG\", \"CTC\", \"CT\", \"C\"]\\nY_test = [True, True, False, False, False]\\n\\nplt.figure(figsize=(8, 5))\\nplt.scatter(\\n    np.arange(len(X_train)),\\n    [1.0 if c else -1.0 for c in Y_train],\\n    s=100,\\n    marker=\"o\",\\n    edgecolor=\"none\",\\n    facecolor=(1, 0.75, 0),\\n    label=\"training\",\\n)\\nplt.scatter(\\n    len(X_train) + np.arange(len(X_test)),\\n    [1.0 if c else -1.0 for c in Y_test],\\n    s=100,\\n    marker=\"o\",\\n    edgecolor=\"none\",\\n    facecolor=\"r\",\\n    label=\"truth\",\\n)\\nplt.scatter(\\n    len(X_train) + np.arange(len(X_test)),\\n    [1.0 if c else -1.0 for c in gp.predict(X_test)],\\n    s=100,\\n    marker=\"x\",\\n    facecolor=\"b\",\\n    linewidth=2,\\n    label=\"prediction\",\\n)\\nplt.xticks(np.arange(len(X_train) + len(X_test)), np.concatenate((X_train, X_test)))\\nplt.yticks([-1, 1], [False, True])\\nplt.title(\"Classification on sequences\")\\nplt.legend()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_noisy.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def target_generator(X, add_noise=False):\\n    target = 0.5 + np.sin(3 * X)\\n    if add_noise:\\n        rng = np.random.RandomState(1)\\n        target += rng.normal(0, 0.3, size=target.shape)\\n    return target.squeeze()'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_noisy.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================================\\nAbility of Gaussian process regression (GPR) to estimate data noise-level\\n=========================================================================\\n\\nThis example shows the ability of the\\n:class:`~sklearn.gaussian_process.kernels.WhiteKernel` to estimate the noise\\nlevel in the data. Moreover, we show the importance of kernel hyperparameters\\ninitialization.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Data generation\\n# ---------------\\n#\\n# We will work in a setting where `X` will contain a single feature. We create a\\n# function that will generate the target to be predicted. We will add an\\n# option to add some noise to the generated target.\\nimport numpy as np\\n\\n\\n# Code for: def target_generator(X, add_noise=False):\\n\\n\\n# %%\\n# Let\\'s have a look to the target generator where we will not add any noise to\\n# observe the signal that we would like to predict.\\nX = np.linspace(0, 5, num=30).reshape(-1, 1)\\ny = target_generator(X, add_noise=False)\\n\\n# %%\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(X, y, label=\"Expected signal\")\\nplt.legend()\\nplt.xlabel(\"X\")\\n_ = plt.ylabel(\"y\")\\n\\n# %%\\n# The target is transforming the input `X` using a sine function. Now, we will\\n# generate few noisy training samples. To illustrate the noise level, we will\\n# plot the true signal together with the noisy training samples.\\nrng = np.random.RandomState(0)\\nX_train = rng.uniform(0, 5, size=20).reshape(-1, 1)\\ny_train = target_generator(X_train, add_noise=True)\\n\\n# %%\\nplt.plot(X, y, label=\"Expected signal\")\\nplt.scatter(\\n    x=X_train[:, 0],\\n    y=y_train,\\n    color=\"black\",\\n    alpha=0.4,\\n    label=\"Observations\",\\n)\\nplt.legend()\\nplt.xlabel(\"X\")\\n_ = plt.ylabel(\"y\")\\n\\n# %%\\n# Optimisation of kernel hyperparameters in GPR\\n# ---------------------------------------------\\n#\\n# Now, we will create a\\n# :class:`~sklearn.gaussian_process.GaussianProcessRegressor`\\n# using an additive kernel adding a\\n# :class:`~sklearn.gaussian_process.kernels.RBF` and\\n# :class:`~sklearn.gaussian_process.kernels.WhiteKernel` kernels.\\n# The :class:`~sklearn.gaussian_process.kernels.WhiteKernel` is a kernel that\\n# will able to estimate the amount of noise present in the data while the\\n# :class:`~sklearn.gaussian_process.kernels.RBF` will serve at fitting the\\n# non-linearity between the data and the target.\\n#\\n# However, we will show that the hyperparameter space contains several local\\n# minima. It will highlights the importance of initial hyperparameter values.\\n#\\n# We will create a model using a kernel with a high noise level and a large\\n# length scale, which will explain all variations in the data by noise.\\nfrom sklearn.gaussian_process import GaussianProcessRegressor\\nfrom sklearn.gaussian_process.kernels import RBF, WhiteKernel'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_noisy.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='kernel = 1.0 * RBF(length_scale=1e1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(\\n    noise_level=1, noise_level_bounds=(1e-5, 1e1)\\n)\\ngpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)\\ngpr.fit(X_train, y_train)\\ny_mean, y_std = gpr.predict(X, return_std=True)\\n\\n# %%\\nplt.plot(X, y, label=\"Expected signal\")\\nplt.scatter(x=X_train[:, 0], y=y_train, color=\"black\", alpha=0.4, label=\"Observations\")\\nplt.errorbar(X, y_mean, y_std)\\nplt.legend()\\nplt.xlabel(\"X\")\\nplt.ylabel(\"y\")\\n_ = plt.title(\\n    (\\n        f\"Initial: {kernel}\\\\nOptimum: {gpr.kernel_}\\\\nLog-Marginal-Likelihood: \"\\n        f\"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}\"\\n    ),\\n    fontsize=8,\\n)\\n# %%\\n# We see that the optimum kernel found still have a high noise level and\\n# an even larger length scale. Furthermore, we observe that the\\n# model does not provide faithful predictions.\\n#\\n# Now, we will initialize the\\n# :class:`~sklearn.gaussian_process.kernels.RBF` with a\\n# larger `length_scale` and the\\n# :class:`~sklearn.gaussian_process.kernels.WhiteKernel`\\n# with a smaller noise level lower bound.\\nkernel = 1.0 * RBF(length_scale=1e-1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(\\n    noise_level=1e-2, noise_level_bounds=(1e-10, 1e1)\\n)\\ngpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)\\ngpr.fit(X_train, y_train)\\ny_mean, y_std = gpr.predict(X, return_std=True)\\n\\n# %%\\nplt.plot(X, y, label=\"Expected signal\")\\nplt.scatter(x=X_train[:, 0], y=y_train, color=\"black\", alpha=0.4, label=\"Observations\")\\nplt.errorbar(X, y_mean, y_std)\\nplt.legend()\\nplt.xlabel(\"X\")\\nplt.ylabel(\"y\")\\n_ = plt.title(\\n    (\\n        f\"Initial: {kernel}\\\\nOptimum: {gpr.kernel_}\\\\nLog-Marginal-Likelihood: \"\\n        f\"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}\"\\n    ),\\n    fontsize=8,\\n)\\n\\n# %%\\n# First, we see that the model\\'s predictions are more precise than the\\n# previous model\\'s: this new model is able to estimate the noise-free\\n# functional relationship.\\n#\\n# Looking at the kernel hyperparameters, we see that the best combination found\\n# has a smaller noise level and shorter length scale than the first model.\\n#\\n# We can inspect the Log-Marginal-Likelihood (LML) of\\n# :class:`~sklearn.gaussian_process.GaussianProcessRegressor`\\n# for different hyperparameters to get a sense of the local minima.\\nfrom matplotlib.colors import LogNorm\\n\\nlength_scale = np.logspace(-2, 4, num=50)\\nnoise_level = np.logspace(-2, 1, num=50)\\nlength_scale_grid, noise_level_grid = np.meshgrid(length_scale, noise_level)\\n\\nlog_marginal_likelihood = [\\n    gpr.log_marginal_likelihood(theta=np.log([0.36, scale, noise]))\\n    for scale, noise in zip(length_scale_grid.ravel(), noise_level_grid.ravel())\\n]\\nlog_marginal_likelihood = np.reshape(\\n    log_marginal_likelihood, newshape=noise_level_grid.shape\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_noisy.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='log_marginal_likelihood = [\\n    gpr.log_marginal_likelihood(theta=np.log([0.36, scale, noise]))\\n    for scale, noise in zip(length_scale_grid.ravel(), noise_level_grid.ravel())\\n]\\nlog_marginal_likelihood = np.reshape(\\n    log_marginal_likelihood, newshape=noise_level_grid.shape\\n)\\n\\n# %%\\nvmin, vmax = (-log_marginal_likelihood).min(), 50\\nlevel = np.around(np.logspace(np.log10(vmin), np.log10(vmax), num=50), decimals=1)\\nplt.contour(\\n    length_scale_grid,\\n    noise_level_grid,\\n    -log_marginal_likelihood,\\n    levels=level,\\n    norm=LogNorm(vmin=vmin, vmax=vmax),\\n)\\nplt.colorbar()\\nplt.xscale(\"log\")\\nplt.yscale(\"log\")\\nplt.xlabel(\"Length-scale\")\\nplt.ylabel(\"Noise-level\")\\nplt.title(\"Log-marginal-likelihood\")\\nplt.show()\\n\\n# %%\\n# We see that there are two local minima that correspond to the combination\\n# of hyperparameters previously found. Depending on the initial values for the\\n# hyperparameters, the gradient-based optimization might converge whether or\\n# not to the best model. It is thus important to repeat the optimization\\n# several times for different initializations.'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_prior_posterior.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_gpr_samples(gpr_model, n_samples, ax):\\n    \"\"\"Plot samples drawn from the Gaussian process model.\\n\\n    If the Gaussian process model is not trained then the drawn samples are\\n    drawn from the prior distribution. Otherwise, the samples are drawn from\\n    the posterior distribution. Be aware that a sample here corresponds to a\\n    function.\\n\\n    Parameters\\n    ----------\\n    gpr_model : `GaussianProcessRegressor`\\n        A :class:`~sklearn.gaussian_process.GaussianProcessRegressor` model.\\n    n_samples : int\\n        The number of samples to draw from the Gaussian process distribution.\\n    ax : matplotlib axis\\n        The matplotlib axis where to plot the samples.\\n    \"\"\"\\n    x = np.linspace(0, 5, 100)\\n    X = x.reshape(-1, 1)\\n\\n    y_mean, y_std = gpr_model.predict(X, return_std=True)\\n    y_samples = gpr_model.sample_y(X, n_samples)\\n\\n    for idx, single_prior in enumerate(y_samples.T):\\n        ax.plot(\\n            x,\\n            single_prior,\\n            linestyle=\"--\",\\n            alpha=0.7,\\n            label=f\"Sampled function #{idx + 1}\",\\n        )\\n    ax.plot(x, y_mean, color=\"black\", label=\"Mean\")\\n    ax.fill_between(\\n        x,\\n        y_mean - y_std,\\n        y_mean + y_std,\\n        alpha=0.1,\\n        color=\"black\",\\n        label=r\"$\\\\pm$ 1 std. dev.\",\\n    )\\n    ax.set_xlabel(\"x\")\\n    ax.set_ylabel(\"y\")\\n    ax.set_ylim([-3, 3])'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_prior_posterior.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================================================================\\nIllustration of prior and posterior Gaussian process for different kernels\\n==========================================================================\\n\\nThis example illustrates the prior and posterior of a\\n:class:`~sklearn.gaussian_process.GaussianProcessRegressor` with different\\nkernels. Mean, standard deviation, and 5 samples are shown for both prior\\nand posterior distributions.\\n\\nHere, we only give some illustration. To know more about kernels\\' formulation,\\nrefer to the :ref:`User Guide <gp_kernels>`.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Helper function\\n# ---------------\\n#\\n# Before presenting each individual kernel available for Gaussian processes,\\n# we will define an helper function allowing us plotting samples drawn from\\n# the Gaussian process.\\n#\\n# This function will take a\\n# :class:`~sklearn.gaussian_process.GaussianProcessRegressor` model and will\\n# drawn sample from the Gaussian process. If the model was not fit, the samples\\n# are drawn from the prior distribution while after model fitting, the samples are\\n# drawn from the posterior distribution.\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n\\n# Code for: def plot_gpr_samples(gpr_model, n_samples, ax):\\n\\n\\n# %%\\n# Dataset and Gaussian process generation\\n# ---------------------------------------\\n# We will create a training dataset that we will use in the different sections.\\nrng = np.random.RandomState(4)\\nX_train = rng.uniform(0, 5, 10).reshape(-1, 1)\\ny_train = np.sin((X_train[:, 0] - 2.5) ** 2)\\nn_samples = 5\\n\\n# %%\\n# Kernel cookbook\\n# ---------------\\n#\\n# In this section, we illustrate some samples drawn from the prior and posterior\\n# distributions of the Gaussian process with different kernels.\\n#\\n# Radial Basis Function kernel\\n# ............................\\nfrom sklearn.gaussian_process import GaussianProcessRegressor\\nfrom sklearn.gaussian_process.kernels import RBF\\n\\nkernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\\ngpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\\n\\nfig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\\n\\n# plot prior\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\\naxs[0].set_title(\"Samples from prior distribution\")\\n\\n# plot posterior\\ngpr.fit(X_train, y_train)\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\\naxs[1].set_title(\"Samples from posterior distribution\")\\n\\nfig.suptitle(\"Radial Basis Function kernel\", fontsize=18)\\nplt.tight_layout()\\n\\n# %%\\nprint(f\"Kernel parameters before fit:\\\\n{kernel})\")\\nprint(\\n    f\"Kernel parameters after fit: \\\\n{gpr.kernel_} \\\\n\"\\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_prior_posterior.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig.suptitle(\"Radial Basis Function kernel\", fontsize=18)\\nplt.tight_layout()\\n\\n# %%\\nprint(f\"Kernel parameters before fit:\\\\n{kernel})\")\\nprint(\\n    f\"Kernel parameters after fit: \\\\n{gpr.kernel_} \\\\n\"\\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\\n)\\n\\n# %%\\n# Rational Quadratic kernel\\n# .........................\\nfrom sklearn.gaussian_process.kernels import RationalQuadratic\\n\\nkernel = 1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1, alpha_bounds=(1e-5, 1e15))\\ngpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\\n\\nfig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\\n\\n# plot prior\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\\naxs[0].set_title(\"Samples from prior distribution\")\\n\\n# plot posterior\\ngpr.fit(X_train, y_train)\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\\naxs[1].set_title(\"Samples from posterior distribution\")\\n\\nfig.suptitle(\"Rational Quadratic kernel\", fontsize=18)\\nplt.tight_layout()\\n\\n# %%\\nprint(f\"Kernel parameters before fit:\\\\n{kernel})\")\\nprint(\\n    f\"Kernel parameters after fit: \\\\n{gpr.kernel_} \\\\n\"\\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\\n)\\n\\n# %%\\n# Exp-Sine-Squared kernel\\n# .......................\\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\\n\\nkernel = 1.0 * ExpSineSquared(\\n    length_scale=1.0,\\n    periodicity=3.0,\\n    length_scale_bounds=(0.1, 10.0),\\n    periodicity_bounds=(1.0, 10.0),\\n)\\ngpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\\n\\nfig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\\n\\n# plot prior\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\\naxs[0].set_title(\"Samples from prior distribution\")\\n\\n# plot posterior\\ngpr.fit(X_train, y_train)\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\\naxs[1].set_title(\"Samples from posterior distribution\")\\n\\nfig.suptitle(\"Exp-Sine-Squared kernel\", fontsize=18)\\nplt.tight_layout()\\n\\n# %%\\nprint(f\"Kernel parameters before fit:\\\\n{kernel})\")\\nprint(\\n    f\"Kernel parameters after fit: \\\\n{gpr.kernel_} \\\\n\"\\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\\n)\\n\\n# %%\\n# Dot-product kernel\\n# ..................\\nfrom sklearn.gaussian_process.kernels import ConstantKernel, DotProduct\\n\\nkernel = ConstantKernel(0.1, (0.01, 10.0)) * (\\n    DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2\\n)\\ngpr = GaussianProcessRegressor(kernel=kernel, random_state=0, normalize_y=True)\\n\\nfig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\\n\\n# plot prior\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\\naxs[0].set_title(\"Samples from prior distribution\")'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_prior_posterior.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\\n\\n# plot prior\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\\naxs[0].set_title(\"Samples from prior distribution\")\\n\\n# plot posterior\\ngpr.fit(X_train, y_train)\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\\naxs[1].set_title(\"Samples from posterior distribution\")\\n\\nfig.suptitle(\"Dot-product kernel\", fontsize=18)\\nplt.tight_layout()\\n\\n# %%\\nprint(f\"Kernel parameters before fit:\\\\n{kernel})\")\\nprint(\\n    f\"Kernel parameters after fit: \\\\n{gpr.kernel_} \\\\n\"\\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\\n)\\n\\n# %%\\n# Mat√©rn kernel\\n# ..............\\nfrom sklearn.gaussian_process.kernels import Matern\\n\\nkernel = 1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0), nu=1.5)\\ngpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\\n\\nfig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\\n\\n# plot prior\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\\naxs[0].set_title(\"Samples from prior distribution\")\\n\\n# plot posterior\\ngpr.fit(X_train, y_train)\\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\\naxs[1].set_title(\"Samples from posterior distribution\")\\n\\nfig.suptitle(\"Mat√©rn kernel\", fontsize=18)\\nplt.tight_layout()\\n\\n# %%\\nprint(f\"Kernel parameters before fit:\\\\n{kernel})\")\\nprint(\\n    f\"Kernel parameters after fit: \\\\n{gpr.kernel_} \\\\n\"\\n    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpc_xor.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n========================================================================\\nIllustration of Gaussian process classification (GPC) on the XOR dataset\\n========================================================================\\n\\nThis example illustrates GPC on XOR data. Compared are a stationary, isotropic\\nkernel (RBF) and a non-stationary kernel (DotProduct). On this particular\\ndataset, the DotProduct kernel obtains considerably better results because the\\nclass-boundaries are linear and coincide with the coordinate axes. In general,\\nstationary kernels often obtain better results.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.gaussian_process import GaussianProcessClassifier\\nfrom sklearn.gaussian_process.kernels import RBF, DotProduct\\n\\nxx, yy = np.meshgrid(np.linspace(-3, 3, 50), np.linspace(-3, 3, 50))\\nrng = np.random.RandomState(0)\\nX = rng.randn(200, 2)\\nY = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\\n\\n# fit the model\\nplt.figure(figsize=(10, 5))\\nkernels = [1.0 * RBF(length_scale=1.15), 1.0 * DotProduct(sigma_0=1.0) ** 2]\\nfor i, kernel in enumerate(kernels):\\n    clf = GaussianProcessClassifier(kernel=kernel, warm_start=True).fit(X, Y)\\n\\n    # plot the decision function for each datapoint on the grid\\n    Z = clf.predict_proba(np.vstack((xx.ravel(), yy.ravel())).T)[:, 1]\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.subplot(1, 2, i + 1)\\n    image = plt.imshow(\\n        Z,\\n        interpolation=\"nearest\",\\n        extent=(xx.min(), xx.max(), yy.min(), yy.max()),\\n        aspect=\"auto\",\\n        origin=\"lower\",\\n        cmap=plt.cm.PuOr_r,\\n    )\\n    contours = plt.contour(xx, yy, Z, levels=[0.5], linewidths=2, colors=[\"k\"])\\n    plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired, edgecolors=(0, 0, 0))\\n    plt.xticks(())\\n    plt.yticks(())\\n    plt.axis([-3, 3, -3, 3])\\n    plt.colorbar(image)\\n    plt.title(\\n        \"%s\\\\n Log-Marginal-Likelihood:%.3f\"\\n        % (clf.kernel_, clf.log_marginal_likelihood(clf.kernel_.theta)),\\n        fontsize=12,\\n    )\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_co2.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================================================================\\nForecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)\\n====================================================================================\\n\\nThis example is based on Section 5.4.3 of \"Gaussian Processes for Machine\\nLearning\" [1]_. It illustrates an example of complex kernel engineering\\nand hyperparameter optimization using gradient ascent on the\\nlog-marginal-likelihood. The data consists of the monthly average atmospheric\\nCO2 concentrations (in parts per million by volume (ppm)) collected at the\\nMauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to\\nmodel the CO2 concentration as a function of the time :math:`t` and extrapolate\\nfor years after 2001.\\n\\n.. rubric:: References\\n\\n.. [1] `Rasmussen, Carl Edward. \"Gaussian processes in machine learning.\"\\n    Summer school on machine learning. Springer, Berlin, Heidelberg, 2003\\n    <http://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_.\\n\"\"\"\\n\\nprint(__doc__)\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Build the dataset\\n# -----------------\\n#\\n# We will derive a dataset from the Mauna Loa Observatory that collected air\\n# samples. We are interested in estimating the concentration of CO2 and\\n# extrapolate it for further year. First, we load the original dataset available\\n# in OpenML as a pandas dataframe. This will be replaced with Polars\\n# once `fetch_openml` adds a native support for it.\\nfrom sklearn.datasets import fetch_openml\\n\\nco2 = fetch_openml(data_id=41187, as_frame=True)\\nco2.frame.head()\\n\\n# %%\\n# First, we process the original dataframe to create a date column and select\\n# it along with the CO2 column.\\nimport polars as pl\\n\\nco2_data = pl.DataFrame(co2.frame[[\"year\", \"month\", \"day\", \"co2\"]]).select(\\n    pl.date(\"year\", \"month\", \"day\"), \"co2\"\\n)\\nco2_data.head()\\n\\n# %%\\nco2_data[\"date\"].min(), co2_data[\"date\"].max()\\n\\n# %%\\n# We see that we get CO2 concentration for some days from March, 1958 to\\n# December, 2001. We can plot these raw information to have a better\\n# understanding.\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(co2_data[\"date\"], co2_data[\"co2\"])\\nplt.xlabel(\"date\")\\nplt.ylabel(\"CO$_2$ concentration (ppm)\")\\n_ = plt.title(\"Raw air samples measurements from the Mauna Loa Observatory\")\\n\\n# %%\\n# We will preprocess the dataset by taking a monthly average and drop month\\n# for which no measurements were collected. Such a processing will have an\\n# smoothing effect on the data.\\n\\nco2_data = (\\n    co2_data.sort(by=\"date\")\\n    .group_by_dynamic(\"date\", every=\"1mo\")\\n    .agg(pl.col(\"co2\").mean())\\n    .drop_nulls()\\n)\\nplt.plot(co2_data[\"date\"], co2_data[\"co2\"])\\nplt.xlabel(\"date\")\\nplt.ylabel(\"Monthly average of CO$_2$ concentration (ppm)\")\\n_ = plt.title(\\n    \"Monthly average of air samples measurements\\\\nfrom the Mauna Loa Observatory\"\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_co2.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The idea in this example will be to predict the CO2 concentration in function\\n# of the date. We are as well interested in extrapolating for upcoming year\\n# after 2001.\\n#\\n# As a first step, we will divide the data and the target to estimate. The data\\n# being a date, we will convert it into a numeric.\\nX = co2_data.select(\\n    pl.col(\"date\").dt.year() + pl.col(\"date\").dt.month() / 12\\n).to_numpy()\\ny = co2_data[\"co2\"].to_numpy()\\n\\n# %%\\n# Design the proper kernel\\n# ------------------------\\n#\\n# To design the kernel to use with our Gaussian process, we can make some\\n# assumption regarding the data at hand. We observe that they have several\\n# characteristics: we see a long term rising trend, a pronounced seasonal\\n# variation and some smaller irregularities. We can use different appropriate\\n# kernel that would capture these features.\\n#\\n# First, the long term rising trend could be fitted using a radial basis\\n# function (RBF) kernel with a large length-scale parameter. The RBF kernel\\n# with a large length-scale enforces this component to be smooth. An trending\\n# increase is not enforced as to give a degree of freedom to our model. The\\n# specific length-scale and the amplitude are free hyperparameters.\\nfrom sklearn.gaussian_process.kernels import RBF\\n\\nlong_term_trend_kernel = 50.0**2 * RBF(length_scale=50.0)\\n\\n# %%\\n# The seasonal variation is explained by the periodic exponential sine squared\\n# kernel with a fixed periodicity of 1 year. The length-scale of this periodic\\n# component, controlling its smoothness, is a free parameter. In order to allow\\n# decaying away from exact periodicity, the product with an RBF kernel is\\n# taken. The length-scale of this RBF component controls the decay time and is\\n# a further free parameter. This type of kernel is also known as locally\\n# periodic kernel.\\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\\n\\nseasonal_kernel = (\\n    2.0**2\\n    * RBF(length_scale=100.0)\\n    * ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds=\"fixed\")\\n)\\n\\n# %%\\n# The small irregularities are to be explained by a rational quadratic kernel\\n# component, whose length-scale and alpha parameter, which quantifies the\\n# diffuseness of the length-scales, are to be determined. A rational quadratic\\n# kernel is equivalent to an RBF kernel with several length-scale and will\\n# better accommodate the different irregularities.\\nfrom sklearn.gaussian_process.kernels import RationalQuadratic\\n\\nirregularities_kernel = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\\n\\n# %%\\n# Finally, the noise in the dataset can be accounted with a kernel consisting\\n# of an RBF kernel contribution, which shall explain the correlated noise\\n# components such as local weather phenomena, and a white kernel contribution\\n# for the white noise. The relative amplitudes and the RBF\\'s length scale are\\n# further free parameters.\\nfrom sklearn.gaussian_process.kernels import WhiteKernel'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_co2.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='noise_kernel = 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(\\n    noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)\\n)\\n\\n# %%\\n# Thus, our final kernel is an addition of all previous kernel.\\nco2_kernel = (\\n    long_term_trend_kernel + seasonal_kernel + irregularities_kernel + noise_kernel\\n)\\nco2_kernel\\n\\n# %%\\n# Model fitting and extrapolation\\n# -------------------------------\\n#\\n# Now, we are ready to use a Gaussian process regressor and fit the available\\n# data. To follow the example from the literature, we will subtract the mean\\n# from the target. We could have used `normalize_y=True`. However, doing so\\n# would have also scaled the target (dividing `y` by its standard deviation).\\n# Thus, the hyperparameters of the different kernel would have had different\\n# meaning since they would not have been expressed in ppm.\\nfrom sklearn.gaussian_process import GaussianProcessRegressor\\n\\ny_mean = y.mean()\\ngaussian_process = GaussianProcessRegressor(kernel=co2_kernel, normalize_y=False)\\ngaussian_process.fit(X, y - y_mean)\\n\\n# %%\\n# Now, we will use the Gaussian process to predict on:\\n#\\n# - training data to inspect the goodness of fit;\\n# - future data to see the extrapolation done by the model.\\n#\\n# Thus, we create synthetic data from 1958 to the current month. In addition,\\n# we need to add the subtracted mean computed during training.\\nimport datetime\\n\\nimport numpy as np\\n\\ntoday = datetime.datetime.now()\\ncurrent_month = today.year + today.month / 12\\nX_test = np.linspace(start=1958, stop=current_month, num=1_000).reshape(-1, 1)\\nmean_y_pred, std_y_pred = gaussian_process.predict(X_test, return_std=True)\\nmean_y_pred += y_mean\\n\\n# %%\\nplt.plot(X, y, color=\"black\", linestyle=\"dashed\", label=\"Measurements\")\\nplt.plot(X_test, mean_y_pred, color=\"tab:blue\", alpha=0.4, label=\"Gaussian process\")\\nplt.fill_between(\\n    X_test.ravel(),\\n    mean_y_pred - std_y_pred,\\n    mean_y_pred + std_y_pred,\\n    color=\"tab:blue\",\\n    alpha=0.2,\\n)\\nplt.legend()\\nplt.xlabel(\"Year\")\\nplt.ylabel(\"Monthly average of CO$_2$ concentration (ppm)\")\\n_ = plt.title(\\n    \"Monthly average of air samples measurements\\\\nfrom the Mauna Loa Observatory\"\\n)\\n\\n# %%\\n# Our fitted model is capable to fit previous data properly and extrapolate to\\n# future year with confidence.\\n#\\n# Interpretation of kernel hyperparameters\\n# ----------------------------------------\\n#\\n# Now, we can have a look at the hyperparameters of the kernel.\\ngaussian_process.kernel_'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_co2.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Our fitted model is capable to fit previous data properly and extrapolate to\\n# future year with confidence.\\n#\\n# Interpretation of kernel hyperparameters\\n# ----------------------------------------\\n#\\n# Now, we can have a look at the hyperparameters of the kernel.\\ngaussian_process.kernel_\\n\\n# %%\\n# Thus, most of the target signal, with the mean subtracted, is explained by a\\n# long-term rising trend for ~45 ppm and a length-scale of ~52 years. The\\n# periodic component has an amplitude of ~2.6ppm, a decay time of ~90 years and\\n# a length-scale of ~1.5. The long decay time indicates that we have a\\n# component very close to a seasonal periodicity. The correlated noise has an\\n# amplitude of ~0.2 ppm with a length scale of ~0.12 years and a white-noise\\n# contribution of ~0.04 ppm. Thus, the overall noise level is very small,\\n# indicating that the data can be very well explained by the model.'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_noisy_targets.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nGaussian Processes regression: basic introductory example\\n=========================================================\\n\\nA simple one-dimensional regression example computed in two different ways:\\n\\n1. A noise-free case\\n2. A noisy case with known noise-level per datapoint\\n\\nIn both cases, the kernel\\'s parameters are estimated using the maximum\\nlikelihood principle.\\n\\nThe figures illustrate the interpolating property of the Gaussian Process model\\nas well as its probabilistic nature in the form of a pointwise 95% confidence\\ninterval.\\n\\nNote that `alpha` is a parameter to control the strength of the Tikhonov\\nregularization on the assumed training points\\' covariance matrix.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Dataset generation\\n# ------------------\\n#\\n# We will start by generating a synthetic dataset. The true generative process\\n# is defined as :math:`f(x) = x \\\\sin(x)`.\\nimport numpy as np\\n\\nX = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\\ny = np.squeeze(X * np.sin(X))\\n\\n# %%\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(X, y, label=r\"$f(x) = x \\\\sin(x)$\", linestyle=\"dotted\")\\nplt.legend()\\nplt.xlabel(\"$x$\")\\nplt.ylabel(\"$f(x)$\")\\n_ = plt.title(\"True generative process\")\\n\\n# %%\\n# We will use this dataset in the next experiment to illustrate how Gaussian\\n# Process regression is working.\\n#\\n# Example with noise-free target\\n# ------------------------------\\n#\\n# In this first example, we will use the true generative process without\\n# adding any noise. For training the Gaussian Process regression, we will only\\n# select few samples.\\nrng = np.random.RandomState(1)\\ntraining_indices = rng.choice(np.arange(y.size), size=6, replace=False)\\nX_train, y_train = X[training_indices], y[training_indices]\\n\\n# %%\\n# Now, we fit a Gaussian process on these few training data samples. We will\\n# use a radial basis function (RBF) kernel and a constant parameter to fit the\\n# amplitude.\\nfrom sklearn.gaussian_process import GaussianProcessRegressor\\nfrom sklearn.gaussian_process.kernels import RBF\\n\\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\\ngaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\\ngaussian_process.fit(X_train, y_train)\\ngaussian_process.kernel_\\n\\n# %%\\n# After fitting our model, we see that the hyperparameters of the kernel have\\n# been optimized. Now, we will use our kernel to compute the mean prediction\\n# of the full dataset and plot the 95% confidence interval.\\nmean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpr_noisy_targets.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# After fitting our model, we see that the hyperparameters of the kernel have\\n# been optimized. Now, we will use our kernel to compute the mean prediction\\n# of the full dataset and plot the 95% confidence interval.\\nmean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\\n\\nplt.plot(X, y, label=r\"$f(x) = x \\\\sin(x)$\", linestyle=\"dotted\")\\nplt.scatter(X_train, y_train, label=\"Observations\")\\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\\nplt.fill_between(\\n    X.ravel(),\\n    mean_prediction - 1.96 * std_prediction,\\n    mean_prediction + 1.96 * std_prediction,\\n    alpha=0.5,\\n    label=r\"95% confidence interval\",\\n)\\nplt.legend()\\nplt.xlabel(\"$x$\")\\nplt.ylabel(\"$f(x)$\")\\n_ = plt.title(\"Gaussian process regression on noise-free dataset\")\\n\\n# %%\\n# We see that for a prediction made on a data point close to the one from the\\n# training set, the 95% confidence has a small amplitude. Whenever a sample\\n# falls far from training data, our model\\'s prediction is less accurate and the\\n# model prediction is less precise (higher uncertainty).\\n#\\n# Example with noisy targets\\n# --------------------------\\n#\\n# We can repeat a similar experiment adding an additional noise to the target\\n# this time. It will allow seeing the effect of the noise on the fitted model.\\n#\\n# We add some random Gaussian noise to the target with an arbitrary\\n# standard deviation.\\nnoise_std = 0.75\\ny_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)\\n\\n# %%\\n# We create a similar Gaussian process model. In addition to the kernel, this\\n# time, we specify the parameter `alpha` which can be interpreted as the\\n# variance of a Gaussian noise.\\ngaussian_process = GaussianProcessRegressor(\\n    kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9\\n)\\ngaussian_process.fit(X_train, y_train_noisy)\\nmean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\\n\\n# %%\\n# Let\\'s plot the mean prediction and the uncertainty region as before.\\nplt.plot(X, y, label=r\"$f(x) = x \\\\sin(x)$\", linestyle=\"dotted\")\\nplt.errorbar(\\n    X_train,\\n    y_train_noisy,\\n    noise_std,\\n    linestyle=\"None\",\\n    color=\"tab:blue\",\\n    marker=\".\",\\n    markersize=10,\\n    label=\"Observations\",\\n)\\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\\nplt.fill_between(\\n    X.ravel(),\\n    mean_prediction - 1.96 * std_prediction,\\n    mean_prediction + 1.96 * std_prediction,\\n    color=\"tab:orange\",\\n    alpha=0.5,\\n    label=r\"95% confidence interval\",\\n)\\nplt.legend()\\nplt.xlabel(\"$x$\")\\nplt.ylabel(\"$f(x)$\")\\n_ = plt.title(\"Gaussian process regression on a noisy dataset\")\\n\\n# %%\\n# The noise affects the predictions close to the training samples: the\\n# predictive uncertainty near to the training samples is larger because we\\n# explicitly model a given level target noise independent of the input\\n# variable.'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_compare_gpr_krr.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================================================\\nComparison of kernel ridge and Gaussian process regression\\n==========================================================\\n\\nThis example illustrates differences between a kernel ridge regression and a\\nGaussian process regression.\\n\\nBoth kernel ridge regression and Gaussian process regression are using a\\nso-called \"kernel trick\" to make their models expressive enough to fit\\nthe training data. However, the machine learning problems solved by the two\\nmethods are drastically different.\\n\\nKernel ridge regression will find the target function that minimizes a loss\\nfunction (the mean squared error).\\n\\nInstead of finding a single target function, the Gaussian process regression\\nemploys a probabilistic approach : a Gaussian posterior distribution over\\ntarget functions is defined based on the Bayes\\' theorem, Thus prior\\nprobabilities on target functions are being combined with a likelihood function\\ndefined by the observed training data to provide estimates of the posterior\\ndistributions.\\n\\nWe will illustrate these differences with an example and we will also focus on\\ntuning the kernel hyperparameters.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generating a dataset\\n# --------------------\\n#\\n# We create a synthetic dataset. The true generative process will take a 1-D\\n# vector and compute its sine. Note that the period of this sine is thus\\n# :math:`2 \\\\pi`. We will reuse this information later in this example.\\nimport numpy as np\\n\\nrng = np.random.RandomState(0)\\ndata = np.linspace(0, 30, num=1_000).reshape(-1, 1)\\ntarget = np.sin(data).ravel()\\n\\n# %%\\n# Now, we can imagine a scenario where we get observations from this true\\n# process. However, we will add some challenges:\\n#\\n# - the measurements will be noisy;\\n# - only samples from the beginning of the signal will be available.\\ntraining_sample_indices = rng.choice(np.arange(0, 400), size=40, replace=False)\\ntraining_data = data[training_sample_indices]\\ntraining_noisy_target = target[training_sample_indices] + 0.5 * rng.randn(\\n    len(training_sample_indices)\\n)\\n\\n# %%\\n# Let\\'s plot the true signal and the noisy measurements available for training.\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(data, target, label=\"True signal\", linewidth=2)\\nplt.scatter(\\n    training_data,\\n    training_noisy_target,\\n    color=\"black\",\\n    label=\"Noisy measurements\",\\n)\\nplt.legend()\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\n_ = plt.title(\\n    \"Illustration of the true generative process and \\\\n\"\\n    \"noisy measurements available during training\"\\n)\\n\\n# %%\\n# Limitations of a simple linear model\\n# ------------------------------------\\n#\\n# First, we would like to highlight the limitations of a linear model given\\n# our dataset. We fit a :class:`~sklearn.linear_model.Ridge` and check the\\n# predictions of this model on our dataset.\\nfrom sklearn.linear_model import Ridge\\n\\nridge = Ridge().fit(training_data, training_noisy_target)'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_compare_gpr_krr.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='ridge = Ridge().fit(training_data, training_noisy_target)\\n\\nplt.plot(data, target, label=\"True signal\", linewidth=2)\\nplt.scatter(\\n    training_data,\\n    training_noisy_target,\\n    color=\"black\",\\n    label=\"Noisy measurements\",\\n)\\nplt.plot(data, ridge.predict(data), label=\"Ridge regression\")\\nplt.legend()\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\n_ = plt.title(\"Limitation of a linear model such as ridge\")\\n\\n# %%\\n# Such a ridge regressor underfits data since it is not expressive enough.\\n#\\n# Kernel methods: kernel ridge and Gaussian process\\n# -------------------------------------------------\\n#\\n# Kernel ridge\\n# ............\\n#\\n# We can make the previous linear model more expressive by using a so-called\\n# kernel. A kernel is an embedding from the original feature space to another\\n# one. Simply put, it is used to map our original data into a newer and more\\n# complex feature space. This new space is explicitly defined by the choice of\\n# kernel.\\n#\\n# In our case, we know that the true generative process is a periodic function.\\n# We can use a :class:`~sklearn.gaussian_process.kernels.ExpSineSquared` kernel\\n# which allows recovering the periodicity. The class\\n# :class:`~sklearn.kernel_ridge.KernelRidge` will accept such a kernel.\\n#\\n# Using this model together with a kernel is equivalent to embed the data\\n# using the mapping function of the kernel and then apply a ridge regression.\\n# In practice, the data are not mapped explicitly; instead the dot product\\n# between samples in the higher dimensional feature space is computed using the\\n# \"kernel trick\".\\n#\\n# Thus, let\\'s use such a :class:`~sklearn.kernel_ridge.KernelRidge`.\\nimport time\\n\\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\\nfrom sklearn.kernel_ridge import KernelRidge\\n\\nkernel_ridge = KernelRidge(kernel=ExpSineSquared())\\n\\nstart_time = time.time()\\nkernel_ridge.fit(training_data, training_noisy_target)\\nprint(\\n    f\"Fitting KernelRidge with default kernel: {time.time() - start_time:.3f} seconds\"\\n)\\n\\n# %%\\nplt.plot(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\\nplt.scatter(\\n    training_data,\\n    training_noisy_target,\\n    color=\"black\",\\n    label=\"Noisy measurements\",\\n)\\nplt.plot(\\n    data,\\n    kernel_ridge.predict(data),\\n    label=\"Kernel ridge\",\\n    linewidth=2,\\n    linestyle=\"dashdot\",\\n)\\nplt.legend(loc=\"lower right\")\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\n_ = plt.title(\\n    \"Kernel ridge regression with an exponential sine squared\\\\n \"\\n    \"kernel using default hyperparameters\"\\n)\\n\\n# %%\\n# This fitted model is not accurate. Indeed, we did not set the parameters of\\n# the kernel and instead used the default ones. We can inspect them.\\nkernel_ridge.kernel'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_compare_gpr_krr.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# This fitted model is not accurate. Indeed, we did not set the parameters of\\n# the kernel and instead used the default ones. We can inspect them.\\nkernel_ridge.kernel\\n\\n# %%\\n# Our kernel has two parameters: the length-scale and the periodicity. For our\\n# dataset, we use `sin` as the generative process, implying a\\n# :math:`2 \\\\pi`-periodicity for the signal. The default value of the parameter\\n# being :math:`1`, it explains the high frequency observed in the predictions of\\n# our model.\\n# Similar conclusions could be drawn with the length-scale parameter. Thus, it\\n# tell us that the kernel parameters need to be tuned. We will use a randomized\\n# search to tune the different parameters the kernel ridge model: the `alpha`\\n# parameter and the kernel parameters.\\n\\n# %%\\nfrom scipy.stats import loguniform\\n\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\nparam_distributions = {\\n    \"alpha\": loguniform(1e0, 1e3),\\n    \"kernel__length_scale\": loguniform(1e-2, 1e2),\\n    \"kernel__periodicity\": loguniform(1e0, 1e1),\\n}\\nkernel_ridge_tuned = RandomizedSearchCV(\\n    kernel_ridge,\\n    param_distributions=param_distributions,\\n    n_iter=500,\\n    random_state=0,\\n)\\nstart_time = time.time()\\nkernel_ridge_tuned.fit(training_data, training_noisy_target)\\nprint(f\"Time for KernelRidge fitting: {time.time() - start_time:.3f} seconds\")\\n\\n# %%\\n# Fitting the model is now more computationally expensive since we have to try\\n# several combinations of hyperparameters. We can have a look at the\\n# hyperparameters found to get some intuitions.\\nkernel_ridge_tuned.best_params_\\n\\n# %%\\n# Looking at the best parameters, we see that they are different from the\\n# defaults. We also see that the periodicity is closer to the expected value:\\n# :math:`2 \\\\pi`. We can now inspect the predictions of our tuned kernel ridge.\\nstart_time = time.time()\\npredictions_kr = kernel_ridge_tuned.predict(data)\\nprint(f\"Time for KernelRidge predict: {time.time() - start_time:.3f} seconds\")\\n\\n# %%\\nplt.plot(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\\nplt.scatter(\\n    training_data,\\n    training_noisy_target,\\n    color=\"black\",\\n    label=\"Noisy measurements\",\\n)\\nplt.plot(\\n    data,\\n    predictions_kr,\\n    label=\"Kernel ridge\",\\n    linewidth=2,\\n    linestyle=\"dashdot\",\\n)\\nplt.legend(loc=\"lower right\")\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\n_ = plt.title(\\n    \"Kernel ridge regression with an exponential sine squared\\\\n \"\\n    \"kernel using tuned hyperparameters\"\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_compare_gpr_krr.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We get a much more accurate model. We still observe some errors mainly due to\\n# the noise added to the dataset.\\n#\\n# Gaussian process regression\\n# ...........................\\n#\\n# Now, we will use a\\n# :class:`~sklearn.gaussian_process.GaussianProcessRegressor` to fit the same\\n# dataset. When training a Gaussian process, the hyperparameters of the kernel\\n# are optimized during the fitting process. There is no need for an external\\n# hyperparameter search. Here, we create a slightly more complex kernel than\\n# for the kernel ridge regressor: we add a\\n# :class:`~sklearn.gaussian_process.kernels.WhiteKernel` that is used to\\n# estimate the noise in the dataset.\\nfrom sklearn.gaussian_process import GaussianProcessRegressor\\nfrom sklearn.gaussian_process.kernels import WhiteKernel\\n\\nkernel = 1.0 * ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) + WhiteKernel(\\n    1e-1\\n)\\ngaussian_process = GaussianProcessRegressor(kernel=kernel)\\nstart_time = time.time()\\ngaussian_process.fit(training_data, training_noisy_target)\\nprint(\\n    f\"Time for GaussianProcessRegressor fitting: {time.time() - start_time:.3f} seconds\"\\n)\\n\\n# %%\\n# The computation cost of training a Gaussian process is much less than the\\n# kernel ridge that uses a randomized search. We can check the parameters of\\n# the kernels that we computed.\\ngaussian_process.kernel_\\n\\n# %%\\n# Indeed, we see that the parameters have been optimized. Looking at the\\n# `periodicity` parameter, we see that we found a period close to the\\n# theoretical value :math:`2 \\\\pi`. We can have a look now at the predictions of\\n# our model.\\nstart_time = time.time()\\nmean_predictions_gpr, std_predictions_gpr = gaussian_process.predict(\\n    data,\\n    return_std=True,\\n)\\nprint(\\n    f\"Time for GaussianProcessRegressor predict: {time.time() - start_time:.3f} seconds\"\\n)\\n\\n# %%\\nplt.plot(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\\nplt.scatter(\\n    training_data,\\n    training_noisy_target,\\n    color=\"black\",\\n    label=\"Noisy measurements\",\\n)\\n# Plot the predictions of the kernel ridge\\nplt.plot(\\n    data,\\n    predictions_kr,\\n    label=\"Kernel ridge\",\\n    linewidth=2,\\n    linestyle=\"dashdot\",\\n)\\n# Plot the predictions of the gaussian process regressor\\nplt.plot(\\n    data,\\n    mean_predictions_gpr,\\n    label=\"Gaussian process regressor\",\\n    linewidth=2,\\n    linestyle=\"dotted\",\\n)\\nplt.fill_between(\\n    data.ravel(),\\n    mean_predictions_gpr - std_predictions_gpr,\\n    mean_predictions_gpr + std_predictions_gpr,\\n    color=\"tab:green\",\\n    alpha=0.2,\\n)\\nplt.legend(loc=\"lower right\")\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\n_ = plt.title(\"Comparison between kernel ridge and gaussian process regressor\")'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_compare_gpr_krr.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We observe that the results of the kernel ridge and the Gaussian process\\n# regressor are close. However, the Gaussian process regressor also provide\\n# an uncertainty information that is not available with a kernel ridge.\\n# Due to the probabilistic formulation of the target functions, the\\n# Gaussian process can output the standard deviation (or the covariance)\\n# together with the mean predictions of the target functions.\\n#\\n# However, it comes at a cost: the time to compute the predictions is higher\\n# with a Gaussian process.\\n#\\n# Final conclusion\\n# ----------------\\n#\\n# We can give a final word regarding the possibility of the two models to\\n# extrapolate. Indeed, we only provided the beginning of the signal as a\\n# training set. Using a periodic kernel forces our model to repeat the pattern\\n# found on the training set. Using this kernel information together with the\\n# capacity of the both models to extrapolate, we observe that the models will\\n# continue to predict the sine pattern.\\n#\\n# Gaussian process allows to combine kernels together. Thus, we could associate\\n# the exponential sine squared kernel together with a radial basis function\\n# kernel.\\nfrom sklearn.gaussian_process.kernels import RBF\\n\\nkernel = 1.0 * ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) * RBF(\\n    length_scale=15, length_scale_bounds=\"fixed\"\\n) + WhiteKernel(1e-1)\\ngaussian_process = GaussianProcessRegressor(kernel=kernel)\\ngaussian_process.fit(training_data, training_noisy_target)\\nmean_predictions_gpr, std_predictions_gpr = gaussian_process.predict(\\n    data,\\n    return_std=True,\\n)\\n\\n# %%\\nplt.plot(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\\nplt.scatter(\\n    training_data,\\n    training_noisy_target,\\n    color=\"black\",\\n    label=\"Noisy measurements\",\\n)\\n# Plot the predictions of the kernel ridge\\nplt.plot(\\n    data,\\n    predictions_kr,\\n    label=\"Kernel ridge\",\\n    linewidth=2,\\n    linestyle=\"dashdot\",\\n)\\n# Plot the predictions of the gaussian process regressor\\nplt.plot(\\n    data,\\n    mean_predictions_gpr,\\n    label=\"Gaussian process regressor\",\\n    linewidth=2,\\n    linestyle=\"dotted\",\\n)\\nplt.fill_between(\\n    data.ravel(),\\n    mean_predictions_gpr - std_predictions_gpr,\\n    mean_predictions_gpr + std_predictions_gpr,\\n    color=\"tab:green\",\\n    alpha=0.2,\\n)\\nplt.legend(loc=\"lower right\")\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\n_ = plt.title(\"Effect of using a radial basis function kernel\")\\n\\n# %%\\n# The effect of using a radial basis function kernel will attenuate the\\n# periodicity effect once that no sample are available in the training.\\n# As testing samples get further away from the training ones, predictions\\n# are converging towards their mean and their standard deviation\\n# also increases.'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpc_isoprobability.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def g(x):\\n    \"\"\"The function to predict (classification will then consist in predicting\\n    whether g(x) <= 0 or not)\"\"\"\\n    return 5.0 - x[:, 1] - 0.5 * x[:, 0] ** 2.0'), Document(metadata={'source': '/content/local_copy_repo/examples/gaussian_process/plot_gpc_isoprobability.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================================\\nIso-probability lines for Gaussian Processes classification (GPC)\\n=================================================================\\n\\nA two-dimensional classification example showing iso-probability lines for\\nthe predicted probabilities.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport numpy as np\\nfrom matplotlib import cm\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn.gaussian_process import GaussianProcessClassifier\\nfrom sklearn.gaussian_process.kernels import ConstantKernel as C\\nfrom sklearn.gaussian_process.kernels import DotProduct\\n\\n# A few constants\\nlim = 8\\n\\n\\n# Code for: def g(x):\\n\\n\\n# Design of experiments\\nX = np.array(\\n    [\\n        [-4.61611719, -6.00099547],\\n        [4.10469096, 5.32782448],\\n        [0.00000000, -0.50000000],\\n        [-6.17289014, -4.6984743],\\n        [1.3109306, -6.93271427],\\n        [-5.03823144, 3.10584743],\\n        [-2.87600388, 6.74310541],\\n        [5.21301203, 4.26386883],\\n    ]\\n)\\n\\n# Observations\\ny = np.array(g(X) > 0, dtype=int)\\n\\n# Instantiate and fit Gaussian Process Model\\nkernel = C(0.1, (1e-5, np.inf)) * DotProduct(sigma_0=0.1) ** 2\\ngp = GaussianProcessClassifier(kernel=kernel)\\ngp.fit(X, y)\\nprint(\"Learned kernel: %s \" % gp.kernel_)\\n\\n# Evaluate real function and the predicted probability\\nres = 50\\nx1, x2 = np.meshgrid(np.linspace(-lim, lim, res), np.linspace(-lim, lim, res))\\nxx = np.vstack([x1.reshape(x1.size), x2.reshape(x2.size)]).T\\n\\ny_true = g(xx)\\ny_prob = gp.predict_proba(xx)[:, 1]\\ny_true = y_true.reshape((res, res))\\ny_prob = y_prob.reshape((res, res))\\n\\n# Plot the probabilistic classification iso-values\\nfig = plt.figure(1)\\nax = fig.gca()\\nax.axes.set_aspect(\"equal\")\\nplt.xticks([])\\nplt.yticks([])\\nax.set_xticklabels([])\\nax.set_yticklabels([])\\nplt.xlabel(\"$x_1$\")\\nplt.ylabel(\"$x_2$\")\\n\\ncax = plt.imshow(y_prob, cmap=cm.gray_r, alpha=0.8, extent=(-lim, lim, -lim, lim))\\nnorm = plt.matplotlib.colors.Normalize(vmin=0.0, vmax=0.9)\\ncb = plt.colorbar(cax, ticks=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], norm=norm)\\ncb.set_label(r\"${\\\\rm \\\\mathbb{P}}\\\\left[\\\\widehat{G}(\\\\mathbf{x}) \\\\leq 0\\\\right]$\")\\nplt.clim(0, 1)\\n\\nplt.plot(X[y <= 0, 0], X[y <= 0, 1], \"r.\", markersize=12)\\n\\nplt.plot(X[y > 0, 0], X[y > 0, 1], \"b.\", markersize=12)\\n\\nplt.contour(x1, x2, y_true, [0.0], colors=\"k\", linestyles=\"dashdot\")\\n\\ncs = plt.contour(x1, x2, y_prob, [0.666], colors=\"b\", linestyles=\"solid\")\\nplt.clabel(cs, fontsize=11)\\n\\ncs = plt.contour(x1, x2, y_prob, [0.5], colors=\"k\", linestyles=\"dashed\")\\nplt.clabel(cs, fontsize=11)\\n\\ncs = plt.contour(x1, x2, y_prob, [0.334], colors=\"r\", linestyles=\"solid\")\\nplt.clabel(cs, fontsize=11)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_digits.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def print_dataframe(filtered_cv_results):\\n    \"\"\"Pretty print for filtered dataframe\"\"\"\\n    for mean_precision, std_precision, mean_recall, std_recall, params in zip(\\n        filtered_cv_results[\"mean_test_precision\"],\\n        filtered_cv_results[\"std_test_precision\"],\\n        filtered_cv_results[\"mean_test_recall\"],\\n        filtered_cv_results[\"std_test_recall\"],\\n        filtered_cv_results[\"params\"],\\n    ):\\n        print(\\n            f\"precision: {mean_precision:0.3f} (¬±{std_precision:0.03f}),\"\\n            f\" recall: {mean_recall:0.3f} (¬±{std_recall:0.03f}),\"\\n            f\" for {params}\"\\n        )\\n    print()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_digits.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def refit_strategy(cv_results):\\n    \"\"\"Define the strategy to select the best estimator.\\n\\n    The strategy defined here is to filter-out all results below a precision threshold\\n    of 0.98, rank the remaining by recall and keep all models with one standard\\n    deviation of the best by recall. Once these models are selected, we can select the\\n    fastest model to predict.\\n\\n    Parameters\\n    ----------\\n    cv_results : dict of numpy (masked) ndarrays\\n        CV results as returned by the `GridSearchCV`.\\n\\n    Returns\\n    -------\\n    best_index : int\\n        The index of the best estimator as it appears in `cv_results`.\\n    \"\"\"\\n    # print the info about the grid-search for the different scores\\n    precision_threshold = 0.98\\n\\n    cv_results_ = pd.DataFrame(cv_results)\\n    print(\"All grid-search results:\")\\n    print_dataframe(cv_results_)\\n\\n    # Filter-out all results below the threshold\\n    high_precision_cv_results = cv_results_[\\n        cv_results_[\"mean_test_precision\"] > precision_threshold\\n    ]\\n\\n    print(f\"Models with a precision higher than {precision_threshold}:\")\\n    print_dataframe(high_precision_cv_results)\\n\\n    high_precision_cv_results = high_precision_cv_results[\\n        [\\n            \"mean_score_time\",\\n            \"mean_test_recall\",\\n            \"std_test_recall\",\\n            \"mean_test_precision\",\\n            \"std_test_precision\",\\n            \"rank_test_recall\",\\n            \"rank_test_precision\",\\n            \"params\",\\n        ]\\n    ]\\n\\n    # Select the most performant models in terms of recall\\n    # (within 1 sigma from the best)\\n    best_recall_std = high_precision_cv_results[\"mean_test_recall\"].std()\\n    best_recall = high_precision_cv_results[\"mean_test_recall\"].max()\\n    best_recall_threshold = best_recall - best_recall_std\\n\\n    high_recall_cv_results = high_precision_cv_results[\\n        high_precision_cv_results[\"mean_test_recall\"] > best_recall_threshold\\n    ]\\n    print(\\n        \"Out of the previously selected high precision models, we keep all the\\\\n\"\\n        \"the models within one standard deviation of the highest recall model:\"\\n    )\\n    print_dataframe(high_recall_cv_results)\\n\\n    # From the best candidates, select the fastest model to predict\\n    fastest_top_recall_high_precision_index = high_recall_cv_results[\\n        \"mean_score_time\"\\n    ].idxmin()\\n\\n    print(\\n        \"\\\\nThe selected final model is the fastest to predict out of the previously\\\\n\"\\n        \"selected subset of best models based on precision and recall.\\\\n\"\\n        \"Its scoring time is:\\\\n\\\\n\"\\n        f\"{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}\"\\n    )\\n\\n    return fastest_top_recall_high_precision_index'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_digits.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================================================\\nCustom refit strategy of a grid search with cross-validation\\n============================================================\\n\\nThis examples shows how a classifier is optimized by cross-validation,\\nwhich is done using the :class:`~sklearn.model_selection.GridSearchCV` object\\non a development set that comprises only half of the available labeled data.\\n\\nThe performance of the selected hyper-parameters and trained model is\\nthen measured on a dedicated evaluation set that was not used during\\nthe model selection step.\\n\\nMore details on tools available for model selection can be found in the\\nsections on :ref:`cross_validation` and :ref:`grid_search`.\\n\"\"\"\\n\\n# %%\\n# The dataset\\n# -----------\\n#\\n# We will work with the `digits` dataset. The goal is to classify handwritten\\n# digits images.\\n# We transform the problem into a binary classification for easier\\n# understanding: the goal is to identify whether a digit is `8` or not.\\nfrom sklearn import datasets\\n\\ndigits = datasets.load_digits()\\n\\n# %%\\n# In order to train a classifier on images, we need to flatten them into vectors.\\n# Each image of 8 by 8 pixels needs to be transformed to a vector of 64 pixels.\\n# Thus, we will get a final data array of shape `(n_images, n_pixels)`.\\nn_samples = len(digits.images)\\nX = digits.images.reshape((n_samples, -1))\\ny = digits.target == 8\\nprint(\\n    f\"The number of images is {X.shape[0]} and each image contains {X.shape[1]} pixels\"\\n)\\n\\n# %%\\n# As presented in the introduction, the data will be split into a training\\n# and a testing set of equal size.\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\\n\\n# %%\\n# Define our grid-search strategy\\n# -------------------------------\\n#\\n# We will select a classifier by searching the best hyper-parameters on folds\\n# of the training set. To do this, we need to define\\n# the scores to select the best candidate.\\n\\nscores = [\"precision\", \"recall\"]\\n\\n# %%\\n# We can also define a function to be passed to the `refit` parameter of the\\n# :class:`~sklearn.model_selection.GridSearchCV` instance. It will implement the\\n# custom strategy to select the best candidate from the `cv_results_` attribute\\n# of the :class:`~sklearn.model_selection.GridSearchCV`. Once the candidate is\\n# selected, it is automatically refitted by the\\n# :class:`~sklearn.model_selection.GridSearchCV` instance.\\n#\\n# Here, the strategy is to short-list the models which are the best in terms of\\n# precision and recall. From the selected models, we finally select the fastest\\n# model at predicting. Notice that these custom choices are completely\\n# arbitrary.\\n\\nimport pandas as pd\\n\\n\\n# Code for: def print_dataframe(filtered_cv_results):\\n\\n\\n# Code for: def refit_strategy(cv_results):'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_digits.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import pandas as pd\\n\\n\\n# Code for: def print_dataframe(filtered_cv_results):\\n\\n\\n# Code for: def refit_strategy(cv_results):\\n\\n\\n# %%\\n#\\n# Tuning hyper-parameters\\n# -----------------------\\n#\\n# Once we defined our strategy to select the best model, we define the values\\n# of the hyper-parameters and create the grid-search instance:\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.svm import SVC\\n\\ntuned_parameters = [\\n    {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100, 1000]},\\n    {\"kernel\": [\"linear\"], \"C\": [1, 10, 100, 1000]},\\n]\\n\\ngrid_search = GridSearchCV(\\n    SVC(), tuned_parameters, scoring=scores, refit=refit_strategy\\n)\\ngrid_search.fit(X_train, y_train)\\n\\n# %%\\n#\\n# The parameters selected by the grid-search with our custom strategy are:\\ngrid_search.best_params_\\n\\n# %%\\n#\\n# Finally, we evaluate the fine-tuned model on the left-out evaluation set: the\\n# `grid_search` object **has automatically been refit** on the full training\\n# set with the parameters selected by our custom refit strategy.\\n#\\n# We can use the classification report to compute standard classification\\n# metrics on the left-out set:\\nfrom sklearn.metrics import classification_report\\n\\ny_pred = grid_search.predict(X_test)\\nprint(classification_report(y_test, y_pred))\\n\\n# %%\\n# .. note::\\n#    The problem is too easy: the hyperparameter plateau is too flat and the\\n#    output model is the same for precision and recall with ties in quality.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_multi_metric_evaluation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================================================================\\nDemonstration of multi-metric evaluation on cross_val_score and GridSearchCV\\n============================================================================\\n\\nMultiple metric parameter search can be done by setting the ``scoring``\\nparameter to a list of metric scorer names or a dict mapping the scorer names\\nto the scorer callables.\\n\\nThe scores of all the scorers are available in the ``cv_results_`` dict at keys\\nending in ``\\'_<scorer_name>\\'`` (``\\'mean_test_precision\\'``,\\n``\\'rank_test_precision\\'``, etc...)\\n\\nThe ``best_estimator_``, ``best_index_``, ``best_score_`` and ``best_params_``\\ncorrespond to the scorer (key) that is set to the ``refit`` attribute.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn.datasets import make_hastie_10_2\\nfrom sklearn.metrics import accuracy_score, make_scorer\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# %%\\n# Running ``GridSearchCV`` using multiple evaluation metrics\\n# ----------------------------------------------------------\\n#\\n\\nX, y = make_hastie_10_2(n_samples=8000, random_state=42)\\n\\n# The scorers can be either one of the predefined metric strings or a scorer\\n# callable, like the one returned by make_scorer\\nscoring = {\"AUC\": \"roc_auc\", \"Accuracy\": make_scorer(accuracy_score)}\\n\\n# Setting refit=\\'AUC\\', refits an estimator on the whole dataset with the\\n# parameter setting that has the best cross-validated AUC score.\\n# That estimator is made available at ``gs.best_estimator_`` along with\\n# parameters like ``gs.best_score_``, ``gs.best_params_`` and\\n# ``gs.best_index_``\\ngs = GridSearchCV(\\n    DecisionTreeClassifier(random_state=42),\\n    param_grid={\"min_samples_split\": range(2, 403, 20)},\\n    scoring=scoring,\\n    refit=\"AUC\",\\n    n_jobs=2,\\n    return_train_score=True,\\n)\\ngs.fit(X, y)\\nresults = gs.cv_results_\\n\\n# %%\\n# Plotting the result\\n# -------------------\\n\\nplt.figure(figsize=(13, 13))\\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\", fontsize=16)\\n\\nplt.xlabel(\"min_samples_split\")\\nplt.ylabel(\"Score\")\\n\\nax = plt.gca()\\nax.set_xlim(0, 402)\\nax.set_ylim(0.73, 1)\\n\\n# Get the regular numpy array from the MaskedArray\\nX_axis = np.array(results[\"param_min_samples_split\"].data, dtype=float)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_multi_metric_evaluation.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.xlabel(\"min_samples_split\")\\nplt.ylabel(\"Score\")\\n\\nax = plt.gca()\\nax.set_xlim(0, 402)\\nax.set_ylim(0.73, 1)\\n\\n# Get the regular numpy array from the MaskedArray\\nX_axis = np.array(results[\"param_min_samples_split\"].data, dtype=float)\\n\\nfor scorer, color in zip(sorted(scoring), [\"g\", \"k\"]):\\n    for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\\n        sample_score_mean = results[\"mean_%s_%s\" % (sample, scorer)]\\n        sample_score_std = results[\"std_%s_%s\" % (sample, scorer)]\\n        ax.fill_between(\\n            X_axis,\\n            sample_score_mean - sample_score_std,\\n            sample_score_mean + sample_score_std,\\n            alpha=0.1 if sample == \"test\" else 0,\\n            color=color,\\n        )\\n        ax.plot(\\n            X_axis,\\n            sample_score_mean,\\n            style,\\n            color=color,\\n            alpha=1 if sample == \"test\" else 0.7,\\n            label=\"%s (%s)\" % (scorer, sample),\\n        )\\n\\n    best_index = np.nonzero(results[\"rank_test_%s\" % scorer] == 1)[0][0]\\n    best_score = results[\"mean_test_%s\" % scorer][best_index]\\n\\n    # Plot a dotted vertical line at the best score for that scorer marked by x\\n    ax.plot(\\n        [\\n            X_axis[best_index],\\n        ]\\n        * 2,\\n        [0, best_score],\\n        linestyle=\"-.\",\\n        color=color,\\n        marker=\"x\",\\n        markeredgewidth=3,\\n        ms=8,\\n    )\\n\\n    # Annotate the best score for that scorer\\n    ax.annotate(\"%0.2f\" % best_score, (X_axis[best_index], best_score + 0.005))\\n\\nplt.legend(loc=\"best\")\\nplt.grid(False)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_likelihood_ratios.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def scoring(estimator, X, y):\\n    y_pred = estimator.predict(X)\\n    pos_lr, neg_lr = class_likelihood_ratios(y, y_pred, raise_warning=False)\\n    return {\"positive_likelihood_ratio\": pos_lr, \"negative_likelihood_ratio\": neg_lr}'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_likelihood_ratios.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def extract_score(cv_results):\\n    lr = pd.DataFrame(\\n        {\\n            \"positive\": cv_results[\"test_positive_likelihood_ratio\"],\\n            \"negative\": cv_results[\"test_negative_likelihood_ratio\"],\\n        }\\n    )\\n    return lr.aggregate([\"mean\", \"std\"])'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_likelihood_ratios.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def scoring_on_bootstrap(estimator, X, y, rng, n_bootstrap=100):\\n    results_for_prevalence = defaultdict(list)\\n    for _ in range(n_bootstrap):\\n        bootstrap_indices = rng.choice(\\n            np.arange(X.shape[0]), size=X.shape[0], replace=True\\n        )\\n        for key, value in scoring(\\n            estimator, X[bootstrap_indices], y[bootstrap_indices]\\n        ).items():\\n            results_for_prevalence[key].append(value)\\n    return pd.DataFrame(results_for_prevalence)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_likelihood_ratios.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================================\\nClass Likelihood Ratios to measure classification performance\\n=============================================================\\n\\nThis example demonstrates the :func:`~sklearn.metrics.class_likelihood_ratios`\\nfunction, which computes the positive and negative likelihood ratios (`LR+`,\\n`LR-`) to assess the predictive power of a binary classifier. As we will see,\\nthese metrics are independent of the proportion between classes in the test set,\\nwhich makes them very useful when the available data for a study has a different'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_likelihood_ratios.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='class proportion than the target application.\\n\\nA typical use is a case-control study in medicine, which has nearly balanced\\nclasses while the general population has large class imbalance. In such\\napplication, the pre-test probability of an individual having the target\\ncondition can be chosen to be the prevalence, i.e. the proportion of a\\nparticular population found to be affected by a medical condition. The post-test\\nprobabilities represent then the probability that the condition is truly present\\ngiven a positive test result.\\n\\nIn this example we first discuss the link between pre-test and post-test odds\\ngiven by the :ref:`class_likelihood_ratios`. Then we evaluate their behavior in\\nsome controlled scenarios. In the last section we plot them as a function of the\\nprevalence of the positive class.\\n\\n\"\"\"\\n\\n# Authors:  Arturo Amor <david-arturo.amor-quiroz@inria.fr>\\n#           Olivier Grisel <olivier.grisel@ensta.org>\\n# %%\\n# Pre-test vs. post-test analysis\\n# ===============================\\n#\\n# Suppose we have a population of subjects with physiological measurements `X`\\n# that can hopefully serve as indirect bio-markers of the disease and actual\\n# disease indicators `y` (ground truth). Most of the people in the population do\\n# not carry the disease but a minority (in this case around 10%) does:\\n\\nfrom sklearn.datasets import make_classification\\n\\nX, y = make_classification(n_samples=10_000, weights=[0.9, 0.1], random_state=0)\\nprint(f\"Percentage of people carrying the disease: {100*y.mean():.2f}%\")\\n\\n# %%\\n# A machine learning model is built to diagnose if a person with some given\\n# physiological measurements is likely to carry the disease of interest. To\\n# evaluate the model, we need to assess its performance on a held-out test set:\\n\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\n# %%\\n# Then we can fit our diagnosis model and compute the positive likelihood\\n# ratio to evaluate the usefulness of this classifier as a disease diagnosis\\n# tool:\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import class_likelihood_ratios\\n\\nestimator = LogisticRegression().fit(X_train, y_train)\\ny_pred = estimator.predict(X_test)\\npos_LR, neg_LR = class_likelihood_ratios(y_test, y_pred)\\nprint(f\"LR+: {pos_LR:.3f}\")\\n\\n# %%\\n# Since the positive class likelihood ratio is much larger than 1.0, it means\\n# that the machine learning-based diagnosis tool is useful: the post-test odds\\n# that the condition is truly present given a positive test result are more than\\n# 12 times larger than the pre-test odds.\\n#\\n# Cross-validation of likelihood ratios\\n# =====================================\\n#\\n# We assess the variability of the measurements for the class likelihood ratios\\n# in some particular cases.\\n\\nimport pandas as pd\\n\\n\\n# Code for: def scoring(estimator, X, y):\\n\\n\\n# Code for: def extract_score(cv_results):'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_likelihood_ratios.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import pandas as pd\\n\\n\\n# Code for: def scoring(estimator, X, y):\\n\\n\\n# Code for: def extract_score(cv_results):\\n\\n\\n# %%\\n# We first validate the :class:`~sklearn.linear_model.LogisticRegression` model\\n# with default hyperparameters as used in the previous section.\\n\\nfrom sklearn.model_selection import cross_validate\\n\\nestimator = LogisticRegression()\\nextract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))\\n\\n# %%\\n# We confirm that the model is useful: the post-test odds are between 12 and 20\\n# times larger than the pre-test odds.\\n#\\n# On the contrary, let\\'s consider a dummy model that will output random\\n# predictions with similar odds as the average disease prevalence in the\\n# training set:\\n\\nfrom sklearn.dummy import DummyClassifier\\n\\nestimator = DummyClassifier(strategy=\"stratified\", random_state=1234)\\nextract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))\\n\\n# %%\\n# Here both class likelihood ratios are compatible with 1.0 which makes this\\n# classifier useless as a diagnostic tool to improve disease detection.\\n#\\n# Another option for the dummy model is to always predict the most frequent\\n# class, which in this case is \"no-disease\".\\n\\nestimator = DummyClassifier(strategy=\"most_frequent\")\\nextract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))\\n\\n# %%\\n# The absence of positive predictions means there will be no true positives nor\\n# false positives, leading to an undefined `LR+` that by no means should be\\n# interpreted as an infinite `LR+` (the classifier perfectly identifying\\n# positive cases). In such situation the\\n# :func:`~sklearn.metrics.class_likelihood_ratios` function returns `nan` and\\n# raises a warning by default. Indeed, the value of `LR-` helps us discard this\\n# model.\\n#\\n# A similar scenario may arise when cross-validating highly imbalanced data with\\n# few samples: some folds will have no samples with the disease and therefore\\n# they will output no true positives nor false negatives when used for testing.\\n# Mathematically this leads to an infinite `LR+`, which should also not be\\n# interpreted as the model perfectly identifying positive cases. Such event\\n# leads to a higher variance of the estimated likelihood ratios, but can still\\n# be interpreted as an increment of the post-test odds of having the condition.\\n\\nestimator = LogisticRegression()\\nX, y = make_classification(n_samples=300, weights=[0.9, 0.1], random_state=0)\\nextract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_likelihood_ratios.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='estimator = LogisticRegression()\\nX, y = make_classification(n_samples=300, weights=[0.9, 0.1], random_state=0)\\nextract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))\\n\\n# %%\\n# Invariance with respect to prevalence\\n# =====================================\\n#\\n# The likelihood ratios are independent of the disease prevalence and can be\\n# extrapolated between populations regardless of any possible class imbalance,\\n# **as long as the same model is applied to all of them**. Notice that in the\\n# plots below **the decision boundary is constant** (see\\n# :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane_unbalanced.py` for\\n# a study of the boundary decision for unbalanced classes).\\n#\\n# Here we train a :class:`~sklearn.linear_model.LogisticRegression` base model\\n# on a case-control study with a prevalence of 50%. It is then evaluated over\\n# populations with varying prevalence. We use the\\n# :func:`~sklearn.datasets.make_classification` function to ensure the\\n# data-generating process is always the same as shown in the plots below. The\\n# label `1` corresponds to the positive class \"disease\", whereas the label `0`\\n# stands for \"no-disease\".\\n\\nfrom collections import defaultdict\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\npopulations = defaultdict(list)\\ncommon_params = {\\n    \"n_samples\": 10_000,\\n    \"n_features\": 2,\\n    \"n_informative\": 2,\\n    \"n_redundant\": 0,\\n    \"random_state\": 0,\\n}\\nweights = np.linspace(0.1, 0.8, 6)\\nweights = weights[::-1]\\n\\n# fit and evaluate base model on balanced classes\\nX, y = make_classification(**common_params, weights=[0.5, 0.5])\\nestimator = LogisticRegression().fit(X, y)\\nlr_base = extract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))\\npos_lr_base, pos_lr_base_std = lr_base[\"positive\"].values\\nneg_lr_base, neg_lr_base_std = lr_base[\"negative\"].values\\n\\n# %%\\n# We will now show the decision boundary for each level of prevalence. Note that\\n# we only plot a subset of the original data to better assess the linear model\\n# decision boundary.\\n\\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 12))\\n\\nfor ax, (n, weight) in zip(axs.ravel(), enumerate(weights)):\\n    X, y = make_classification(\\n        **common_params,\\n        weights=[weight, 1 - weight],\\n    )\\n    prevalence = y.mean()\\n    populations[\"prevalence\"].append(prevalence)\\n    populations[\"X\"].append(X)\\n    populations[\"y\"].append(y)\\n\\n    # down-sample for plotting\\n    rng = np.random.RandomState(1)\\n    plot_indices = rng.choice(np.arange(X.shape[0]), size=500, replace=True)\\n    X_plot, y_plot = X[plot_indices], y[plot_indices]'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_likelihood_ratios.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# down-sample for plotting\\n    rng = np.random.RandomState(1)\\n    plot_indices = rng.choice(np.arange(X.shape[0]), size=500, replace=True)\\n    X_plot, y_plot = X[plot_indices], y[plot_indices]\\n\\n    # plot fixed decision boundary of base model with varying prevalence\\n    disp = DecisionBoundaryDisplay.from_estimator(\\n        estimator,\\n        X_plot,\\n        response_method=\"predict\",\\n        alpha=0.5,\\n        ax=ax,\\n    )\\n    scatter = disp.ax_.scatter(X_plot[:, 0], X_plot[:, 1], c=y_plot, edgecolor=\"k\")\\n    disp.ax_.set_title(f\"prevalence = {y_plot.mean():.2f}\")\\n    disp.ax_.legend(*scatter.legend_elements())\\n\\n# %%\\n# We define a function for bootstrapping.\\n\\n\\n# Code for: def scoring_on_bootstrap(estimator, X, y, rng, n_bootstrap=100):\\n\\n\\n# %%\\n# We score the base model for each prevalence using bootstrapping.\\n\\nresults = defaultdict(list)\\nn_bootstrap = 100\\nrng = np.random.default_rng(seed=0)\\n\\nfor prevalence, X, y in zip(\\n    populations[\"prevalence\"], populations[\"X\"], populations[\"y\"]\\n):\\n    results_for_prevalence = scoring_on_bootstrap(\\n        estimator, X, y, rng, n_bootstrap=n_bootstrap\\n    )\\n    results[\"prevalence\"].append(prevalence)\\n    results[\"metrics\"].append(\\n        results_for_prevalence.aggregate([\"mean\", \"std\"]).unstack()\\n    )\\n\\nresults = pd.DataFrame(results[\"metrics\"], index=results[\"prevalence\"])\\nresults.index.name = \"prevalence\"\\nresults\\n\\n# %%\\n# In the plots below we observe that the class likelihood ratios re-computed\\n# with different prevalences are indeed constant within one standard deviation\\n# of those computed with on balanced classes.\\n\\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\\nresults[\"positive_likelihood_ratio\"][\"mean\"].plot(\\n    ax=ax1, color=\"r\", label=\"extrapolation through populations\"\\n)\\nax1.axhline(y=pos_lr_base + pos_lr_base_std, color=\"r\", linestyle=\"--\")\\nax1.axhline(\\n    y=pos_lr_base - pos_lr_base_std,\\n    color=\"r\",\\n    linestyle=\"--\",\\n    label=\"base model confidence band\",\\n)\\nax1.fill_between(\\n    results.index,\\n    results[\"positive_likelihood_ratio\"][\"mean\"]\\n    - results[\"positive_likelihood_ratio\"][\"std\"],\\n    results[\"positive_likelihood_ratio\"][\"mean\"]\\n    + results[\"positive_likelihood_ratio\"][\"std\"],\\n    color=\"r\",\\n    alpha=0.3,\\n)\\nax1.set(\\n    title=\"Positive likelihood ratio\",\\n    ylabel=\"LR+\",\\n    ylim=[0, 5],\\n)\\nax1.legend(loc=\"lower right\")'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_likelihood_ratios.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='ax2 = results[\"negative_likelihood_ratio\"][\"mean\"].plot(\\n    ax=ax2, color=\"b\", label=\"extrapolation through populations\"\\n)\\nax2.axhline(y=neg_lr_base + neg_lr_base_std, color=\"b\", linestyle=\"--\")\\nax2.axhline(\\n    y=neg_lr_base - neg_lr_base_std,\\n    color=\"b\",\\n    linestyle=\"--\",\\n    label=\"base model confidence band\",\\n)\\nax2.fill_between(\\n    results.index,\\n    results[\"negative_likelihood_ratio\"][\"mean\"]\\n    - results[\"negative_likelihood_ratio\"][\"std\"],\\n    results[\"negative_likelihood_ratio\"][\"mean\"]\\n    + results[\"negative_likelihood_ratio\"][\"std\"],\\n    color=\"b\",\\n    alpha=0.3,\\n)\\nax2.set(\\n    title=\"Negative likelihood ratio\",\\n    ylabel=\"LR-\",\\n    ylim=[0, 0.5],\\n)\\nax2.legend(loc=\"lower right\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_train_error_vs_test_error.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================\\nTrain error vs Test error\\n=========================\\n\\nIllustration of how the performance of an estimator on unseen data (test data)\\nis not the same as the performance on training data. As the regularization\\nincreases the performance on train decreases while the performance on test\\nis optimal within a range of values of the regularization parameter.\\nThe example with an Elastic-Net regression model and the performance is\\nmeasured using the explained variance a.k.a. R^2.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate sample data\\n# --------------------\\nimport numpy as np\\n\\nfrom sklearn import linear_model\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.model_selection import train_test_split\\n\\nn_samples_train, n_samples_test, n_features = 75, 150, 500\\nX, y, coef = make_regression(\\n    n_samples=n_samples_train + n_samples_test,\\n    n_features=n_features,\\n    n_informative=50,\\n    shuffle=False,\\n    noise=1.0,\\n    coef=True,\\n)\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, train_size=n_samples_train, test_size=n_samples_test, shuffle=False\\n)\\n# %%\\n# Compute train and test errors\\n# -----------------------------\\nalphas = np.logspace(-5, 1, 60)\\nenet = linear_model.ElasticNet(l1_ratio=0.7, max_iter=10000)\\ntrain_errors = list()\\ntest_errors = list()\\nfor alpha in alphas:\\n    enet.set_params(alpha=alpha)\\n    enet.fit(X_train, y_train)\\n    train_errors.append(enet.score(X_train, y_train))\\n    test_errors.append(enet.score(X_test, y_test))\\n\\ni_alpha_optim = np.argmax(test_errors)\\nalpha_optim = alphas[i_alpha_optim]\\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\\n\\n# Estimate the coef_ on full data with optimal regularization parameter\\nenet.set_params(alpha=alpha_optim)\\ncoef_ = enet.fit(X, y).coef_\\n\\n# %%\\n# Plot results functions\\n# ----------------------\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.subplot(2, 1, 1)\\nplt.semilogx(alphas, train_errors, label=\"Train\")\\nplt.semilogx(alphas, test_errors, label=\"Test\")\\nplt.vlines(\\n    alpha_optim,\\n    plt.ylim()[0],\\n    np.max(test_errors),\\n    color=\"k\",\\n    linewidth=3,\\n    label=\"Optimum on test\",\\n)\\nplt.legend(loc=\"lower right\")\\nplt.ylim([0, 1.2])\\nplt.xlabel(\"Regularization parameter\")\\nplt.ylabel(\"Performance\")\\n\\n# Show estimated coef_ vs true coef\\nplt.subplot(2, 1, 2)\\nplt.plot(coef, label=\"True coef\")\\nplt.plot(coef_, label=\"Estimated coef\")\\nplt.legend()\\nplt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.26)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_det.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================\\nDetection error tradeoff (DET) curve\\n====================================\\n\\nIn this example, we compare two binary classification multi-threshold metrics:\\nthe Receiver Operating Characteristic (ROC) and the Detection Error Tradeoff\\n(DET). For such purpose, we evaluate two different classifiers for the same\\nclassification task.\\n\\nROC curves feature true positive rate (TPR) on the Y axis, and false positive\\nrate (FPR) on the X axis. This means that the top left corner of the plot is the\\n\"ideal\" point - a FPR of zero, and a TPR of one.\\n\\nDET curves are a variation of ROC curves where False Negative Rate (FNR) is\\nplotted on the y-axis instead of the TPR. In this case the origin (bottom left\\ncorner) is the \"ideal\" point.\\n\\n.. note::\\n\\n    - See :func:`sklearn.metrics.roc_curve` for further information about ROC\\n      curves.\\n\\n    - See :func:`sklearn.metrics.det_curve` for further information about\\n      DET curves.\\n\\n    - This example is loosely based on\\n      :ref:`sphx_glr_auto_examples_classification_plot_classifier_comparison.py`\\n      example.\\n\\n    - See :ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py` for\\n      an example estimating the variance of the ROC curves and ROC-AUC.\\n\\n\"\"\"\\n\\n# %%\\n# Generate synthetic data\\n# -----------------------\\n\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\nX, y = make_classification(\\n    n_samples=1_000,\\n    n_features=2,\\n    n_redundant=0,\\n    n_informative=2,\\n    random_state=1,\\n    n_clusters_per_class=1,\\n)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\\n\\n# %%\\n# Define the classifiers\\n# ----------------------\\n#\\n# Here we define two different classifiers. The goal is to visually compare their\\n# statistical performance across thresholds using the ROC and DET curves. There\\n# is no particular reason why these classifiers are chosen other classifiers\\n# available in scikit-learn.\\n\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.svm import LinearSVC\\n\\nclassifiers = {\\n    \"Linear SVM\": make_pipeline(StandardScaler(), LinearSVC(C=0.025)),\\n    \"Random Forest\": RandomForestClassifier(\\n        max_depth=5, n_estimators=10, max_features=1\\n    ),\\n}\\n\\n# %%\\n# Plot ROC and DET curves\\n# -----------------------\\n#\\n# DET curves are commonly plotted in normal deviate scale. To achieve this the\\n# DET display transforms the error rates as returned by the\\n# :func:`~sklearn.metrics.det_curve` and the axis scale using\\n# `scipy.stats.norm`.\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.metrics import DetCurveDisplay, RocCurveDisplay\\n\\nfig, [ax_roc, ax_det] = plt.subplots(1, 2, figsize=(11, 5))\\n\\nfor name, clf in classifiers.items():\\n    clf.fit(X_train, y_train)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_det.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import matplotlib.pyplot as plt\\n\\nfrom sklearn.metrics import DetCurveDisplay, RocCurveDisplay\\n\\nfig, [ax_roc, ax_det] = plt.subplots(1, 2, figsize=(11, 5))\\n\\nfor name, clf in classifiers.items():\\n    clf.fit(X_train, y_train)\\n\\n    RocCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_roc, name=name)\\n    DetCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_det, name=name)\\n\\nax_roc.set_title(\"Receiver Operating Characteristic (ROC) curves\")\\nax_det.set_title(\"Detection Error Tradeoff (DET) curves\")\\n\\nax_roc.grid(linestyle=\"--\")\\nax_det.grid(linestyle=\"--\")\\n\\nplt.legend()\\nplt.show()\\n\\n# %%\\n# Notice that it is easier to visually assess the overall performance of\\n# different classification algorithms using DET curves than using ROC curves. As\\n# ROC curves are plot in a linear scale, different classifiers usually appear\\n# similar for a large part of the plot and differ the most in the top left\\n# corner of the graph. On the other hand, because DET curves represent straight\\n# lines in normal deviate scale, they tend to be distinguishable as a whole and\\n# the area of interest spans a large part of the plot.\\n#\\n# DET curves give direct feedback of the detection error tradeoff to aid in\\n# operating point analysis. The user can then decide the FNR they are willing to\\n# accept at the expense of the FPR (or vice-versa).'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cv_indices.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def visualize_groups(classes, groups, name):\\n    # Visualize dataset groups\\n    fig, ax = plt.subplots()\\n    ax.scatter(\\n        range(len(groups)),\\n        [0.5] * len(groups),\\n        c=groups,\\n        marker=\"_\",\\n        lw=50,\\n        cmap=cmap_data,\\n    )\\n    ax.scatter(\\n        range(len(groups)),\\n        [3.5] * len(groups),\\n        c=classes,\\n        marker=\"_\",\\n        lw=50,\\n        cmap=cmap_data,\\n    )\\n    ax.set(\\n        ylim=[-1, 5],\\n        yticks=[0.5, 3.5],\\n        yticklabels=[\"Data\\\\ngroup\", \"Data\\\\nclass\"],\\n        xlabel=\"Sample index\",\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cv_indices.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\\n    use_groups = \"Group\" in type(cv).__name__\\n    groups = group if use_groups else None\\n    # Generate the training/testing visualizations for each CV split\\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=groups)):\\n        # Fill in indices with the training/test groups\\n        indices = np.array([np.nan] * len(X))\\n        indices[tt] = 1\\n        indices[tr] = 0\\n\\n        # Visualize the results\\n        ax.scatter(\\n            range(len(indices)),\\n            [ii + 0.5] * len(indices),\\n            c=indices,\\n            marker=\"_\",\\n            lw=lw,\\n            cmap=cmap_cv,\\n            vmin=-0.2,\\n            vmax=1.2,\\n        )\\n\\n    # Plot the data classes and groups at the end\\n    ax.scatter(\\n        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\\n    )\\n\\n    ax.scatter(\\n        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data\\n    )\\n\\n    # Formatting\\n    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\\n    ax.set(\\n        yticks=np.arange(n_splits + 2) + 0.5,\\n        yticklabels=yticklabels,\\n        xlabel=\"Sample index\",\\n        ylabel=\"CV iteration\",\\n        ylim=[n_splits + 2.2, -0.2],\\n        xlim=[0, 100],\\n    )\\n    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\\n    return ax'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cv_indices.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\nVisualizing cross-validation behavior in scikit-learn\\n=====================================================\\n\\nChoosing the right cross-validation object is a crucial part of fitting a\\nmodel properly. There are many ways to split data into training and test\\nsets in order to avoid model overfitting, to standardize the number of\\ngroups in test sets, etc.\\n\\nThis example visualizes the behavior of several common scikit-learn objects\\nfor comparison.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib.patches import Patch\\n\\nfrom sklearn.model_selection import (\\n    GroupKFold,\\n    GroupShuffleSplit,\\n    KFold,\\n    ShuffleSplit,\\n    StratifiedGroupKFold,\\n    StratifiedKFold,\\n    StratifiedShuffleSplit,\\n    TimeSeriesSplit,\\n)\\n\\nrng = np.random.RandomState(1338)\\ncmap_data = plt.cm.Paired\\ncmap_cv = plt.cm.coolwarm\\nn_splits = 4\\n\\n# %%\\n# Visualize our data\\n# ------------------\\n#\\n# First, we must understand the structure of our data. It has 100 randomly\\n# generated input datapoints, 3 classes split unevenly across datapoints,\\n# and 10 \"groups\" split evenly across datapoints.\\n#\\n# As we\\'ll see, some cross-validation objects do specific things with\\n# labeled data, others behave differently with grouped data, and others\\n# do not use this information.\\n#\\n# To begin, we\\'ll visualize our data.\\n\\n# Generate the class/group data\\nn_points = 100\\nX = rng.randn(100, 10)\\n\\npercentiles_classes = [0.1, 0.3, 0.6]\\ny = np.hstack([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])\\n\\n# Generate uneven groups\\ngroup_prior = rng.dirichlet([2] * 10)\\ngroups = np.repeat(np.arange(10), rng.multinomial(100, group_prior))\\n\\n\\n# Code for: def visualize_groups(classes, groups, name):\\n\\n\\nvisualize_groups(y, groups, \"no groups\")\\n\\n# %%\\n# Define a function to visualize cross-validation behavior\\n# --------------------------------------------------------\\n#\\n# We\\'ll define a function that lets us visualize the behavior of each\\n# cross-validation object. We\\'ll perform 4 splits of the data. On each\\n# split, we\\'ll visualize the indices chosen for the training set\\n# (in blue) and the test set (in red).\\n\\n\\n# Code for: def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\\n\\n\\n# %%\\n# Let\\'s see how it looks for the :class:`~sklearn.model_selection.KFold`\\n# cross-validation object:\\n\\nfig, ax = plt.subplots()\\ncv = KFold(n_splits)\\nplot_cv_indices(cv, X, y, groups, ax, n_splits)\\n\\n# %%\\n# As you can see, by default the KFold cross-validation iterator does not\\n# take either datapoint class or group into consideration. We can change this\\n# by using either:\\n#\\n# - ``StratifiedKFold`` to preserve the percentage of samples for each class.\\n# - ``GroupKFold`` to ensure that the same group will not appear in two\\n#   different folds.\\n# - ``StratifiedGroupKFold`` to keep the constraint of ``GroupKFold`` while\\n#   attempting to return stratified folds.\\ncvs = [StratifiedKFold, GroupKFold, StratifiedGroupKFold]'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cv_indices.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for cv in cvs:\\n    fig, ax = plt.subplots(figsize=(6, 3))\\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\\n    ax.legend(\\n        [Patch(color=cmap_cv(0.8)), Patch(color=cmap_cv(0.02))],\\n        [\"Testing set\", \"Training set\"],\\n        loc=(1.02, 0.8),\\n    )\\n    # Make the legend fit\\n    plt.tight_layout()\\n    fig.subplots_adjust(right=0.7)\\n\\n# %%\\n# Next we\\'ll visualize this behavior for a number of CV iterators.\\n#\\n# Visualize cross-validation indices for many CV objects\\n# ------------------------------------------------------\\n#\\n# Let\\'s visually compare the cross validation behavior for many\\n# scikit-learn cross-validation objects. Below we will loop through several\\n# common cross-validation objects, visualizing the behavior of each.\\n#\\n# Note how some use the group/class information while others do not.\\n\\ncvs = [\\n    KFold,\\n    GroupKFold,\\n    ShuffleSplit,\\n    StratifiedKFold,\\n    StratifiedGroupKFold,\\n    GroupShuffleSplit,\\n    StratifiedShuffleSplit,\\n    TimeSeriesSplit,\\n]\\n\\n\\nfor cv in cvs:\\n    this_cv = cv(n_splits=n_splits)\\n    fig, ax = plt.subplots(figsize=(6, 3))\\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\\n\\n    ax.legend(\\n        [Patch(color=cmap_cv(0.8)), Patch(color=cmap_cv(0.02))],\\n        [\"Testing set\", \"Training set\"],\\n        loc=(1.02, 0.8),\\n    )\\n    # Make the legend fit\\n    plt.tight_layout()\\n    fig.subplots_adjust(right=0.7)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_roc.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================================\\nMulticlass Receiver Operating Characteristic (ROC)\\n==================================================\\n\\nThis example describes the use of the Receiver Operating Characteristic (ROC)\\nmetric to evaluate the quality of multiclass classifiers.\\n\\nROC curves typically feature true positive rate (TPR) on the Y axis, and false\\npositive rate (FPR) on the X axis. This means that the top left corner of the\\nplot is the \"ideal\" point - a FPR of zero, and a TPR of one. This is not very\\nrealistic, but it does mean that a larger area under the curve (AUC) is usually\\nbetter. The \"steepness\" of ROC curves is also important, since it is ideal to\\nmaximize the TPR while minimizing the FPR.\\n\\nROC curves are typically used in binary classification, where the TPR and FPR\\ncan be defined unambiguously. In the case of multiclass classification, a notion\\nof TPR or FPR is obtained only after binarizing the output. This can be done in\\n2 different ways:\\n\\n- the One-vs-Rest scheme compares each class against all the others (assumed as\\n  one);\\n- the One-vs-One scheme compares every unique pairwise combination of classes.\\n\\nIn this example we explore both schemes and demo the concepts of micro and macro\\naveraging as different ways of summarizing the information of the multiclass ROC\\ncurves.\\n\\n.. note::\\n\\n    See :ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py` for\\n    an extension of the present example estimating the variance of the ROC\\n    curves and their respective AUC.\\n\"\"\"\\n\\n# %%\\n# Load and prepare data\\n# =====================\\n#\\n# We import the :ref:`iris_dataset` which contains 3 classes, each one\\n# corresponding to a type of iris plant. One class is linearly separable from\\n# the other 2; the latter are **not** linearly separable from each other.\\n#\\n# Here we binarize the output and add noisy features to make the problem harder.\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\n\\niris = load_iris()\\ntarget_names = iris.target_names\\nX, y = iris.data, iris.target\\ny = iris.target_names[y]\\n\\nrandom_state = np.random.RandomState(0)\\nn_samples, n_features = X.shape\\nn_classes = len(np.unique(y))\\nX = np.concatenate([X, random_state.randn(n_samples, 200 * n_features)], axis=1)\\n(\\n    X_train,\\n    X_test,\\n    y_train,\\n    y_test,\\n) = train_test_split(X, y, test_size=0.5, stratify=y, random_state=0)\\n\\n# %%\\n# We train a :class:`~sklearn.linear_model.LogisticRegression` model which can\\n# naturally handle multiclass problems, thanks to the use of the multinomial\\n# formulation.\\n\\nfrom sklearn.linear_model import LogisticRegression\\n\\nclassifier = LogisticRegression()\\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_roc.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.linear_model import LogisticRegression\\n\\nclassifier = LogisticRegression()\\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)\\n\\n# %%\\n# One-vs-Rest multiclass ROC\\n# ==========================\\n#\\n# The One-vs-the-Rest (OvR) multiclass strategy, also known as one-vs-all,\\n# consists in computing a ROC curve per each of the `n_classes`. In each step, a\\n# given class is regarded as the positive class and the remaining classes are\\n# regarded as the negative class as a bulk.\\n#\\n# .. note:: One should not confuse the OvR strategy used for the **evaluation**\\n#     of multiclass classifiers with the OvR strategy used to **train** a\\n#     multiclass classifier by fitting a set of binary classifiers (for instance\\n#     via the :class:`~sklearn.multiclass.OneVsRestClassifier` meta-estimator).\\n#     The OvR ROC evaluation can be used to scrutinize any kind of classification\\n#     models irrespectively of how they were trained (see :ref:`multiclass`).\\n#\\n# In this section we use a :class:`~sklearn.preprocessing.LabelBinarizer` to\\n# binarize the target by one-hot-encoding in a OvR fashion. This means that the\\n# target of shape (`n_samples`,) is mapped to a target of shape (`n_samples`,\\n# `n_classes`).\\n\\nfrom sklearn.preprocessing import LabelBinarizer\\n\\nlabel_binarizer = LabelBinarizer().fit(y_train)\\ny_onehot_test = label_binarizer.transform(y_test)\\ny_onehot_test.shape  # (n_samples, n_classes)\\n\\n# %%\\n# We can as well easily check the encoding of a specific class:\\n\\nlabel_binarizer.transform([\"virginica\"])\\n\\n# %%\\n# ROC curve showing a specific class\\n# ----------------------------------\\n#\\n# In the following plot we show the resulting ROC curve when regarding the iris\\n# flowers as either \"virginica\" (`class_id=2`) or \"non-virginica\" (the rest).\\n\\nclass_of_interest = \"virginica\"\\nclass_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\\nclass_id\\n\\n# %%\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.metrics import RocCurveDisplay\\n\\ndisplay = RocCurveDisplay.from_predictions(\\n    y_onehot_test[:, class_id],\\n    y_score[:, class_id],\\n    name=f\"{class_of_interest} vs the rest\",\\n    color=\"darkorange\",\\n    plot_chance_level=True,\\n)\\n_ = display.ax_.set(\\n    xlabel=\"False Positive Rate\",\\n    ylabel=\"True Positive Rate\",\\n    title=\"One-vs-Rest ROC curves:\\\\nVirginica vs (Setosa & Versicolor)\",\\n)\\n\\n# %%\\n# ROC curve using micro-averaged OvR\\n# ----------------------------------\\n#\\n# Micro-averaging aggregates the contributions from all the classes (using\\n# :func:`numpy.ravel`) to compute the average metrics as follows:\\n#\\n# :math:`TPR=\\\\frac{\\\\sum_{c}TP_c}{\\\\sum_{c}(TP_c + FN_c)}` ;\\n#\\n# :math:`FPR=\\\\frac{\\\\sum_{c}FP_c}{\\\\sum_{c}(FP_c + TN_c)}` .\\n#\\n# We can briefly demo the effect of :func:`numpy.ravel`:\\n\\nprint(f\"y_score:\\\\n{y_score[0:2,:]}\")\\nprint()\\nprint(f\"y_score.ravel():\\\\n{y_score[0:2,:].ravel()}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_roc.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(f\"y_score:\\\\n{y_score[0:2,:]}\")\\nprint()\\nprint(f\"y_score.ravel():\\\\n{y_score[0:2,:].ravel()}\")\\n\\n# %%\\n# In a multi-class classification setup with highly imbalanced classes,\\n# micro-averaging is preferable over macro-averaging. In such cases, one can\\n# alternatively use a weighted macro-averaging, not demoed here.\\n\\ndisplay = RocCurveDisplay.from_predictions(\\n    y_onehot_test.ravel(),\\n    y_score.ravel(),\\n    name=\"micro-average OvR\",\\n    color=\"darkorange\",\\n    plot_chance_level=True,\\n)\\n_ = display.ax_.set(\\n    xlabel=\"False Positive Rate\",\\n    ylabel=\"True Positive Rate\",\\n    title=\"Micro-averaged One-vs-Rest\\\\nReceiver Operating Characteristic\",\\n)\\n\\n# %%\\n# In the case where the main interest is not the plot but the ROC-AUC score\\n# itself, we can reproduce the value shown in the plot using\\n# :class:`~sklearn.metrics.roc_auc_score`.\\n\\nfrom sklearn.metrics import roc_auc_score\\n\\nmicro_roc_auc_ovr = roc_auc_score(\\n    y_test,\\n    y_score,\\n    multi_class=\"ovr\",\\n    average=\"micro\",\\n)\\n\\nprint(f\"Micro-averaged One-vs-Rest ROC AUC score:\\\\n{micro_roc_auc_ovr:.2f}\")\\n\\n# %%\\n# This is equivalent to computing the ROC curve with\\n# :class:`~sklearn.metrics.roc_curve` and then the area under the curve with\\n# :class:`~sklearn.metrics.auc` for the raveled true and predicted classes.\\n\\nfrom sklearn.metrics import auc, roc_curve\\n\\n# store the fpr, tpr, and roc_auc for all averaging strategies\\nfpr, tpr, roc_auc = dict(), dict(), dict()\\n# Compute micro-average ROC curve and ROC area\\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_score.ravel())\\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\\n\\nprint(f\"Micro-averaged One-vs-Rest ROC AUC score:\\\\n{roc_auc[\\'micro\\']:.2f}\")\\n\\n# %%\\n# .. note:: By default, the computation of the ROC curve adds a single point at\\n#     the maximal false positive rate by using linear interpolation and the\\n#     McClish correction [:doi:`Analyzing a portion of the ROC curve Med Decis\\n#     Making. 1989 Jul-Sep; 9(3):190-5.<10.1177/0272989x8900900307>`].\\n#\\n# ROC curve using the OvR macro-average\\n# -------------------------------------\\n#\\n# Obtaining the macro-average requires computing the metric independently for\\n# each class and then taking the average over them, hence treating all classes\\n# equally a priori. We first aggregate the true/false positive rates per class:\\n\\nfor i in range(n_classes):\\n    fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i])\\n    roc_auc[i] = auc(fpr[i], tpr[i])\\n\\nfpr_grid = np.linspace(0.0, 1.0, 1000)\\n\\n# Interpolate all ROC curves at these points\\nmean_tpr = np.zeros_like(fpr_grid)\\n\\nfor i in range(n_classes):\\n    mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\\n\\n# Average it and compute AUC\\nmean_tpr /= n_classes\\n\\nfpr[\"macro\"] = fpr_grid\\ntpr[\"macro\"] = mean_tpr\\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\\n\\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\\\n{roc_auc[\\'macro\\']:.2f}\")\\n\\n# %%\\n# This computation is equivalent to simply calling'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_roc.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Average it and compute AUC\\nmean_tpr /= n_classes\\n\\nfpr[\"macro\"] = fpr_grid\\ntpr[\"macro\"] = mean_tpr\\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\\n\\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\\\n{roc_auc[\\'macro\\']:.2f}\")\\n\\n# %%\\n# This computation is equivalent to simply calling\\n\\nmacro_roc_auc_ovr = roc_auc_score(\\n    y_test,\\n    y_score,\\n    multi_class=\"ovr\",\\n    average=\"macro\",\\n)\\n\\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\\\n{macro_roc_auc_ovr:.2f}\")\\n\\n# %%\\n# Plot all OvR ROC curves together\\n# --------------------------------\\n\\nfrom itertools import cycle\\n\\nfig, ax = plt.subplots(figsize=(6, 6))\\n\\nplt.plot(\\n    fpr[\"micro\"],\\n    tpr[\"micro\"],\\n    label=f\"micro-average ROC curve (AUC = {roc_auc[\\'micro\\']:.2f})\",\\n    color=\"deeppink\",\\n    linestyle=\":\",\\n    linewidth=4,\\n)\\n\\nplt.plot(\\n    fpr[\"macro\"],\\n    tpr[\"macro\"],\\n    label=f\"macro-average ROC curve (AUC = {roc_auc[\\'macro\\']:.2f})\",\\n    color=\"navy\",\\n    linestyle=\":\",\\n    linewidth=4,\\n)\\n\\ncolors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\\nfor class_id, color in zip(range(n_classes), colors):\\n    RocCurveDisplay.from_predictions(\\n        y_onehot_test[:, class_id],\\n        y_score[:, class_id],\\n        name=f\"ROC curve for {target_names[class_id]}\",\\n        color=color,\\n        ax=ax,\\n        plot_chance_level=(class_id == 2),\\n    )\\n\\n_ = ax.set(\\n    xlabel=\"False Positive Rate\",\\n    ylabel=\"True Positive Rate\",\\n    title=\"Extension of Receiver Operating Characteristic\\\\nto One-vs-Rest multiclass\",\\n)\\n\\n# %%\\n# One-vs-One multiclass ROC\\n# =========================\\n#\\n# The One-vs-One (OvO) multiclass strategy consists in fitting one classifier\\n# per class pair. Since it requires to train `n_classes` * (`n_classes` - 1) / 2\\n# classifiers, this method is usually slower than One-vs-Rest due to its\\n# O(`n_classes` ^2) complexity.\\n#\\n# In this section, we demonstrate the macro-averaged AUC using the OvO scheme\\n# for the 3 possible combinations in the :ref:`iris_dataset`: \"setosa\" vs\\n# \"versicolor\", \"versicolor\" vs \"virginica\" and  \"virginica\" vs \"setosa\". Notice\\n# that micro-averaging is not defined for the OvO scheme.\\n#\\n# ROC curve using the OvO macro-average\\n# -------------------------------------\\n#\\n# In the OvO scheme, the first step is to identify all possible unique\\n# combinations of pairs. The computation of scores is done by treating one of\\n# the elements in a given pair as the positive class and the other element as\\n# the negative class, then re-computing the score by inversing the roles and\\n# taking the mean of both scores.\\n\\nfrom itertools import combinations\\n\\npair_list = list(combinations(np.unique(y), 2))\\nprint(pair_list)\\n\\n# %%\\npair_scores = []\\nmean_tpr = dict()\\n\\nfor ix, (label_a, label_b) in enumerate(pair_list):\\n    a_mask = y_test == label_a\\n    b_mask = y_test == label_b\\n    ab_mask = np.logical_or(a_mask, b_mask)\\n\\n    a_true = a_mask[ab_mask]\\n    b_true = b_mask[ab_mask]'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_roc.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\npair_scores = []\\nmean_tpr = dict()\\n\\nfor ix, (label_a, label_b) in enumerate(pair_list):\\n    a_mask = y_test == label_a\\n    b_mask = y_test == label_b\\n    ab_mask = np.logical_or(a_mask, b_mask)\\n\\n    a_true = a_mask[ab_mask]\\n    b_true = b_mask[ab_mask]\\n\\n    idx_a = np.flatnonzero(label_binarizer.classes_ == label_a)[0]\\n    idx_b = np.flatnonzero(label_binarizer.classes_ == label_b)[0]\\n\\n    fpr_a, tpr_a, _ = roc_curve(a_true, y_score[ab_mask, idx_a])\\n    fpr_b, tpr_b, _ = roc_curve(b_true, y_score[ab_mask, idx_b])\\n\\n    mean_tpr[ix] = np.zeros_like(fpr_grid)\\n    mean_tpr[ix] += np.interp(fpr_grid, fpr_a, tpr_a)\\n    mean_tpr[ix] += np.interp(fpr_grid, fpr_b, tpr_b)\\n    mean_tpr[ix] /= 2\\n    mean_score = auc(fpr_grid, mean_tpr[ix])\\n    pair_scores.append(mean_score)\\n\\n    fig, ax = plt.subplots(figsize=(6, 6))\\n    plt.plot(\\n        fpr_grid,\\n        mean_tpr[ix],\\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\\n        linestyle=\":\",\\n        linewidth=4,\\n    )\\n    RocCurveDisplay.from_predictions(\\n        a_true,\\n        y_score[ab_mask, idx_a],\\n        ax=ax,\\n        name=f\"{label_a} as positive class\",\\n    )\\n    RocCurveDisplay.from_predictions(\\n        b_true,\\n        y_score[ab_mask, idx_b],\\n        ax=ax,\\n        name=f\"{label_b} as positive class\",\\n        plot_chance_level=True,\\n    )\\n    ax.set(\\n        xlabel=\"False Positive Rate\",\\n        ylabel=\"True Positive Rate\",\\n        title=f\"{target_names[idx_a]} vs {label_b} ROC curves\",\\n    )\\n\\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\\\n{np.average(pair_scores):.2f}\")\\n\\n# %%\\n# One can also assert that the macro-average we computed \"by hand\" is equivalent\\n# to the implemented `average=\"macro\"` option of the\\n# :class:`~sklearn.metrics.roc_auc_score` function.\\n\\nmacro_roc_auc_ovo = roc_auc_score(\\n    y_test,\\n    y_score,\\n    multi_class=\"ovo\",\\n    average=\"macro\",\\n)\\n\\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\\\n{macro_roc_auc_ovo:.2f}\")\\n\\n# %%\\n# Plot all OvO ROC curves together\\n# --------------------------------\\n\\novo_tpr = np.zeros_like(fpr_grid)\\n\\nfig, ax = plt.subplots(figsize=(6, 6))\\nfor ix, (label_a, label_b) in enumerate(pair_list):\\n    ovo_tpr += mean_tpr[ix]\\n    ax.plot(\\n        fpr_grid,\\n        mean_tpr[ix],\\n        label=f\"Mean {label_a} vs {label_b} (AUC = {pair_scores[ix]:.2f})\",\\n    )\\n\\novo_tpr /= sum(1 for pair in enumerate(pair_list))\\n\\nax.plot(\\n    fpr_grid,\\n    ovo_tpr,\\n    label=f\"One-vs-One macro-average (AUC = {macro_roc_auc_ovo:.2f})\",\\n    linestyle=\":\",\\n    linewidth=4,\\n)\\nax.plot([0, 1], [0, 1], \"k--\", label=\"Chance level (AUC = 0.5)\")\\n_ = ax.set(\\n    xlabel=\"False Positive Rate\",\\n    ylabel=\"True Positive Rate\",\\n    title=\"Extension of Receiver Operating Characteristic\\\\nto One-vs-One multiclass\",\\n    aspect=\"equal\",\\n    xlim=(-0.01, 1.01),\\n    ylim=(-0.01, 1.01),\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_roc.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We confirm that the classes \"versicolor\" and \"virginica\" are not well\\n# identified by a linear classifier. Notice that the \"virginica\"-vs-the-rest\\n# ROC-AUC score (0.77) is between the OvO ROC-AUC scores for \"versicolor\" vs\\n# \"virginica\" (0.64) and \"setosa\" vs \"virginica\" (0.90). Indeed, the OvO\\n# strategy gives additional information on the confusion between a pair of\\n# classes, at the expense of computational cost when the number of classes\\n# is large.\\n#\\n# The OvO strategy is recommended if the user is mainly interested in correctly\\n# identifying a particular class or subset of classes, whereas evaluating the\\n# global performance of a classifier can still be summarized via a given\\n# averaging strategy.\\n#\\n# Micro-averaged OvR ROC is dominated by the more frequent class, since the\\n# counts are pooled. The macro-averaged alternative better reflects the\\n# statistics of the less frequent classes, and then is more appropriate when\\n# performance on all the classes is deemed equally important.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def fpr_score(y, y_pred, neg_label, pos_label):\\n    cm = confusion_matrix(y, y_pred, labels=[neg_label, pos_label])\\n    tn, fp, _, _ = cm.ravel()\\n    tnr = tn / (tn + fp)\\n    return 1 - tnr'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def credit_gain_score(y, y_pred, neg_label, pos_label):\\n    cm = confusion_matrix(y, y_pred, labels=[neg_label, pos_label])\\n    # The rows of the confusion matrix hold the counts of observed classes\\n    # while the columns hold counts of predicted classes. Recall that here we\\n    # consider \"bad\" as the positive class (second row and column).\\n    # Scikit-learn model selection tools expect that we follow a convention\\n    # that \"higher\" means \"better\", hence the following gain matrix assigns\\n    # negative gains (costs) to the two kinds of prediction errors:\\n    # - a gain of -1 for each false positive (\"good\" credit labeled as \"bad\"),\\n    # - a gain of -5 for each false negative (\"bad\" credit labeled as \"good\"),\\n    # The true positives and true negatives are assigned null gains in this\\n    # metric.\\n    #\\n    # Note that theoretically, given that our model is calibrated and our data\\n    # set representative and large enough, we do not need to tune the\\n    # threshold, but can safely set it to the cost ration 1/5, as stated by Eq.\\n    # (2) in Elkan paper [2]_.\\n    gain_matrix = np.array(\\n        [\\n            [0, -1],  # -1 gain for false positives\\n            [-5, 0],  # -5 gain for false negatives\\n        ]\\n    )\\n    return np.sum(cm * gain_matrix)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_roc_pr_curves(vanilla_model, tuned_model, *, title):\\n    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(21, 6))\\n\\n    linestyles = (\"dashed\", \"dotted\")\\n    markerstyles = (\"o\", \">\")\\n    colors = (\"tab:blue\", \"tab:orange\")\\n    names = (\"Vanilla GBDT\", \"Tuned GBDT\")\\n    for idx, (est, linestyle, marker, color, name) in enumerate(\\n        zip((vanilla_model, tuned_model), linestyles, markerstyles, colors, names)\\n    ):\\n        decision_threshold = getattr(est, \"best_threshold_\", 0.5)\\n        PrecisionRecallDisplay.from_estimator(\\n            est,\\n            X_test,\\n            y_test,\\n            pos_label=pos_label,\\n            linestyle=linestyle,\\n            color=color,\\n            ax=axs[0],\\n            name=name,\\n        )\\n        axs[0].plot(\\n            scoring[\"recall\"](est, X_test, y_test),\\n            scoring[\"precision\"](est, X_test, y_test),\\n            marker,\\n            markersize=10,\\n            color=color,\\n            label=f\"Cut-off point at probability of {decision_threshold:.2f}\",\\n        )\\n        RocCurveDisplay.from_estimator(\\n            est,\\n            X_test,\\n            y_test,\\n            pos_label=pos_label,\\n            linestyle=linestyle,\\n            color=color,\\n            ax=axs[1],\\n            name=name,\\n            plot_chance_level=idx == 1,\\n        )\\n        axs[1].plot(\\n            scoring[\"fpr\"](est, X_test, y_test),\\n            scoring[\"tpr\"](est, X_test, y_test),\\n            marker,\\n            markersize=10,\\n            color=color,\\n            label=f\"Cut-off point at probability of {decision_threshold:.2f}\",\\n        )\\n\\n    axs[0].set_title(\"Precision-Recall curve\")\\n    axs[0].legend()\\n    axs[1].set_title(\"ROC curve\")\\n    axs[1].legend()\\n\\n    axs[2].plot(\\n        tuned_model.cv_results_[\"thresholds\"],\\n        tuned_model.cv_results_[\"scores\"],\\n        color=\"tab:orange\",\\n    )\\n    axs[2].plot(\\n        tuned_model.best_threshold_,\\n        tuned_model.best_score_,\\n        \"o\",\\n        markersize=10,\\n        color=\"tab:orange\",\\n        label=\"Optimal cut-off point for the business metric\",\\n    )\\n    axs[2].legend()\\n    axs[2].set_xlabel(\"Decision threshold (probability)\")\\n    axs[2].set_ylabel(\"Objective score (using cost-matrix)\")\\n    axs[2].set_title(\"Objective score as a function of the decision threshold\")\\n    fig.suptitle(title)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def business_metric(y_true, y_pred, amount):\\n    mask_true_positive = (y_true == 1) & (y_pred == 1)\\n    mask_true_negative = (y_true == 0) & (y_pred == 0)\\n    mask_false_positive = (y_true == 0) & (y_pred == 1)\\n    mask_false_negative = (y_true == 1) & (y_pred == 0)\\n    fraudulent_refuse = mask_true_positive.sum() * 50\\n    fraudulent_accept = -amount[mask_false_negative].sum()\\n    legitimate_refuse = mask_false_positive.sum() * -5\\n    legitimate_accept = (amount[mask_true_negative] * 0.02).sum()\\n    return fraudulent_refuse + fraudulent_accept + legitimate_refuse + legitimate_accept'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================================\\nPost-tuning the decision threshold for cost-sensitive learning\\n==============================================================\\n\\nOnce a classifier is trained, the output of the :term:`predict` method outputs class\\nlabel predictions corresponding to a thresholding of either the\\n:term:`decision_function` or the :term:`predict_proba` output. For a binary classifier,\\nthe default threshold is defined as a posterior probability estimate of 0.5 or a\\ndecision score of 0.0.\\n\\nHowever, this default strategy is most likely not optimal for the task at hand.\\nHere, we use the \"Statlog\" German credit dataset [1]_ to illustrate a use case.\\nIn this dataset, the task is to predict whether a person has a \"good\" or \"bad\" credit.\\nIn addition, a cost-matrix is provided that specifies the cost of\\nmisclassification. Specifically, misclassifying a \"bad\" credit as \"good\" is five\\ntimes more costly on average than misclassifying a \"good\" credit as \"bad\".\\n\\nWe use the :class:`~sklearn.model_selection.TunedThresholdClassifierCV` to select the\\ncut-off point of the decision function that minimizes the provided business\\ncost.\\n\\nIn the second part of the example, we further extend this approach by\\nconsidering the problem of fraud detection in credit card transactions: in this\\ncase, the business metric depends on the amount of each individual transaction.\\n\\n.. rubric :: References\\n\\n.. [1] \"Statlog (German Credit Data) Data Set\", UCI Machine Learning Repository,\\n    `Link <https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29>`_.\\n\\n.. [2] `Charles Elkan, \"The Foundations of Cost-Sensitive Learning\",\\n    International joint conference on artificial intelligence.\\n    Vol. 17. No. 1. Lawrence Erlbaum Associates Ltd, 2001.\\n    <https://cseweb.ucsd.edu/~elkan/rescale.pdf>`_\\n\"\"\"\\n\\n# %%\\n# Cost-sensitive learning with constant gains and costs\\n# -----------------------------------------------------\\n#\\n# In this first section, we illustrate the use of the\\n# :class:`~sklearn.model_selection.TunedThresholdClassifierCV` in a setting of\\n# cost-sensitive learning when the gains and costs associated to each entry of the\\n# confusion matrix are constant. We use the problematic presented in [2]_ using the\\n# \"Statlog\" German credit dataset [1]_.\\n#\\n# \"Statlog\" German credit dataset\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# We fetch the German credit dataset from OpenML.\\nimport sklearn\\nfrom sklearn.datasets import fetch_openml\\n\\nsklearn.set_config(transform_output=\"pandas\")\\n\\ngerman_credit = fetch_openml(data_id=31, as_frame=True, parser=\"pandas\")\\nX, y = german_credit.data, german_credit.target\\n\\n# %%\\n# We check the feature types available in `X`.\\nX.info()\\n\\n# %%\\n# Many features are categorical and usually string-encoded. We need to encode\\n# these categories when we develop our predictive model. Let\\'s check the targets.\\ny.value_counts()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We check the feature types available in `X`.\\nX.info()\\n\\n# %%\\n# Many features are categorical and usually string-encoded. We need to encode\\n# these categories when we develop our predictive model. Let\\'s check the targets.\\ny.value_counts()\\n\\n# %%\\n# Another observation is that the dataset is imbalanced. We would need to be careful\\n# when evaluating our predictive model and use a family of metrics that are adapted\\n# to this setting.\\n#\\n# In addition, we observe that the target is string-encoded. Some metrics\\n# (e.g. precision and recall) require to provide the label of interest also called\\n# the \"positive label\". Here, we define that our goal is to predict whether or not\\n# a sample is a \"bad\" credit.\\npos_label, neg_label = \"bad\", \"good\"\\n\\n# %%\\n# To carry our analysis, we split our dataset using a single stratified split.\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n\\n# %%\\n# We are ready to design our predictive model and the associated evaluation strategy.\\n#\\n# Evaluation metrics\\n# ^^^^^^^^^^^^^^^^^^\\n#\\n# In this section, we define a set of metrics that we use later. To see\\n# the effect of tuning the cut-off point, we evaluate the predictive model using\\n# the Receiver Operating Characteristic (ROC) curve and the Precision-Recall curve.\\n# The values reported on these plots are therefore the true positive rate (TPR),\\n# also known as the recall or the sensitivity, and the false positive rate (FPR),\\n# also known as the specificity, for the ROC curve and the precision and recall for\\n# the Precision-Recall curve.\\n#\\n# From these four metrics, scikit-learn does not provide a scorer for the FPR. We\\n# therefore need to define a small custom function to compute it.\\nfrom sklearn.metrics import confusion_matrix\\n\\n\\n# Code for: def fpr_score(y, y_pred, neg_label, pos_label):\\n\\n\\n# %%\\n# As previously stated, the \"positive label\" is not defined as the value \"1\" and calling\\n# some of the metrics with this non-standard value raise an error. We need to\\n# provide the indication of the \"positive label\" to the metrics.\\n#\\n# We therefore need to define a scikit-learn scorer using\\n# :func:`~sklearn.metrics.make_scorer` where the information is passed. We store all\\n# the custom scorers in a dictionary. To use them, we need to pass the fitted model,\\n# the data and the target on which we want to evaluate the predictive model.\\nfrom sklearn.metrics import make_scorer, precision_score, recall_score\\n\\ntpr_score = recall_score  # TPR and recall are the same metric\\nscoring = {\\n    \"precision\": make_scorer(precision_score, pos_label=pos_label),\\n    \"recall\": make_scorer(recall_score, pos_label=pos_label),\\n    \"fpr\": make_scorer(fpr_score, neg_label=neg_label, pos_label=pos_label),\\n    \"tpr\": make_scorer(tpr_score, pos_label=pos_label),\\n}'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# In addition, the original research [1]_ defines a custom business metric. We\\n# call a \"business metric\" any metric function that aims at quantifying how the\\n# predictions (correct or wrong) might impact the business value of deploying a\\n# given machine learning model in a specific application context. For our\\n# credit prediction task, the authors provide a custom cost-matrix which\\n# encodes that classifying a a \"bad\" credit as \"good\" is 5 times more costly on\\n# average than the opposite: it is less costly for the financing institution to\\n# not grant a credit to a potential customer that will not default (and\\n# therefore miss a good customer that would have otherwise both reimbursed the\\n# credit and payed interests) than to grant a credit to a customer that will\\n# default.\\n#\\n# We define a python function that weight the confusion matrix and return the\\n# overall cost.\\nimport numpy as np\\n\\n\\n# Code for: def credit_gain_score(y, y_pred, neg_label, pos_label):\\n\\n\\nscoring[\"credit_gain\"] = make_scorer(\\n    credit_gain_score, neg_label=neg_label, pos_label=pos_label\\n)\\n# %%\\n# Vanilla predictive model\\n# ^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# We use :class:`~sklearn.ensemble.HistGradientBoostingClassifier` as a predictive model\\n# that natively handles categorical features and missing values.\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\nmodel = HistGradientBoostingClassifier(\\n    categorical_features=\"from_dtype\", random_state=0\\n).fit(X_train, y_train)\\nmodel\\n\\n# %%\\n# We evaluate the performance of our predictive model using the ROC and Precision-Recall\\n# curves.\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\\n\\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\\n\\nPrecisionRecallDisplay.from_estimator(\\n    model, X_test, y_test, pos_label=pos_label, ax=axs[0], name=\"GBDT\"\\n)\\naxs[0].plot(\\n    scoring[\"recall\"](model, X_test, y_test),\\n    scoring[\"precision\"](model, X_test, y_test),\\n    marker=\"o\",\\n    markersize=10,\\n    color=\"tab:blue\",\\n    label=\"Default cut-off point at a probability of 0.5\",\\n)\\naxs[0].set_title(\"Precision-Recall curve\")\\naxs[0].legend()\\n\\nRocCurveDisplay.from_estimator(\\n    model,\\n    X_test,\\n    y_test,\\n    pos_label=pos_label,\\n    ax=axs[1],\\n    name=\"GBDT\",\\n    plot_chance_level=True,\\n)\\naxs[1].plot(\\n    scoring[\"fpr\"](model, X_test, y_test),\\n    scoring[\"tpr\"](model, X_test, y_test),\\n    marker=\"o\",\\n    markersize=10,\\n    color=\"tab:blue\",\\n    label=\"Default cut-off point at a probability of 0.5\",\\n)\\naxs[1].set_title(\"ROC curve\")\\naxs[1].legend()\\n_ = fig.suptitle(\"Evaluation of the vanilla GBDT model\")'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We recall that these curves give insights on the statistical performance of the\\n# predictive model for different cut-off points. For the Precision-Recall curve, the\\n# reported metrics are the precision and recall and for the ROC curve, the reported\\n# metrics are the TPR (same as recall) and FPR.\\n#\\n# Here, the different cut-off points correspond to different levels of posterior\\n# probability estimates ranging between 0 and 1. By default, `model.predict` uses a\\n# cut-off point at a probability estimate of 0.5. The metrics for such a cut-off point\\n# are reported with the blue dot on the curves: it corresponds to the statistical\\n# performance of the model when using `model.predict`.\\n#\\n# However, we recall that the original aim was to minimize the cost (or maximize the\\n# gain) as defined by the business metric. We can compute the value of the business\\n# metric:\\nprint(f\"Business defined metric: {scoring[\\'credit_gain\\'](model, X_test, y_test)}\")\\n\\n# %%\\n# At this stage we don\\'t know if any other cut-off can lead to a greater gain. To find\\n# the optimal one, we need to compute the cost-gain using the business metric for all\\n# possible cut-off points and choose the best. This strategy can be quite tedious to\\n# implement by hand, but the\\n# :class:`~sklearn.model_selection.TunedThresholdClassifierCV` class is here to help us.\\n# It automatically computes the cost-gain for all possible cut-off points and optimizes\\n# for the `scoring`.\\n#\\n# .. _cost_sensitive_learning_example:\\n#\\n# Tuning the cut-off point\\n# ^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# We use :class:`~sklearn.model_selection.TunedThresholdClassifierCV` to tune the\\n# cut-off point. We need to provide the business metric to optimize as well as the\\n# positive label. Internally, the optimum cut-off point is chosen such that it maximizes\\n# the business metric via cross-validation. By default a 5-fold stratified\\n# cross-validation is used.\\nfrom sklearn.model_selection import TunedThresholdClassifierCV\\n\\ntuned_model = TunedThresholdClassifierCV(\\n    estimator=model,\\n    scoring=scoring[\"credit_gain\"],\\n    store_cv_results=True,  # necessary to inspect all results\\n)\\ntuned_model.fit(X_train, y_train)\\nprint(f\"{tuned_model.best_threshold_=:0.2f}\")\\n\\n# %%\\n# We plot the ROC and Precision-Recall curves for the vanilla model and the tuned model.\\n# Also we plot the cut-off points that would be used by each model. Because, we are\\n# reusing the same code later, we define a function that generates the plots.\\n\\n\\n# Code for: def plot_roc_pr_curves(vanilla_model, tuned_model, *, title):\\n\\n\\n# %%\\ntitle = \"Comparison of the cut-off point for the vanilla and tuned GBDT model\"\\nplot_roc_pr_curves(model, tuned_model, title=title)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def plot_roc_pr_curves(vanilla_model, tuned_model, *, title):\\n\\n\\n# %%\\ntitle = \"Comparison of the cut-off point for the vanilla and tuned GBDT model\"\\nplot_roc_pr_curves(model, tuned_model, title=title)\\n\\n# %%\\n# The first remark is that both classifiers have exactly the same ROC and\\n# Precision-Recall curves. It is expected because by default, the classifier is fitted\\n# on the same training data. In a later section, we discuss more in detail the\\n# available options regarding model refitting and cross-validation.\\n#\\n# The second remark is that the cut-off points of the vanilla and tuned model are\\n# different. To understand why the tuned model has chosen this cut-off point, we can\\n# look at the right-hand side plot that plots the objective score that is our exactly\\n# the same as our business metric. We see that the optimum threshold corresponds to the\\n# maximum of the objective score. This maximum is reached for a decision threshold\\n# much lower than 0.5: the tuned model enjoys a much higher recall at the cost of\\n# of significantly lower precision: the tuned model is much more eager to\\n# predict the \"bad\" class label to larger fraction of individuals.\\n#\\n# We can now check if choosing this cut-off point leads to a better score on the testing\\n# set:\\nprint(f\"Business defined metric: {scoring[\\'credit_gain\\'](tuned_model, X_test, y_test)}\")\\n\\n# %%\\n# We observe that tuning the decision threshold almost improves our business gains\\n# by factor of 2.\\n#\\n# .. _TunedThresholdClassifierCV_no_cv:\\n#\\n# Consideration regarding model refitting and cross-validation\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# In the above experiment, we used the default setting of the\\n# :class:`~sklearn.model_selection.TunedThresholdClassifierCV`. In particular, the\\n# cut-off point is tuned using a 5-fold stratified cross-validation. Also, the\\n# underlying predictive model is refitted on the entire training data once the cut-off\\n# point is chosen.\\n#\\n# These two strategies can be changed by providing the `refit` and `cv` parameters.\\n# For instance, one could provide a fitted `estimator` and set `cv=\"prefit\"`, in which\\n# case the cut-off point is found on the entire dataset provided at fitting time.\\n# Also, the underlying classifier is not be refitted by setting `refit=False`. Here, we\\n# can try to do such experiment.\\nmodel.fit(X_train, y_train)\\ntuned_model.set_params(cv=\"prefit\", refit=False).fit(X_train, y_train)\\nprint(f\"{tuned_model.best_threshold_=:0.2f}\")\\n\\n\\n# %%\\n# Then, we evaluate our model with the same approach as before:\\ntitle = \"Tuned GBDT model without refitting and using the entire dataset\"\\nplot_roc_pr_curves(model, tuned_model, title=title)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Then, we evaluate our model with the same approach as before:\\ntitle = \"Tuned GBDT model without refitting and using the entire dataset\"\\nplot_roc_pr_curves(model, tuned_model, title=title)\\n\\n# %%\\n# We observe the that the optimum cut-off point is different from the one found\\n# in the previous experiment. If we look at the right-hand side plot, we\\n# observe that the business gain has large plateau of near-optimal 0 gain for a\\n# large span of decision thresholds. This behavior is symptomatic of an\\n# overfitting. Because we disable cross-validation, we tuned the cut-off point\\n# on the same set as the model was trained on, and this is the reason for the\\n# observed overfitting.\\n#\\n# This option should therefore be used with caution. One needs to make sure that the\\n# data provided at fitting time to the\\n# :class:`~sklearn.model_selection.TunedThresholdClassifierCV` is not the same as the\\n# data used to train the underlying classifier. This could happen sometimes when the\\n# idea is just to tune the predictive model on a completely new validation set without a\\n# costly complete refit.\\n#\\n# When cross-validation is too costly, a potential alternative is to use a\\n# single train-test split by providing a floating number in range `[0, 1]` to the `cv`\\n# parameter. It splits the data into a training and testing set. Let\\'s explore this\\n# option:\\ntuned_model.set_params(cv=0.75).fit(X_train, y_train)\\n\\n# %%\\ntitle = \"Tuned GBDT model without refitting and using the entire dataset\"\\nplot_roc_pr_curves(model, tuned_model, title=title)\\n\\n# %%\\n# Regarding the cut-off point, we observe that the optimum is similar to the multiple\\n# repeated cross-validation case. However, be aware that a single split does not account\\n# for the variability of the fit/predict process and thus we are unable to know if there\\n# is any variance in the cut-off point. The repeated cross-validation averages out\\n# this effect.\\n#\\n# Another observation concerns the ROC and Precision-Recall curves of the tuned model.\\n# As expected, these curves differ from those of the vanilla model, given that we\\n# trained the underlying classifier on a subset of the data provided during fitting and\\n# reserved a validation set for tuning the cut-off point.\\n#\\n# Cost-sensitive learning when gains and costs are not constant\\n# -------------------------------------------------------------\\n#\\n# As stated in [2]_, gains and costs are generally not constant in real-world problems.\\n# In this section, we use a similar example as in [2]_ for the problem of\\n# detecting fraud in credit card transaction records.\\n#\\n# The credit card dataset\\n# ^^^^^^^^^^^^^^^^^^^^^^^\\ncredit_card = fetch_openml(data_id=1597, as_frame=True, parser=\"pandas\")\\ncredit_card.frame.info()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The dataset contains information about credit card records from which some are\\n# fraudulent and others are legitimate. The goal is therefore to predict whether or\\n# not a credit card record is fraudulent.\\ncolumns_to_drop = [\"Class\"]\\ndata = credit_card.frame.drop(columns=columns_to_drop)\\ntarget = credit_card.frame[\"Class\"].astype(int)\\n\\n# %%\\n# First, we check the class distribution of the datasets.\\ntarget.value_counts(normalize=True)\\n\\n# %%\\n# The dataset is highly imbalanced with fraudulent transaction representing only 0.17%\\n# of the data. Since we are interested in training a machine learning model, we should\\n# also make sure that we have enough samples in the minority class to train the model.\\ntarget.value_counts()\\n\\n# %%\\n# We observe that we have around 500 samples that is on the low end of the number of\\n# samples required to train a machine learning model. In addition of the target\\n# distribution, we check the distribution of the amount of the\\n# fraudulent transactions.\\nfraud = target == 1\\namount_fraud = data[\"Amount\"][fraud]\\n_, ax = plt.subplots()\\nax.hist(amount_fraud, bins=30)\\nax.set_title(\"Amount of fraud transaction\")\\n_ = ax.set_xlabel(\"Amount (‚Ç¨)\")\\n\\n# %%\\n# Addressing the problem with a business metric\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# Now, we create the business metric that depends on the amount of each transaction. We\\n# define the cost matrix similarly to [2]_. Accepting a legitimate transaction provides\\n# a gain of 2% of the amount of the transaction. However, accepting a fraudulent\\n# transaction result in a loss of the amount of the transaction. As stated in [2]_, the\\n# gain and loss related to refusals (of fraudulent and legitimate transactions) are not\\n# trivial to define. Here, we define that a refusal of a legitimate transaction\\n# is estimated to a loss of 5‚Ç¨ while the refusal of a fraudulent transaction is\\n# estimated to a gain of 50‚Ç¨. Therefore, we define the following function to\\n# compute the total benefit of a given decision:\\n\\n\\n# Code for: def business_metric(y_true, y_pred, amount):\\n\\n\\n# %%\\n# From this business metric, we create a scikit-learn scorer that given a fitted\\n# classifier and a test set compute the business metric. In this regard, we use\\n# the :func:`~sklearn.metrics.make_scorer` factory. The variable `amount` is an\\n# additional metadata to be passed to the scorer and we need to use\\n# :ref:`metadata routing <metadata_routing>` to take into account this information.\\nsklearn.set_config(enable_metadata_routing=True)\\nbusiness_scorer = make_scorer(business_metric).set_score_request(amount=True)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# So at this stage, we observe that the amount of the transaction is used twice: once\\n# as a feature to train our predictive model and once as a metadata to compute the\\n# the business metric and thus the statistical performance of our model. When used as a\\n# feature, we are only required to have a column in `data` that contains the amount of\\n# each transaction. To use this information as metadata, we need to have an external\\n# variable that we can pass to the scorer or the model that internally routes this\\n# metadata to the scorer. So let\\'s create this variable.\\namount = credit_card.frame[\"Amount\"].to_numpy()\\n\\n# %%\\nfrom sklearn.model_selection import train_test_split\\n\\ndata_train, data_test, target_train, target_test, amount_train, amount_test = (\\n    train_test_split(\\n        data, target, amount, stratify=target, test_size=0.5, random_state=42\\n    )\\n)\\n\\n# %%\\n# We first evaluate some baseline policies to serve as reference. Recall that\\n# class \"0\" is the legitimate class and class \"1\" is the fraudulent class.\\nfrom sklearn.dummy import DummyClassifier\\n\\nalways_accept_policy = DummyClassifier(strategy=\"constant\", constant=0)\\nalways_accept_policy.fit(data_train, target_train)\\nbenefit = business_scorer(\\n    always_accept_policy, data_test, target_test, amount=amount_test\\n)\\nprint(f\"Benefit of the \\'always accept\\' policy: {benefit:,.2f}‚Ç¨\")\\n\\n# %%\\n# A policy that considers all transactions as legitimate would create a profit of\\n# around 220,000‚Ç¨. We make the same evaluation for a classifier that predicts all\\n# transactions as fraudulent.\\nalways_reject_policy = DummyClassifier(strategy=\"constant\", constant=1)\\nalways_reject_policy.fit(data_train, target_train)\\nbenefit = business_scorer(\\n    always_reject_policy, data_test, target_test, amount=amount_test\\n)\\nprint(f\"Benefit of the \\'always reject\\' policy: {benefit:,.2f}‚Ç¨\")\\n\\n\\n# %%\\n# Such a policy would entail a catastrophic loss: around 670,000‚Ç¨. This is\\n# expected since the vast majority of the transactions are legitimate and the\\n# policy would refuse them at a non-trivial cost.\\n#\\n# A predictive model that adapts the accept/reject decisions on a per\\n# transaction basis should ideally allow us to make a profit larger than the\\n# 220,000‚Ç¨ of the best of our constant baseline policies.\\n#\\n# We start with a logistic regression model with the default decision threshold\\n# at 0.5. Here we tune the hyperparameter `C` of the logistic regression with a\\n# proper scoring rule (the log loss) to ensure that the model\\'s probabilistic\\n# predictions returned by its `predict_proba` method are as accurate as\\n# possible, irrespectively of the choice of the value of the decision\\n# threshold.\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='logistic_regression = make_pipeline(StandardScaler(), LogisticRegression())\\nparam_grid = {\"logisticregression__C\": np.logspace(-6, 6, 13)}\\nmodel = GridSearchCV(logistic_regression, param_grid, scoring=\"neg_log_loss\").fit(\\n    data_train, target_train\\n)\\nmodel\\n\\n# %%\\nprint(\\n    \"Benefit of logistic regression with default threshold: \"\\n    f\"{business_scorer(model, data_test, target_test, amount=amount_test):,.2f}‚Ç¨\"\\n)\\n\\n# %%\\n# The business metric shows that our predictive model with a default decision\\n# threshold is already winning over the baseline in terms of profit and it would be\\n# already beneficial to use it to accept or reject transactions instead of\\n# accepting all transactions.\\n#\\n# Tuning the decision threshold\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# Now the question is: is our model optimum for the type of decision that we want to do?\\n# Up to now, we did not optimize the decision threshold. We use the\\n# :class:`~sklearn.model_selection.TunedThresholdClassifierCV` to optimize the decision\\n# given our business scorer. To avoid a nested cross-validation, we will use the\\n# best estimator found during the previous grid-search.\\ntuned_model = TunedThresholdClassifierCV(\\n    estimator=model.best_estimator_,\\n    scoring=business_scorer,\\n    thresholds=100,\\n    n_jobs=2,\\n)\\n\\n# %%\\n# Since our business scorer requires the amount of each transaction, we need to pass\\n# this information in the `fit` method. The\\n# :class:`~sklearn.model_selection.TunedThresholdClassifierCV` is in charge of\\n# automatically dispatching this metadata to the underlying scorer.\\ntuned_model.fit(data_train, target_train, amount=amount_train)\\n\\n# %%\\n# We observe that the tuned decision threshold is far away from the default 0.5:\\nprint(f\"Tuned decision threshold: {tuned_model.best_threshold_:.2f}\")\\n\\n# %%\\nprint(\\n    \"Benefit of logistic regression with a tuned threshold: \"\\n    f\"{business_scorer(tuned_model, data_test, target_test, amount=amount_test):,.2f}‚Ç¨\"\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cost_sensitive_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\nprint(\\n    \"Benefit of logistic regression with a tuned threshold: \"\\n    f\"{business_scorer(tuned_model, data_test, target_test, amount=amount_test):,.2f}‚Ç¨\"\\n)\\n\\n# %%\\n# We observe that tuning the decision threshold increases the expected profit\\n# when deploying our model - as indicated by the business metric. It is therefore\\n# valuable, whenever possible, to optimize the decision threshold with respect\\n# to the business metric.\\n#\\n# Manually setting the decision threshold instead of tuning it\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# In the previous example, we used the\\n# :class:`~sklearn.model_selection.TunedThresholdClassifierCV` to find the optimal\\n# decision threshold. However, in some cases, we might have some prior knowledge about\\n# the problem at hand and we might be happy to set the decision threshold manually.\\n#\\n# The class :class:`~sklearn.model_selection.FixedThresholdClassifier` allows us to\\n# manually set the decision threshold. At prediction time, it behave as the previous\\n# tuned model but no search is performed during the fitting process.\\n#\\n# Here, we will reuse the decision threshold found in the previous section to create a\\n# new model and check that it gives the same results.\\nfrom sklearn.model_selection import FixedThresholdClassifier\\n\\nmodel_fixed_threshold = FixedThresholdClassifier(\\n    estimator=model, threshold=tuned_model.best_threshold_, prefit=True\\n).fit(data_train, target_train)\\n\\n# %%\\nbusiness_score = business_scorer(\\n    model_fixed_threshold, data_test, target_test, amount=amount_test\\n)\\nprint(f\"Benefit of logistic regression with a tuned threshold:  {business_score:,.2f}‚Ç¨\")\\n\\n# %%\\n# We observe that we obtained the exact same results but the fitting process\\n# was much faster since we did not perform any hyper-parameter search.\\n#\\n# Finally, the estimate of the (average) business metric itself can be unreliable, in\\n# particular when the number of data points in the minority class is very small.\\n# Any business impact estimated by cross-validation of a business metric on\\n# historical data (offline evaluation) should ideally be confirmed by A/B testing\\n# on live data (online evaluation). Note however that A/B testing models is\\n# beyond the scope of the scikit-learn library itself.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_successive_halving_iterations.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\nSuccessive Halving Iterations\\n=============================\\n\\nThis example illustrates how a successive halving search\\n(:class:`~sklearn.model_selection.HalvingGridSearchCV` and\\n:class:`~sklearn.model_selection.HalvingRandomSearchCV`)\\niteratively chooses the best parameter combination out of\\nmultiple candidates.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy.stats import randint\\n\\nfrom sklearn import datasets\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\\nfrom sklearn.model_selection import HalvingRandomSearchCV\\n\\n# %%\\n# We first define the parameter space and train a\\n# :class:`~sklearn.model_selection.HalvingRandomSearchCV` instance.\\n\\nrng = np.random.RandomState(0)\\n\\nX, y = datasets.make_classification(n_samples=400, n_features=12, random_state=rng)\\n\\nclf = RandomForestClassifier(n_estimators=20, random_state=rng)\\n\\nparam_dist = {\\n    \"max_depth\": [3, None],\\n    \"max_features\": randint(1, 6),\\n    \"min_samples_split\": randint(2, 11),\\n    \"bootstrap\": [True, False],\\n    \"criterion\": [\"gini\", \"entropy\"],\\n}\\n\\nrsh = HalvingRandomSearchCV(\\n    estimator=clf, param_distributions=param_dist, factor=2, random_state=rng\\n)\\nrsh.fit(X, y)\\n\\n# %%\\n# We can now use the `cv_results_` attribute of the search estimator to inspect\\n# and plot the evolution of the search.\\n\\nresults = pd.DataFrame(rsh.cv_results_)\\nresults[\"params_str\"] = results.params.apply(str)\\nresults.drop_duplicates(subset=(\"params_str\", \"iter\"), inplace=True)\\nmean_scores = results.pivot(\\n    index=\"iter\", columns=\"params_str\", values=\"mean_test_score\"\\n)\\nax = mean_scores.plot(legend=False, alpha=0.6)\\n\\nlabels = [\\n    f\"iter={i}\\\\nn_samples={rsh.n_resources_[i]}\\\\nn_candidates={rsh.n_candidates_[i]}\"\\n    for i in range(rsh.n_iterations_)\\n]\\n\\nax.set_xticks(range(rsh.n_iterations_))\\nax.set_xticklabels(labels, rotation=45, multialignment=\"left\")\\nax.set_title(\"Scores of candidates over iterations\")\\nax.set_ylabel(\"mean test score\", fontsize=15)\\nax.set_xlabel(\"iterations\", fontsize=15)\\nplt.tight_layout()\\nplt.show()\\n\\n# %%\\n# Number of candidates and amount of resource at each iteration\\n# -------------------------------------------------------------\\n#\\n# At the first iteration, a small amount of resources is used. The resource\\n# here is the number of samples that the estimators are trained on. All\\n# candidates are evaluated.\\n#\\n# At the second iteration, only the best half of the candidates is evaluated.\\n# The number of allocated resources is doubled: candidates are evaluated on\\n# twice as many samples.\\n#\\n# This process is repeated until the last iteration, where only 2 candidates\\n# are left. The best candidate is the candidate that has the best score at the\\n# last iteration.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_refit_callable.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def lower_bound(cv_results):\\n    \"\"\"\\n    Calculate the lower bound within 1 standard deviation\\n    of the best `mean_test_scores`.\\n\\n    Parameters\\n    ----------\\n    cv_results : dict of numpy(masked) ndarrays\\n        See attribute cv_results_ of `GridSearchCV`\\n\\n    Returns\\n    -------\\n    float\\n        Lower bound within 1 standard deviation of the\\n        best `mean_test_score`.\\n    \"\"\"\\n    best_score_idx = np.argmax(cv_results[\"mean_test_score\"])\\n\\n    return (\\n        cv_results[\"mean_test_score\"][best_score_idx]\\n        - cv_results[\"std_test_score\"][best_score_idx]\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_refit_callable.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def best_low_complexity(cv_results):\\n    \"\"\"\\n    Balance model complexity with cross-validated score.\\n\\n    Parameters\\n    ----------\\n    cv_results : dict of numpy(masked) ndarrays\\n        See attribute cv_results_ of `GridSearchCV`.\\n\\n    Return\\n    ------\\n    int\\n        Index of a model that has the fewest PCA components\\n        while has its test score within 1 standard deviation of the best\\n        `mean_test_score`.\\n    \"\"\"\\n    threshold = lower_bound(cv_results)\\n    candidate_idx = np.flatnonzero(cv_results[\"mean_test_score\"] >= threshold)\\n    best_idx = candidate_idx[\\n        cv_results[\"param_reduce_dim__n_components\"][candidate_idx].argmin()\\n    ]\\n    return best_idx'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_refit_callable.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================================\\nBalance model complexity and cross-validated score\\n==================================================\\n\\nThis example balances model complexity and cross-validated score by\\nfinding a decent accuracy within 1 standard deviation of the best accuracy\\nscore while minimising the number of PCA components [1].\\n\\nThe figure shows the trade-off between cross-validated score and the number\\nof PCA components. The balanced case is when n_components=10 and accuracy=0.88,\\nwhich falls into the range within 1 standard deviation of the best accuracy\\nscore.\\n\\n[1] Hastie, T., Tibshirani, R.,, Friedman, J. (2001). Model Assessment and\\nSelection. The Elements of Statistical Learning (pp. 219-260). New York,\\nNY, USA: Springer New York Inc..\\n\\n\"\"\"\\n\\n# Author: Wenhao Zhang <wenhaoz@ucla.edu>\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.svm import LinearSVC\\n\\n\\n# Code for: def lower_bound(cv_results):\\n\\n\\n# Code for: def best_low_complexity(cv_results):\\n\\n\\npipe = Pipeline(\\n    [\\n        (\"reduce_dim\", PCA(random_state=42)),\\n        (\"classify\", LinearSVC(random_state=42, C=0.01)),\\n    ]\\n)\\n\\nparam_grid = {\"reduce_dim__n_components\": [6, 8, 10, 12, 14]}\\n\\ngrid = GridSearchCV(\\n    pipe,\\n    cv=10,\\n    n_jobs=1,\\n    param_grid=param_grid,\\n    scoring=\"accuracy\",\\n    refit=best_low_complexity,\\n)\\nX, y = load_digits(return_X_y=True)\\ngrid.fit(X, y)\\n\\nn_components = grid.cv_results_[\"param_reduce_dim__n_components\"]\\ntest_scores = grid.cv_results_[\"mean_test_score\"]\\n\\nplt.figure()\\nplt.bar(n_components, test_scores, width=1.3, color=\"b\")\\n\\nlower = lower_bound(grid.cv_results_)\\nplt.axhline(np.max(test_scores), linestyle=\"--\", color=\"y\", label=\"Best score\")\\nplt.axhline(lower, linestyle=\"--\", color=\".5\", label=\"Best score - 1 std\")\\n\\nplt.title(\"Balance model complexity and cross-validated score\")\\nplt.xlabel(\"Number of PCA components used\")\\nplt.ylabel(\"Digit classification accuracy\")\\nplt.xticks(n_components.tolist())\\nplt.ylim((0, 1.0))\\nplt.legend(loc=\"upper left\")\\n\\nbest_index_ = grid.best_index_\\n\\nprint(\"The best_index_ is %d\" % best_index_)\\nprint(\"The n_components selected is %d\" % n_components[best_index_])\\nprint(\\n    \"The corresponding accuracy score is %.2f\"\\n    % grid.cv_results_[\"mean_test_score\"][best_index_]\\n)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_tuned_decision_threshold.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================================\\nPost-hoc tuning the cut-off point of decision function\\n======================================================\\n\\nOnce a binary classifier is trained, the :term:`predict` method outputs class label\\npredictions corresponding to a thresholding of either the :term:`decision_function` or\\nthe :term:`predict_proba` output. The default threshold is defined as a posterior\\nprobability estimate of 0.5 or a decision score of 0.0. However, this default strategy\\nmay not be optimal for the task at hand.\\n\\nThis example shows how to use the\\n:class:`~sklearn.model_selection.TunedThresholdClassifierCV` to tune the decision\\nthreshold, depending on a metric of interest.\\n\"\"\"\\n\\n# %%\\n# The diabetes dataset\\n# --------------------\\n#\\n# To illustrate the tuning of the decision threshold, we will use the diabetes dataset.\\n# This dataset is available on OpenML: https://www.openml.org/d/37. We use the\\n# :func:`~sklearn.datasets.fetch_openml` function to fetch this dataset.\\nfrom sklearn.datasets import fetch_openml\\n\\ndiabetes = fetch_openml(data_id=37, as_frame=True, parser=\"pandas\")\\ndata, target = diabetes.data, diabetes.target\\n\\n# %%\\n# We look at the target to understand the type of problem we are dealing with.\\ntarget.value_counts()\\n\\n# %%\\n# We can see that we are dealing with a binary classification problem. Since the\\n# labels are not encoded as 0 and 1, we make it explicit that we consider the class\\n# labeled \"tested_negative\" as the negative class (which is also the most frequent)\\n# and the class labeled \"tested_positive\" the positive as the positive class:\\nneg_label, pos_label = target.value_counts().index\\n\\n# %%\\n# We can also observe that this binary problem is slightly imbalanced where we have\\n# around twice more samples from the negative class than from the positive class. When\\n# it comes to evaluation, we should consider this aspect to interpret the results.\\n#\\n# Our vanilla classifier\\n# ----------------------\\n#\\n# We define a basic predictive model composed of a scaler followed by a logistic\\n# regression classifier.\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\\nmodel\\n\\n# %%\\n# We evaluate our model using cross-validation. We use the accuracy and the balanced\\n# accuracy to report the performance of our model. The balanced accuracy is a metric\\n# that is less sensitive to class imbalance and will allow us to put the accuracy\\n# score in perspective.\\n#\\n# Cross-validation allows us to study the variance of the decision threshold across\\n# different splits of the data. However, the dataset is rather small and it would be\\n# detrimental to use more than 5 folds to evaluate the dispersion. Therefore, we use\\n# a :class:`~sklearn.model_selection.RepeatedStratifiedKFold` where we apply several\\n# repetitions of 5-fold cross-validation.\\nimport pandas as pd'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_tuned_decision_threshold.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\\n\\nscoring = [\"accuracy\", \"balanced_accuracy\"]\\ncv_scores = [\\n    \"train_accuracy\",\\n    \"test_accuracy\",\\n    \"train_balanced_accuracy\",\\n    \"test_balanced_accuracy\",\\n]\\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\\ncv_results_vanilla_model = pd.DataFrame(\\n    cross_validate(\\n        model,\\n        data,\\n        target,\\n        scoring=scoring,\\n        cv=cv,\\n        return_train_score=True,\\n        return_estimator=True,\\n    )\\n)\\ncv_results_vanilla_model[cv_scores].aggregate([\"mean\", \"std\"]).T\\n\\n# %%\\n# Our predictive model succeeds to grasp the relationship between the data and the\\n# target. The training and testing scores are close to each other, meaning that our\\n# predictive model is not overfitting. We can also observe that the balanced accuracy is\\n# lower than the accuracy, due to the class imbalance previously mentioned.\\n#\\n# For this classifier, we let the decision threshold, used convert the probability of\\n# the positive class into a class prediction, to its default value: 0.5. However, this\\n# threshold might not be optimal. If our interest is to maximize the balanced accuracy,\\n# we should select another threshold that would maximize this metric.\\n#\\n# The :class:`~sklearn.model_selection.TunedThresholdClassifierCV` meta-estimator allows\\n# to tune the decision threshold of a classifier given a metric of interest.\\n#\\n# Tuning the decision threshold\\n# -----------------------------\\n#\\n# We create a :class:`~sklearn.model_selection.TunedThresholdClassifierCV` and\\n# configure it to maximize the balanced accuracy. We evaluate the model using the same\\n# cross-validation strategy as previously.\\nfrom sklearn.model_selection import TunedThresholdClassifierCV\\n\\ntuned_model = TunedThresholdClassifierCV(estimator=model, scoring=\"balanced_accuracy\")\\ncv_results_tuned_model = pd.DataFrame(\\n    cross_validate(\\n        tuned_model,\\n        data,\\n        target,\\n        scoring=scoring,\\n        cv=cv,\\n        return_train_score=True,\\n        return_estimator=True,\\n    )\\n)\\ncv_results_tuned_model[cv_scores].aggregate([\"mean\", \"std\"]).T\\n\\n# %%\\n# In comparison with the vanilla model, we observe that the balanced accuracy score\\n# increased. Of course, it comes at the cost of a lower accuracy score. It means that\\n# our model is now more sensitive to the positive class but makes more mistakes on the\\n# negative class.\\n#\\n# However, it is important to note that this tuned predictive model is internally the\\n# same model as the vanilla model: they have the same fitted coefficients.\\nimport matplotlib.pyplot as plt\\n\\nvanilla_model_coef = pd.DataFrame(\\n    [est[-1].coef_.ravel() for est in cv_results_vanilla_model[\"estimator\"]],\\n    columns=diabetes.feature_names,\\n)\\ntuned_model_coef = pd.DataFrame(\\n    [est.estimator_[-1].coef_.ravel() for est in cv_results_tuned_model[\"estimator\"]],\\n    columns=diabetes.feature_names,\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_tuned_decision_threshold.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig, ax = plt.subplots(ncols=2, figsize=(12, 4), sharex=True, sharey=True)\\nvanilla_model_coef.boxplot(ax=ax[0])\\nax[0].set_ylabel(\"Coefficient value\")\\nax[0].set_title(\"Vanilla model\")\\ntuned_model_coef.boxplot(ax=ax[1])\\nax[1].set_title(\"Tuned model\")\\n_ = fig.suptitle(\"Coefficients of the predictive models\")\\n\\n# %%\\n# Only the decision threshold of each model was changed during the cross-validation.\\ndecision_threshold = pd.Series(\\n    [est.best_threshold_ for est in cv_results_tuned_model[\"estimator\"]],\\n)\\nax = decision_threshold.plot.kde()\\nax.axvline(\\n    decision_threshold.mean(),\\n    color=\"k\",\\n    linestyle=\"--\",\\n    label=f\"Mean decision threshold: {decision_threshold.mean():.2f}\",\\n)\\nax.set_xlabel(\"Decision threshold\")\\nax.legend(loc=\"upper right\")\\n_ = ax.set_title(\\n    \"Distribution of the decision threshold \\\\nacross different cross-validation folds\"\\n)\\n\\n# %%\\n# In average, a decision threshold around 0.32 maximizes the balanced accuracy, which is\\n# different from the default decision threshold of 0.5. Thus tuning the decision\\n# threshold is particularly important when the output of the predictive model\\n# is used to make decisions. Besides, the metric used to tune the decision threshold\\n# should be chosen carefully. Here, we used the balanced accuracy but it might not be\\n# the most appropriate metric for the problem at hand. The choice of the \"right\" metric\\n# is usually problem-dependent and might require some domain knowledge. Refer to the\\n# example entitled,\\n# :ref:`sphx_glr_auto_examples_model_selection_plot_cost_sensitive_learning.py`,\\n# for more details.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_randomized_search.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def report(results, n_top=3):\\n    for i in range(1, n_top + 1):\\n        candidates = np.flatnonzero(results[\"rank_test_score\"] == i)\\n        for candidate in candidates:\\n            print(\"Model with rank: {0}\".format(i))\\n            print(\\n                \"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\\n                    results[\"mean_test_score\"][candidate],\\n                    results[\"std_test_score\"][candidate],\\n                )\\n            )\\n            print(\"Parameters: {0}\".format(results[\"params\"][candidate]))\\n            print(\"\")'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_randomized_search.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================================\\nComparing randomized search and grid search for hyperparameter estimation\\n=========================================================================\\n\\nCompare randomized search and grid search for optimizing hyperparameters of a\\nlinear SVM with SGD training.\\nAll parameters that influence the learning are searched simultaneously\\n(except for the number of estimators, which poses a time / quality tradeoff).\\n\\nThe randomized search and the grid search explore exactly the same space of\\nparameters. The result in parameter settings is quite similar, while the run\\ntime for randomized search is drastically lower.\\n\\nThe performance is may slightly worse for the randomized search, and is likely\\ndue to a noise effect and would not carry over to a held-out test set.\\n\\nNote that in practice, one would not search over this many different parameters\\nsimultaneously using grid search, but pick only the ones deemed most important.\\n\\n\"\"\"\\n\\nfrom time import time\\n\\nimport numpy as np\\nimport scipy.stats as stats\\n\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\\n\\n# get some data\\nX, y = load_digits(return_X_y=True, n_class=3)\\n\\n# build a classifier\\nclf = SGDClassifier(loss=\"hinge\", penalty=\"elasticnet\", fit_intercept=True)\\n\\n\\n# Utility function to report best scores\\n# Code for: def report(results, n_top=3):\\n\\n\\n# specify parameters and distributions to sample from\\nparam_dist = {\\n    \"average\": [True, False],\\n    \"l1_ratio\": stats.uniform(0, 1),\\n    \"alpha\": stats.loguniform(1e-2, 1e0),\\n}\\n\\n# run randomized search\\nn_iter_search = 15\\nrandom_search = RandomizedSearchCV(\\n    clf, param_distributions=param_dist, n_iter=n_iter_search\\n)\\n\\nstart = time()\\nrandom_search.fit(X, y)\\nprint(\\n    \"RandomizedSearchCV took %.2f seconds for %d candidates parameter settings.\"\\n    % ((time() - start), n_iter_search)\\n)\\nreport(random_search.cv_results_)\\n\\n# use a full grid over all parameters\\nparam_grid = {\\n    \"average\": [True, False],\\n    \"l1_ratio\": np.linspace(0, 1, num=10),\\n    \"alpha\": np.power(10, np.arange(-2, 1, dtype=float)),\\n}\\n\\n# run grid search\\ngrid_search = GridSearchCV(clf, param_grid=param_grid)\\nstart = time()\\ngrid_search.fit(X, y)\\n\\nprint(\\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\\n    % (time() - start, len(grid_search.cv_results_[\"params\"]))\\n)\\nreport(grid_search.cv_results_)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_precision_recall.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================\\nPrecision-Recall\\n================\\n\\nExample of Precision-Recall metric to evaluate classifier output quality.\\n\\nPrecision-Recall is a useful measure of success of prediction when the\\nclasses are very imbalanced. In information retrieval, precision is a\\nmeasure of the fraction of relevant items among actually returned items while recall\\nis a measure of the fraction of items that were returned among all items that should\\nhave been returned. \\'Relevancy\\' here refers to items that are\\npostively labeled, i.e., true positives and false negatives.\\n\\nPrecision (:math:`P`) is defined as the number of true positives (:math:`T_p`)\\nover the number of true positives plus the number of false positives\\n(:math:`F_p`).\\n\\n.. math::\\n    P = \\\\\\\\frac{T_p}{T_p+F_p}\\n\\nRecall (:math:`R`) is defined as the number of true positives (:math:`T_p`)\\nover the number of true positives plus the number of false negatives\\n(:math:`F_n`).\\n\\n.. math::\\n    R = \\\\\\\\frac{T_p}{T_p + F_n}\\n\\nThe precision-recall curve shows the tradeoff between precision and\\nrecall for different thresholds. A high area under the curve represents\\nboth high recall and high precision. High precision is achieved by having\\nfew false positives in the returned results, and high recall is achieved by\\nhaving few false negatives in the relevant results.\\nHigh scores for both show that the classifier is returning\\naccurate results (high precision), as well as returning a majority of all relevant\\nresults (high recall).\\n\\nA system with high recall but low precision returns most of the relevant items, but\\nthe proportion of returned results that are incorrectly labeled is high. A\\nsystem with high precision but low recall is just the opposite, returning very\\nfew of the relevant items, but most of its predicted labels are correct when compared\\nto the actual labels. An ideal system with high precision and high recall will\\nreturn most of the relevant items, with most results labeled correctly.\\n\\nThe definition of precision (:math:`\\\\\\\\frac{T_p}{T_p + F_p}`) shows that lowering\\nthe threshold of a classifier may increase the denominator, by increasing the\\nnumber of results returned. If the threshold was previously set too high, the\\nnew results may all be true positives, which will increase precision. If the\\nprevious threshold was about right or too low, further lowering the threshold\\nwill introduce false positives, decreasing precision.\\n\\nRecall is defined as :math:`\\\\\\\\frac{T_p}{T_p+F_n}`, where :math:`T_p+F_n` does\\nnot depend on the classifier threshold. Changing the classifier threshold can only\\nchange the numerator, :math:`T_p`. Lowering the classifier\\nthreshold may increase recall, by increasing the number of true positive\\nresults. It is also possible that lowering the threshold may leave recall\\nunchanged, while the precision fluctuates. Thus, precision does not necessarily\\ndecrease with recall.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_precision_recall.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='The relationship between recall and precision can be observed in the\\nstairstep area of the plot - at the edges of these steps a small change\\nin the threshold considerably reduces precision, with only a minor gain in\\nrecall.\\n\\n**Average precision** (AP) summarizes such a plot as the weighted mean of\\nprecisions achieved at each threshold, with the increase in recall from the\\nprevious threshold used as the weight:\\n\\n:math:`\\\\\\\\text{AP} = \\\\\\\\sum_n (R_n - R_{n-1}) P_n`\\n\\nwhere :math:`P_n` and :math:`R_n` are the precision and recall at the\\nnth threshold. A pair :math:`(R_k, P_k)` is referred to as an\\n*operating point*.\\n\\nAP and the trapezoidal area under the operating points\\n(:func:`sklearn.metrics.auc`) are common ways to summarize a precision-recall\\ncurve that lead to different results. Read more in the\\n:ref:`User Guide <precision_recall_f_measure_metrics>`.\\n\\nPrecision-recall curves are typically used in binary classification to study\\nthe output of a classifier. In order to extend the precision-recall curve and\\naverage precision to multi-class or multi-label classification, it is necessary\\nto binarize the output. One curve can be drawn per label, but one can also draw\\na precision-recall curve by considering each element of the label indicator\\nmatrix as a binary prediction (:ref:`micro-averaging <average>`).\\n\\n.. note::\\n\\n    See also :func:`sklearn.metrics.average_precision_score`,\\n             :func:`sklearn.metrics.recall_score`,\\n             :func:`sklearn.metrics.precision_score`,\\n             :func:`sklearn.metrics.f1_score`\\n\"\"\"\\n\\n# %%\\n# In binary classification settings\\n# ---------------------------------\\n#\\n# Dataset and model\\n# .................\\n#\\n# We will use a Linear SVC classifier to differentiate two types of irises.\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_iris(return_X_y=True)\\n\\n# Add noisy features\\nrandom_state = np.random.RandomState(0)\\nn_samples, n_features = X.shape\\nX = np.concatenate([X, random_state.randn(n_samples, 200 * n_features)], axis=1)\\n\\n# Limit to the two first classes, and split into training and test\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X[y < 2], y[y < 2], test_size=0.5, random_state=random_state\\n)\\n\\n# %%\\n# Linear SVC will expect each feature to have a similar range of values. Thus,\\n# we will first scale the data using a\\n# :class:`~sklearn.preprocessing.StandardScaler`.\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import LinearSVC\\n\\nclassifier = make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))\\nclassifier.fit(X_train, y_train)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_precision_recall.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='classifier = make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))\\nclassifier.fit(X_train, y_train)\\n\\n# %%\\n# Plot the Precision-Recall curve\\n# ...............................\\n#\\n# To plot the precision-recall curve, you should use\\n# :class:`~sklearn.metrics.PrecisionRecallDisplay`. Indeed, there is two\\n# methods available depending if you already computed the predictions of the\\n# classifier or not.\\n#\\n# Let\\'s first plot the precision-recall curve without the classifier\\n# predictions. We use\\n# :func:`~sklearn.metrics.PrecisionRecallDisplay.from_estimator` that\\n# computes the predictions for us before plotting the curve.\\nfrom sklearn.metrics import PrecisionRecallDisplay\\n\\ndisplay = PrecisionRecallDisplay.from_estimator(\\n    classifier, X_test, y_test, name=\"LinearSVC\", plot_chance_level=True\\n)\\n_ = display.ax_.set_title(\"2-class Precision-Recall curve\")\\n\\n# %%\\n# If we already got the estimated probabilities or scores for\\n# our model, then we can use\\n# :func:`~sklearn.metrics.PrecisionRecallDisplay.from_predictions`.\\ny_score = classifier.decision_function(X_test)\\n\\ndisplay = PrecisionRecallDisplay.from_predictions(\\n    y_test, y_score, name=\"LinearSVC\", plot_chance_level=True\\n)\\n_ = display.ax_.set_title(\"2-class Precision-Recall curve\")\\n\\n# %%\\n# In multi-label settings\\n# -----------------------\\n#\\n# The precision-recall curve does not support the multilabel setting. However,\\n# one can decide how to handle this case. We show such an example below.\\n#\\n# Create multi-label data, fit, and predict\\n# .........................................\\n#\\n# We create a multi-label dataset, to illustrate the precision-recall in\\n# multi-label settings.\\n\\nfrom sklearn.preprocessing import label_binarize\\n\\n# Use label_binarize to be multi-label like settings\\nY = label_binarize(y, classes=[0, 1, 2])\\nn_classes = Y.shape[1]\\n\\n# Split into training and test\\nX_train, X_test, Y_train, Y_test = train_test_split(\\n    X, Y, test_size=0.5, random_state=random_state\\n)\\n\\n# %%\\n# We use :class:`~sklearn.multiclass.OneVsRestClassifier` for multi-label\\n# prediction.\\nfrom sklearn.multiclass import OneVsRestClassifier\\n\\nclassifier = OneVsRestClassifier(\\n    make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))\\n)\\nclassifier.fit(X_train, Y_train)\\ny_score = classifier.decision_function(X_test)\\n\\n\\n# %%\\n# The average precision score in multi-label settings\\n# ...................................................\\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\\n\\n# For each class\\nprecision = dict()\\nrecall = dict()\\naverage_precision = dict()\\nfor i in range(n_classes):\\n    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i], y_score[:, i])\\n    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_precision_recall.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# For each class\\nprecision = dict()\\nrecall = dict()\\naverage_precision = dict()\\nfor i in range(n_classes):\\n    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i], y_score[:, i])\\n    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\\n\\n# A \"micro-average\": quantifying score on all classes jointly\\nprecision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\\n    Y_test.ravel(), y_score.ravel()\\n)\\naverage_precision[\"micro\"] = average_precision_score(Y_test, y_score, average=\"micro\")\\n\\n# %%\\n# Plot the micro-averaged Precision-Recall curve\\n# ..............................................\\nfrom collections import Counter\\n\\ndisplay = PrecisionRecallDisplay(\\n    recall=recall[\"micro\"],\\n    precision=precision[\"micro\"],\\n    average_precision=average_precision[\"micro\"],\\n    prevalence_pos_label=Counter(Y_test.ravel())[1] / Y_test.size,\\n)\\ndisplay.plot(plot_chance_level=True)\\n_ = display.ax_.set_title(\"Micro-averaged over all classes\")\\n\\n# %%\\n# Plot Precision-Recall curve for each class and iso-f1 curves\\n# ............................................................\\nfrom itertools import cycle\\n\\nimport matplotlib.pyplot as plt\\n\\n# setup plot details\\ncolors = cycle([\"navy\", \"turquoise\", \"darkorange\", \"cornflowerblue\", \"teal\"])\\n\\n_, ax = plt.subplots(figsize=(7, 8))\\n\\nf_scores = np.linspace(0.2, 0.8, num=4)\\nlines, labels = [], []\\nfor f_score in f_scores:\\n    x = np.linspace(0.01, 1)\\n    y = f_score * x / (2 * x - f_score)\\n    (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\\n    plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\\n\\ndisplay = PrecisionRecallDisplay(\\n    recall=recall[\"micro\"],\\n    precision=precision[\"micro\"],\\n    average_precision=average_precision[\"micro\"],\\n)\\ndisplay.plot(ax=ax, name=\"Micro-average precision-recall\", color=\"gold\")\\n\\nfor i, color in zip(range(n_classes), colors):\\n    display = PrecisionRecallDisplay(\\n        recall=recall[i],\\n        precision=precision[i],\\n        average_precision=average_precision[i],\\n    )\\n    display.plot(ax=ax, name=f\"Precision-recall for class {i}\", color=color)\\n\\n# add the legend for the iso-f1 curves\\nhandles, labels = display.ax_.get_legend_handles_labels()\\nhandles.extend([l])\\nlabels.extend([\"iso-f1 curves\"])\\n# set the legend and the axes\\nax.legend(handles=handles, labels=labels, loc=\"best\")\\nax.set_title(\"Extension of Precision-Recall curve to multi-class\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_permutation_tests_for_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================================\\nTest with permutations the significance of a classification score\\n=================================================================\\n\\nThis example demonstrates the use of\\n:func:`~sklearn.model_selection.permutation_test_score` to evaluate the\\nsignificance of a cross-validated score using permutations.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Dataset\\n# -------\\n#\\n# We will use the :ref:`iris_dataset`, which consists of measurements taken\\n# from 3 types of irises.\\n\\nfrom sklearn.datasets import load_iris\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\n\\n# %%\\n# We will also generate some random feature data (i.e., 20 features),\\n# uncorrelated with the class labels in the iris dataset.\\n\\nimport numpy as np\\n\\nn_uncorrelated_features = 20\\nrng = np.random.RandomState(seed=0)\\n# Use same number of samples as in iris and 20 features\\nX_rand = rng.normal(size=(X.shape[0], n_uncorrelated_features))\\n\\n# %%\\n# Permutation test score\\n# ----------------------\\n#\\n# Next, we calculate the\\n# :func:`~sklearn.model_selection.permutation_test_score` using the original\\n# iris dataset, which strongly predict the labels and\\n# the randomly generated features and iris labels, which should have\\n# no dependency between features and labels. We use the\\n# :class:`~sklearn.svm.SVC` classifier and :ref:`accuracy_score` to evaluate\\n# the model at each round.\\n#\\n# :func:`~sklearn.model_selection.permutation_test_score` generates a null\\n# distribution by calculating the accuracy of the classifier\\n# on 1000 different permutations of the dataset, where features\\n# remain the same but labels undergo different permutations. This is the\\n# distribution for the null hypothesis which states there is no dependency\\n# between the features and labels. An empirical p-value is then calculated as\\n# the percentage of permutations for which the score obtained is greater\\n# that the score obtained using the original data.\\n\\nfrom sklearn.model_selection import StratifiedKFold, permutation_test_score\\nfrom sklearn.svm import SVC\\n\\nclf = SVC(kernel=\"linear\", random_state=7)\\ncv = StratifiedKFold(2, shuffle=True, random_state=0)\\n\\nscore_iris, perm_scores_iris, pvalue_iris = permutation_test_score(\\n    clf, X, y, scoring=\"accuracy\", cv=cv, n_permutations=1000\\n)\\n\\nscore_rand, perm_scores_rand, pvalue_rand = permutation_test_score(\\n    clf, X_rand, y, scoring=\"accuracy\", cv=cv, n_permutations=1000\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_permutation_tests_for_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='score_iris, perm_scores_iris, pvalue_iris = permutation_test_score(\\n    clf, X, y, scoring=\"accuracy\", cv=cv, n_permutations=1000\\n)\\n\\nscore_rand, perm_scores_rand, pvalue_rand = permutation_test_score(\\n    clf, X_rand, y, scoring=\"accuracy\", cv=cv, n_permutations=1000\\n)\\n\\n# %%\\n# Original data\\n# ^^^^^^^^^^^^^\\n#\\n# Below we plot a histogram of the permutation scores (the null\\n# distribution). The red line indicates the score obtained by the classifier\\n# on the original data. The score is much better than those obtained by\\n# using permuted data and the p-value is thus very low. This indicates that\\n# there is a low likelihood that this good score would be obtained by chance\\n# alone. It provides evidence that the iris dataset contains real dependency\\n# between features and labels and the classifier was able to utilize this\\n# to obtain good results.\\n\\nimport matplotlib.pyplot as plt\\n\\nfig, ax = plt.subplots()\\n\\nax.hist(perm_scores_iris, bins=20, density=True)\\nax.axvline(score_iris, ls=\"--\", color=\"r\")\\nscore_label = f\"Score on original\\\\ndata: {score_iris:.2f}\\\\n(p-value: {pvalue_iris:.3f})\"\\nax.text(0.7, 10, score_label, fontsize=12)\\nax.set_xlabel(\"Accuracy score\")\\n_ = ax.set_ylabel(\"Probability density\")\\n\\n# %%\\n# Random data\\n# ^^^^^^^^^^^\\n#\\n# Below we plot the null distribution for the randomized data. The permutation\\n# scores are similar to those obtained using the original iris dataset\\n# because the permutation always destroys any feature label dependency present.\\n# The score obtained on the original randomized data in this case though, is\\n# very poor. This results in a large p-value, confirming that there was no\\n# feature label dependency in the original data.\\n\\nfig, ax = plt.subplots()\\n\\nax.hist(perm_scores_rand, bins=20, density=True)\\nax.set_xlim(0.13)\\nax.axvline(score_rand, ls=\"--\", color=\"r\")\\nscore_label = f\"Score on original\\\\ndata: {score_rand:.2f}\\\\n(p-value: {pvalue_rand:.3f})\"\\nax.text(0.14, 7.5, score_label, fontsize=12)\\nax.set_xlabel(\"Accuracy score\")\\nax.set_ylabel(\"Probability density\")\\nplt.show()\\n\\n# %%\\n# Another possible reason for obtaining a high p-value is that the classifier\\n# was not able to use the structure in the data. In this case, the p-value\\n# would only be low for classifiers that are able to utilize the dependency\\n# present. In our case above, where the data is random, all classifiers would\\n# have a high p-value as there is no structure present in the data.\\n#\\n# Finally, note that this test has been shown to produce low p-values even\\n# if there is only weak structure in the data [1]_.\\n#\\n# .. rubric:: References\\n#\\n# .. [1] Ojala and Garriga. `Permutation Tests for Studying Classifier\\n#        Performance\\n#        <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. The\\n#        Journal of Machine Learning Research (2010) vol. 11\\n#'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_underfitting_overfitting.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def true_fun(X):\\n    return np.cos(1.5 * np.pi * X)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_underfitting_overfitting.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================\\nUnderfitting vs. Overfitting\\n============================\\n\\nThis example demonstrates the problems of underfitting and overfitting and\\nhow we can use linear regression with polynomial features to approximate\\nnonlinear functions. The plot shows the function that we want to approximate,\\nwhich is a part of the cosine function. In addition, the samples from the\\nreal function and the approximations of different models are displayed. The\\nmodels have polynomial features of different degrees. We can see that a\\nlinear function (polynomial with degree 1) is not sufficient to fit the\\ntraining samples. This is called **underfitting**. A polynomial of degree 4\\napproximates the true function almost perfectly. However, for higher degrees\\nthe model will **overfit** the training data, i.e. it learns the noise of the\\ntraining data.\\nWe evaluate quantitatively **overfitting** / **underfitting** by using\\ncross-validation. We calculate the mean squared error (MSE) on the validation\\nset, the higher, the less likely the model generalizes correctly from the\\ntraining data.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import PolynomialFeatures\\n\\n\\n# Code for: def true_fun(X):\\n\\n\\nnp.random.seed(0)\\n\\nn_samples = 30\\ndegrees = [1, 4, 15]\\n\\nX = np.sort(np.random.rand(n_samples))\\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\\n\\nplt.figure(figsize=(14, 5))\\nfor i in range(len(degrees)):\\n    ax = plt.subplot(1, len(degrees), i + 1)\\n    plt.setp(ax, xticks=(), yticks=())\\n\\n    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\\n    linear_regression = LinearRegression()\\n    pipeline = Pipeline(\\n        [\\n            (\"polynomial_features\", polynomial_features),\\n            (\"linear_regression\", linear_regression),\\n        ]\\n    )\\n    pipeline.fit(X[:, np.newaxis], y)\\n\\n    # Evaluate the models using crossvalidation\\n    scores = cross_val_score(\\n        pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10\\n    )\\n\\n    X_test = np.linspace(0, 1, 100)\\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\\n    plt.plot(X_test, true_fun(X_test), label=\"True function\")\\n    plt.scatter(X, y, edgecolor=\"b\", s=20, label=\"Samples\")\\n    plt.xlabel(\"x\")\\n    plt.ylabel(\"y\")\\n    plt.xlim((0, 1))\\n    plt.ylim((-2, 2))\\n    plt.legend(loc=\"best\")\\n    plt.title(\\n        \"Degree {}\\\\nMSE = {:.2e}(+/- {:.2e})\".format(\\n            degrees[i], -scores.mean(), scores.std()\\n        )\\n    )\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_nested_cross_validation_iris.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================\\nNested versus non-nested cross-validation\\n=========================================\\n\\nThis example compares non-nested and nested cross-validation strategies on a\\nclassifier of the iris data set. Nested cross-validation (CV) is often used to\\ntrain a model in which hyperparameters also need to be optimized. Nested CV\\nestimates the generalization error of the underlying model and its\\n(hyper)parameter search. Choosing the parameters that maximize non-nested CV\\nbiases the model to the dataset, yielding an overly-optimistic score.\\n\\nModel selection without nested CV uses the same data to tune model parameters\\nand evaluate model performance. Information may thus \"leak\" into the model\\nand overfit the data. The magnitude of this effect is primarily dependent on\\nthe size of the dataset and the stability of the model. See Cawley and Talbot\\n[1]_ for an analysis of these issues.\\n\\nTo avoid this problem, nested CV effectively uses a series of\\ntrain/validation/test set splits. In the inner loop (here executed by\\n:class:`GridSearchCV <sklearn.model_selection.GridSearchCV>`), the score is\\napproximately maximized by fitting a model to each training set, and then\\ndirectly maximized in selecting (hyper)parameters over the validation set. In\\nthe outer loop (here in :func:`cross_val_score\\n<sklearn.model_selection.cross_val_score>`), generalization error is estimated\\nby averaging test set scores over several dataset splits.\\n\\nThe example below uses a support vector classifier with a non-linear kernel to\\nbuild a model with optimized hyperparameters by grid search. We compare the\\nperformance of non-nested and nested CV strategies by taking the difference\\nbetween their scores.\\n\\n.. seealso::\\n\\n    - :ref:`cross_validation`\\n    - :ref:`grid_search`\\n\\n.. rubric:: References\\n\\n.. [1] `Cawley, G.C.; Talbot, N.L.C. On over-fitting in model selection and\\n    subsequent selection bias in performance evaluation.\\n    J. Mach. Learn. Res 2010,11, 2079-2107.\\n    <http://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf>`_\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\\nfrom sklearn.svm import SVC\\n\\n# Number of random trials\\nNUM_TRIALS = 30\\n\\n# Load the dataset\\niris = load_iris()\\nX_iris = iris.data\\ny_iris = iris.target\\n\\n# Set up possible values of parameters to optimize over\\np_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\\n\\n# We will use a Support Vector Classifier with \"rbf\" kernel\\nsvm = SVC(kernel=\"rbf\")\\n\\n# Arrays to store scores\\nnon_nested_scores = np.zeros(NUM_TRIALS)\\nnested_scores = np.zeros(NUM_TRIALS)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_nested_cross_validation_iris.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Set up possible values of parameters to optimize over\\np_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\\n\\n# We will use a Support Vector Classifier with \"rbf\" kernel\\nsvm = SVC(kernel=\"rbf\")\\n\\n# Arrays to store scores\\nnon_nested_scores = np.zeros(NUM_TRIALS)\\nnested_scores = np.zeros(NUM_TRIALS)\\n\\n# Loop for each trial\\nfor i in range(NUM_TRIALS):\\n    # Choose cross-validation techniques for the inner and outer loops,\\n    # independently of the dataset.\\n    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\\n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\\n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\\n\\n    # Non_nested parameter search and scoring\\n    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=outer_cv)\\n    clf.fit(X_iris, y_iris)\\n    non_nested_scores[i] = clf.best_score_\\n\\n    # Nested CV with parameter optimization\\n    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)\\n    nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)\\n    nested_scores[i] = nested_score.mean()\\n\\nscore_difference = non_nested_scores - nested_scores\\n\\nprint(\\n    \"Average difference of {:6f} with std. dev. of {:6f}.\".format(\\n        score_difference.mean(), score_difference.std()\\n    )\\n)\\n\\n# Plot scores on each trial for nested and non-nested CV\\nplt.figure()\\nplt.subplot(211)\\n(non_nested_scores_line,) = plt.plot(non_nested_scores, color=\"r\")\\n(nested_line,) = plt.plot(nested_scores, color=\"b\")\\nplt.ylabel(\"score\", fontsize=\"14\")\\nplt.legend(\\n    [non_nested_scores_line, nested_line],\\n    [\"Non-Nested CV\", \"Nested CV\"],\\n    bbox_to_anchor=(0, 0.4, 0.5, 0),\\n)\\nplt.title(\\n    \"Non-Nested and Nested Cross Validation on Iris Dataset\",\\n    x=0.5,\\n    y=1.1,\\n    fontsize=\"15\",\\n)\\n\\n# Plot bar chart of the difference.\\nplt.subplot(212)\\ndifference_plot = plt.bar(range(NUM_TRIALS), score_difference)\\nplt.xlabel(\"Individual Trial #\")\\nplt.legend(\\n    [difference_plot],\\n    [\"Non-Nested CV - Nested CV Score\"],\\n    bbox_to_anchor=(0, 1, 0.8, 0),\\n)\\nplt.ylabel(\"score difference\", fontsize=\"14\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def corrected_std(differences, n_train, n_test):\\n    \"\"\"Corrects standard deviation using Nadeau and Bengio\\'s approach.\\n\\n    Parameters\\n    ----------\\n    differences : ndarray of shape (n_samples,)\\n        Vector containing the differences in the score metrics of two models.\\n    n_train : int\\n        Number of samples in the training set.\\n    n_test : int\\n        Number of samples in the testing set.\\n\\n    Returns\\n    -------\\n    corrected_std : float\\n        Variance-corrected standard deviation of the set of differences.\\n    \"\"\"\\n    # kr = k times r, r times repeated k-fold crossvalidation,\\n    # kr equals the number of times the model was evaluated\\n    kr = len(differences)\\n    corrected_var = np.var(differences, ddof=1) * (1 / kr + n_test / n_train)\\n    corrected_std = np.sqrt(corrected_var)\\n    return corrected_std'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def compute_corrected_ttest(differences, df, n_train, n_test):\\n    \"\"\"Computes right-tailed paired t-test with corrected variance.\\n\\n    Parameters\\n    ----------\\n    differences : array-like of shape (n_samples,)\\n        Vector containing the differences in the score metrics of two models.\\n    df : int\\n        Degrees of freedom.\\n    n_train : int\\n        Number of samples in the training set.\\n    n_test : int\\n        Number of samples in the testing set.\\n\\n    Returns\\n    -------\\n    t_stat : float\\n        Variance-corrected t-statistic.\\n    p_val : float\\n        Variance-corrected p-value.\\n    \"\"\"\\n    mean = np.mean(differences)\\n    std = corrected_std(differences, n_train, n_test)\\n    t_stat = mean / std\\n    p_val = t.sf(np.abs(t_stat), df)  # right-tailed t-test\\n    return t_stat, p_val'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================================\\nStatistical comparison of models using grid search\\n==================================================\\n\\nThis example illustrates how to statistically compare the performance of models\\ntrained and evaluated using :class:`~sklearn.model_selection.GridSearchCV`.\\n\\n\"\"\"\\n\\n# %%\\n# We will start by simulating moon shaped data (where the ideal separation\\n# between classes is non-linear), adding to it a moderate degree of noise.\\n# Datapoints will belong to one of two possible classes to be predicted by two\\n# features. We will simulate 50 samples for each class:\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\nfrom sklearn.datasets import make_moons\\n\\nX, y = make_moons(noise=0.352, random_state=1, n_samples=100)\\n\\nsns.scatterplot(\\n    x=X[:, 0], y=X[:, 1], hue=y, marker=\"o\", s=25, edgecolor=\"k\", legend=False\\n).set_title(\"Data\")\\nplt.show()\\n\\n# %%\\n# We will compare the performance of :class:`~sklearn.svm.SVC` estimators that\\n# vary on their `kernel` parameter, to decide which choice of this\\n# hyper-parameter predicts our simulated data best.\\n# We will evaluate the performance of the models using\\n# :class:`~sklearn.model_selection.RepeatedStratifiedKFold`, repeating 10 times\\n# a 10-fold stratified cross validation using a different randomization of the\\n# data in each repetition. The performance will be evaluated using\\n# :class:`~sklearn.metrics.roc_auc_score`.\\n\\nfrom sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\\nfrom sklearn.svm import SVC\\n\\nparam_grid = [\\n    {\"kernel\": [\"linear\"]},\\n    {\"kernel\": [\"poly\"], \"degree\": [2, 3]},\\n    {\"kernel\": [\"rbf\"]},\\n]\\n\\nsvc = SVC(random_state=0)\\n\\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=0)\\n\\nsearch = GridSearchCV(estimator=svc, param_grid=param_grid, scoring=\"roc_auc\", cv=cv)\\nsearch.fit(X, y)\\n\\n# %%\\n# We can now inspect the results of our search, sorted by their\\n# `mean_test_score`:\\n\\nimport pandas as pd\\n\\nresults_df = pd.DataFrame(search.cv_results_)\\nresults_df = results_df.sort_values(by=[\"rank_test_score\"])\\nresults_df = results_df.set_index(\\n    results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\\n).rename_axis(\"kernel\")\\nresults_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We can see that the estimator using the `\\'rbf\\'` kernel performed best,\\n# closely followed by `\\'linear\\'`. Both estimators with a `\\'poly\\'` kernel\\n# performed worse, with the one using a two-degree polynomial achieving a much\\n# lower performance than all other models.\\n#\\n# Usually, the analysis just ends here, but half the story is missing. The\\n# output of :class:`~sklearn.model_selection.GridSearchCV` does not provide\\n# information on the certainty of the differences between the models.\\n# We don\\'t know if these are **statistically** significant.\\n# To evaluate this, we need to conduct a statistical test.\\n# Specifically, to contrast the performance of two models we should\\n# statistically compare their AUC scores. There are 100 samples (AUC\\n# scores) for each model as we repreated 10 times a 10-fold cross-validation.\\n#\\n# However, the scores of the models are not independent: all models are\\n# evaluated on the **same** 100 partitions, increasing the correlation\\n# between the performance of the models.\\n# Since some partitions of the data can make the distinction of the classes\\n# particularly easy or hard to find for all models, the models scores will\\n# co-vary.\\n#\\n# Let\\'s inspect this partition effect by plotting the performance of all models\\n# in each fold, and calculating the correlation between models across folds:\\n\\n# create df of model scores ordered by performance\\nmodel_scores = results_df.filter(regex=r\"split\\\\d*_test_score\")\\n\\n# plot 30 examples of dependency between cv fold and AUC scores\\nfig, ax = plt.subplots()\\nsns.lineplot(\\n    data=model_scores.transpose().iloc[:30],\\n    dashes=False,\\n    palette=\"Set1\",\\n    marker=\"o\",\\n    alpha=0.5,\\n    ax=ax,\\n)\\nax.set_xlabel(\"CV test fold\", size=12, labelpad=10)\\nax.set_ylabel(\"Model AUC\", size=12)\\nax.tick_params(bottom=True, labelbottom=False)\\nplt.show()\\n\\n# print correlation of AUC scores across folds\\nprint(f\"Correlation of models:\\\\n {model_scores.transpose().corr()}\")\\n\\n# %%\\n# We can observe that the performance of the models highly depends on the fold.\\n#\\n# As a consequence, if we assume independence between samples we will be\\n# underestimating the variance computed in our statistical tests, increasing\\n# the number of false positive errors (i.e. detecting a significant difference\\n# between models when such does not exist) [1]_.\\n#\\n# Several variance-corrected statistical tests have been developed for these\\n# cases. In this example we will show how to implement one of them (the so\\n# called Nadeau and Bengio\\'s corrected t-test) under two different statistical\\n# frameworks: frequentist and Bayesian.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Comparing two models: frequentist approach\\n# ------------------------------------------\\n#\\n# We can start by asking: \"Is the first model significantly better than the\\n# second model (when ranked by `mean_test_score`)?\"\\n#\\n# To answer this question using a frequentist approach we could\\n# run a paired t-test and compute the p-value. This is also known as\\n# Diebold-Mariano test in the forecast literature [5]_.\\n# Many variants of such a t-test have been developed to account for the\\n# \\'non-independence of samples problem\\'\\n# described in the previous section. We will use the one proven to obtain the\\n# highest replicability scores (which rate how similar the performance of a\\n# model is when evaluating it on different random partitions of the same\\n# dataset) while maintaining a low rate of false positives and false negatives:\\n# the Nadeau and Bengio\\'s corrected t-test [2]_ that uses a 10 times repeated\\n# 10-fold cross validation [3]_.\\n#\\n# This corrected paired t-test is computed as:\\n#\\n# .. math::\\n#    t=\\\\frac{\\\\frac{1}{k \\\\cdot r}\\\\sum_{i=1}^{k}\\\\sum_{j=1}^{r}x_{ij}}\\n#    {\\\\sqrt{(\\\\frac{1}{k \\\\cdot r}+\\\\frac{n_{test}}{n_{train}})\\\\hat{\\\\sigma}^2}}\\n#\\n# where :math:`k` is the number of folds,\\n# :math:`r` the number of repetitions in the cross-validation,\\n# :math:`x` is the difference in performance of the models,\\n# :math:`n_{test}` is the number of samples used for testing,\\n# :math:`n_{train}` is the number of samples used for training,\\n# and :math:`\\\\hat{\\\\sigma}^2` represents the variance of the observed\\n# differences.\\n#\\n# Let\\'s implement a corrected right-tailed paired t-test to evaluate if the\\n# performance of the first model is significantly better than that of the\\n# second model. Our null hypothesis is that the second model performs at least\\n# as good as the first model.\\n\\nimport numpy as np\\nfrom scipy.stats import t\\n\\n\\n# Code for: def corrected_std(differences, n_train, n_test):\\n\\n\\n# Code for: def compute_corrected_ttest(differences, df, n_train, n_test):\\n\\n\\n# %%\\nmodel_1_scores = model_scores.iloc[0].values  # scores of the best model\\nmodel_2_scores = model_scores.iloc[1].values  # scores of the second-best model\\n\\ndifferences = model_1_scores - model_2_scores\\n\\nn = differences.shape[0]  # number of test sets\\ndf = n - 1\\nn_train = len(list(cv.split(X, y))[0][0])\\nn_test = len(list(cv.split(X, y))[0][1])\\n\\nt_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\\nprint(f\"Corrected t-value: {t_stat:.3f}\\\\nCorrected p-value: {p_val:.3f}\")\\n\\n# %%\\n# We can compare the corrected t- and p-values with the uncorrected ones:\\n\\nt_stat_uncorrected = np.mean(differences) / np.sqrt(np.var(differences, ddof=1) / n)\\np_val_uncorrected = t.sf(np.abs(t_stat_uncorrected), df)\\n\\nprint(\\n    f\"Uncorrected t-value: {t_stat_uncorrected:.3f}\\\\n\"\\n    f\"Uncorrected p-value: {p_val_uncorrected:.3f}\"\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='t_stat_uncorrected = np.mean(differences) / np.sqrt(np.var(differences, ddof=1) / n)\\np_val_uncorrected = t.sf(np.abs(t_stat_uncorrected), df)\\n\\nprint(\\n    f\"Uncorrected t-value: {t_stat_uncorrected:.3f}\\\\n\"\\n    f\"Uncorrected p-value: {p_val_uncorrected:.3f}\"\\n)\\n\\n# %%\\n# Using the conventional significance alpha level at `p=0.05`, we observe that\\n# the uncorrected t-test concludes that the first model is significantly better\\n# than the second.\\n#\\n# With the corrected approach, in contrast, we fail to detect this difference.\\n#\\n# In the latter case, however, the frequentist approach does not let us\\n# conclude that the first and second model have an equivalent performance. If\\n# we wanted to make this assertion we need to use a Bayesian approach.\\n\\n# %%\\n# Comparing two models: Bayesian approach\\n# ---------------------------------------\\n# We can use Bayesian estimation to calculate the probability that the first\\n# model is better than the second. Bayesian estimation will output a\\n# distribution followed by the mean :math:`\\\\mu` of the differences in the\\n# performance of two models.\\n#\\n# To obtain the posterior distribution we need to define a prior that models\\n# our beliefs of how the mean is distributed before looking at the data,\\n# and multiply it by a likelihood function that computes how likely our\\n# observed differences are, given the values that the mean of differences\\n# could take.\\n#\\n# Bayesian estimation can be carried out in many forms to answer our question,\\n# but in this example we will implement the approach suggested by Benavoli and\\n# colleagues [4]_.\\n#\\n# One way of defining our posterior using a closed-form expression is to select\\n# a prior conjugate to the likelihood function. Benavoli and colleagues [4]_\\n# show that when comparing the performance of two classifiers we can model the\\n# prior as a Normal-Gamma distribution (with both mean and variance unknown)\\n# conjugate to a normal likelihood, to thus express the posterior as a normal\\n# distribution.\\n# Marginalizing out the variance from this normal posterior, we can define the\\n# posterior of the mean parameter as a Student\\'s t-distribution. Specifically:\\n#\\n# .. math::\\n#    St(\\\\mu;n-1,\\\\overline{x},(\\\\frac{1}{n}+\\\\frac{n_{test}}{n_{train}})\\n#    \\\\hat{\\\\sigma}^2)\\n#\\n# where :math:`n` is the total number of samples,\\n# :math:`\\\\overline{x}` represents the mean difference in the scores,\\n# :math:`n_{test}` is the number of samples used for testing,\\n# :math:`n_{train}` is the number of samples used for training,\\n# and :math:`\\\\hat{\\\\sigma}^2` represents the variance of the observed\\n# differences.\\n#\\n# Notice that we are using Nadeau and Bengio\\'s corrected variance in our\\n# Bayesian approach as well.\\n#\\n# Let\\'s compute and plot the posterior:\\n\\n# initialize random variable\\nt_post = t(\\n    df, loc=np.mean(differences), scale=corrected_std(differences, n_train, n_test)\\n)\\n\\n# %%\\n# Let\\'s plot the posterior distribution:\\n\\nx = np.linspace(t_post.ppf(0.001), t_post.ppf(0.999), 100)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# initialize random variable\\nt_post = t(\\n    df, loc=np.mean(differences), scale=corrected_std(differences, n_train, n_test)\\n)\\n\\n# %%\\n# Let\\'s plot the posterior distribution:\\n\\nx = np.linspace(t_post.ppf(0.001), t_post.ppf(0.999), 100)\\n\\nplt.plot(x, t_post.pdf(x))\\nplt.xticks(np.arange(-0.04, 0.06, 0.01))\\nplt.fill_between(x, t_post.pdf(x), 0, facecolor=\"blue\", alpha=0.2)\\nplt.ylabel(\"Probability density\")\\nplt.xlabel(r\"Mean difference ($\\\\mu$)\")\\nplt.title(\"Posterior distribution\")\\nplt.show()\\n\\n# %%\\n# We can calculate the probability that the first model is better than the\\n# second by computing the area under the curve of the posterior distribution\\n# from zero to infinity. And also the reverse: we can calculate the probability\\n# that the second model is better than the first by computing the area under\\n# the curve from minus infinity to zero.\\n\\nbetter_prob = 1 - t_post.cdf(0)\\n\\nprint(\\n    f\"Probability of {model_scores.index[0]} being more accurate than \"\\n    f\"{model_scores.index[1]}: {better_prob:.3f}\"\\n)\\nprint(\\n    f\"Probability of {model_scores.index[1]} being more accurate than \"\\n    f\"{model_scores.index[0]}: {1 - better_prob:.3f}\"\\n)\\n\\n# %%\\n# In contrast with the frequentist approach, we can compute the probability\\n# that one model is better than the other.\\n#\\n# Note that we obtained similar results as those in the frequentist approach.\\n# Given our choice of priors, we are essentially performing the same\\n# computations, but we are allowed to make different assertions.\\n\\n# %%\\n# Region of Practical Equivalence\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n# Sometimes we are interested in determining the probabilities that our models\\n# have an equivalent performance, where \"equivalent\" is defined in a practical\\n# way. A naive approach [4]_ would be to define estimators as practically\\n# equivalent when they differ by less than 1% in their accuracy. But we could\\n# also define this practical equivalence taking into account the problem we are\\n# trying to solve. For example, a difference of 5% in accuracy would mean an\\n# increase of $1000 in sales, and we consider any quantity above that as\\n# relevant for our business.\\n#\\n# In this example we are going to define the\\n# Region of Practical Equivalence (ROPE) to be :math:`[-0.01, 0.01]`. That is,\\n# we will consider two models as practically equivalent if they differ by less\\n# than 1% in their performance.\\n#\\n# To compute the probabilities of the classifiers being practically equivalent,\\n# we calculate the area under the curve of the posterior over the ROPE\\n# interval:\\n\\nrope_interval = [-0.01, 0.01]\\nrope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\\n\\nprint(\\n    f\"Probability of {model_scores.index[0]} and {model_scores.index[1]} \"\\n    f\"being practically equivalent: {rope_prob:.3f}\"\\n)\\n\\n# %%\\n# We can plot how the posterior is distributed over the ROPE interval:\\n\\nx_rope = np.linspace(rope_interval[0], rope_interval[1], 100)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(\\n    f\"Probability of {model_scores.index[0]} and {model_scores.index[1]} \"\\n    f\"being practically equivalent: {rope_prob:.3f}\"\\n)\\n\\n# %%\\n# We can plot how the posterior is distributed over the ROPE interval:\\n\\nx_rope = np.linspace(rope_interval[0], rope_interval[1], 100)\\n\\nplt.plot(x, t_post.pdf(x))\\nplt.xticks(np.arange(-0.04, 0.06, 0.01))\\nplt.vlines([-0.01, 0.01], ymin=0, ymax=(np.max(t_post.pdf(x)) + 1))\\nplt.fill_between(x_rope, t_post.pdf(x_rope), 0, facecolor=\"blue\", alpha=0.2)\\nplt.ylabel(\"Probability density\")\\nplt.xlabel(r\"Mean difference ($\\\\mu$)\")\\nplt.title(\"Posterior distribution under the ROPE\")\\nplt.show()\\n\\n# %%\\n# As suggested in [4]_, we can further interpret these probabilities using the\\n# same criteria as the frequentist approach: is the probability of falling\\n# inside the ROPE bigger than 95% (alpha value of 5%)?  In that case we can\\n# conclude that both models are practically equivalent.\\n\\n# %%\\n# The Bayesian estimation approach also allows us to compute how uncertain we\\n# are about our estimation of the difference. This can be calculated using\\n# credible intervals. For a given probability, they show the range of values\\n# that the estimated quantity, in our case the mean difference in\\n# performance, can take.\\n# For example, a 50% credible interval [x, y] tells us that there is a 50%\\n# probability that the true (mean) difference of performance between models is\\n# between x and y.\\n#\\n# Let\\'s determine the credible intervals of our data using 50%, 75% and 95%:\\n\\ncred_intervals = []\\nintervals = [0.5, 0.75, 0.95]\\n\\nfor interval in intervals:\\n    cred_interval = list(t_post.interval(interval))\\n    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\\n\\ncred_int_df = pd.DataFrame(\\n    cred_intervals, columns=[\"interval\", \"lower value\", \"upper value\"]\\n).set_index(\"interval\")\\ncred_int_df\\n\\n# %%\\n# As shown in the table, there is a 50% probability that the true mean\\n# difference between models will be between 0.000977 and 0.019023, 70%\\n# probability that it will be between -0.005422 and 0.025422, and 95%\\n# probability that it will be between -0.016445\\tand 0.036445.\\n\\n# %%\\n# Pairwise comparison of all models: frequentist approach\\n# -------------------------------------------------------\\n#\\n# We could also be interested in comparing the performance of all our models\\n# evaluated with :class:`~sklearn.model_selection.GridSearchCV`. In this case\\n# we would be running our statistical test multiple times, which leads us to\\n# the `multiple comparisons problem\\n# <https://en.wikipedia.org/wiki/Multiple_comparisons_problem>`_.\\n#\\n# There are many possible ways to tackle this problem, but a standard approach\\n# is to apply a `Bonferroni correction\\n# <https://en.wikipedia.org/wiki/Bonferroni_correction>`_. Bonferroni can be\\n# computed by multiplying the p-value by the number of comparisons we are\\n# testing.\\n#\\n# Let\\'s compare the performance of the models using the corrected t-test:\\n\\nfrom itertools import combinations\\nfrom math import factorial'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from itertools import combinations\\nfrom math import factorial\\n\\nn_comparisons = factorial(len(model_scores)) / (\\n    factorial(2) * factorial(len(model_scores) - 2)\\n)\\npairwise_t_test = []\\n\\nfor model_i, model_k in combinations(range(len(model_scores)), 2):\\n    model_i_scores = model_scores.iloc[model_i].values\\n    model_k_scores = model_scores.iloc[model_k].values\\n    differences = model_i_scores - model_k_scores\\n    t_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\\n    p_val *= n_comparisons  # implement Bonferroni correction\\n    # Bonferroni can output p-values higher than 1\\n    p_val = 1 if p_val > 1 else p_val\\n    pairwise_t_test.append(\\n        [model_scores.index[model_i], model_scores.index[model_k], t_stat, p_val]\\n    )\\n\\npairwise_comp_df = pd.DataFrame(\\n    pairwise_t_test, columns=[\"model_1\", \"model_2\", \"t_stat\", \"p_val\"]\\n).round(3)\\npairwise_comp_df\\n\\n# %%\\n# We observe that after correcting for multiple comparisons, the only model\\n# that significantly differs from the others is `\\'2_poly\\'`.\\n# `\\'rbf\\'`, the model ranked first by\\n# :class:`~sklearn.model_selection.GridSearchCV`, does not significantly\\n# differ from `\\'linear\\'` or `\\'3_poly\\'`.\\n\\n# %%\\n# Pairwise comparison of all models: Bayesian approach\\n# ----------------------------------------------------\\n#\\n# When using Bayesian estimation to compare multiple models, we don\\'t need to\\n# correct for multiple comparisons (for reasons why see [4]_).\\n#\\n# We can carry out our pairwise comparisons the same way as in the first\\n# section:\\n\\npairwise_bayesian = []\\n\\nfor model_i, model_k in combinations(range(len(model_scores)), 2):\\n    model_i_scores = model_scores.iloc[model_i].values\\n    model_k_scores = model_scores.iloc[model_k].values\\n    differences = model_i_scores - model_k_scores\\n    t_post = t(\\n        df, loc=np.mean(differences), scale=corrected_std(differences, n_train, n_test)\\n    )\\n    worse_prob = t_post.cdf(rope_interval[0])\\n    better_prob = 1 - t_post.cdf(rope_interval[1])\\n    rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\\n\\n    pairwise_bayesian.append([worse_prob, better_prob, rope_prob])\\n\\npairwise_bayesian_df = pd.DataFrame(\\n    pairwise_bayesian, columns=[\"worse_prob\", \"better_prob\", \"rope_prob\"]\\n).round(3)\\n\\npairwise_comp_df = pairwise_comp_df.join(pairwise_bayesian_df)\\npairwise_comp_df'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='pairwise_bayesian.append([worse_prob, better_prob, rope_prob])\\n\\npairwise_bayesian_df = pd.DataFrame(\\n    pairwise_bayesian, columns=[\"worse_prob\", \"better_prob\", \"rope_prob\"]\\n).round(3)\\n\\npairwise_comp_df = pairwise_comp_df.join(pairwise_bayesian_df)\\npairwise_comp_df\\n\\n# %%\\n# Using the Bayesian approach we can compute the probability that a model\\n# performs better, worse or practically equivalent to another.\\n#\\n# Results show that the model ranked first by\\n# :class:`~sklearn.model_selection.GridSearchCV` `\\'rbf\\'`, has approximately a\\n# 6.8% chance of being worse than `\\'linear\\'`, and a 1.8% chance of being worse\\n# than `\\'3_poly\\'`.\\n# `\\'rbf\\'` and `\\'linear\\'` have a 43% probability of being practically\\n# equivalent, while `\\'rbf\\'` and `\\'3_poly\\'` have a 10% chance of being so.\\n#\\n# Similarly to the conclusions obtained using the frequentist approach, all\\n# models have a 100% probability of being better than `\\'2_poly\\'`, and none have\\n# a practically equivalent performance with the latter.\\n\\n# %%\\n# Take-home messages\\n# ------------------\\n# - Small differences in performance measures might easily turn out to be\\n#   merely by chance, but not because one model predicts systematically better\\n#   than the other. As shown in this example, statistics can tell you how\\n#   likely that is.\\n# - When statistically comparing the performance of two models evaluated in\\n#   GridSearchCV, it is necessary to correct the calculated variance which\\n#   could be underestimated since the scores of the models are not independent\\n#   from each other.\\n# - A frequentist approach that uses a (variance-corrected) paired t-test can\\n#   tell us if the performance of one model is better than another with a\\n#   degree of certainty above chance.\\n# - A Bayesian approach can provide the probabilities of one model being\\n#   better, worse or practically equivalent than another. It can also tell us\\n#   how confident we are of knowing that the true differences of our models\\n#   fall under a certain range of values.\\n# - If multiple models are statistically compared, a multiple comparisons\\n#   correction is needed when using the frequentist approach.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_stats.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# .. rubric:: References\\n#\\n# .. [1] Dietterich, T. G. (1998). `Approximate statistical tests for\\n#        comparing supervised classification learning algorithms\\n#        <http://web.cs.iastate.edu/~jtian/cs573/Papers/Dietterich-98.pdf>`_.\\n#        Neural computation, 10(7).\\n# .. [2] Nadeau, C., & Bengio, Y. (2000). `Inference for the generalization\\n#        error\\n#        <https://papers.nips.cc/paper/1661-inference-for-the-generalization-error.pdf>`_.\\n#        In Advances in neural information processing systems.\\n# .. [3] Bouckaert, R. R., & Frank, E. (2004). `Evaluating the replicability\\n#        of significance tests for comparing learning algorithms\\n#        <https://www.cms.waikato.ac.nz/~ml/publications/2004/bouckaert-frank.pdf>`_.\\n#        In Pacific-Asia Conference on Knowledge Discovery and Data Mining.\\n# .. [4] Benavoli, A., Corani, G., Dem≈°ar, J., & Zaffalon, M. (2017). `Time\\n#        for a change: a tutorial for comparing multiple classifiers through\\n#        Bayesian analysis\\n#        <http://www.jmlr.org/papers/volume18/16-305/16-305.pdf>`_.\\n#        The Journal of Machine Learning Research, 18(1). See the Python\\n#        library that accompanies this paper `here\\n#        <https://github.com/janezd/baycomp>`_.\\n# .. [5] Diebold, F.X. & Mariano R.S. (1995). `Comparing predictive accuracy\\n#        <http://www.est.uc3m.es/esp/nueva_docencia/comp_col_get/lade/tecnicas_prediccion/Practicas0708/Comparing%20Predictive%20Accuracy%20(Dielbold).pdf>`_\\n#        Journal of Business & economic statistics, 20(1), 134-144.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_successive_halving_heatmap.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def make_heatmap(ax, gs, is_sh=False, make_cbar=False):\\n    \"\"\"Helper to make a heatmap.\"\"\"\\n    results = pd.DataFrame(gs.cv_results_)\\n    results[[\"param_C\", \"param_gamma\"]] = results[[\"param_C\", \"param_gamma\"]].astype(\\n        np.float64\\n    )\\n    if is_sh:\\n        # SH dataframe: get mean_test_score values for the highest iter\\n        scores_matrix = results.sort_values(\"iter\").pivot_table(\\n            index=\"param_gamma\",\\n            columns=\"param_C\",\\n            values=\"mean_test_score\",\\n            aggfunc=\"last\",\\n        )\\n    else:\\n        scores_matrix = results.pivot(\\n            index=\"param_gamma\", columns=\"param_C\", values=\"mean_test_score\"\\n        )\\n\\n    im = ax.imshow(scores_matrix)\\n\\n    ax.set_xticks(np.arange(len(Cs)))\\n    ax.set_xticklabels([\"{:.0E}\".format(x) for x in Cs])\\n    ax.set_xlabel(\"C\", fontsize=15)\\n\\n    ax.set_yticks(np.arange(len(gammas)))\\n    ax.set_yticklabels([\"{:.0E}\".format(x) for x in gammas])\\n    ax.set_ylabel(\"gamma\", fontsize=15)\\n\\n    # Rotate the tick labels and set their alignment.\\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\\n\\n    if is_sh:\\n        iterations = results.pivot_table(\\n            index=\"param_gamma\", columns=\"param_C\", values=\"iter\", aggfunc=\"max\"\\n        ).values\\n        for i in range(len(gammas)):\\n            for j in range(len(Cs)):\\n                ax.text(\\n                    j,\\n                    i,\\n                    iterations[i, j],\\n                    ha=\"center\",\\n                    va=\"center\",\\n                    color=\"w\",\\n                    fontsize=20,\\n                )\\n\\n    if make_cbar:\\n        fig.subplots_adjust(right=0.8)\\n        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\\n        fig.colorbar(im, cax=cbar_ax)\\n        cbar_ax.set_ylabel(\"mean_test_score\", rotation=-90, va=\"bottom\", fontsize=15)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_successive_halving_heatmap.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\nComparison between grid search and successive halving\\n=====================================================\\n\\nThis example compares the parameter search performed by\\n:class:`~sklearn.model_selection.HalvingGridSearchCV` and\\n:class:`~sklearn.model_selection.GridSearchCV`.\\n\\n\"\"\"\\n\\nfrom time import time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom sklearn import datasets\\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\\nfrom sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\\nfrom sklearn.svm import SVC\\n\\n# %%\\n# We first define the parameter space for an :class:`~sklearn.svm.SVC`\\n# estimator, and compute the time required to train a\\n# :class:`~sklearn.model_selection.HalvingGridSearchCV` instance, as well as a\\n# :class:`~sklearn.model_selection.GridSearchCV` instance.\\n\\nrng = np.random.RandomState(0)\\nX, y = datasets.make_classification(n_samples=1000, random_state=rng)\\n\\ngammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\\nCs = [1, 10, 100, 1e3, 1e4, 1e5]\\nparam_grid = {\"gamma\": gammas, \"C\": Cs}\\n\\nclf = SVC(random_state=rng)\\n\\ntic = time()\\ngsh = HalvingGridSearchCV(\\n    estimator=clf, param_grid=param_grid, factor=2, random_state=rng\\n)\\ngsh.fit(X, y)\\ngsh_time = time() - tic\\n\\ntic = time()\\ngs = GridSearchCV(estimator=clf, param_grid=param_grid)\\ngs.fit(X, y)\\ngs_time = time() - tic\\n\\n# %%\\n# We now plot heatmaps for both search estimators.\\n\\n\\n# Code for: def make_heatmap(ax, gs, is_sh=False, make_cbar=False):\\n\\n\\nfig, axes = plt.subplots(ncols=2, sharey=True)\\nax1, ax2 = axes\\n\\nmake_heatmap(ax1, gsh, is_sh=True)\\nmake_heatmap(ax2, gs, make_cbar=True)\\n\\nax1.set_title(\"Successive Halving\\\\ntime = {:.3f}s\".format(gsh_time), fontsize=15)\\nax2.set_title(\"GridSearch\\\\ntime = {:.3f}s\".format(gs_time), fontsize=15)\\n\\nplt.show()\\n\\n# %%\\n# The heatmaps show the mean test score of the parameter combinations for an\\n# :class:`~sklearn.svm.SVC` instance. The\\n# :class:`~sklearn.model_selection.HalvingGridSearchCV` also shows the\\n# iteration at which the combinations where last used. The combinations marked\\n# as ``0`` were only evaluated at the first iteration, while the ones with\\n# ``5`` are the parameter combinations that are considered the best ones.\\n#\\n# We can see that the :class:`~sklearn.model_selection.HalvingGridSearchCV`\\n# class is able to find parameter combinations that are just as accurate as\\n# :class:`~sklearn.model_selection.GridSearchCV`, in much less time.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_cv_predict.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================\\nPlotting Cross-Validated Predictions\\n====================================\\n\\nThis example shows how to use\\n:func:`~sklearn.model_selection.cross_val_predict` together with\\n:class:`~sklearn.metrics.PredictionErrorDisplay` to visualize prediction\\nerrors.\\n\"\"\"\\n\\n# %%\\n# We will load the diabetes dataset and create an instance of a linear\\n# regression model.\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.linear_model import LinearRegression\\n\\nX, y = load_diabetes(return_X_y=True)\\nlr = LinearRegression()\\n\\n# %%\\n# :func:`~sklearn.model_selection.cross_val_predict` returns an array of the\\n# same size of `y` where each entry is a prediction obtained by cross\\n# validation.\\nfrom sklearn.model_selection import cross_val_predict\\n\\ny_pred = cross_val_predict(lr, X, y, cv=10)\\n\\n# %%\\n# Since `cv=10`, it means that we trained 10 models and each model was\\n# used to predict on one of the 10 folds. We can now use the\\n# :class:`~sklearn.metrics.PredictionErrorDisplay` to visualize the\\n# prediction errors.\\n#\\n# On the left axis, we plot the observed values :math:`y` vs. the predicted\\n# values :math:`\\\\hat{y}` given by the models. On the right axis, we plot the\\n# residuals (i.e. the difference between the observed values and the predicted\\n# values) vs. the predicted values.\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.metrics import PredictionErrorDisplay\\n\\nfig, axs = plt.subplots(ncols=2, figsize=(8, 4))\\nPredictionErrorDisplay.from_predictions(\\n    y,\\n    y_pred=y_pred,\\n    kind=\"actual_vs_predicted\",\\n    subsample=100,\\n    ax=axs[0],\\n    random_state=0,\\n)\\naxs[0].set_title(\"Actual vs. Predicted values\")\\nPredictionErrorDisplay.from_predictions(\\n    y,\\n    y_pred=y_pred,\\n    kind=\"residual_vs_predicted\",\\n    subsample=100,\\n    ax=axs[1],\\n    random_state=0,\\n)\\naxs[1].set_title(\"Residuals vs. Predicted Values\")\\nfig.suptitle(\"Plotting cross-validated predictions\")\\nplt.tight_layout()\\nplt.show()\\n\\n# %%\\n# It is important to note that we used\\n# :func:`~sklearn.model_selection.cross_val_predict` for visualization\\n# purpose only in this example.\\n#\\n# It would be problematic to\\n# quantitatively assess the model performance by computing a single\\n# performance metric from the concatenated predictions returned by\\n# :func:`~sklearn.model_selection.cross_val_predict`\\n# when the different CV folds vary by size and distributions.\\n#\\n# It is recommended to compute per-fold performance metrics using:\\n# :func:`~sklearn.model_selection.cross_val_score` or\\n# :func:`~sklearn.model_selection.cross_validate` instead.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_validation_curve.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================\\nPlotting Validation Curves\\n==========================\\n\\nIn this plot you can see the training scores and validation scores of an SVM\\nfor different values of the kernel parameter gamma. For very low values of\\ngamma, you can see that both the training score and the validation score are\\nlow. This is called underfitting. Medium values of gamma will result in high\\nvalues for both scores, i.e. the classifier is performing fairly well. If gamma\\nis too high, the classifier will overfit, which means that the training score\\nis good but the validation score is poor.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import ValidationCurveDisplay\\nfrom sklearn.svm import SVC\\n\\nX, y = load_digits(return_X_y=True)\\nsubset_mask = np.isin(y, [1, 2])  # binary classification: 1 vs 2\\nX, y = X[subset_mask], y[subset_mask]\\n\\ndisp = ValidationCurveDisplay.from_estimator(\\n    SVC(),\\n    X,\\n    y,\\n    param_name=\"gamma\",\\n    param_range=np.logspace(-6, -1, 5),\\n    score_type=\"both\",\\n    n_jobs=2,\\n    score_name=\"Accuracy\",\\n)\\ndisp.ax_.set_title(\"Validation Curve for SVM with an RBF kernel\")\\ndisp.ax_.set_xlabel(r\"gamma (inverse radius of the RBF kernel)\")\\ndisp.ax_.set_ylim(0.0, 1.1)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_roc_crossval.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================================\\nReceiver Operating Characteristic (ROC) with cross validation\\n=============================================================\\n\\nThis example presents how to estimate and visualize the variance of the Receiver\\nOperating Characteristic (ROC) metric using cross-validation.\\n\\nROC curves typically feature true positive rate (TPR) on the Y axis, and false\\npositive rate (FPR) on the X axis. This means that the top left corner of the\\nplot is the \"ideal\" point - a FPR of zero, and a TPR of one. This is not very\\nrealistic, but it does mean that a larger Area Under the Curve (AUC) is usually\\nbetter. The \"steepness\" of ROC curves is also important, since it is ideal to\\nmaximize the TPR while minimizing the FPR.\\n\\nThis example shows the ROC response of different datasets, created from K-fold\\ncross-validation. Taking all of these curves, it is possible to calculate the\\nmean AUC, and see the variance of the curve when the\\ntraining set is split into different subsets. This roughly shows how the\\nclassifier output is affected by changes in the training data, and how different\\nthe splits generated by K-fold cross-validation are from one another.\\n\\n.. note::\\n\\n    See :ref:`sphx_glr_auto_examples_model_selection_plot_roc.py` for a\\n    complement of the present example explaining the averaging strategies to\\n    generalize the metrics for multiclass classifiers.\\n\"\"\"\\n\\n# %%\\n# Load and prepare data\\n# =====================\\n#\\n# We import the :ref:`iris_dataset` which contains 3 classes, each one\\n# corresponding to a type of iris plant. One class is linearly separable from\\n# the other 2; the latter are **not** linearly separable from each other.\\n#\\n# In the following we binarize the dataset by dropping the \"virginica\" class\\n# (`class_id=2`). This means that the \"versicolor\" class (`class_id=1`) is\\n# regarded as the positive class and \"setosa\" as the negative class\\n# (`class_id=0`).\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\niris = load_iris()\\ntarget_names = iris.target_names\\nX, y = iris.data, iris.target\\nX, y = X[y != 2], y[y != 2]\\nn_samples, n_features = X.shape\\n\\n# %%\\n# We also add noisy features to make the problem harder.\\nrandom_state = np.random.RandomState(0)\\nX = np.concatenate([X, random_state.randn(n_samples, 200 * n_features)], axis=1)\\n\\n# %%\\n# Classification and ROC analysis\\n# -------------------------------\\n#\\n# Here we run a :class:`~sklearn.svm.SVC` classifier with cross-validation and\\n# plot the ROC curves fold-wise. Notice that the baseline to define the chance\\n# level (dashed ROC curve) is a classifier that would always predict the most\\n# frequent class.\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import svm\\nfrom sklearn.metrics import RocCurveDisplay, auc\\nfrom sklearn.model_selection import StratifiedKFold\\n\\nn_splits = 6\\ncv = StratifiedKFold(n_splits=n_splits)\\nclassifier = svm.SVC(kernel=\"linear\", probability=True, random_state=random_state)'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_roc_crossval.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import matplotlib.pyplot as plt\\n\\nfrom sklearn import svm\\nfrom sklearn.metrics import RocCurveDisplay, auc\\nfrom sklearn.model_selection import StratifiedKFold\\n\\nn_splits = 6\\ncv = StratifiedKFold(n_splits=n_splits)\\nclassifier = svm.SVC(kernel=\"linear\", probability=True, random_state=random_state)\\n\\ntprs = []\\naucs = []\\nmean_fpr = np.linspace(0, 1, 100)\\n\\nfig, ax = plt.subplots(figsize=(6, 6))\\nfor fold, (train, test) in enumerate(cv.split(X, y)):\\n    classifier.fit(X[train], y[train])\\n    viz = RocCurveDisplay.from_estimator(\\n        classifier,\\n        X[test],\\n        y[test],\\n        name=f\"ROC fold {fold}\",\\n        alpha=0.3,\\n        lw=1,\\n        ax=ax,\\n        plot_chance_level=(fold == n_splits - 1),\\n    )\\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\\n    interp_tpr[0] = 0.0\\n    tprs.append(interp_tpr)\\n    aucs.append(viz.roc_auc)\\n\\nmean_tpr = np.mean(tprs, axis=0)\\nmean_tpr[-1] = 1.0\\nmean_auc = auc(mean_fpr, mean_tpr)\\nstd_auc = np.std(aucs)\\nax.plot(\\n    mean_fpr,\\n    mean_tpr,\\n    color=\"b\",\\n    label=r\"Mean ROC (AUC = %0.2f $\\\\pm$ %0.2f)\" % (mean_auc, std_auc),\\n    lw=2,\\n    alpha=0.8,\\n)\\n\\nstd_tpr = np.std(tprs, axis=0)\\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\\nax.fill_between(\\n    mean_fpr,\\n    tprs_lower,\\n    tprs_upper,\\n    color=\"grey\",\\n    alpha=0.2,\\n    label=r\"$\\\\pm$ 1 std. dev.\",\\n)\\n\\nax.set(\\n    xlabel=\"False Positive Rate\",\\n    ylabel=\"True Positive Rate\",\\n    title=f\"Mean ROC curve with variability\\\\n(Positive label \\'{target_names[1]}\\')\",\\n)\\nax.legend(loc=\"lower right\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_confusion_matrix.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================\\nConfusion matrix\\n================\\n\\nExample of confusion matrix usage to evaluate the quality\\nof the output of a classifier on the iris data set. The\\ndiagonal elements represent the number of points for which\\nthe predicted label is equal to the true label, while\\noff-diagonal elements are those that are mislabeled by the\\nclassifier. The higher the diagonal values of the confusion\\nmatrix the better, indicating many correct predictions.\\n\\nThe figures show the confusion matrix with and without\\nnormalization by class support size (number of elements\\nin each class). This kind of normalization can be\\ninteresting in case of class imbalance to have a more\\nvisual interpretation of which class is being misclassified.\\n\\nHere the results are not as good as they could be as our\\nchoice for the regularization parameter C was not the best.\\nIn real life applications this parameter is usually chosen\\nusing :ref:`grid_search`.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets, svm\\nfrom sklearn.metrics import ConfusionMatrixDisplay\\nfrom sklearn.model_selection import train_test_split\\n\\n# import some data to play with\\niris = datasets.load_iris()\\nX = iris.data\\ny = iris.target\\nclass_names = iris.target_names\\n\\n# Split the data into a training set and a test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\n# Run classifier, using a model that is too regularized (C too low) to see\\n# the impact on the results\\nclassifier = svm.SVC(kernel=\"linear\", C=0.01).fit(X_train, y_train)\\n\\nnp.set_printoptions(precision=2)\\n\\n# Plot non-normalized confusion matrix\\ntitles_options = [\\n    (\"Confusion matrix, without normalization\", None),\\n    (\"Normalized confusion matrix\", \"true\"),\\n]\\nfor title, normalize in titles_options:\\n    disp = ConfusionMatrixDisplay.from_estimator(\\n        classifier,\\n        X_test,\\n        y_test,\\n        display_labels=class_names,\\n        cmap=plt.cm.Blues,\\n        normalize=normalize,\\n    )\\n    disp.ax_.set_title(title)\\n\\n    print(title)\\n    print(disp.confusion_matrix)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_text_feature_extraction.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def shorten_param(param_name):\\n    \"\"\"Remove components\\' prefixes in param_name.\"\"\"\\n    if \"__\" in param_name:\\n        return param_name.rsplit(\"__\", 1)[1]\\n    return param_name'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_text_feature_extraction.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================================================\\nSample pipeline for text feature extraction and evaluation\\n==========================================================\\n\\nThe dataset used in this example is :ref:`20newsgroups_dataset` which will be\\nautomatically downloaded, cached and reused for the document classification\\nexample.\\n\\nIn this example, we tune the hyperparameters of a particular classifier using a\\n:class:`~sklearn.model_selection.RandomizedSearchCV`. For a demo on the\\nperformance of some other classifiers, see the\\n:ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\\nnotebook.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Data loading\\n# ------------\\n# We load two categories from the training set. You can adjust the number of\\n# categories by adding their names to the list or setting `categories=None` when\\n# calling the dataset loader :func:`~sklearn.datasets.fetch_20newsgroups` to get\\n# the 20 of them.\\n\\nfrom sklearn.datasets import fetch_20newsgroups\\n\\ncategories = [\\n    \"alt.atheism\",\\n    \"talk.religion.misc\",\\n]\\n\\ndata_train = fetch_20newsgroups(\\n    subset=\"train\",\\n    categories=categories,\\n    shuffle=True,\\n    random_state=42,\\n    remove=(\"headers\", \"footers\", \"quotes\"),\\n)\\n\\ndata_test = fetch_20newsgroups(\\n    subset=\"test\",\\n    categories=categories,\\n    shuffle=True,\\n    random_state=42,\\n    remove=(\"headers\", \"footers\", \"quotes\"),\\n)\\n\\nprint(f\"Loading 20 newsgroups dataset for {len(data_train.target_names)} categories:\")\\nprint(data_train.target_names)\\nprint(f\"{len(data_train.data)} documents\")\\n\\n# %%\\n# Pipeline with hyperparameter tuning\\n# -----------------------------------\\n#\\n# We define a pipeline combining a text feature vectorizer with a simple\\n# classifier yet effective for text classification.\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.naive_bayes import ComplementNB\\nfrom sklearn.pipeline import Pipeline\\n\\npipeline = Pipeline(\\n    [\\n        (\"vect\", TfidfVectorizer()),\\n        (\"clf\", ComplementNB()),\\n    ]\\n)\\npipeline\\n\\n# %%\\n# We define a grid of hyperparameters to be explored by the\\n# :class:`~sklearn.model_selection.RandomizedSearchCV`. Using a\\n# :class:`~sklearn.model_selection.GridSearchCV` instead would explore all the\\n# possible combinations on the grid, which can be costly to compute, whereas the\\n# parameter `n_iter` of the :class:`~sklearn.model_selection.RandomizedSearchCV`\\n# controls the number of different random combination that are evaluated. Notice\\n# that setting `n_iter` larger than the number of possible combinations in a\\n# grid would lead to repeating already-explored combinations. We search for the\\n# best parameter combination for both the feature extraction (`vect__`) and the\\n# classifier (`clf__`).\\n\\nimport numpy as np'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_text_feature_extraction.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import numpy as np\\n\\nparameter_grid = {\\n    \"vect__max_df\": (0.2, 0.4, 0.6, 0.8, 1.0),\\n    \"vect__min_df\": (1, 3, 5, 10),\\n    \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\\n    \"vect__norm\": (\"l1\", \"l2\"),\\n    \"clf__alpha\": np.logspace(-6, 6, 13),\\n}\\n\\n# %%\\n# In this case `n_iter=40` is not an exhaustive search of the hyperparameters\\'\\n# grid. In practice it would be interesting to increase the parameter `n_iter`\\n# to get a more informative analysis. As a consequence, the computional time\\n# increases. We can reduce it by taking advantage of the parallelisation over\\n# the parameter combinations evaluation by increasing the number of CPUs used\\n# via the parameter `n_jobs`.\\n\\nfrom pprint import pprint\\n\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\nrandom_search = RandomizedSearchCV(\\n    estimator=pipeline,\\n    param_distributions=parameter_grid,\\n    n_iter=40,\\n    random_state=0,\\n    n_jobs=2,\\n    verbose=1,\\n)\\n\\nprint(\"Performing grid search...\")\\nprint(\"Hyperparameters to be evaluated:\")\\npprint(parameter_grid)\\n\\n# %%\\nfrom time import time\\n\\nt0 = time()\\nrandom_search.fit(data_train.data, data_train.target)\\nprint(f\"Done in {time() - t0:.3f}s\")\\n\\n# %%\\nprint(\"Best parameters combination found:\")\\nbest_parameters = random_search.best_estimator_.get_params()\\nfor param_name in sorted(parameter_grid.keys()):\\n    print(f\"{param_name}: {best_parameters[param_name]}\")\\n\\n# %%\\ntest_accuracy = random_search.score(data_test.data, data_test.target)\\nprint(\\n    \"Accuracy of the best parameters using the inner CV of \"\\n    f\"the random search: {random_search.best_score_:.3f}\"\\n)\\nprint(f\"Accuracy on test set: {test_accuracy:.3f}\")\\n\\n# %%\\n# The prefixes `vect` and `clf` are required to avoid possible ambiguities in\\n# the pipeline, but are not necessary for visualizing the results. Because of\\n# this, we define a function that will rename the tuned hyperparameters and\\n# improve the readability.\\n\\nimport pandas as pd\\n\\n\\n# Code for: def shorten_param(param_name):\\n\\n\\ncv_results = pd.DataFrame(random_search.cv_results_)\\ncv_results = cv_results.rename(shorten_param, axis=1)\\n\\n# %%\\n# We can use a `plotly.express.scatter\\n# <https://plotly.com/python-api-reference/generated/plotly.express.scatter.html>`_\\n# to visualize the trade-off between scoring time and mean test score (i.e. \"CV\\n# score\"). Passing the cursor over a given point displays the corresponding\\n# parameters. Error bars correspond to one standard deviation as computed in the\\n# different folds of the cross-validation.\\n\\nimport plotly.express as px'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_text_feature_extraction.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import plotly.express as px\\n\\nparam_names = [shorten_param(name) for name in parameter_grid.keys()]\\nlabels = {\\n    \"mean_score_time\": \"CV Score time (s)\",\\n    \"mean_test_score\": \"CV score (accuracy)\",\\n}\\nfig = px.scatter(\\n    cv_results,\\n    x=\"mean_score_time\",\\n    y=\"mean_test_score\",\\n    error_x=\"std_score_time\",\\n    error_y=\"std_test_score\",\\n    hover_data=param_names,\\n    labels=labels,\\n)\\nfig.update_layout(\\n    title={\\n        \"text\": \"trade-off between scoring time and mean test score\",\\n        \"y\": 0.95,\\n        \"x\": 0.5,\\n        \"xanchor\": \"center\",\\n        \"yanchor\": \"top\",\\n    }\\n)\\nfig\\n\\n# %%\\n# Notice that the cluster of models in the upper-left corner of the plot have\\n# the best trade-off between accuracy and scoring time. In this case, using\\n# bigrams increases the required scoring time without improving considerably the\\n# accuracy of the pipeline.\\n#\\n# .. note:: For more information on how to customize an automated tuning to\\n#    maximize score and minimize scoring time, see the example notebook\\n#    :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`.\\n#\\n# We can also use a `plotly.express.parallel_coordinates\\n# <https://plotly.com/python-api-reference/generated/plotly.express.parallel_coordinates.html>`_\\n# to further visualize the mean test score as a function of the tuned\\n# hyperparameters. This helps finding interactions between more than two\\n# hyperparameters and provide intuition on their relevance for improving the\\n# performance of a pipeline.\\n#\\n# We apply a `math.log10` transformation on the `alpha` axis to spread the\\n# active range and improve the readability of the plot. A value :math:`x` on\\n# said axis is to be understood as :math:`10^x`.\\n\\nimport math\\n\\ncolumn_results = param_names + [\"mean_test_score\", \"mean_score_time\"]\\n\\ntransform_funcs = dict.fromkeys(column_results, lambda x: x)\\n# Using a logarithmic scale for alpha\\ntransform_funcs[\"alpha\"] = math.log10\\n# L1 norms are mapped to index 1, and L2 norms to index 2\\ntransform_funcs[\"norm\"] = lambda x: 2 if x == \"l2\" else 1\\n# Unigrams are mapped to index 1 and bigrams to index 2\\ntransform_funcs[\"ngram_range\"] = lambda x: x[1]\\n\\nfig = px.parallel_coordinates(\\n    cv_results[column_results].apply(transform_funcs),\\n    color=\"mean_test_score\",\\n    color_continuous_scale=px.colors.sequential.Viridis_r,\\n    labels=labels,\\n)\\nfig.update_layout(\\n    title={\\n        \"text\": \"Parallel coordinates plot of text classifier pipeline\",\\n        \"y\": 0.99,\\n        \"x\": 0.5,\\n        \"xanchor\": \"center\",\\n        \"yanchor\": \"top\",\\n    }\\n)\\nfig'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_grid_search_text_feature_extraction.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The parallel coordinates plot displays the values of the hyperparameters on\\n# different columns while the performance metric is color coded. It is possible\\n# to select a range of results by clicking and holding on any axis of the\\n# parallel coordinate plot. You can then slide (move) the range selection and\\n# cross two selections to see the intersections. You can undo a selection by\\n# clicking once again on the same axis.\\n#\\n# In particular for this hyperparameter search, it is interesting to notice that\\n# the top performing models do not seem to depend on the regularization `norm`,\\n# but they do depend on a trade-off between `max_df`, `min_df` and the\\n# regularization strength `alpha`. The reason is that including noisy features\\n# (i.e. `max_df` close to :math:`1.0` or `min_df` close to :math:`0`) tend to\\n# overfit and therefore require a stronger regularization to compensate. Having\\n# less features require less regularization and less scoring time.\\n#\\n# The best accuracy scores are obtained when `alpha` is between :math:`10^{-6}`\\n# and :math:`10^0`, regardless of the hyperparameter `norm`.'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_learning_curve.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nPlotting Learning Curves and Checking Models\\' Scalability\\n=========================================================\\n\\nIn this example, we show how to use the class\\n:class:`~sklearn.model_selection.LearningCurveDisplay` to easily plot learning\\ncurves. In addition, we give an interpretation to the learning curves obtained\\nfor a naive Bayes and SVM classifiers.\\n\\nThen, we explore and draw some conclusions about the scalability of these predictive\\nmodels by looking at their computational cost and not only at their statistical\\naccuracy.\\n\"\"\"\\n\\n# %%\\n# Learning Curve\\n# ==============\\n#\\n# Learning curves show the effect of adding more samples during the training\\n# process. The effect is depicted by checking the statistical performance of\\n# the model in terms of training score and testing score.\\n#\\n# Here, we compute the learning curve of a naive Bayes classifier and a SVM\\n# classifier with a RBF kernel using the digits dataset.\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.svm import SVC\\n\\nX, y = load_digits(return_X_y=True)\\nnaive_bayes = GaussianNB()\\nsvc = SVC(kernel=\"rbf\", gamma=0.001)\\n\\n# %%\\n# The :meth:`~sklearn.model_selection.LearningCurveDisplay.from_estimator`\\n# displays the learning curve given the dataset and the predictive model to\\n# analyze. To get an estimate of the scores uncertainty, this method uses\\n# a cross-validation procedure.\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\\n\\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 6), sharey=True)\\n\\ncommon_params = {\\n    \"X\": X,\\n    \"y\": y,\\n    \"train_sizes\": np.linspace(0.1, 1.0, 5),\\n    \"cv\": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),\\n    \"score_type\": \"both\",\\n    \"n_jobs\": 4,\\n    \"line_kw\": {\"marker\": \"o\"},\\n    \"std_display_style\": \"fill_between\",\\n    \"score_name\": \"Accuracy\",\\n}\\n\\nfor ax_idx, estimator in enumerate([naive_bayes, svc]):\\n    LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[ax_idx])\\n    handles, label = ax[ax_idx].get_legend_handles_labels()\\n    ax[ax_idx].legend(handles[:2], [\"Training Score\", \"Test Score\"])\\n    ax[ax_idx].set_title(f\"Learning Curve for {estimator.__class__.__name__}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_learning_curve.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# We first analyze the learning curve of the naive Bayes classifier. Its shape\\n# can be found in more complex datasets very often: the training score is very\\n# high when using few samples for training and decreases when increasing the\\n# number of samples, whereas the test score is very low at the beginning and\\n# then increases when adding samples. The training and test scores become more\\n# realistic when all the samples are used for training.\\n#\\n# We see another typical learning curve for the SVM classifier with RBF kernel.\\n# The training score remains high regardless of the size of the training set.\\n# On the other hand, the test score increases with the size of the training\\n# dataset. Indeed, it increases up to a point where it reaches a plateau.\\n# Observing such a plateau is an indication that it might not be useful to\\n# acquire new data to train the model since the generalization performance of\\n# the model will not increase anymore.\\n#\\n# Complexity analysis\\n# ===================\\n#\\n# In addition to these learning curves, it is also possible to look at the\\n# scalability of the predictive models in terms of training and scoring times.\\n#\\n# The :class:`~sklearn.model_selection.LearningCurveDisplay` class does not\\n# provide such information. We need to resort to the\\n# :func:`~sklearn.model_selection.learning_curve` function instead and make\\n# the plot manually.\\n\\n# %%\\nfrom sklearn.model_selection import learning_curve\\n\\ncommon_params = {\\n    \"X\": X,\\n    \"y\": y,\\n    \"train_sizes\": np.linspace(0.1, 1.0, 5),\\n    \"cv\": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),\\n    \"n_jobs\": 4,\\n    \"return_times\": True,\\n}\\n\\ntrain_sizes, _, test_scores_nb, fit_times_nb, score_times_nb = learning_curve(\\n    naive_bayes, **common_params\\n)\\ntrain_sizes, _, test_scores_svm, fit_times_svm, score_times_svm = learning_curve(\\n    svc, **common_params\\n)\\n\\n# %%\\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 12), sharex=True)\\n\\nfor ax_idx, (fit_times, score_times, estimator) in enumerate(\\n    zip(\\n        [fit_times_nb, fit_times_svm],\\n        [score_times_nb, score_times_svm],\\n        [naive_bayes, svc],\\n    )\\n):\\n    # scalability regarding the fit time\\n    ax[0, ax_idx].plot(train_sizes, fit_times.mean(axis=1), \"o-\")\\n    ax[0, ax_idx].fill_between(\\n        train_sizes,\\n        fit_times.mean(axis=1) - fit_times.std(axis=1),\\n        fit_times.mean(axis=1) + fit_times.std(axis=1),\\n        alpha=0.3,\\n    )\\n    ax[0, ax_idx].set_ylabel(\"Fit time (s)\")\\n    ax[0, ax_idx].set_title(\\n        f\"Scalability of the {estimator.__class__.__name__} classifier\"\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/model_selection/plot_learning_curve.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# scalability regarding the score time\\n    ax[1, ax_idx].plot(train_sizes, score_times.mean(axis=1), \"o-\")\\n    ax[1, ax_idx].fill_between(\\n        train_sizes,\\n        score_times.mean(axis=1) - score_times.std(axis=1),\\n        score_times.mean(axis=1) + score_times.std(axis=1),\\n        alpha=0.3,\\n    )\\n    ax[1, ax_idx].set_ylabel(\"Score time (s)\")\\n    ax[1, ax_idx].set_xlabel(\"Number of training samples\")\\n\\n# %%\\n# We see that the scalability of the SVM and naive Bayes classifiers is very\\n# different. The SVM classifier complexity at fit and score time increases\\n# rapidly with the number of samples. Indeed, it is known that the fit time\\n# complexity of this classifier is more than quadratic with the number of\\n# samples which makes it hard to scale to dataset with more than a few\\n# 10,000 samples. In contrast, the naive Bayes classifier scales much better\\n# with a lower complexity at fit and score time.\\n#\\n# Subsequently, we can check the trade-off between increased training time and\\n# the cross-validation score.\\n\\n# %%\\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\\n\\nfor ax_idx, (fit_times, test_scores, estimator) in enumerate(\\n    zip(\\n        [fit_times_nb, fit_times_svm],\\n        [test_scores_nb, test_scores_svm],\\n        [naive_bayes, svc],\\n    )\\n):\\n    ax[ax_idx].plot(fit_times.mean(axis=1), test_scores.mean(axis=1), \"o-\")\\n    ax[ax_idx].fill_between(\\n        fit_times.mean(axis=1),\\n        test_scores.mean(axis=1) - test_scores.std(axis=1),\\n        test_scores.mean(axis=1) + test_scores.std(axis=1),\\n        alpha=0.3,\\n    )\\n    ax[ax_idx].set_ylabel(\"Accuracy\")\\n    ax[ax_idx].set_xlabel(\"Fit time (s)\")\\n    ax[ax_idx].set_title(\\n        f\"Performance of the {estimator.__class__.__name__} classifier\"\\n    )\\n\\nplt.show()\\n\\n# %%\\n# In these plots, we can look for the inflection point for which the\\n# cross-validation score does not increase anymore and only the training time\\n# increases.'), Document(metadata={'source': '/content/local_copy_repo/examples/neural_networks/plot_mnist_filters.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================================\\nVisualization of MLP weights on MNIST\\n=====================================\\n\\nSometimes looking at the learned coefficients of a neural network can provide\\ninsight into the learning behavior. For example if weights look unstructured,\\nmaybe some were not used at all, or if very large coefficients exist, maybe\\nregularization was too low or the learning rate too high.\\n\\nThis example shows how to plot some of the first layer weights in a\\nMLPClassifier trained on the MNIST dataset.\\n\\nThe input data consists of 28x28 pixel handwritten digits, leading to 784\\nfeatures in the dataset. Therefore the first layer weight matrix has the shape\\n(784, hidden_layer_sizes[0]).  We can therefore visualize a single column of\\nthe weight matrix as a 28x28 pixel image.\\n\\nTo make the example run faster, we use very few hidden units, and train only\\nfor a very short time. Training longer would result in weights with a much\\nsmoother spatial appearance. The example will throw a warning because it\\ndoesn\\'t converge, in this case this is what we want because of resource\\nusage constraints on our Continuous Integration infrastructure that is used\\nto build this documentation on a regular basis.\\n\"\"\"\\n\\nimport warnings\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.exceptions import ConvergenceWarning\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neural_network import MLPClassifier\\n\\n# Load data from https://www.openml.org/d/554\\nX, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\\nX = X / 255.0\\n\\n# Split data into train partition and test partition\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.7)\\n\\nmlp = MLPClassifier(\\n    hidden_layer_sizes=(40,),\\n    max_iter=8,\\n    alpha=1e-4,\\n    solver=\"sgd\",\\n    verbose=10,\\n    random_state=1,\\n    learning_rate_init=0.2,\\n)\\n\\n# this example won\\'t converge because of resource usage constraints on\\n# our Continuous Integration infrastructure, so we catch the warning and\\n# ignore it here\\nwith warnings.catch_warnings():\\n    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\\n    mlp.fit(X_train, y_train)\\n\\nprint(\"Training set score: %f\" % mlp.score(X_train, y_train))\\nprint(\"Test set score: %f\" % mlp.score(X_test, y_test))\\n\\nfig, axes = plt.subplots(4, 4)\\n# use global min / max to ensure all weights are shown on the same scale\\nvmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\\nfor coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\\n    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\\n    ax.set_xticks(())\\n    ax.set_yticks(())\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neural_networks/plot_mlp_training_curves.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_on_dataset(X, y, ax, name):\\n    # for each dataset, plot learning for each learning strategy\\n    print(\"\\\\nlearning on dataset %s\" % name)\\n    ax.set_title(name)\\n\\n    X = MinMaxScaler().fit_transform(X)\\n    mlps = []\\n    if name == \"digits\":\\n        # digits is larger but converges fairly quickly\\n        max_iter = 15\\n    else:\\n        max_iter = 400\\n\\n    for label, param in zip(labels, params):\\n        print(\"training: %s\" % label)\\n        mlp = MLPClassifier(random_state=0, max_iter=max_iter, **param)\\n\\n        # some parameter combinations will not converge as can be seen on the\\n        # plots so they are ignored here\\n        with warnings.catch_warnings():\\n            warnings.filterwarnings(\\n                \"ignore\", category=ConvergenceWarning, module=\"sklearn\"\\n            )\\n            mlp.fit(X, y)\\n\\n        mlps.append(mlp)\\n        print(\"Training set score: %f\" % mlp.score(X, y))\\n        print(\"Training set loss: %f\" % mlp.loss_)\\n    for mlp, label, args in zip(mlps, labels, plot_args):\\n        ax.plot(mlp.loss_curve_, label=label, **args)'), Document(metadata={'source': '/content/local_copy_repo/examples/neural_networks/plot_mlp_training_curves.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n========================================================\\nCompare Stochastic learning strategies for MLPClassifier\\n========================================================\\n\\nThis example visualizes some training loss curves for different stochastic\\nlearning strategies, including SGD and Adam. Because of time-constraints, we\\nuse several small datasets, for which L-BFGS might be more suitable. The\\ngeneral trend shown in these examples seems to carry over to larger datasets,\\nhowever.\\n\\nNote that those results can be highly dependent on the value of\\n``learning_rate_init``.\\n\\n\"\"\"\\n\\nimport warnings\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import datasets\\nfrom sklearn.exceptions import ConvergenceWarning\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# different learning rate schedules and momentum parameters\\nparams = [\\n    {\\n        \"solver\": \"sgd\",\\n        \"learning_rate\": \"constant\",\\n        \"momentum\": 0,\\n        \"learning_rate_init\": 0.2,\\n    },\\n    {\\n        \"solver\": \"sgd\",\\n        \"learning_rate\": \"constant\",\\n        \"momentum\": 0.9,\\n        \"nesterovs_momentum\": False,\\n        \"learning_rate_init\": 0.2,\\n    },\\n    {\\n        \"solver\": \"sgd\",\\n        \"learning_rate\": \"constant\",\\n        \"momentum\": 0.9,\\n        \"nesterovs_momentum\": True,\\n        \"learning_rate_init\": 0.2,\\n    },\\n    {\\n        \"solver\": \"sgd\",\\n        \"learning_rate\": \"invscaling\",\\n        \"momentum\": 0,\\n        \"learning_rate_init\": 0.2,\\n    },\\n    {\\n        \"solver\": \"sgd\",\\n        \"learning_rate\": \"invscaling\",\\n        \"momentum\": 0.9,\\n        \"nesterovs_momentum\": False,\\n        \"learning_rate_init\": 0.2,\\n    },\\n    {\\n        \"solver\": \"sgd\",\\n        \"learning_rate\": \"invscaling\",\\n        \"momentum\": 0.9,\\n        \"nesterovs_momentum\": True,\\n        \"learning_rate_init\": 0.2,\\n    },\\n    {\"solver\": \"adam\", \"learning_rate_init\": 0.01},\\n]\\n\\nlabels = [\\n    \"constant learning-rate\",\\n    \"constant with momentum\",\\n    \"constant with Nesterov\\'s momentum\",\\n    \"inv-scaling learning-rate\",\\n    \"inv-scaling with momentum\",\\n    \"inv-scaling with Nesterov\\'s momentum\",\\n    \"adam\",\\n]\\n\\nplot_args = [\\n    {\"c\": \"red\", \"linestyle\": \"-\"},\\n    {\"c\": \"green\", \"linestyle\": \"-\"},\\n    {\"c\": \"blue\", \"linestyle\": \"-\"},\\n    {\"c\": \"red\", \"linestyle\": \"--\"},\\n    {\"c\": \"green\", \"linestyle\": \"--\"},\\n    {\"c\": \"blue\", \"linestyle\": \"--\"},\\n    {\"c\": \"black\", \"linestyle\": \"-\"},\\n]\\n\\n\\n# Code for: def plot_on_dataset(X, y, ax, name):\\n\\n\\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n# load / generate some toy datasets\\niris = datasets.load_iris()\\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\\ndata_sets = [\\n    (iris.data, iris.target),\\n    (X_digits, y_digits),\\n    datasets.make_circles(noise=0.2, factor=0.5, random_state=1),\\n    datasets.make_moons(noise=0.3, random_state=0),\\n]\\n\\nfor ax, data, name in zip(\\n    axes.ravel(), data_sets, [\"iris\", \"digits\", \"circles\", \"moons\"]\\n):\\n    plot_on_dataset(*data, ax=ax, name=name)'), Document(metadata={'source': '/content/local_copy_repo/examples/neural_networks/plot_mlp_training_curves.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for ax, data, name in zip(\\n    axes.ravel(), data_sets, [\"iris\", \"digits\", \"circles\", \"moons\"]\\n):\\n    plot_on_dataset(*data, ax=ax, name=name)\\n\\nfig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neural_networks/plot_rbm_logistic_classification.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def nudge_dataset(X, Y):\\n    \"\"\"\\n    This produces a dataset 5 times bigger than the original one,\\n    by moving the 8x8 images in X around by 1px to left, right, down, up\\n    \"\"\"\\n    direction_vectors = [\\n        [[0, 1, 0], [0, 0, 0], [0, 0, 0]],\\n        [[0, 0, 0], [1, 0, 0], [0, 0, 0]],\\n        [[0, 0, 0], [0, 0, 1], [0, 0, 0]],\\n        [[0, 0, 0], [0, 0, 0], [0, 1, 0]],\\n    ]\\n\\n    def shift(x, w):\\n        return convolve(x.reshape((8, 8)), mode=\"constant\", weights=w).ravel()\\n\\n    X = np.concatenate(\\n        [X] + [np.apply_along_axis(shift, 1, X, vector) for vector in direction_vectors]\\n    )\\n    Y = np.concatenate([Y for _ in range(5)], axis=0)\\n    return X, Y'), Document(metadata={'source': '/content/local_copy_repo/examples/neural_networks/plot_rbm_logistic_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================================\\nRestricted Boltzmann Machine features for digit classification\\n==============================================================\\n\\nFor greyscale image data where pixel values can be interpreted as degrees of\\nblackness on a white background, like handwritten digit recognition, the\\nBernoulli Restricted Boltzmann machine model (:class:`BernoulliRBM\\n<sklearn.neural_network.BernoulliRBM>`) can perform effective non-linear\\nfeature extraction.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate data\\n# -------------\\n#\\n# In order to learn good latent representations from a small dataset, we\\n# artificially generate more labeled data by perturbing the training data with\\n# linear shifts of 1 pixel in each direction.\\n\\nimport numpy as np\\nfrom scipy.ndimage import convolve\\n\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import minmax_scale\\n\\n\\n# Code for: def nudge_dataset(X, Y):\\n\\n\\nX, y = datasets.load_digits(return_X_y=True)\\nX = np.asarray(X, \"float32\")\\nX, Y = nudge_dataset(X, y)\\nX = minmax_scale(X, feature_range=(0, 1))  # 0-1 scaling\\n\\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\\n\\n# %%\\n# Models definition\\n# -----------------\\n#\\n# We build a classification pipeline with a BernoulliRBM feature extractor and\\n# a :class:`LogisticRegression <sklearn.linear_model.LogisticRegression>`\\n# classifier.\\n\\nfrom sklearn import linear_model\\nfrom sklearn.neural_network import BernoulliRBM\\nfrom sklearn.pipeline import Pipeline\\n\\nlogistic = linear_model.LogisticRegression(solver=\"newton-cg\", tol=1)\\nrbm = BernoulliRBM(random_state=0, verbose=True)\\n\\nrbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\", logistic)])\\n\\n# %%\\n# Training\\n# --------\\n#\\n# The hyperparameters of the entire model (learning rate, hidden layer size,\\n# regularization) were optimized by grid search, but the search is not\\n# reproduced here because of runtime constraints.\\n\\nfrom sklearn.base import clone\\n\\n# Hyper-parameters. These were set by cross-validation,\\n# using a GridSearchCV. Here we are not performing cross-validation to\\n# save time.\\nrbm.learning_rate = 0.06\\nrbm.n_iter = 10\\n\\n# More components tend to give better prediction performance, but larger\\n# fitting time\\nrbm.n_components = 100\\nlogistic.C = 6000\\n\\n# Training RBM-Logistic Pipeline\\nrbm_features_classifier.fit(X_train, Y_train)\\n\\n# Training the Logistic regression classifier directly on the pixel\\nraw_pixel_classifier = clone(logistic)\\nraw_pixel_classifier.C = 100.0\\nraw_pixel_classifier.fit(X_train, Y_train)\\n\\n# %%\\n# Evaluation\\n# ----------\\n\\nfrom sklearn import metrics\\n\\nY_pred = rbm_features_classifier.predict(X_test)\\nprint(\\n    \"Logistic regression using RBM features:\\\\n%s\\\\n\"\\n    % (metrics.classification_report(Y_test, Y_pred))\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/neural_networks/plot_rbm_logistic_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Evaluation\\n# ----------\\n\\nfrom sklearn import metrics\\n\\nY_pred = rbm_features_classifier.predict(X_test)\\nprint(\\n    \"Logistic regression using RBM features:\\\\n%s\\\\n\"\\n    % (metrics.classification_report(Y_test, Y_pred))\\n)\\n\\n# %%\\nY_pred = raw_pixel_classifier.predict(X_test)\\nprint(\\n    \"Logistic regression using raw pixel features:\\\\n%s\\\\n\"\\n    % (metrics.classification_report(Y_test, Y_pred))\\n)\\n\\n# %%\\n# The features extracted by the BernoulliRBM help improve the classification\\n# accuracy with respect to the logistic regression on raw pixels.\\n\\n# %%\\n# Plotting\\n# --------\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.figure(figsize=(4.2, 4))\\nfor i, comp in enumerate(rbm.components_):\\n    plt.subplot(10, 10, i + 1)\\n    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r, interpolation=\"nearest\")\\n    plt.xticks(())\\n    plt.yticks(())\\nplt.suptitle(\"100 components extracted by RBM\", fontsize=16)\\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neural_networks/plot_mlp_alpha.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================================\\nVarying regularization in Multi-layer Perceptron\\n================================================\\n\\nA comparison of different values for regularization parameter \\'alpha\\' on\\nsynthetic datasets. The plot shows that different alphas yield different\\ndecision functions.\\n\\nAlpha is a parameter for regularization term, aka penalty term, that combats\\noverfitting by constraining the size of the weights. Increasing alpha may fix\\nhigh variance (a sign of overfitting) by encouraging smaller weights, resulting\\nin a decision boundary plot that appears with lesser curvatures.\\nSimilarly, decreasing alpha may fix high bias (a sign of underfitting) by\\nencouraging larger weights, potentially resulting in a more complicated\\ndecision boundary.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nfrom matplotlib.colors import ListedColormap\\n\\nfrom sklearn.datasets import make_circles, make_classification, make_moons\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nh = 0.02  # step size in the mesh\\n\\nalphas = np.logspace(-1, 1, 5)\\n\\nclassifiers = []\\nnames = []\\nfor alpha in alphas:\\n    classifiers.append(\\n        make_pipeline(\\n            StandardScaler(),\\n            MLPClassifier(\\n                solver=\"lbfgs\",\\n                alpha=alpha,\\n                random_state=1,\\n                max_iter=2000,\\n                early_stopping=True,\\n                hidden_layer_sizes=[10, 10],\\n            ),\\n        )\\n    )\\n    names.append(f\"alpha {alpha:.2f}\")\\n\\nX, y = make_classification(\\n    n_features=2, n_redundant=0, n_informative=2, random_state=0, n_clusters_per_class=1\\n)\\nrng = np.random.RandomState(2)\\nX += 2 * rng.uniform(size=X.shape)\\nlinearly_separable = (X, y)\\n\\ndatasets = [\\n    make_moons(noise=0.3, random_state=0),\\n    make_circles(noise=0.2, factor=0.5, random_state=1),\\n    linearly_separable,\\n]\\n\\nfigure = plt.figure(figsize=(17, 9))\\ni = 1\\n# iterate over datasets\\nfor X, y in datasets:\\n    # split into training and test part\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=0.4, random_state=42\\n    )\\n\\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))'), Document(metadata={'source': '/content/local_copy_repo/examples/neural_networks/plot_mlp_alpha.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n\\n    # just plot the dataset first\\n    cm = plt.cm.RdBu\\n    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\\n    # Plot the training points\\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\\n    # and testing points\\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\\n    ax.set_xlim(xx.min(), xx.max())\\n    ax.set_ylim(yy.min(), yy.max())\\n    ax.set_xticks(())\\n    ax.set_yticks(())\\n    i += 1\\n\\n    # iterate over classifiers\\n    for name, clf in zip(names, classifiers):\\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\\n        clf.fit(X_train, y_train)\\n        score = clf.score(X_test, y_test)\\n\\n        # Plot the decision boundary. For that, we will assign a color to each\\n        # point in the mesh [x_min, x_max] x [y_min, y_max].\\n        if hasattr(clf, \"decision_function\"):\\n            Z = clf.decision_function(np.column_stack([xx.ravel(), yy.ravel()]))\\n        else:\\n            Z = clf.predict_proba(np.column_stack([xx.ravel(), yy.ravel()]))[:, 1]\\n\\n        # Put the result into a color plot\\n        Z = Z.reshape(xx.shape)\\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)\\n\\n        # Plot also the training points\\n        ax.scatter(\\n            X_train[:, 0],\\n            X_train[:, 1],\\n            c=y_train,\\n            cmap=cm_bright,\\n            edgecolors=\"black\",\\n            s=25,\\n        )\\n        # and testing points\\n        ax.scatter(\\n            X_test[:, 0],\\n            X_test[:, 1],\\n            c=y_test,\\n            cmap=cm_bright,\\n            alpha=0.6,\\n            edgecolors=\"black\",\\n            s=25,\\n        )\\n\\n        ax.set_xlim(xx.min(), xx.max())\\n        ax.set_ylim(yy.min(), yy.max())\\n        ax.set_xticks(())\\n        ax.set_yticks(())\\n        ax.set_title(name)\\n        ax.text(\\n            xx.max() - 0.3,\\n            yy.min() + 0.3,\\n            f\"{score:.3f}\".lstrip(\"0\"),\\n            size=15,\\n            horizontalalignment=\"right\",\\n        )\\n        i += 1\\n\\nfigure.subplots_adjust(left=0.02, right=0.98)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_map_data_to_normal.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================\\nMap data to a normal distribution\\n=================================\\n\\n.. currentmodule:: sklearn.preprocessing\\n\\nThis example demonstrates the use of the Box-Cox and Yeo-Johnson transforms\\nthrough :class:`~PowerTransformer` to map data from various\\ndistributions to a normal distribution.\\n\\nThe power transform is useful as a transformation in modeling problems where\\nhomoscedasticity and normality are desired. Below are examples of Box-Cox and\\nYeo-Johnwon applied to six different probability distributions: Lognormal,\\nChi-squared, Weibull, Gaussian, Uniform, and Bimodal.\\n\\nNote that the transformations successfully map the data to a normal\\ndistribution when applied to certain datasets, but are ineffective with others.\\nThis highlights the importance of visualizing the data before and after\\ntransformation.\\n\\nAlso note that even though Box-Cox seems to perform better than Yeo-Johnson for\\nlognormal and chi-squared distributions, keep in mind that Box-Cox does not\\nsupport inputs with negative values.\\n\\nFor comparison, we also add the output from\\n:class:`~QuantileTransformer`. It can force any arbitrary\\ndistribution into a gaussian, provided that there are enough training samples\\n(thousands). Because it is a non-parametric method, it is harder to interpret\\nthan the parametric ones (Box-Cox and Yeo-Johnson).\\n\\nOn \"small\" datasets (less than a few hundred points), the quantile transformer\\nis prone to overfitting. The use of the power transform is then recommended.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import PowerTransformer, QuantileTransformer\\n\\nN_SAMPLES = 1000\\nFONT_SIZE = 6\\nBINS = 30\\n\\n\\nrng = np.random.RandomState(304)\\nbc = PowerTransformer(method=\"box-cox\")\\nyj = PowerTransformer(method=\"yeo-johnson\")\\n# n_quantiles is set to the training set size rather than the default value\\n# to avoid a warning being raised by this example\\nqt = QuantileTransformer(\\n    n_quantiles=500, output_distribution=\"normal\", random_state=rng\\n)\\nsize = (N_SAMPLES, 1)\\n\\n\\n# lognormal distribution\\nX_lognormal = rng.lognormal(size=size)\\n\\n# chi-squared distribution\\ndf = 3\\nX_chisq = rng.chisquare(df=df, size=size)\\n\\n# weibull distribution\\na = 50\\nX_weibull = rng.weibull(a=a, size=size)\\n\\n# gaussian distribution\\nloc = 100\\nX_gaussian = rng.normal(loc=loc, size=size)\\n\\n# uniform distribution\\nX_uniform = rng.uniform(low=0, high=1, size=size)\\n\\n# bimodal distribution\\nloc_a, loc_b = 100, 105\\nX_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\\nX_bimodal = np.concatenate([X_a, X_b], axis=0)\\n\\n\\n# create plots\\ndistributions = [\\n    (\"Lognormal\", X_lognormal),\\n    (\"Chi-squared\", X_chisq),\\n    (\"Weibull\", X_weibull),\\n    (\"Gaussian\", X_gaussian),\\n    (\"Uniform\", X_uniform),\\n    (\"Bimodal\", X_bimodal),\\n]'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_map_data_to_normal.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# create plots\\ndistributions = [\\n    (\"Lognormal\", X_lognormal),\\n    (\"Chi-squared\", X_chisq),\\n    (\"Weibull\", X_weibull),\\n    (\"Gaussian\", X_gaussian),\\n    (\"Uniform\", X_uniform),\\n    (\"Bimodal\", X_bimodal),\\n]\\n\\ncolors = [\"#D81B60\", \"#0188FF\", \"#FFC107\", \"#B7A2FF\", \"#000000\", \"#2EC5AC\"]\\n\\nfig, axes = plt.subplots(nrows=8, ncols=3, figsize=plt.figaspect(2))\\naxes = axes.flatten()\\naxes_idxs = [\\n    (0, 3, 6, 9),\\n    (1, 4, 7, 10),\\n    (2, 5, 8, 11),\\n    (12, 15, 18, 21),\\n    (13, 16, 19, 22),\\n    (14, 17, 20, 23),\\n]\\naxes_list = [(axes[i], axes[j], axes[k], axes[l]) for (i, j, k, l) in axes_idxs]\\n\\n\\nfor distribution, color, axes in zip(distributions, colors, axes_list):\\n    name, X = distribution\\n    X_train, X_test = train_test_split(X, test_size=0.5)\\n\\n    # perform power transforms and quantile transform\\n    X_trans_bc = bc.fit(X_train).transform(X_test)\\n    lmbda_bc = round(bc.lambdas_[0], 2)\\n    X_trans_yj = yj.fit(X_train).transform(X_test)\\n    lmbda_yj = round(yj.lambdas_[0], 2)\\n    X_trans_qt = qt.fit(X_train).transform(X_test)\\n\\n    ax_original, ax_bc, ax_yj, ax_qt = axes\\n\\n    ax_original.hist(X_train, color=color, bins=BINS)\\n    ax_original.set_title(name, fontsize=FONT_SIZE)\\n    ax_original.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\\n\\n    for ax, X_trans, meth_name, lmbda in zip(\\n        (ax_bc, ax_yj, ax_qt),\\n        (X_trans_bc, X_trans_yj, X_trans_qt),\\n        (\"Box-Cox\", \"Yeo-Johnson\", \"Quantile transform\"),\\n        (lmbda_bc, lmbda_yj, None),\\n    ):\\n        ax.hist(X_trans, color=color, bins=BINS)\\n        title = \"After {}\".format(meth_name)\\n        if lmbda is not None:\\n            title += \"\\\\n$\\\\\\\\lambda$ = {}\".format(lmbda)\\n        ax.set_title(title, fontsize=FONT_SIZE)\\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\\n        ax.set_xlim([-3.5, 3.5])\\n\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_target_encoder_cross_val.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=======================================\\nTarget Encoder\\'s Internal Cross fitting\\n=======================================\\n\\n.. currentmodule:: sklearn.preprocessing\\n\\nThe :class:`TargetEncoder` replaces each category of a categorical feature with\\nthe shrunk mean of the target variable for that category. This method is useful\\nin cases where there is a strong relationship between the categorical feature\\nand the target. To prevent overfitting, :meth:`TargetEncoder.fit_transform` uses\\nan internal :term:`cross fitting` scheme to encode the training data to be used\\nby a downstream model. This scheme involves splitting the data into *k* folds\\nand encoding each fold using the encodings learnt using the other *k-1* folds.\\nIn this example, we demonstrate the importance of the cross\\nfitting procedure to prevent overfitting.\\n\"\"\"\\n\\n# %%\\n# Create Synthetic Dataset\\n# ========================\\n# For this example, we build a dataset with three categorical features:\\n#\\n# * an informative feature with medium cardinality (\"informative\")\\n# * an uninformative feature with medium cardinality (\"shuffled\")\\n# * an uninformative feature with high cardinality (\"near_unique\")\\n#\\n# First, we generate the informative feature:\\nimport numpy as np\\n\\nfrom sklearn.preprocessing import KBinsDiscretizer\\n\\nn_samples = 50_000\\n\\nrng = np.random.RandomState(42)\\ny = rng.randn(n_samples)\\nnoise = 0.5 * rng.randn(n_samples)\\nn_categories = 100\\n\\nkbins = KBinsDiscretizer(\\n    n_bins=n_categories,\\n    encode=\"ordinal\",\\n    strategy=\"uniform\",\\n    random_state=rng,\\n    subsample=None,\\n)\\nX_informative = kbins.fit_transform((y + noise).reshape(-1, 1))\\n\\n# Remove the linear relationship between y and the bin index by permuting the\\n# values of X_informative:\\npermuted_categories = rng.permutation(n_categories)\\nX_informative = permuted_categories[X_informative.astype(np.int32)]\\n\\n# %%\\n# The uninformative feature with medium cardinality is generated by permuting the\\n# informative feature and removing the relationship with the target:\\nX_shuffled = rng.permutation(X_informative)\\n\\n# %%\\n# The uninformative feature with high cardinality is generated so that it is\\n# independent of the target variable. We will show that target encoding without\\n# :term:`cross fitting` will cause catastrophic overfitting for the downstream\\n# regressor. These high cardinality features are basically unique identifiers\\n# for samples which should generally be removed from machine learning datasets.\\n# In this example, we generate them to show how :class:`TargetEncoder`\\'s default\\n# :term:`cross fitting` behavior mitigates the overfitting issue automatically.\\nX_near_unique_categories = rng.choice(\\n    int(0.9 * n_samples), size=n_samples, replace=True\\n).reshape(-1, 1)\\n\\n# %%\\n# Finally, we assemble the dataset and perform a train test split:\\nimport pandas as pd\\n\\nfrom sklearn.model_selection import train_test_split'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_target_encoder_cross_val.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Finally, we assemble the dataset and perform a train test split:\\nimport pandas as pd\\n\\nfrom sklearn.model_selection import train_test_split\\n\\nX = pd.DataFrame(\\n    np.concatenate(\\n        [X_informative, X_shuffled, X_near_unique_categories],\\n        axis=1,\\n    ),\\n    columns=[\"informative\", \"shuffled\", \"near_unique\"],\\n)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\n# %%\\n# Training a Ridge Regressor\\n# ==========================\\n# In this section, we train a ridge regressor on the dataset with and without\\n# encoding and explore the influence of target encoder with and without the\\n# internal :term:`cross fitting`. First, we see the Ridge model trained on the\\n# raw features will have low performance. This is because we permuted the order\\n# of the informative feature meaning `X_informative` is not informative when\\n# raw:\\nimport sklearn\\nfrom sklearn.linear_model import Ridge\\n\\n# Configure transformers to always output DataFrames\\nsklearn.set_config(transform_output=\"pandas\")\\n\\nridge = Ridge(alpha=1e-6, solver=\"lsqr\", fit_intercept=False)\\n\\nraw_model = ridge.fit(X_train, y_train)\\nprint(\"Raw Model score on training set: \", raw_model.score(X_train, y_train))\\nprint(\"Raw Model score on test set: \", raw_model.score(X_test, y_test))\\n\\n# %%\\n# Next, we create a pipeline with the target encoder and ridge model. The pipeline\\n# uses :meth:`TargetEncoder.fit_transform` which uses :term:`cross fitting`. We\\n# see that the model fits the data well and generalizes to the test set:\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import TargetEncoder\\n\\nmodel_with_cf = make_pipeline(TargetEncoder(random_state=0), ridge)\\nmodel_with_cf.fit(X_train, y_train)\\nprint(\"Model with CF on train set: \", model_with_cf.score(X_train, y_train))\\nprint(\"Model with CF on test set: \", model_with_cf.score(X_test, y_test))\\n\\n# %%\\n# The coefficients of the linear model shows that most of the weight is on the\\n# feature at column index 0, which is the informative feature\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\nplt.rcParams[\"figure.constrained_layout.use\"] = True\\n\\ncoefs_cf = pd.Series(\\n    model_with_cf[-1].coef_, index=model_with_cf[-1].feature_names_in_\\n).sort_values()\\nax = coefs_cf.plot(kind=\"barh\")\\n_ = ax.set(\\n    title=\"Target encoded with cross fitting\",\\n    xlabel=\"Ridge coefficient\",\\n    ylabel=\"Feature\",\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_target_encoder_cross_val.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='coefs_cf = pd.Series(\\n    model_with_cf[-1].coef_, index=model_with_cf[-1].feature_names_in_\\n).sort_values()\\nax = coefs_cf.plot(kind=\"barh\")\\n_ = ax.set(\\n    title=\"Target encoded with cross fitting\",\\n    xlabel=\"Ridge coefficient\",\\n    ylabel=\"Feature\",\\n)\\n\\n# %%\\n# While :meth:`TargetEncoder.fit_transform` uses an internal\\n# :term:`cross fitting` scheme to learn encodings for the training set,\\n# :meth:`TargetEncoder.transform` itself does not.\\n# It uses the complete training set to learn encodings and to transform the\\n# categorical features. Thus, we can use :meth:`TargetEncoder.fit` followed by\\n# :meth:`TargetEncoder.transform` to disable the :term:`cross fitting`. This\\n# encoding is then passed to the ridge model.\\ntarget_encoder = TargetEncoder(random_state=0)\\ntarget_encoder.fit(X_train, y_train)\\nX_train_no_cf_encoding = target_encoder.transform(X_train)\\nX_test_no_cf_encoding = target_encoder.transform(X_test)\\n\\nmodel_no_cf = ridge.fit(X_train_no_cf_encoding, y_train)\\n\\n# %%\\n# We evaluate the model that did not use :term:`cross fitting` when encoding and\\n# see that it overfits:\\nprint(\\n    \"Model without CF on training set: \",\\n    model_no_cf.score(X_train_no_cf_encoding, y_train),\\n)\\nprint(\\n    \"Model without CF on test set: \",\\n    model_no_cf.score(\\n        X_test_no_cf_encoding,\\n        y_test,\\n    ),\\n)\\n\\n# %%\\n# The ridge model overfits because it assigns much more weight to the\\n# uninformative extremely high cardinality (\"near_unique\") and medium\\n# cardinality (\"shuffled\") features than when the model used\\n# :term:`cross fitting` to encode the features.\\ncoefs_no_cf = pd.Series(\\n    model_no_cf.coef_, index=model_no_cf.feature_names_in_\\n).sort_values()\\nax = coefs_no_cf.plot(kind=\"barh\")\\n_ = ax.set(\\n    title=\"Target encoded without cross fitting\",\\n    xlabel=\"Ridge coefficient\",\\n    ylabel=\"Feature\",\\n)\\n\\n# %%\\n# Conclusion\\n# ==========\\n# This example demonstrates the importance of :class:`TargetEncoder`\\'s internal\\n# :term:`cross fitting`. It is important to use\\n# :meth:`TargetEncoder.fit_transform` to encode training data before passing it\\n# to a machine learning model. When a :class:`TargetEncoder` is a part of a\\n# :class:`~sklearn.pipeline.Pipeline` and the pipeline is fitted, the pipeline\\n# will correctly call :meth:`TargetEncoder.fit_transform` and use\\n# :term:`cross fitting` when encoding the training data.'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_scaling_importance.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def fit_and_plot_model(X_plot, y, clf, ax):\\n    clf.fit(X_plot, y)\\n    disp = DecisionBoundaryDisplay.from_estimator(\\n        clf,\\n        X_plot,\\n        response_method=\"predict\",\\n        alpha=0.5,\\n        ax=ax,\\n    )\\n    disp.ax_.scatter(X_plot[\"proline\"], X_plot[\"hue\"], c=y, s=20, edgecolor=\"k\")\\n    disp.ax_.set_xlim((X_plot[\"proline\"].min(), X_plot[\"proline\"].max()))\\n    disp.ax_.set_ylim((X_plot[\"hue\"].min(), X_plot[\"hue\"].max()))\\n    return disp.ax_'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_scaling_importance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================\\nImportance of Feature Scaling\\n=============================\\n\\nFeature scaling through standardization, also called Z-score normalization, is\\nan important preprocessing step for many machine learning algorithms. It\\ninvolves rescaling each feature such that it has a standard deviation of 1 and a\\nmean of 0.\\n\\nEven if tree based models are (almost) not affected by scaling, many other\\nalgorithms require features to be normalized, often for different reasons: to\\nease the convergence (such as a non-penalized logistic regression), to create a\\ncompletely different model fit compared to the fit with unscaled data (such as\\nKNeighbors models). The latter is demoed on the first part of the present\\nexample.\\n\\nOn the second part of the example we show how Principal Component Analysis (PCA)\\nis impacted by normalization of features. To illustrate this, we compare the\\nprincipal components found using :class:`~sklearn.decomposition.PCA` on unscaled\\ndata with those obatined when using a\\n:class:`~sklearn.preprocessing.StandardScaler` to scale data first.\\n\\nIn the last part of the example we show the effect of the normalization on the\\naccuracy of a model trained on PCA-reduced data.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Load and prepare data\\n# =====================\\n#\\n# The dataset used is the :ref:`wine_dataset` available at UCI. This dataset has\\n# continuous features that are heterogeneous in scale due to differing\\n# properties that they measure (e.g. alcohol content and malic acid).\\n\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\nX, y = load_wine(return_X_y=True, as_frame=True)\\nscaler = StandardScaler().set_output(transform=\"pandas\")\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.30, random_state=42\\n)\\nscaled_X_train = scaler.fit_transform(X_train)\\n\\n# %%\\n# .. _neighbors_scaling:\\n#\\n# Effect of rescaling on a k-neighbors models\\n# ===========================================\\n#\\n# For the sake of visualizing the decision boundary of a\\n# :class:`~sklearn.neighbors.KNeighborsClassifier`, in this section we select a\\n# subset of 2 features that have values with different orders of magnitude.\\n#\\n# Keep in mind that using a subset of the features to train the model may likely\\n# leave out feature with high predictive impact, resulting in a decision\\n# boundary that is much worse in comparison to a model trained on the full set\\n# of features.\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.neighbors import KNeighborsClassifier\\n\\nX_plot = X[[\"proline\", \"hue\"]]\\nX_plot_scaled = scaler.fit_transform(X_plot)\\nclf = KNeighborsClassifier(n_neighbors=20)\\n\\n\\n# Code for: def fit_and_plot_model(X_plot, y, clf, ax):\\n\\n\\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_scaling_importance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X_plot = X[[\"proline\", \"hue\"]]\\nX_plot_scaled = scaler.fit_transform(X_plot)\\nclf = KNeighborsClassifier(n_neighbors=20)\\n\\n\\n# Code for: def fit_and_plot_model(X_plot, y, clf, ax):\\n\\n\\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\\n\\nfit_and_plot_model(X_plot, y, clf, ax1)\\nax1.set_title(\"KNN without scaling\")\\n\\nfit_and_plot_model(X_plot_scaled, y, clf, ax2)\\nax2.set_xlabel(\"scaled proline\")\\nax2.set_ylabel(\"scaled hue\")\\n_ = ax2.set_title(\"KNN with scaling\")\\n\\n# %%\\n# Here the decision boundary shows that fitting scaled or non-scaled data lead\\n# to completely different models. The reason is that the variable \"proline\" has\\n# values which vary between 0 and 1,000; whereas the variable \"hue\" varies\\n# between 1 and 10. Because of this, distances between samples are mostly\\n# impacted by differences in values of \"proline\", while values of the \"hue\" will\\n# be comparatively ignored. If one uses\\n# :class:`~sklearn.preprocessing.StandardScaler` to normalize this database,\\n# both scaled values lay approximately between -3 and 3 and the neighbors\\n# structure will be impacted more or less equivalently by both variables.\\n#\\n# Effect of rescaling on a PCA dimensional reduction\\n# ==================================================\\n#\\n# Dimensional reduction using :class:`~sklearn.decomposition.PCA` consists of\\n# finding the features that maximize the variance. If one feature varies more\\n# than the others only because of their respective scales,\\n# :class:`~sklearn.decomposition.PCA` would determine that such feature\\n# dominates the direction of the principal components.\\n#\\n# We can inspect the first principal components using all the original features:\\n\\nimport pandas as pd\\n\\nfrom sklearn.decomposition import PCA\\n\\npca = PCA(n_components=2).fit(X_train)\\nscaled_pca = PCA(n_components=2).fit(scaled_X_train)\\nX_train_transformed = pca.transform(X_train)\\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\\n\\nfirst_pca_component = pd.DataFrame(\\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\\n)\\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\\nfirst_pca_component.plot.bar(\\n    title=\"Weights of the first principal component\", figsize=(6, 8)\\n)\\n\\n_ = plt.tight_layout()\\n\\n# %%\\n# Indeed we find that the \"proline\" feature dominates the direction of the first\\n# principal component without scaling, being about two orders of magnitude above\\n# the other features. This is contrasted when observing the first principal\\n# component for the scaled version of the data, where the orders of magnitude\\n# are roughly the same across all the features.\\n#\\n# We can visualize the distribution of the principal components in both cases:\\n\\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\\n\\ntarget_classes = range(0, 3)\\ncolors = (\"blue\", \"red\", \"green\")\\nmarkers = (\"^\", \"s\", \"o\")'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_scaling_importance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\\n\\ntarget_classes = range(0, 3)\\ncolors = (\"blue\", \"red\", \"green\")\\nmarkers = (\"^\", \"s\", \"o\")\\n\\nfor target_class, color, marker in zip(target_classes, colors, markers):\\n    ax1.scatter(\\n        x=X_train_transformed[y_train == target_class, 0],\\n        y=X_train_transformed[y_train == target_class, 1],\\n        color=color,\\n        label=f\"class {target_class}\",\\n        alpha=0.5,\\n        marker=marker,\\n    )\\n\\n    ax2.scatter(\\n        x=X_train_std_transformed[y_train == target_class, 0],\\n        y=X_train_std_transformed[y_train == target_class, 1],\\n        color=color,\\n        label=f\"class {target_class}\",\\n        alpha=0.5,\\n        marker=marker,\\n    )\\n\\nax1.set_title(\"Unscaled training dataset after PCA\")\\nax2.set_title(\"Standardized training dataset after PCA\")\\n\\nfor ax in (ax1, ax2):\\n    ax.set_xlabel(\"1st principal component\")\\n    ax.set_ylabel(\"2nd principal component\")\\n    ax.legend(loc=\"upper right\")\\n    ax.grid()\\n\\n_ = plt.tight_layout()\\n\\n# %%\\n# From the plot above we observe that scaling the features before reducing the\\n# dimensionality results in components with the same order of magnitude. In this\\n# case it also improves the separability of the classes. Indeed, in the next\\n# section we confirm that a better separability has a good repercussion on the\\n# overall model\\'s performance.\\n#\\n# Effect of rescaling on model\\'s performance\\n# ==========================================\\n#\\n# First we show how the optimal regularization of a\\n# :class:`~sklearn.linear_model.LogisticRegressionCV` depends on the scaling or\\n# non-scaling of the data:\\n\\nimport numpy as np\\n\\nfrom sklearn.linear_model import LogisticRegressionCV\\nfrom sklearn.pipeline import make_pipeline\\n\\nCs = np.logspace(-5, 5, 20)\\n\\nunscaled_clf = make_pipeline(pca, LogisticRegressionCV(Cs=Cs))\\nunscaled_clf.fit(X_train, y_train)\\n\\nscaled_clf = make_pipeline(scaler, pca, LogisticRegressionCV(Cs=Cs))\\nscaled_clf.fit(X_train, y_train)\\n\\nprint(f\"Optimal C for the unscaled PCA: {unscaled_clf[-1].C_[0]:.4f}\\\\n\")\\nprint(f\"Optimal C for the standardized data with PCA: {scaled_clf[-1].C_[0]:.2f}\")\\n\\n# %%\\n# The need for regularization is higher (lower values of `C`) for the data that\\n# was not scaled before applying PCA. We now evaluate the effect of scaling on\\n# the accuracy and the mean log-loss of the optimal models:\\n\\nfrom sklearn.metrics import accuracy_score, log_loss\\n\\ny_pred = unscaled_clf.predict(X_test)\\ny_pred_scaled = scaled_clf.predict(X_test)\\ny_proba = unscaled_clf.predict_proba(X_test)\\ny_proba_scaled = scaled_clf.predict_proba(X_test)\\n\\nprint(\"Test accuracy for the unscaled PCA\")\\nprint(f\"{accuracy_score(y_test, y_pred):.2%}\\\\n\")\\nprint(\"Test accuracy for the standardized data with PCA\")\\nprint(f\"{accuracy_score(y_test, y_pred_scaled):.2%}\\\\n\")\\nprint(\"Log-loss for the unscaled PCA\")\\nprint(f\"{log_loss(y_test, y_proba):.3}\\\\n\")\\nprint(\"Log-loss for the standardized data with PCA\")\\nprint(f\"{log_loss(y_test, y_proba_scaled):.3}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_scaling_importance.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# A clear difference in prediction accuracies is observed when the data is\\n# scaled before :class:`~sklearn.decomposition.PCA`, as it vastly outperforms\\n# the unscaled version. This corresponds to the intuition obtained from the plot\\n# in the previous section, where the components become linearly separable when\\n# scaling before using :class:`~sklearn.decomposition.PCA`.\\n#\\n# Notice that in this case the models with scaled features perform better than\\n# the models with non-scaled features because all the variables are expected to\\n# be predictive and we rather avoid some of them being comparatively ignored.\\n#\\n# If the variables in lower scales were not predictive, one may experience a\\n# decrease of the performance after scaling the features: noisy features would\\n# contribute more to the prediction after scaling and therefore scaling would\\n# increase overfitting.\\n#\\n# Last but not least, we observe that one achieves a lower log-loss by means of\\n# the scaling step.'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_discretization_strategies.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================================================\\nDemonstrating the different strategies of KBinsDiscretizer\\n==========================================================\\n\\nThis example presents the different strategies implemented in KBinsDiscretizer:\\n\\n- \\'uniform\\': The discretization is uniform in each feature, which means that\\n  the bin widths are constant in each dimension.\\n- quantile\\': The discretization is done on the quantiled values, which means\\n  that each bin has approximately the same number of samples.\\n- \\'kmeans\\': The discretization is based on the centroids of a KMeans clustering\\n  procedure.\\n\\nThe plot shows the regions where the discretized encoding is constant.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.preprocessing import KBinsDiscretizer\\n\\nstrategies = [\"uniform\", \"quantile\", \"kmeans\"]\\n\\nn_samples = 200\\ncenters_0 = np.array([[0, 0], [0, 5], [2, 4], [8, 8]])\\ncenters_1 = np.array([[0, 0], [3, 1]])\\n\\n# construct the datasets\\nrandom_state = 42\\nX_list = [\\n    np.random.RandomState(random_state).uniform(-3, 3, size=(n_samples, 2)),\\n    make_blobs(\\n        n_samples=[\\n            n_samples // 10,\\n            n_samples * 4 // 10,\\n            n_samples // 10,\\n            n_samples * 4 // 10,\\n        ],\\n        cluster_std=0.5,\\n        centers=centers_0,\\n        random_state=random_state,\\n    )[0],\\n    make_blobs(\\n        n_samples=[n_samples // 5, n_samples * 4 // 5],\\n        cluster_std=0.5,\\n        centers=centers_1,\\n        random_state=random_state,\\n    )[0],\\n]\\n\\nfigure = plt.figure(figsize=(14, 9))\\ni = 1\\nfor ds_cnt, X in enumerate(X_list):\\n    ax = plt.subplot(len(X_list), len(strategies) + 1, i)\\n    ax.scatter(X[:, 0], X[:, 1], edgecolors=\"k\")\\n    if ds_cnt == 0:\\n        ax.set_title(\"Input data\", size=14)\\n\\n    xx, yy = np.meshgrid(\\n        np.linspace(X[:, 0].min(), X[:, 0].max(), 300),\\n        np.linspace(X[:, 1].min(), X[:, 1].max(), 300),\\n    )\\n    grid = np.c_[xx.ravel(), yy.ravel()]\\n\\n    ax.set_xlim(xx.min(), xx.max())\\n    ax.set_ylim(yy.min(), yy.max())\\n    ax.set_xticks(())\\n    ax.set_yticks(())\\n\\n    i += 1\\n    # transform the dataset with KBinsDiscretizer\\n    for strategy in strategies:\\n        enc = KBinsDiscretizer(n_bins=4, encode=\"ordinal\", strategy=strategy)\\n        enc.fit(X)\\n        grid_encoded = enc.transform(grid)\\n\\n        ax = plt.subplot(len(X_list), len(strategies) + 1, i)\\n\\n        # horizontal stripes\\n        horizontal = grid_encoded[:, 0].reshape(xx.shape)\\n        ax.contourf(xx, yy, horizontal, alpha=0.5)\\n        # vertical stripes\\n        vertical = grid_encoded[:, 1].reshape(xx.shape)\\n        ax.contourf(xx, yy, vertical, alpha=0.5)'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_discretization_strategies.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# horizontal stripes\\n        horizontal = grid_encoded[:, 0].reshape(xx.shape)\\n        ax.contourf(xx, yy, horizontal, alpha=0.5)\\n        # vertical stripes\\n        vertical = grid_encoded[:, 1].reshape(xx.shape)\\n        ax.contourf(xx, yy, vertical, alpha=0.5)\\n\\n        ax.scatter(X[:, 0], X[:, 1], edgecolors=\"k\")\\n        ax.set_xlim(xx.min(), xx.max())\\n        ax.set_ylim(yy.min(), yy.max())\\n        ax.set_xticks(())\\n        ax.set_yticks(())\\n        if ds_cnt == 0:\\n            ax.set_title(\"strategy=\\'%s\\'\" % (strategy,), size=14)\\n\\n        i += 1\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_discretization_classification.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_name(estimator):\\n    name = estimator.__class__.__name__\\n    if name == \"Pipeline\":\\n        name = [get_name(est[1]) for est in estimator.steps]\\n        name = \" + \".join(name)\\n    return name'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_discretization_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================\\nFeature discretization\\n======================\\n\\nA demonstration of feature discretization on synthetic classification datasets.\\nFeature discretization decomposes each feature into a set of bins, here equally\\ndistributed in width. The discrete values are then one-hot encoded, and given\\nto a linear classifier. This preprocessing enables a non-linear behavior even\\nthough the classifier is linear.\\n\\nOn this example, the first two rows represent linearly non-separable datasets\\n(moons and concentric circles) while the third is approximately linearly\\nseparable. On the two linearly non-separable datasets, feature discretization\\nlargely increases the performance of linear classifiers. On the linearly\\nseparable dataset, feature discretization decreases the performance of linear\\nclassifiers. Two non-linear classifiers are also shown for comparison.\\n\\nThis example should be taken with a grain of salt, as the intuition conveyed\\ndoes not necessarily carry over to real datasets. Particularly in\\nhigh-dimensional spaces, data can more easily be separated linearly. Moreover,\\nusing feature discretization and one-hot encoding increases the number of\\nfeatures, which easily lead to overfitting when the number of samples is small.\\n\\nThe plots show training points in solid colors and testing points\\nsemi-transparent. The lower right shows the classification accuracy on the test\\nset.\\n\\n\"\"\"\\n\\n# Code source: Tom Dupr√© la Tour\\n# Adapted from plot_classifier_comparison by Ga√´l Varoquaux and Andreas M√ºller\\n#\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib.colors import ListedColormap\\n\\nfrom sklearn.datasets import make_circles, make_classification, make_moons\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.exceptions import ConvergenceWarning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import KBinsDiscretizer, StandardScaler\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.utils._testing import ignore_warnings\\n\\nh = 0.02  # step size in the mesh\\n\\n\\n# Code for: def get_name(estimator):'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_discretization_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='h = 0.02  # step size in the mesh\\n\\n\\n# Code for: def get_name(estimator):\\n\\n\\n# list of (estimator, param_grid), where param_grid is used in GridSearchCV\\n# The parameter spaces in this example are limited to a narrow band to reduce\\n# its runtime. In a real use case, a broader search space for the algorithms\\n# should be used.\\nclassifiers = [\\n    (\\n        make_pipeline(StandardScaler(), LogisticRegression(random_state=0)),\\n        {\"logisticregression__C\": np.logspace(-1, 1, 3)},\\n    ),\\n    (\\n        make_pipeline(StandardScaler(), LinearSVC(random_state=0)),\\n        {\"linearsvc__C\": np.logspace(-1, 1, 3)},\\n    ),\\n    (\\n        make_pipeline(\\n            StandardScaler(),\\n            KBinsDiscretizer(encode=\"onehot\", random_state=0),\\n            LogisticRegression(random_state=0),\\n        ),\\n        {\\n            \"kbinsdiscretizer__n_bins\": np.arange(5, 8),\\n            \"logisticregression__C\": np.logspace(-1, 1, 3),\\n        },\\n    ),\\n    (\\n        make_pipeline(\\n            StandardScaler(),\\n            KBinsDiscretizer(encode=\"onehot\", random_state=0),\\n            LinearSVC(random_state=0),\\n        ),\\n        {\\n            \"kbinsdiscretizer__n_bins\": np.arange(5, 8),\\n            \"linearsvc__C\": np.logspace(-1, 1, 3),\\n        },\\n    ),\\n    (\\n        make_pipeline(\\n            StandardScaler(), GradientBoostingClassifier(n_estimators=5, random_state=0)\\n        ),\\n        {\"gradientboostingclassifier__learning_rate\": np.logspace(-2, 0, 5)},\\n    ),\\n    (\\n        make_pipeline(StandardScaler(), SVC(random_state=0)),\\n        {\"svc__C\": np.logspace(-1, 1, 3)},\\n    ),\\n]\\n\\nnames = [get_name(e).replace(\"StandardScaler + \", \"\") for e, _ in classifiers]\\n\\nn_samples = 100\\ndatasets = [\\n    make_moons(n_samples=n_samples, noise=0.2, random_state=0),\\n    make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),\\n    make_classification(\\n        n_samples=n_samples,\\n        n_features=2,\\n        n_redundant=0,\\n        n_informative=2,\\n        random_state=2,\\n        n_clusters_per_class=1,\\n    ),\\n]\\n\\nfig, axes = plt.subplots(\\n    nrows=len(datasets), ncols=len(classifiers) + 1, figsize=(21, 9)\\n)\\n\\ncm_piyg = plt.cm.PiYG\\ncm_bright = ListedColormap([\"#b30065\", \"#178000\"])\\n\\n# iterate over datasets\\nfor ds_cnt, (X, y) in enumerate(datasets):\\n    print(f\"\\\\ndataset {ds_cnt}\\\\n---------\")\\n\\n    # split into training and test part\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=0.5, random_state=42\\n    )\\n\\n    # create the grid for background colors\\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_discretization_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# create the grid for background colors\\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n\\n    # plot the dataset first\\n    ax = axes[ds_cnt, 0]\\n    if ds_cnt == 0:\\n        ax.set_title(\"Input data\")\\n    # plot the training points\\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\\n    # and testing points\\n    ax.scatter(\\n        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\\n    )\\n    ax.set_xlim(xx.min(), xx.max())\\n    ax.set_ylim(yy.min(), yy.max())\\n    ax.set_xticks(())\\n    ax.set_yticks(())\\n\\n    # iterate over classifiers\\n    for est_idx, (name, (estimator, param_grid)) in enumerate(zip(names, classifiers)):\\n        ax = axes[ds_cnt, est_idx + 1]\\n\\n        clf = GridSearchCV(estimator=estimator, param_grid=param_grid)\\n        with ignore_warnings(category=ConvergenceWarning):\\n            clf.fit(X_train, y_train)\\n        score = clf.score(X_test, y_test)\\n        print(f\"{name}: {score:.2f}\")\\n\\n        # plot the decision boundary. For that, we will assign a color to each\\n        # point in the mesh [x_min, x_max]*[y_min, y_max].\\n        if hasattr(clf, \"decision_function\"):\\n            Z = clf.decision_function(np.column_stack([xx.ravel(), yy.ravel()]))\\n        else:\\n            Z = clf.predict_proba(np.column_stack([xx.ravel(), yy.ravel()]))[:, 1]\\n\\n        # put the result into a color plot\\n        Z = Z.reshape(xx.shape)\\n        ax.contourf(xx, yy, Z, cmap=cm_piyg, alpha=0.8)\\n\\n        # plot the training points\\n        ax.scatter(\\n            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\\n        )\\n        # and testing points\\n        ax.scatter(\\n            X_test[:, 0],\\n            X_test[:, 1],\\n            c=y_test,\\n            cmap=cm_bright,\\n            edgecolors=\"k\",\\n            alpha=0.6,\\n        )\\n        ax.set_xlim(xx.min(), xx.max())\\n        ax.set_ylim(yy.min(), yy.max())\\n        ax.set_xticks(())\\n        ax.set_yticks(())\\n\\n        if ds_cnt == 0:\\n            ax.set_title(name.replace(\" + \", \"\\\\n\"))\\n        ax.text(\\n            0.95,\\n            0.06,\\n            (f\"{score:.2f}\").lstrip(\"0\"),\\n            size=15,\\n            bbox=dict(boxstyle=\"round\", alpha=0.8, facecolor=\"white\"),\\n            transform=ax.transAxes,\\n            horizontalalignment=\"right\",\\n        )\\n\\n\\nplt.tight_layout()\\n\\n# Add suptitles above the figure\\nplt.subplots_adjust(top=0.90)\\nsuptitles = [\\n    \"Linear classifiers\",\\n    \"Feature discretization and linear classifiers\",\\n    \"Non-linear classifiers\",\\n]\\nfor i, suptitle in zip([1, 3, 5], suptitles):\\n    ax = axes[0, i]\\n    ax.text(\\n        1.05,\\n        1.25,\\n        suptitle,\\n        transform=ax.transAxes,\\n        horizontalalignment=\"center\",\\n        size=\"x-large\",\\n    )\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_all_scaling.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def create_axes(title, figsize=(16, 6)):\\n    fig = plt.figure(figsize=figsize)\\n    fig.suptitle(title)\\n\\n    # define the axis for the first plot\\n    left, width = 0.1, 0.22\\n    bottom, height = 0.1, 0.7\\n    bottom_h = height + 0.15\\n    left_h = left + width + 0.02\\n\\n    rect_scatter = [left, bottom, width, height]\\n    rect_histx = [left, bottom_h, width, 0.1]\\n    rect_histy = [left_h, bottom, 0.05, height]\\n\\n    ax_scatter = plt.axes(rect_scatter)\\n    ax_histx = plt.axes(rect_histx)\\n    ax_histy = plt.axes(rect_histy)\\n\\n    # define the axis for the zoomed-in plot\\n    left = width + left + 0.2\\n    left_h = left + width + 0.02\\n\\n    rect_scatter = [left, bottom, width, height]\\n    rect_histx = [left, bottom_h, width, 0.1]\\n    rect_histy = [left_h, bottom, 0.05, height]\\n\\n    ax_scatter_zoom = plt.axes(rect_scatter)\\n    ax_histx_zoom = plt.axes(rect_histx)\\n    ax_histy_zoom = plt.axes(rect_histy)\\n\\n    # define the axis for the colorbar\\n    left, width = width + left + 0.13, 0.01\\n\\n    rect_colorbar = [left, bottom, width, height]\\n    ax_colorbar = plt.axes(rect_colorbar)\\n\\n    return (\\n        (ax_scatter, ax_histy, ax_histx),\\n        (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\\n        ax_colorbar,\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_all_scaling.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_distribution(axes, X, y, hist_nbins=50, title=\"\", x0_label=\"\", x1_label=\"\"):\\n    ax, hist_X1, hist_X0 = axes\\n\\n    ax.set_title(title)\\n    ax.set_xlabel(x0_label)\\n    ax.set_ylabel(x1_label)\\n\\n    # The scatter plot\\n    colors = cmap(y)\\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker=\"o\", s=5, lw=0, c=colors)\\n\\n    # Removing the top and the right spine for aesthetics\\n    # make nice axis layout\\n    ax.spines[\"top\"].set_visible(False)\\n    ax.spines[\"right\"].set_visible(False)\\n    ax.get_xaxis().tick_bottom()\\n    ax.get_yaxis().tick_left()\\n    ax.spines[\"left\"].set_position((\"outward\", 10))\\n    ax.spines[\"bottom\"].set_position((\"outward\", 10))\\n\\n    # Histogram for axis X1 (feature 5)\\n    hist_X1.set_ylim(ax.get_ylim())\\n    hist_X1.hist(\\n        X[:, 1], bins=hist_nbins, orientation=\"horizontal\", color=\"grey\", ec=\"grey\"\\n    )\\n    hist_X1.axis(\"off\")\\n\\n    # Histogram for axis X0 (feature 0)\\n    hist_X0.set_xlim(ax.get_xlim())\\n    hist_X0.hist(\\n        X[:, 0], bins=hist_nbins, orientation=\"vertical\", color=\"grey\", ec=\"grey\"\\n    )\\n    hist_X0.axis(\"off\")'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_all_scaling.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def make_plot(item_idx):\\n    title, X = distributions[item_idx]\\n    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\\n    axarr = (ax_zoom_out, ax_zoom_in)\\n    plot_distribution(\\n        axarr[0],\\n        X,\\n        y,\\n        hist_nbins=200,\\n        x0_label=feature_mapping[features[0]],\\n        x1_label=feature_mapping[features[1]],\\n        title=\"Full data\",\\n    )\\n\\n    # zoom-in\\n    zoom_in_percentile_range = (0, 99)\\n    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\\n    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\\n\\n    non_outliers_mask = np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) & np.all(\\n        X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1\\n    )\\n    plot_distribution(\\n        axarr[1],\\n        X[non_outliers_mask],\\n        y[non_outliers_mask],\\n        hist_nbins=50,\\n        x0_label=feature_mapping[features[0]],\\n        x1_label=feature_mapping[features[1]],\\n        title=\"Zoom-in\",\\n    )\\n\\n    norm = mpl.colors.Normalize(y_full.min(), y_full.max())\\n    mpl.colorbar.ColorbarBase(\\n        ax_colorbar,\\n        cmap=cmap,\\n        norm=norm,\\n        orientation=\"vertical\",\\n        label=\"Color mapping for values of y\",\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_all_scaling.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================================\\nCompare the effect of different scalers on data with outliers\\n=============================================================\\n\\nFeature 0 (median income in a block) and feature 5 (average house occupancy) of\\nthe :ref:`california_housing_dataset` have very\\ndifferent scales and contain some very large outliers. These two\\ncharacteristics lead to difficulties to visualize the data and, more\\nimportantly, they can degrade the predictive performance of many machine\\nlearning algorithms. Unscaled data can also slow down or even prevent the\\nconvergence of many gradient-based estimators.\\n\\nIndeed many estimators are designed with the assumption that each feature takes\\nvalues close to zero or more importantly that all features vary on comparable\\nscales. In particular, metric-based and gradient-based estimators often assume\\napproximately standardized data (centered features with unit variances). A\\nnotable exception are decision tree-based estimators that are robust to\\narbitrary scaling of the data.\\n\\nThis example uses different scalers, transformers, and normalizers to bring the\\ndata within a pre-defined range.\\n\\nScalers are linear (or more precisely affine) transformers and differ from each\\nother in the way they estimate the parameters used to shift and scale each\\nfeature.\\n\\n:class:`~sklearn.preprocessing.QuantileTransformer` provides non-linear\\ntransformations in which distances\\nbetween marginal outliers and inliers are shrunk.\\n:class:`~sklearn.preprocessing.PowerTransformer` provides\\nnon-linear transformations in which data is mapped to a normal distribution to\\nstabilize variance and minimize skewness.\\n\\nUnlike the previous transformations, normalization refers to a per sample\\ntransformation instead of a per feature transformation.\\n\\nThe following code is a bit verbose, feel free to jump directly to the analysis\\nof the results_.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib as mpl\\nimport numpy as np\\nfrom matplotlib import cm\\nfrom matplotlib import pyplot as plt\\n\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.preprocessing import (\\n    MaxAbsScaler,\\n    MinMaxScaler,\\n    Normalizer,\\n    PowerTransformer,\\n    QuantileTransformer,\\n    RobustScaler,\\n    StandardScaler,\\n    minmax_scale,\\n)\\n\\ndataset = fetch_california_housing()\\nX_full, y_full = dataset.data, dataset.target\\nfeature_names = dataset.feature_names\\n\\nfeature_mapping = {\\n    \"MedInc\": \"Median income in block\",\\n    \"HouseAge\": \"Median house age in block\",\\n    \"AveRooms\": \"Average number of rooms\",\\n    \"AveBedrms\": \"Average number of bedrooms\",\\n    \"Population\": \"Block population\",\\n    \"AveOccup\": \"Average house occupancy\",\\n    \"Latitude\": \"House block latitude\",\\n    \"Longitude\": \"House block longitude\",\\n}'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_all_scaling.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Take only 2 features to make visualization easier\\n# Feature MedInc has a long tail distribution.\\n# Feature AveOccup has a few but very large outliers.\\nfeatures = [\"MedInc\", \"AveOccup\"]\\nfeatures_idx = [feature_names.index(feature) for feature in features]\\nX = X_full[:, features_idx]\\ndistributions = [\\n    (\"Unscaled data\", X),\\n    (\"Data after standard scaling\", StandardScaler().fit_transform(X)),\\n    (\"Data after min-max scaling\", MinMaxScaler().fit_transform(X)),\\n    (\"Data after max-abs scaling\", MaxAbsScaler().fit_transform(X)),\\n    (\\n        \"Data after robust scaling\",\\n        RobustScaler(quantile_range=(25, 75)).fit_transform(X),\\n    ),\\n    (\\n        \"Data after power transformation (Yeo-Johnson)\",\\n        PowerTransformer(method=\"yeo-johnson\").fit_transform(X),\\n    ),\\n    (\\n        \"Data after power transformation (Box-Cox)\",\\n        PowerTransformer(method=\"box-cox\").fit_transform(X),\\n    ),\\n    (\\n        \"Data after quantile transformation (uniform pdf)\",\\n        QuantileTransformer(\\n            output_distribution=\"uniform\", random_state=42\\n        ).fit_transform(X),\\n    ),\\n    (\\n        \"Data after quantile transformation (gaussian pdf)\",\\n        QuantileTransformer(\\n            output_distribution=\"normal\", random_state=42\\n        ).fit_transform(X),\\n    ),\\n    (\"Data after sample-wise L2 normalizing\", Normalizer().fit_transform(X)),\\n]\\n\\n# scale the output between 0 and 1 for the colorbar\\ny = minmax_scale(y_full)\\n\\n# plasma does not exist in matplotlib < 1.5\\ncmap = getattr(cm, \"plasma_r\", cm.hot_r)\\n\\n\\n# Code for: def create_axes(title, figsize=(16, 6)):\\n\\n\\n# Code for: def plot_distribution(axes, X, y, hist_nbins=50, title=\"\", x0_label=\"\", x1_label=\"\"):\\n\\n\\n# %%\\n# Two plots will be shown for each scaler/normalizer/transformer. The left\\n# figure will show a scatter plot of the full data set while the right figure\\n# will exclude the extreme values considering only 99 % of the data set,\\n# excluding marginal outliers. In addition, the marginal distributions for each\\n# feature will be shown on the sides of the scatter plot.\\n\\n\\n# Code for: def make_plot(item_idx):\\n\\n\\n# %%\\n# .. _results:\\n#\\n# Original data\\n# -------------\\n#\\n# Each transformation is plotted showing two transformed features, with the\\n# left plot showing the entire dataset, and the right zoomed-in to show the\\n# dataset without the marginal outliers. A large majority of the samples are\\n# compacted to a specific range, [0, 10] for the median income and [0, 6] for\\n# the average house occupancy. Note that there are some marginal outliers (some\\n# blocks have average occupancy of more than 1200). Therefore, a specific\\n# pre-processing can be very beneficial depending of the application. In the\\n# following, we present some insights and behaviors of those pre-processing\\n# methods in the presence of marginal outliers.\\n\\nmake_plot(0)'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_all_scaling.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='make_plot(0)\\n\\n# %%\\n# .. _plot_all_scaling_standard_scaler_section:\\n#\\n# StandardScaler\\n# --------------\\n#\\n# :class:`~sklearn.preprocessing.StandardScaler` removes the mean and scales\\n# the data to unit variance. The scaling shrinks the range of the feature\\n# values as shown in the left figure below.\\n# However, the outliers have an influence when computing the empirical mean and\\n# standard deviation. Note in particular that because the outliers on each\\n# feature have different magnitudes, the spread of the transformed data on\\n# each feature is very different: most of the data lie in the [-2, 4] range for\\n# the transformed median income feature while the same data is squeezed in the\\n# smaller [-0.2, 0.2] range for the transformed average house occupancy.\\n#\\n# :class:`~sklearn.preprocessing.StandardScaler` therefore cannot guarantee\\n# balanced feature scales in the\\n# presence of outliers.\\n\\nmake_plot(1)\\n\\n# %%\\n# .. _plot_all_scaling_minmax_scaler_section:\\n#\\n# MinMaxScaler\\n# ------------\\n#\\n# :class:`~sklearn.preprocessing.MinMaxScaler` rescales the data set such that\\n# all feature values are in\\n# the range [0, 1] as shown in the right panel below. However, this scaling\\n# compresses all inliers into the narrow range [0, 0.005] for the transformed\\n# average house occupancy.\\n#\\n# Both :class:`~sklearn.preprocessing.StandardScaler` and\\n# :class:`~sklearn.preprocessing.MinMaxScaler` are very sensitive to the\\n# presence of outliers.\\n\\nmake_plot(2)\\n\\n# %%\\n# .. _plot_all_scaling_max_abs_scaler_section:\\n#\\n# MaxAbsScaler\\n# ------------\\n#\\n# :class:`~sklearn.preprocessing.MaxAbsScaler` is similar to\\n# :class:`~sklearn.preprocessing.MinMaxScaler` except that the\\n# values are mapped across several ranges depending on whether negative\\n# OR positive values are present. If only positive values are present, the\\n# range is [0, 1]. If only negative values are present, the range is [-1, 0].\\n# If both negative and positive values are present, the range is [-1, 1].\\n# On positive only data, both :class:`~sklearn.preprocessing.MinMaxScaler`\\n# and :class:`~sklearn.preprocessing.MaxAbsScaler` behave similarly.\\n# :class:`~sklearn.preprocessing.MaxAbsScaler` therefore also suffers from\\n# the presence of large outliers.\\n\\nmake_plot(3)'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_all_scaling.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"make_plot(3)\\n\\n# %%\\n# .. _plot_all_scaling_robust_scaler_section:\\n#\\n# RobustScaler\\n# ------------\\n#\\n# Unlike the previous scalers, the centering and scaling statistics of\\n# :class:`~sklearn.preprocessing.RobustScaler`\\n# are based on percentiles and are therefore not influenced by a small\\n# number of very large marginal outliers. Consequently, the resulting range of\\n# the transformed feature values is larger than for the previous scalers and,\\n# more importantly, are approximately similar: for both features most of the\\n# transformed values lie in a [-2, 3] range as seen in the zoomed-in figure.\\n# Note that the outliers themselves are still present in the transformed data.\\n# If a separate outlier clipping is desirable, a non-linear transformation is\\n# required (see below).\\n\\nmake_plot(4)\\n\\n# %%\\n# .. _plot_all_scaling_power_transformer_section:\\n#\\n# PowerTransformer\\n# ----------------\\n#\\n# :class:`~sklearn.preprocessing.PowerTransformer` applies a power\\n# transformation to each feature to make the data more Gaussian-like in order\\n# to stabilize variance and minimize skewness. Currently the Yeo-Johnson\\n# and Box-Cox transforms are supported and the optimal\\n# scaling factor is determined via maximum likelihood estimation in both\\n# methods. By default, :class:`~sklearn.preprocessing.PowerTransformer` applies\\n# zero-mean, unit variance normalization. Note that\\n# Box-Cox can only be applied to strictly positive data. Income and average\\n# house occupancy happen to be strictly positive, but if negative values are\\n# present the Yeo-Johnson transformed is preferred.\\n\\nmake_plot(5)\\nmake_plot(6)\\n\\n# %%\\n# .. _plot_all_scaling_quantile_transformer_section:\\n#\\n# QuantileTransformer (uniform output)\\n# ------------------------------------\\n#\\n# :class:`~sklearn.preprocessing.QuantileTransformer` applies a non-linear\\n# transformation such that the\\n# probability density function of each feature will be mapped to a uniform\\n# or Gaussian distribution. In this case, all the data, including outliers,\\n# will be mapped to a uniform distribution with the range [0, 1], making\\n# outliers indistinguishable from inliers.\\n#\\n# :class:`~sklearn.preprocessing.RobustScaler` and\\n# :class:`~sklearn.preprocessing.QuantileTransformer` are robust to outliers in\\n# the sense that adding or removing outliers in the training set will yield\\n# approximately the same transformation. But contrary to\\n# :class:`~sklearn.preprocessing.RobustScaler`,\\n# :class:`~sklearn.preprocessing.QuantileTransformer` will also automatically\\n# collapse any outlier by setting them to the a priori defined range boundaries\\n# (0 and 1). This can result in saturation artifacts for extreme values.\\n\\nmake_plot(7)\\n\\n##############################################################################\\n# QuantileTransformer (Gaussian output)\\n# -------------------------------------\\n#\\n# To map to a Gaussian distribution, set the parameter\\n# ``output_distribution='normal'``.\\n\\nmake_plot(8)\"), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_all_scaling.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content=\"make_plot(7)\\n\\n##############################################################################\\n# QuantileTransformer (Gaussian output)\\n# -------------------------------------\\n#\\n# To map to a Gaussian distribution, set the parameter\\n# ``output_distribution='normal'``.\\n\\nmake_plot(8)\\n\\n# %%\\n# .. _plot_all_scaling_normalizer_section:\\n#\\n# Normalizer\\n# ----------\\n#\\n# The :class:`~sklearn.preprocessing.Normalizer` rescales the vector for each\\n# sample to have unit norm,\\n# independently of the distribution of the samples. It can be seen on both\\n# figures below where all samples are mapped onto the unit circle. In our\\n# example the two selected features have only positive values; therefore the\\n# transformed data only lie in the positive quadrant. This would not be the\\n# case if some original features had a mix of positive and negative values.\\n\\nmake_plot(9)\\n\\nplt.show()\"), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_target_encoder.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def evaluate_model_and_store(name, pipe):\\n    result = cross_validate(\\n        pipe,\\n        X,\\n        y,\\n        scoring=\"neg_root_mean_squared_error\",\\n        cv=n_cv_folds,\\n        return_train_score=True,\\n    )\\n    rmse_test_score = -result[\"test_score\"]\\n    rmse_train_score = -result[\"train_score\"]\\n    results.append(\\n        {\\n            \"preprocessor\": name,\\n            \"rmse_test_mean\": rmse_test_score.mean(),\\n            \"rmse_test_std\": rmse_train_score.std(),\\n            \"rmse_train_mean\": rmse_train_score.mean(),\\n            \"rmse_train_std\": rmse_train_score.std(),\\n        }\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_target_encoder.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================================\\nComparing Target Encoder with Other Encoders\\n============================================\\n\\n.. currentmodule:: sklearn.preprocessing\\n\\nThe :class:`TargetEncoder` uses the value of the target to encode each\\ncategorical feature. In this example, we will compare three different approaches\\nfor handling categorical features: :class:`TargetEncoder`,\\n:class:`OrdinalEncoder`, :class:`OneHotEncoder` and dropping the category.\\n\\n.. note::\\n    `fit(X, y).transform(X)` does not equal `fit_transform(X, y)` because a\\n    cross fitting scheme is used in `fit_transform` for encoding. See the\\n    :ref:`User Guide <target_encoder>`. for details.\\n\"\"\"\\n\\n# %%\\n# Loading Data from OpenML\\n# ========================\\n# First, we load the wine reviews dataset, where the target is the points given\\n# be a reviewer:\\nfrom sklearn.datasets import fetch_openml\\n\\nwine_reviews = fetch_openml(data_id=42074, as_frame=True)\\n\\ndf = wine_reviews.frame\\ndf.head()\\n\\n# %%\\n# For this example, we use the following subset of numerical and categorical\\n# features in the data. The target are continuous values from 80 to 100:\\nnumerical_features = [\"price\"]\\ncategorical_features = [\\n    \"country\",\\n    \"province\",\\n    \"region_1\",\\n    \"region_2\",\\n    \"variety\",\\n    \"winery\",\\n]\\ntarget_name = \"points\"\\n\\nX = df[numerical_features + categorical_features]\\ny = df[target_name]\\n\\n_ = y.hist()\\n\\n# %%\\n# Training and Evaluating Pipelines with Different Encoders\\n# =========================================================\\n# In this section, we will evaluate pipelines with\\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor` with different encoding\\n# strategies. First, we list out the encoders we will be using to preprocess\\n# the categorical features:\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, TargetEncoder\\n\\ncategorical_preprocessors = [\\n    (\"drop\", \"drop\"),\\n    (\"ordinal\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\\n    (\\n        \"one_hot\",\\n        OneHotEncoder(handle_unknown=\"ignore\", max_categories=20, sparse_output=False),\\n    ),\\n    (\"target\", TargetEncoder(target_type=\"continuous\")),\\n]\\n\\n# %%\\n# Next, we evaluate the models using cross validation and record the results:\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\nfrom sklearn.model_selection import cross_validate\\nfrom sklearn.pipeline import make_pipeline\\n\\nn_cv_folds = 3\\nmax_iter = 20\\nresults = []\\n\\n\\n# Code for: def evaluate_model_and_store(name, pipe):\\n\\n\\nfor name, categorical_preprocessor in categorical_preprocessors:\\n    preprocessor = ColumnTransformer(\\n        [\\n            (\"numerical\", \"passthrough\", numerical_features),\\n            (\"categorical\", categorical_preprocessor, categorical_features),\\n        ]\\n    )\\n    pipe = make_pipeline(\\n        preprocessor, HistGradientBoostingRegressor(random_state=0, max_iter=max_iter)\\n    )\\n    evaluate_model_and_store(name, pipe)'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_target_encoder.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Native Categorical Feature Support\\n# ==================================\\n# In this section, we build and evaluate a pipeline that uses native categorical\\n# feature support in :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,\\n# which only supports up to 255 unique categories. In our dataset, the most of\\n# the categorical features have more than 255 unique categories:\\nn_unique_categories = df[categorical_features].nunique().sort_values(ascending=False)\\nn_unique_categories\\n\\n# %%\\n# To workaround the limitation above, we group the categorical features into\\n# low cardinality and high cardinality features. The high cardinality features\\n# will be target encoded and the low cardinality features will use the native\\n# categorical feature in gradient boosting.\\nhigh_cardinality_features = n_unique_categories[n_unique_categories > 255].index\\nlow_cardinality_features = n_unique_categories[n_unique_categories <= 255].index\\nmixed_encoded_preprocessor = ColumnTransformer(\\n    [\\n        (\"numerical\", \"passthrough\", numerical_features),\\n        (\\n            \"high_cardinality\",\\n            TargetEncoder(target_type=\"continuous\"),\\n            high_cardinality_features,\\n        ),\\n        (\\n            \"low_cardinality\",\\n            OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\\n            low_cardinality_features,\\n        ),\\n    ],\\n    verbose_feature_names_out=False,\\n)\\n\\n# The output of the of the preprocessor must be set to pandas so the\\n# gradient boosting model can detect the low cardinality features.\\nmixed_encoded_preprocessor.set_output(transform=\"pandas\")\\nmixed_pipe = make_pipeline(\\n    mixed_encoded_preprocessor,\\n    HistGradientBoostingRegressor(\\n        random_state=0, max_iter=max_iter, categorical_features=low_cardinality_features\\n    ),\\n)\\nmixed_pipe\\n\\n# %%\\n# Finally, we evaluate the pipeline using cross validation and record the results:\\nevaluate_model_and_store(\"mixed_target\", mixed_pipe)\\n\\n# %%\\n# Plotting the Results\\n# ====================\\n# In this section, we display the results by plotting the test and train scores:\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\nresults_df = (\\n    pd.DataFrame(results).set_index(\"preprocessor\").sort_values(\"rmse_test_mean\")\\n)\\n\\nfig, (ax1, ax2) = plt.subplots(\\n    1, 2, figsize=(12, 8), sharey=True, constrained_layout=True\\n)\\nxticks = range(len(results_df))\\nname_to_color = dict(\\n    zip((r[\"preprocessor\"] for r in results), [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\"])\\n)\\n\\nfor subset, ax in zip([\"test\", \"train\"], [ax1, ax2]):\\n    mean, std = f\"rmse_{subset}_mean\", f\"rmse_{subset}_std\"\\n    data = results_df[[mean, std]].sort_values(mean)\\n    ax.bar(\\n        x=xticks,\\n        height=data[mean],\\n        yerr=data[std],\\n        width=0.9,\\n        color=[name_to_color[name] for name in data.index],\\n    )\\n    ax.set(\\n        title=f\"RMSE ({subset.title()})\",\\n        xlabel=\"Encoding Scheme\",\\n        xticks=xticks,\\n        xticklabels=data.index,\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_target_encoder.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# When evaluating the predictive performance on the test set, dropping the\\n# categories perform the worst and the target encoders performs the best. This\\n# can be explained as follows:\\n#\\n# - Dropping the categorical features makes the pipeline less expressive and\\n#   underfitting as a result;\\n# - Due to the high cardinality and to reduce the training time, the one-hot\\n#   encoding scheme uses `max_categories=20` which prevents the features from\\n#   expanding too much, which can result in underfitting.\\n# - If we had not set `max_categories=20`, the one-hot encoding scheme would have\\n#   likely made the pipeline overfitting as the number of features explodes with rare\\n#   category occurrences that are correlated with the target by chance (on the training\\n#   set only);\\n# - The ordinal encoding imposes an arbitrary order to the features which are then\\n#   treated as numerical values by the\\n#   :class:`~sklearn.ensemble.HistGradientBoostingRegressor`. Since this\\n#   model groups numerical features in 256 bins per feature, many unrelated categories\\n#   can be grouped together and as a result overall pipeline can underfit;\\n# - When using the target encoder, the same binning happens, but since the encoded\\n#   values are statistically ordered by marginal association with the target variable,\\n#   the binning use by the :class:`~sklearn.ensemble.HistGradientBoostingRegressor`\\n#   makes sense and leads to good results: the combination of smoothed target\\n#   encoding and binning works as a good regularizing strategy against\\n#   overfitting while not limiting the expressiveness of the pipeline too much.'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_discretization.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================================================\\nUsing KBinsDiscretizer to discretize continuous features\\n================================================================\\n\\nThe example compares prediction result of linear regression (linear model)\\nand decision tree (tree based model) with and without discretization of\\nreal-valued features.\\n\\nAs is shown in the result before discretization, linear model is fast to\\nbuild and relatively straightforward to interpret, but can only model\\nlinear relationships, while decision tree can build a much more complex model\\nof the data. One way to make linear model more powerful on continuous data\\nis to use discretization (also known as binning). In the example, we\\ndiscretize the feature and one-hot encode the transformed data. Note that if\\nthe bins are not reasonably wide, there would appear to be a substantially\\nincreased risk of overfitting, so the discretizer parameters should usually\\nbe tuned under cross validation.\\n\\nAfter discretization, linear regression and decision tree make exactly the\\nsame prediction. As features are constant within each bin, any model must\\npredict the same value for all points within a bin. Compared with the result\\nbefore discretization, linear model become much more flexible while decision\\ntree gets much less flexible. Note that binning features generally has no\\nbeneficial effect for tree-based models, as these models can learn to split\\nup the data anywhere.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.preprocessing import KBinsDiscretizer\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\n# construct the dataset\\nrnd = np.random.RandomState(42)\\nX = rnd.uniform(-3, 3, size=100)\\ny = np.sin(X) + rnd.normal(size=len(X)) / 3\\nX = X.reshape(-1, 1)\\n\\n# transform the dataset with KBinsDiscretizer\\nenc = KBinsDiscretizer(n_bins=10, encode=\"onehot\")\\nX_binned = enc.fit_transform(X)\\n\\n# predict with original dataset\\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))\\nline = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\\nreg = LinearRegression().fit(X, y)\\nax1.plot(line, reg.predict(line), linewidth=2, color=\"green\", label=\"linear regression\")\\nreg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X, y)\\nax1.plot(line, reg.predict(line), linewidth=2, color=\"red\", label=\"decision tree\")\\nax1.plot(X[:, 0], y, \"o\", c=\"k\")\\nax1.legend(loc=\"best\")\\nax1.set_ylabel(\"Regression output\")\\nax1.set_xlabel(\"Input feature\")\\nax1.set_title(\"Result before discretization\")'), Document(metadata={'source': '/content/local_copy_repo/examples/preprocessing/plot_discretization.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# predict with transformed dataset\\nline_binned = enc.transform(line)\\nreg = LinearRegression().fit(X_binned, y)\\nax2.plot(\\n    line,\\n    reg.predict(line_binned),\\n    linewidth=2,\\n    color=\"green\",\\n    linestyle=\"-\",\\n    label=\"linear regression\",\\n)\\nreg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X_binned, y)\\nax2.plot(\\n    line,\\n    reg.predict(line_binned),\\n    linewidth=2,\\n    color=\"red\",\\n    linestyle=\":\",\\n    label=\"decision tree\",\\n)\\nax2.plot(X[:, 0], y, \"o\", c=\"k\")\\nax2.vlines(enc.bin_edges_[0], *plt.gca().get_ylim(), linewidth=1, alpha=0.2)\\nax2.legend(loc=\"best\")\\nax2.set_xlabel(\"Input feature\")\\nax2.set_title(\"Result after discretization\")\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_swissroll.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================\\nSwiss Roll And Swiss-Hole Reduction\\n===================================\\nThis notebook seeks to compare two popular non-linear dimensionality\\ntechniques, T-distributed Stochastic Neighbor Embedding (t-SNE) and\\nLocally Linear Embedding (LLE), on the classic Swiss Roll dataset.\\nThen, we will explore how they both deal with the addition of a hole\\nin the data.\\n\"\"\"\\n\\n# %%\\n# Swiss Roll\\n# ---------------------------------------------------\\n#\\n# We start by generating the Swiss Roll dataset.\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import datasets, manifold\\n\\nsr_points, sr_color = datasets.make_swiss_roll(n_samples=1500, random_state=0)\\n\\n# %%\\n# Now, let\\'s take a look at our data:\\n\\nfig = plt.figure(figsize=(8, 6))\\nax = fig.add_subplot(111, projection=\"3d\")\\nfig.add_axes(ax)\\nax.scatter(\\n    sr_points[:, 0], sr_points[:, 1], sr_points[:, 2], c=sr_color, s=50, alpha=0.8\\n)\\nax.set_title(\"Swiss Roll in Ambient Space\")\\nax.view_init(azim=-66, elev=12)\\n_ = ax.text2D(0.8, 0.05, s=\"n_samples=1500\", transform=ax.transAxes)\\n\\n# %%\\n# Computing the LLE and t-SNE embeddings, we find that LLE seems to unroll the\\n# Swiss Roll pretty effectively. t-SNE on the other hand, is able\\n# to preserve the general structure of the data, but, poorly represents the\\n# continuous nature of our original data. Instead, it seems to unnecessarily\\n# clump sections of points together.\\n\\nsr_lle, sr_err = manifold.locally_linear_embedding(\\n    sr_points, n_neighbors=12, n_components=2\\n)\\n\\nsr_tsne = manifold.TSNE(n_components=2, perplexity=40, random_state=0).fit_transform(\\n    sr_points\\n)\\n\\nfig, axs = plt.subplots(figsize=(8, 8), nrows=2)\\naxs[0].scatter(sr_lle[:, 0], sr_lle[:, 1], c=sr_color)\\naxs[0].set_title(\"LLE Embedding of Swiss Roll\")\\naxs[1].scatter(sr_tsne[:, 0], sr_tsne[:, 1], c=sr_color)\\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss Roll\")\\n\\n# %%\\n# .. note::\\n#\\n#     LLE seems to be stretching the points from the center (purple)\\n#     of the swiss roll. However, we observe that this is simply a byproduct\\n#     of how the data was generated. There is a higher density of points near the\\n#     center of the roll, which ultimately affects how LLE reconstructs the\\n#     data in a lower dimension.\\n\\n# %%\\n# Swiss-Hole\\n# ---------------------------------------------------\\n#\\n# Now let\\'s take a look at how both algorithms deal with us adding a hole to\\n# the data. First, we generate the Swiss-Hole dataset and plot it:\\n\\nsh_points, sh_color = datasets.make_swiss_roll(\\n    n_samples=1500, hole=True, random_state=0\\n)\\n\\nfig = plt.figure(figsize=(8, 6))\\nax = fig.add_subplot(111, projection=\"3d\")\\nfig.add_axes(ax)\\nax.scatter(\\n    sh_points[:, 0], sh_points[:, 1], sh_points[:, 2], c=sh_color, s=50, alpha=0.8\\n)\\nax.set_title(\"Swiss-Hole in Ambient Space\")\\nax.view_init(azim=-66, elev=12)\\n_ = ax.text2D(0.8, 0.05, s=\"n_samples=1500\", transform=ax.transAxes)'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_swissroll.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Computing the LLE and t-SNE embeddings, we obtain similar results to the\\n# Swiss Roll. LLE very capably unrolls the data and even preserves\\n# the hole. t-SNE, again seems to clump sections of points together, but, we\\n# note that it preserves the general topology of the original data.\\n\\n\\nsh_lle, sh_err = manifold.locally_linear_embedding(\\n    sh_points, n_neighbors=12, n_components=2\\n)\\n\\nsh_tsne = manifold.TSNE(\\n    n_components=2, perplexity=40, init=\"random\", random_state=0\\n).fit_transform(sh_points)\\n\\nfig, axs = plt.subplots(figsize=(8, 8), nrows=2)\\naxs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)\\naxs[0].set_title(\"LLE Embedding of Swiss-Hole\")\\naxs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)\\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss-Hole\")\\n\\n# %%\\n#\\n# Concluding remarks\\n# ------------------\\n#\\n# We note that t-SNE benefits from testing more combinations of parameters.\\n# Better results could probably have been obtained by better tuning these\\n# parameters.\\n#\\n# We observe that, as seen in the \"Manifold learning on\\n# handwritten digits\" example, t-SNE generally performs better than LLE\\n# on real world data.'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_t_sne_perplexity.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================================================\\nt-SNE: The effect of various perplexity values on the shape\\n=============================================================================\\n\\nAn illustration of t-SNE on the two concentric circles and the S-curve\\ndatasets for different perplexity values.\\n\\nWe observe a tendency towards clearer shapes as the perplexity value increases.\\n\\nThe size, the distance and the shape of clusters may vary upon initialization,\\nperplexity values and does not always convey a meaning.\\n\\nAs shown below, t-SNE for higher perplexities finds meaningful topology of\\ntwo concentric circles, however the size and the distance of the circles varies\\nslightly from the original. Contrary to the two circles dataset, the shapes\\nvisually diverge from S-curve topology on the S-curve dataset even for\\nlarger perplexity values.\\n\\nFor further details, \"How to Use t-SNE Effectively\"\\nhttps://distill.pub/2016/misread-tsne/ provides a good discussion of the\\neffects of various parameters, as well as interactive plots to explore\\nthose effects.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom time import time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib.ticker import NullFormatter\\n\\nfrom sklearn import datasets, manifold\\n\\nn_samples = 150\\nn_components = 2\\n(fig, subplots) = plt.subplots(3, 5, figsize=(15, 8))\\nperplexities = [5, 30, 50, 100]\\n\\nX, y = datasets.make_circles(\\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=0\\n)\\n\\nred = y == 0\\ngreen = y == 1\\n\\nax = subplots[0][0]\\nax.scatter(X[red, 0], X[red, 1], c=\"r\")\\nax.scatter(X[green, 0], X[green, 1], c=\"g\")\\nax.xaxis.set_major_formatter(NullFormatter())\\nax.yaxis.set_major_formatter(NullFormatter())\\nplt.axis(\"tight\")\\n\\nfor i, perplexity in enumerate(perplexities):\\n    ax = subplots[0][i + 1]\\n\\n    t0 = time()\\n    tsne = manifold.TSNE(\\n        n_components=n_components,\\n        init=\"random\",\\n        random_state=0,\\n        perplexity=perplexity,\\n        max_iter=300,\\n    )\\n    Y = tsne.fit_transform(X)\\n    t1 = time()\\n    print(\"circles, perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))\\n    ax.set_title(\"Perplexity=%d\" % perplexity)\\n    ax.scatter(Y[red, 0], Y[red, 1], c=\"r\")\\n    ax.scatter(Y[green, 0], Y[green, 1], c=\"g\")\\n    ax.xaxis.set_major_formatter(NullFormatter())\\n    ax.yaxis.set_major_formatter(NullFormatter())\\n    ax.axis(\"tight\")\\n\\n# Another example using s-curve\\nX, color = datasets.make_s_curve(n_samples, random_state=0)\\n\\nax = subplots[1][0]\\nax.scatter(X[:, 0], X[:, 2], c=color)\\nax.xaxis.set_major_formatter(NullFormatter())\\nax.yaxis.set_major_formatter(NullFormatter())\\n\\nfor i, perplexity in enumerate(perplexities):\\n    ax = subplots[1][i + 1]'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_t_sne_perplexity.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='ax = subplots[1][0]\\nax.scatter(X[:, 0], X[:, 2], c=color)\\nax.xaxis.set_major_formatter(NullFormatter())\\nax.yaxis.set_major_formatter(NullFormatter())\\n\\nfor i, perplexity in enumerate(perplexities):\\n    ax = subplots[1][i + 1]\\n\\n    t0 = time()\\n    tsne = manifold.TSNE(\\n        n_components=n_components,\\n        init=\"random\",\\n        random_state=0,\\n        perplexity=perplexity,\\n        learning_rate=\"auto\",\\n        max_iter=300,\\n    )\\n    Y = tsne.fit_transform(X)\\n    t1 = time()\\n    print(\"S-curve, perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))\\n\\n    ax.set_title(\"Perplexity=%d\" % perplexity)\\n    ax.scatter(Y[:, 0], Y[:, 1], c=color)\\n    ax.xaxis.set_major_formatter(NullFormatter())\\n    ax.yaxis.set_major_formatter(NullFormatter())\\n    ax.axis(\"tight\")\\n\\n\\n# Another example using a 2D uniform grid\\nx = np.linspace(0, 1, int(np.sqrt(n_samples)))\\nxx, yy = np.meshgrid(x, x)\\nX = np.hstack(\\n    [\\n        xx.ravel().reshape(-1, 1),\\n        yy.ravel().reshape(-1, 1),\\n    ]\\n)\\ncolor = xx.ravel()\\nax = subplots[2][0]\\nax.scatter(X[:, 0], X[:, 1], c=color)\\nax.xaxis.set_major_formatter(NullFormatter())\\nax.yaxis.set_major_formatter(NullFormatter())\\n\\nfor i, perplexity in enumerate(perplexities):\\n    ax = subplots[2][i + 1]\\n\\n    t0 = time()\\n    tsne = manifold.TSNE(\\n        n_components=n_components,\\n        init=\"random\",\\n        random_state=0,\\n        perplexity=perplexity,\\n        max_iter=400,\\n    )\\n    Y = tsne.fit_transform(X)\\n    t1 = time()\\n    print(\"uniform grid, perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))\\n\\n    ax.set_title(\"Perplexity=%d\" % perplexity)\\n    ax.scatter(Y[:, 0], Y[:, 1], c=color)\\n    ax.xaxis.set_major_formatter(NullFormatter())\\n    ax.yaxis.set_major_formatter(NullFormatter())\\n    ax.axis(\"tight\")\\n\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_manifold_sphere.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================\\nManifold Learning methods on a severed sphere\\n=============================================\\n\\nAn application of the different :ref:`manifold` techniques\\non a spherical data-set. Here one can see the use of\\ndimensionality reduction in order to gain some intuition\\nregarding the manifold learning methods. Regarding the dataset,\\nthe poles are cut from the sphere, as well as a thin slice down its\\nside. This enables the manifold learning techniques to\\n\\'spread it open\\' whilst projecting it onto two dimensions.\\n\\nFor a similar example, where the methods are applied to the\\nS-curve dataset, see :ref:`sphx_glr_auto_examples_manifold_plot_compare_methods.py`\\n\\nNote that the purpose of the :ref:`MDS <multidimensional_scaling>` is\\nto find a low-dimensional representation of the data (here 2D) in\\nwhich the distances respect well the distances in the original\\nhigh-dimensional space, unlike other manifold-learning algorithms,\\nit does not seeks an isotropic representation of the data in\\nthe low-dimensional space. Here the manifold problem matches fairly\\nthat of representing a flat map of the Earth, as with\\n`map projection <https://en.wikipedia.org/wiki/Map_projection>`_\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom time import time\\n\\nimport matplotlib.pyplot as plt\\n\\n# Unused but required import for doing 3d projections with matplotlib < 3.2\\nimport mpl_toolkits.mplot3d  # noqa: F401\\nimport numpy as np\\nfrom matplotlib.ticker import NullFormatter\\n\\nfrom sklearn import manifold\\nfrom sklearn.utils import check_random_state\\n\\n# Variables for manifold learning.\\nn_neighbors = 10\\nn_samples = 1000\\n\\n# Create our sphere.\\nrandom_state = check_random_state(0)\\np = random_state.rand(n_samples) * (2 * np.pi - 0.55)\\nt = random_state.rand(n_samples) * np.pi\\n\\n# Sever the poles from the sphere.\\nindices = (t < (np.pi - (np.pi / 8))) & (t > ((np.pi / 8)))\\ncolors = p[indices]\\nx, y, z = (\\n    np.sin(t[indices]) * np.cos(p[indices]),\\n    np.sin(t[indices]) * np.sin(p[indices]),\\n    np.cos(t[indices]),\\n)\\n\\n# Plot our dataset.\\nfig = plt.figure(figsize=(15, 8))\\nplt.suptitle(\\n    \"Manifold Learning with %i points, %i neighbors\" % (1000, n_neighbors), fontsize=14\\n)\\n\\nax = fig.add_subplot(251, projection=\"3d\")\\nax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)\\nax.view_init(40, -10)\\n\\nsphere_data = np.array([x, y, z]).T\\n\\n# Perform Locally Linear Embedding Manifold learning\\nmethods = [\"standard\", \"ltsa\", \"hessian\", \"modified\"]\\nlabels = [\"LLE\", \"LTSA\", \"Hessian LLE\", \"Modified LLE\"]\\n\\nfor i, method in enumerate(methods):\\n    t0 = time()\\n    trans_data = (\\n        manifold.LocallyLinearEmbedding(\\n            n_neighbors=n_neighbors, n_components=2, method=method, random_state=42\\n        )\\n        .fit_transform(sphere_data)\\n        .T\\n    )\\n    t1 = time()\\n    print(\"%s: %.2g sec\" % (methods[i], t1 - t0))'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_manifold_sphere.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='ax = fig.add_subplot(252 + i)\\n    plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\\n    plt.title(\"%s (%.2g sec)\" % (labels[i], t1 - t0))\\n    ax.xaxis.set_major_formatter(NullFormatter())\\n    ax.yaxis.set_major_formatter(NullFormatter())\\n    plt.axis(\"tight\")\\n\\n# Perform Isomap Manifold learning.\\nt0 = time()\\ntrans_data = (\\n    manifold.Isomap(n_neighbors=n_neighbors, n_components=2)\\n    .fit_transform(sphere_data)\\n    .T\\n)\\nt1 = time()\\nprint(\"%s: %.2g sec\" % (\"ISO\", t1 - t0))\\n\\nax = fig.add_subplot(257)\\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\\nplt.title(\"%s (%.2g sec)\" % (\"Isomap\", t1 - t0))\\nax.xaxis.set_major_formatter(NullFormatter())\\nax.yaxis.set_major_formatter(NullFormatter())\\nplt.axis(\"tight\")\\n\\n# Perform Multi-dimensional scaling.\\nt0 = time()\\nmds = manifold.MDS(2, max_iter=100, n_init=1, random_state=42)\\ntrans_data = mds.fit_transform(sphere_data).T\\nt1 = time()\\nprint(\"MDS: %.2g sec\" % (t1 - t0))\\n\\nax = fig.add_subplot(258)\\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\\nplt.title(\"MDS (%.2g sec)\" % (t1 - t0))\\nax.xaxis.set_major_formatter(NullFormatter())\\nax.yaxis.set_major_formatter(NullFormatter())\\nplt.axis(\"tight\")\\n\\n# Perform Spectral Embedding.\\nt0 = time()\\nse = manifold.SpectralEmbedding(\\n    n_components=2, n_neighbors=n_neighbors, random_state=42\\n)\\ntrans_data = se.fit_transform(sphere_data).T\\nt1 = time()\\nprint(\"Spectral Embedding: %.2g sec\" % (t1 - t0))\\n\\nax = fig.add_subplot(259)\\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\\nplt.title(\"Spectral Embedding (%.2g sec)\" % (t1 - t0))\\nax.xaxis.set_major_formatter(NullFormatter())\\nax.yaxis.set_major_formatter(NullFormatter())\\nplt.axis(\"tight\")\\n\\n# Perform t-distributed stochastic neighbor embedding.\\nt0 = time()\\ntsne = manifold.TSNE(n_components=2, random_state=0)\\ntrans_data = tsne.fit_transform(sphere_data).T\\nt1 = time()\\nprint(\"t-SNE: %.2g sec\" % (t1 - t0))\\n\\nax = fig.add_subplot(2, 5, 10)\\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\\nplt.title(\"t-SNE (%.2g sec)\" % (t1 - t0))\\nax.xaxis.set_major_formatter(NullFormatter())\\nax.yaxis.set_major_formatter(NullFormatter())\\nplt.axis(\"tight\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_compare_methods.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_3d(points, points_color, title):\\n    x, y, z = points.T\\n\\n    fig, ax = plt.subplots(\\n        figsize=(6, 6),\\n        facecolor=\"white\",\\n        tight_layout=True,\\n        subplot_kw={\"projection\": \"3d\"},\\n    )\\n    fig.suptitle(title, size=16)\\n    col = ax.scatter(x, y, z, c=points_color, s=50, alpha=0.8)\\n    ax.view_init(azim=-60, elev=9)\\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\\n    ax.zaxis.set_major_locator(ticker.MultipleLocator(1))\\n\\n    fig.colorbar(col, ax=ax, orientation=\"horizontal\", shrink=0.6, aspect=60, pad=0.01)\\n    plt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_compare_methods.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_2d(points, points_color, title):\\n    fig, ax = plt.subplots(figsize=(3, 3), facecolor=\"white\", constrained_layout=True)\\n    fig.suptitle(title, size=16)\\n    add_2d_scatter(ax, points, points_color)\\n    plt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_compare_methods.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def add_2d_scatter(ax, points, points_color, title=None):\\n    x, y = points.T\\n    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)\\n    ax.set_title(title)\\n    ax.xaxis.set_major_formatter(ticker.NullFormatter())\\n    ax.yaxis.set_major_formatter(ticker.NullFormatter())'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_compare_methods.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================\\nComparison of Manifold Learning methods\\n=========================================\\n\\nAn illustration of dimensionality reduction on the S-curve dataset\\nwith various manifold learning methods.\\n\\nFor a discussion and comparison of these algorithms, see the\\n:ref:`manifold module page <manifold>`\\n\\nFor a similar example, where the methods are applied to a\\nsphere dataset, see :ref:`sphx_glr_auto_examples_manifold_plot_manifold_sphere.py`\\n\\nNote that the purpose of the MDS is to find a low-dimensional\\nrepresentation of the data (here 2D) in which the distances respect well\\nthe distances in the original high-dimensional space, unlike other\\nmanifold-learning algorithms, it does not seeks an isotropic\\nrepresentation of the data in the low-dimensional space.\\n\\n\"\"\"\\n\\n# Author: Jake Vanderplas -- <vanderplas@astro.washington.edu>\\n\\n# %%\\n# Dataset preparation\\n# -------------------\\n#\\n# We start by generating the S-curve dataset.\\n\\nimport matplotlib.pyplot as plt\\n\\n# unused but required import for doing 3d projections with matplotlib < 3.2\\nimport mpl_toolkits.mplot3d  # noqa: F401\\nfrom matplotlib import ticker\\n\\nfrom sklearn import datasets, manifold\\n\\nn_samples = 1500\\nS_points, S_color = datasets.make_s_curve(n_samples, random_state=0)\\n\\n# %%\\n# Let\\'s look at the original data. Also define some helping\\n# functions, which we will use further on.\\n\\n\\n# Code for: def plot_3d(points, points_color, title):\\n\\n\\n# Code for: def plot_2d(points, points_color, title):\\n\\n\\n# Code for: def add_2d_scatter(ax, points, points_color, title=None):\\n\\n\\nplot_3d(S_points, S_color, \"Original S-curve samples\")\\n\\n# %%\\n# Define algorithms for the manifold learning\\n# -------------------------------------------\\n#\\n# Manifold learning is an approach to non-linear dimensionality reduction.\\n# Algorithms for this task are based on the idea that the dimensionality of\\n# many data sets is only artificially high.\\n#\\n# Read more in the :ref:`User Guide <manifold>`.\\n\\nn_neighbors = 12  # neighborhood which is used to recover the locally linear structure\\nn_components = 2  # number of coordinates for the manifold\\n\\n# %%\\n# Locally Linear Embeddings\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# Locally linear embedding (LLE) can be thought of as a series of local\\n# Principal Component Analyses which are globally compared to find the\\n# best non-linear embedding.\\n# Read more in the :ref:`User Guide <locally_linear_embedding>`.\\n\\nparams = {\\n    \"n_neighbors\": n_neighbors,\\n    \"n_components\": n_components,\\n    \"eigen_solver\": \"auto\",\\n    \"random_state\": 0,\\n}\\n\\nlle_standard = manifold.LocallyLinearEmbedding(method=\"standard\", **params)\\nS_standard = lle_standard.fit_transform(S_points)\\n\\nlle_ltsa = manifold.LocallyLinearEmbedding(method=\"ltsa\", **params)\\nS_ltsa = lle_ltsa.fit_transform(S_points)\\n\\nlle_hessian = manifold.LocallyLinearEmbedding(method=\"hessian\", **params)\\nS_hessian = lle_hessian.fit_transform(S_points)'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_compare_methods.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='lle_ltsa = manifold.LocallyLinearEmbedding(method=\"ltsa\", **params)\\nS_ltsa = lle_ltsa.fit_transform(S_points)\\n\\nlle_hessian = manifold.LocallyLinearEmbedding(method=\"hessian\", **params)\\nS_hessian = lle_hessian.fit_transform(S_points)\\n\\nlle_mod = manifold.LocallyLinearEmbedding(method=\"modified\", **params)\\nS_mod = lle_mod.fit_transform(S_points)\\n\\n# %%\\nfig, axs = plt.subplots(\\n    nrows=2, ncols=2, figsize=(7, 7), facecolor=\"white\", constrained_layout=True\\n)\\nfig.suptitle(\"Locally Linear Embeddings\", size=16)\\n\\nlle_methods = [\\n    (\"Standard locally linear embedding\", S_standard),\\n    (\"Local tangent space alignment\", S_ltsa),\\n    (\"Hessian eigenmap\", S_hessian),\\n    (\"Modified locally linear embedding\", S_mod),\\n]\\nfor ax, method in zip(axs.flat, lle_methods):\\n    name, points = method\\n    add_2d_scatter(ax, points, S_color, name)\\n\\nplt.show()\\n\\n# %%\\n# Isomap Embedding\\n# ^^^^^^^^^^^^^^^^\\n#\\n# Non-linear dimensionality reduction through Isometric Mapping.\\n# Isomap seeks a lower-dimensional embedding which maintains geodesic\\n# distances between all points. Read more in the :ref:`User Guide <isomap>`.\\n\\nisomap = manifold.Isomap(n_neighbors=n_neighbors, n_components=n_components, p=1)\\nS_isomap = isomap.fit_transform(S_points)\\n\\nplot_2d(S_isomap, S_color, \"Isomap Embedding\")\\n\\n# %%\\n# Multidimensional scaling\\n# ^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# Multidimensional scaling (MDS) seeks a low-dimensional representation\\n# of the data in which the distances respect well the distances in the\\n# original high-dimensional space.\\n# Read more in the :ref:`User Guide <multidimensional_scaling>`.\\n\\nmd_scaling = manifold.MDS(\\n    n_components=n_components,\\n    max_iter=50,\\n    n_init=4,\\n    random_state=0,\\n    normalized_stress=False,\\n)\\nS_scaling = md_scaling.fit_transform(S_points)\\n\\nplot_2d(S_scaling, S_color, \"Multidimensional scaling\")\\n\\n# %%\\n# Spectral embedding for non-linear dimensionality reduction\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# This implementation uses Laplacian Eigenmaps, which finds a low dimensional\\n# representation of the data using a spectral decomposition of the graph Laplacian.\\n# Read more in the :ref:`User Guide <spectral_embedding>`.\\n\\nspectral = manifold.SpectralEmbedding(\\n    n_components=n_components, n_neighbors=n_neighbors, random_state=42\\n)\\nS_spectral = spectral.fit_transform(S_points)\\n\\nplot_2d(S_spectral, S_color, \"Spectral Embedding\")\\n\\n# %%\\n# T-distributed Stochastic Neighbor Embedding\\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n#\\n# It converts similarities between data points to joint probabilities and\\n# tries to minimize the Kullback-Leibler divergence between the joint probabilities\\n# of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost\\n# function that is not convex, i.e. with different initializations we can get\\n# different results. Read more in the :ref:`User Guide <t_sne>`.'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_compare_methods.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='t_sne = manifold.TSNE(\\n    n_components=n_components,\\n    perplexity=30,\\n    init=\"random\",\\n    max_iter=250,\\n    random_state=0,\\n)\\nS_t_sne = t_sne.fit_transform(S_points)\\n\\nplot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\\\n Neighbor Embedding\")\\n\\n# %%'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_lle_digits.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_embedding(X, title):\\n    _, ax = plt.subplots()\\n    X = MinMaxScaler().fit_transform(X)\\n\\n    for digit in digits.target_names:\\n        ax.scatter(\\n            *X[y == digit].T,\\n            marker=f\"${digit}$\",\\n            s=60,\\n            color=plt.cm.Dark2(digit),\\n            alpha=0.425,\\n            zorder=2,\\n        )\\n    shown_images = np.array([[1.0, 1.0]])  # just something big\\n    for i in range(X.shape[0]):\\n        # plot every digit on the embedding\\n        # show an annotation box for a group of digits\\n        dist = np.sum((X[i] - shown_images) ** 2, 1)\\n        if np.min(dist) < 4e-3:\\n            # don\\'t show points that are too close\\n            continue\\n        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\\n        imagebox = offsetbox.AnnotationBbox(\\n            offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), X[i]\\n        )\\n        imagebox.set(zorder=1)\\n        ax.add_artist(imagebox)\\n\\n    ax.set_title(title)\\n    ax.axis(\"off\")'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_lle_digits.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================================================\\nManifold learning on handwritten digits: Locally Linear Embedding, Isomap...\\n=============================================================================\\n\\nWe illustrate various embedding techniques on the digits dataset.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n\\n# %%\\n# Load digits dataset\\n# -------------------\\n# We will load the digits dataset and only use six first of the ten available classes.\\nfrom sklearn.datasets import load_digits\\n\\ndigits = load_digits(n_class=6)\\nX, y = digits.data, digits.target\\nn_samples, n_features = X.shape\\nn_neighbors = 30\\n\\n# %%\\n# We can plot the first hundred digits from this data set.\\nimport matplotlib.pyplot as plt\\n\\nfig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\\nfor idx, ax in enumerate(axs.ravel()):\\n    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\\n    ax.axis(\"off\")\\n_ = fig.suptitle(\"A selection from the 64-dimensional digits dataset\", fontsize=16)\\n\\n# %%\\n# Helper function to plot embedding\\n# ---------------------------------\\n# Below, we will use different techniques to embed the digits dataset. We will plot\\n# the projection of the original data onto each embedding. It will allow us to\\n# check whether or digits are grouped together in the embedding space, or\\n# scattered across it.\\nimport numpy as np\\nfrom matplotlib import offsetbox\\n\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n\\n# Code for: def plot_embedding(X, title):\\n\\n\\n# %%\\n# Embedding techniques comparison\\n# -------------------------------\\n#\\n# Below, we compare different techniques. However, there are a couple of things\\n# to note:\\n#\\n# * the :class:`~sklearn.ensemble.RandomTreesEmbedding` is not\\n#   technically a manifold embedding method, as it learn a high-dimensional\\n#   representation on which we apply a dimensionality reduction method.\\n#   However, it is often useful to cast a dataset into a representation in\\n#   which the classes are linearly-separable.\\n# * the :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis` and\\n#   the :class:`~sklearn.neighbors.NeighborhoodComponentsAnalysis`, are supervised\\n#   dimensionality reduction method, i.e. they make use of the provided labels,\\n#   contrary to other methods.\\n# * the :class:`~sklearn.manifold.TSNE` is initialized with the embedding that is\\n#   generated by PCA in this example. It ensures global stability  of the embedding,\\n#   i.e., the embedding does not depend on random initialization.\\nfrom sklearn.decomposition import TruncatedSVD\\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\nfrom sklearn.ensemble import RandomTreesEmbedding\\nfrom sklearn.manifold import (\\n    MDS,\\n    TSNE,\\n    Isomap,\\n    LocallyLinearEmbedding,\\n    SpectralEmbedding,\\n)\\nfrom sklearn.neighbors import NeighborhoodComponentsAnalysis\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.random_projection import SparseRandomProjection'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_lle_digits.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='embeddings = {\\n    \"Random projection embedding\": SparseRandomProjection(\\n        n_components=2, random_state=42\\n    ),\\n    \"Truncated SVD embedding\": TruncatedSVD(n_components=2),\\n    \"Linear Discriminant Analysis embedding\": LinearDiscriminantAnalysis(\\n        n_components=2\\n    ),\\n    \"Isomap embedding\": Isomap(n_neighbors=n_neighbors, n_components=2),\\n    \"Standard LLE embedding\": LocallyLinearEmbedding(\\n        n_neighbors=n_neighbors, n_components=2, method=\"standard\"\\n    ),\\n    \"Modified LLE embedding\": LocallyLinearEmbedding(\\n        n_neighbors=n_neighbors, n_components=2, method=\"modified\"\\n    ),\\n    \"Hessian LLE embedding\": LocallyLinearEmbedding(\\n        n_neighbors=n_neighbors, n_components=2, method=\"hessian\"\\n    ),\\n    \"LTSA LLE embedding\": LocallyLinearEmbedding(\\n        n_neighbors=n_neighbors, n_components=2, method=\"ltsa\"\\n    ),\\n    \"MDS embedding\": MDS(n_components=2, n_init=1, max_iter=120, n_jobs=2),\\n    \"Random Trees embedding\": make_pipeline(\\n        RandomTreesEmbedding(n_estimators=200, max_depth=5, random_state=0),\\n        TruncatedSVD(n_components=2),\\n    ),\\n    \"Spectral embedding\": SpectralEmbedding(\\n        n_components=2, random_state=0, eigen_solver=\"arpack\"\\n    ),\\n    \"t-SNE embedding\": TSNE(\\n        n_components=2,\\n        max_iter=500,\\n        n_iter_without_progress=150,\\n        n_jobs=2,\\n        random_state=0,\\n    ),\\n    \"NCA embedding\": NeighborhoodComponentsAnalysis(\\n        n_components=2, init=\"pca\", random_state=0\\n    ),\\n}\\n\\n# %%\\n# Once we declared all the methods of interest, we can run and perform the projection\\n# of the original data. We will store the projected data as well as the computational\\n# time needed to perform each projection.\\nfrom time import time\\n\\nprojections, timing = {}, {}\\nfor name, transformer in embeddings.items():\\n    if name.startswith(\"Linear Discriminant Analysis\"):\\n        data = X.copy()\\n        data.flat[:: X.shape[1] + 1] += 0.01  # Make X invertible\\n    else:\\n        data = X\\n\\n    print(f\"Computing {name}...\")\\n    start_time = time()\\n    projections[name] = transformer.fit_transform(data, y)\\n    timing[name] = time() - start_time\\n\\n# %%\\n# Finally, we can plot the resulting projection given by each method.\\nfor name in timing:\\n    title = f\"{name} (time {timing[name]:.3f}s)\"\\n    plot_embedding(projections[name], title)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/manifold/plot_mds.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================\\nMulti-dimensional scaling\\n=========================\\n\\nAn illustration of the metric and non-metric MDS on generated noisy data.\\n\\nThe reconstructed points using the metric MDS and non metric MDS are slightly\\nshifted to avoid overlapping.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nfrom matplotlib.collections import LineCollection\\n\\nfrom sklearn import manifold\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.metrics import euclidean_distances\\n\\nEPSILON = np.finfo(np.float32).eps\\nn_samples = 20\\nseed = np.random.RandomState(seed=3)\\nX_true = seed.randint(0, 20, 2 * n_samples).astype(float)\\nX_true = X_true.reshape((n_samples, 2))\\n# Center the data\\nX_true -= X_true.mean()\\n\\nsimilarities = euclidean_distances(X_true)\\n\\n# Add noise to the similarities\\nnoise = np.random.rand(n_samples, n_samples)\\nnoise = noise + noise.T\\nnoise[np.arange(noise.shape[0]), np.arange(noise.shape[0])] = 0\\nsimilarities += noise\\n\\nmds = manifold.MDS(\\n    n_components=2,\\n    max_iter=3000,\\n    eps=1e-9,\\n    random_state=seed,\\n    dissimilarity=\"precomputed\",\\n    n_jobs=1,\\n)\\npos = mds.fit(similarities).embedding_\\n\\nnmds = manifold.MDS(\\n    n_components=2,\\n    metric=False,\\n    max_iter=3000,\\n    eps=1e-12,\\n    dissimilarity=\"precomputed\",\\n    random_state=seed,\\n    n_jobs=1,\\n    n_init=1,\\n)\\nnpos = nmds.fit_transform(similarities, init=pos)\\n\\n# Rescale the data\\npos *= np.sqrt((X_true**2).sum()) / np.sqrt((pos**2).sum())\\nnpos *= np.sqrt((X_true**2).sum()) / np.sqrt((npos**2).sum())\\n\\n# Rotate the data\\nclf = PCA(n_components=2)\\nX_true = clf.fit_transform(X_true)\\n\\npos = clf.fit_transform(pos)\\n\\nnpos = clf.fit_transform(npos)\\n\\nfig = plt.figure(1)\\nax = plt.axes([0.0, 0.0, 1.0, 1.0])\\n\\ns = 100\\nplt.scatter(X_true[:, 0], X_true[:, 1], color=\"navy\", s=s, lw=0, label=\"True Position\")\\nplt.scatter(pos[:, 0], pos[:, 1], color=\"turquoise\", s=s, lw=0, label=\"MDS\")\\nplt.scatter(npos[:, 0], npos[:, 1], color=\"darkorange\", s=s, lw=0, label=\"NMDS\")\\nplt.legend(scatterpoints=1, loc=\"best\", shadow=False)\\n\\nsimilarities = similarities.max() / (similarities + EPSILON) * 100\\nnp.fill_diagonal(similarities, 0)\\n# Plot the edges\\nstart_idx, end_idx = np.where(pos)\\n# a sequence of (*line0*, *line1*, *line2*), where::\\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\\nsegments = [\\n    [X_true[i, :], X_true[j, :]] for i in range(len(pos)) for j in range(len(pos))\\n]\\nvalues = np.abs(similarities)\\nlc = LineCollection(\\n    segments, zorder=0, cmap=plt.cm.Blues, norm=plt.Normalize(0, values.max())\\n)\\nlc.set_array(similarities.flatten())\\nlc.set_linewidths(np.full(len(segments), 0.5))\\nax.add_collection(lc)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_concentration_prior.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_ellipses(ax, weights, means, covars):\\n    for n in range(means.shape[0]):\\n        eig_vals, eig_vecs = np.linalg.eigh(covars[n])\\n        unit_eig_vec = eig_vecs[0] / np.linalg.norm(eig_vecs[0])\\n        angle = np.arctan2(unit_eig_vec[1], unit_eig_vec[0])\\n        # Ellipse needs degrees\\n        angle = 180 * angle / np.pi\\n        # eigenvector normalization\\n        eig_vals = 2 * np.sqrt(2) * np.sqrt(eig_vals)\\n        ell = mpl.patches.Ellipse(\\n            means[n], eig_vals[0], eig_vals[1], angle=180 + angle, edgecolor=\"black\"\\n        )\\n        ell.set_clip_box(ax.bbox)\\n        ell.set_alpha(weights[n])\\n        ell.set_facecolor(\"#56B4E9\")\\n        ax.add_artist(ell)'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_concentration_prior.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_results(ax1, ax2, estimator, X, y, title, plot_title=False):\\n    ax1.set_title(title)\\n    ax1.scatter(X[:, 0], X[:, 1], s=5, marker=\"o\", color=colors[y], alpha=0.8)\\n    ax1.set_xlim(-2.0, 2.0)\\n    ax1.set_ylim(-3.0, 3.0)\\n    ax1.set_xticks(())\\n    ax1.set_yticks(())\\n    plot_ellipses(ax1, estimator.weights_, estimator.means_, estimator.covariances_)\\n\\n    ax2.get_xaxis().set_tick_params(direction=\"out\")\\n    ax2.yaxis.grid(True, alpha=0.7)\\n    for k, w in enumerate(estimator.weights_):\\n        ax2.bar(\\n            k,\\n            w,\\n            width=0.9,\\n            color=\"#56B4E9\",\\n            zorder=3,\\n            align=\"center\",\\n            edgecolor=\"black\",\\n        )\\n        ax2.text(k, w + 0.007, \"%.1f%%\" % (w * 100.0), horizontalalignment=\"center\")\\n    ax2.set_xlim(-0.6, 2 * n_components - 0.4)\\n    ax2.set_ylim(0.0, 1.1)\\n    ax2.tick_params(axis=\"y\", which=\"both\", left=False, right=False, labelleft=False)\\n    ax2.tick_params(axis=\"x\", which=\"both\", top=False)\\n\\n    if plot_title:\\n        ax1.set_ylabel(\"Estimated Mixtures\")\\n        ax2.set_ylabel(\"Weight of each component\")'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_concentration_prior.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n========================================================================\\nConcentration Prior Type Analysis of Variation Bayesian Gaussian Mixture\\n========================================================================\\n\\nThis example plots the ellipsoids obtained from a toy dataset (mixture of three\\nGaussians) fitted by the ``BayesianGaussianMixture`` class models with a\\nDirichlet distribution prior\\n(``weight_concentration_prior_type=\\'dirichlet_distribution\\'``) and a Dirichlet\\nprocess prior (``weight_concentration_prior_type=\\'dirichlet_process\\'``). On\\neach figure, we plot the results for three different values of the weight\\nconcentration prior.\\n\\nThe ``BayesianGaussianMixture`` class can adapt its number of mixture\\ncomponents automatically. The parameter ``weight_concentration_prior`` has a\\ndirect link with the resulting number of components with non-zero weights.\\nSpecifying a low value for the concentration prior will make the model put most\\nof the weight on few components set the remaining components weights very close\\nto zero. High values of the concentration prior will allow a larger number of\\ncomponents to be active in the mixture.\\n\\nThe Dirichlet process prior allows to define an infinite number of components\\nand automatically selects the correct number of components: it activates a\\ncomponent only if it is necessary.\\n\\nOn the contrary the classical finite mixture model with a Dirichlet\\ndistribution prior will favor more uniformly weighted components and therefore\\ntends to divide natural clusters into unnecessary sub-components.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib as mpl\\nimport matplotlib.gridspec as gridspec\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.mixture import BayesianGaussianMixture\\n\\n\\n# Code for: def plot_ellipses(ax, weights, means, covars):\\n\\n\\n# Code for: def plot_results(ax1, ax2, estimator, X, y, title, plot_title=False):\\n\\n\\n# Parameters of the dataset\\nrandom_state, n_components, n_features = 2, 3, 2\\ncolors = np.array([\"#0072B2\", \"#F0E442\", \"#D55E00\"])\\n\\ncovars = np.array(\\n    [[[0.7, 0.0], [0.0, 0.1]], [[0.5, 0.0], [0.0, 0.1]], [[0.5, 0.0], [0.0, 0.1]]]\\n)\\nsamples = np.array([200, 500, 200])\\nmeans = np.array([[0.0, -0.70], [0.0, 0.0], [0.0, 0.70]])'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_concentration_prior.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='covars = np.array(\\n    [[[0.7, 0.0], [0.0, 0.1]], [[0.5, 0.0], [0.0, 0.1]], [[0.5, 0.0], [0.0, 0.1]]]\\n)\\nsamples = np.array([200, 500, 200])\\nmeans = np.array([[0.0, -0.70], [0.0, 0.0], [0.0, 0.70]])\\n\\n# mean_precision_prior= 0.8 to minimize the influence of the prior\\nestimators = [\\n    (\\n        \"Finite mixture with a Dirichlet distribution\\\\nprior and \" r\"$\\\\gamma_0=$\",\\n        BayesianGaussianMixture(\\n            weight_concentration_prior_type=\"dirichlet_distribution\",\\n            n_components=2 * n_components,\\n            reg_covar=0,\\n            init_params=\"random\",\\n            max_iter=1500,\\n            mean_precision_prior=0.8,\\n            random_state=random_state,\\n        ),\\n        [0.001, 1, 1000],\\n    ),\\n    (\\n        \"Infinite mixture with a Dirichlet process\\\\n prior and\" r\"$\\\\gamma_0=$\",\\n        BayesianGaussianMixture(\\n            weight_concentration_prior_type=\"dirichlet_process\",\\n            n_components=2 * n_components,\\n            reg_covar=0,\\n            init_params=\"random\",\\n            max_iter=1500,\\n            mean_precision_prior=0.8,\\n            random_state=random_state,\\n        ),\\n        [1, 1000, 100000],\\n    ),\\n]\\n\\n# Generate data\\nrng = np.random.RandomState(random_state)\\nX = np.vstack(\\n    [\\n        rng.multivariate_normal(means[j], covars[j], samples[j])\\n        for j in range(n_components)\\n    ]\\n)\\ny = np.concatenate([np.full(samples[j], j, dtype=int) for j in range(n_components)])\\n\\n# Plot results in two different figures\\nfor title, estimator, concentrations_prior in estimators:\\n    plt.figure(figsize=(4.7 * 3, 8))\\n    plt.subplots_adjust(\\n        bottom=0.04, top=0.90, hspace=0.05, wspace=0.05, left=0.03, right=0.99\\n    )\\n\\n    gs = gridspec.GridSpec(3, len(concentrations_prior))\\n    for k, concentration in enumerate(concentrations_prior):\\n        estimator.weight_concentration_prior = concentration\\n        estimator.fit(X)\\n        plot_results(\\n            plt.subplot(gs[0:2, k]),\\n            plt.subplot(gs[2, k]),\\n            estimator,\\n            X,\\n            y,\\n            r\"%s$%.1e$\" % (title, concentration),\\n            plot_title=k == 0,\\n        )\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_pdf.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================\\nDensity Estimation for a Gaussian mixture\\n=========================================\\n\\nPlot the density estimation of a mixture of two Gaussians. Data is\\ngenerated from two Gaussians with different centers and covariance\\nmatrices.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib.colors import LogNorm\\n\\nfrom sklearn import mixture\\n\\nn_samples = 300\\n\\n# generate random sample, two components\\nnp.random.seed(0)\\n\\n# generate spherical data centered on (20, 20)\\nshifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])\\n\\n# generate zero centered stretched Gaussian data\\nC = np.array([[0.0, -0.7], [3.5, 0.7]])\\nstretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)\\n\\n# concatenate the two datasets into the final training set\\nX_train = np.vstack([shifted_gaussian, stretched_gaussian])\\n\\n# fit a Gaussian Mixture Model with two components\\nclf = mixture.GaussianMixture(n_components=2, covariance_type=\"full\")\\nclf.fit(X_train)\\n\\n# display predicted scores by the model as a contour plot\\nx = np.linspace(-20.0, 30.0)\\ny = np.linspace(-20.0, 40.0)\\nX, Y = np.meshgrid(x, y)\\nXX = np.array([X.ravel(), Y.ravel()]).T\\nZ = -clf.score_samples(XX)\\nZ = Z.reshape(X.shape)\\n\\nCS = plt.contour(\\n    X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0), levels=np.logspace(0, 3, 10)\\n)\\nCB = plt.colorbar(CS, shrink=0.8, extend=\"both\")\\nplt.scatter(X_train[:, 0], X_train[:, 1], 0.8)\\n\\nplt.title(\"Negative log-likelihood predicted by a GMM\")\\nplt.axis(\"tight\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_selection.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def gmm_bic_score(estimator, X):\\n    \"\"\"Callable to pass to GridSearchCV that will use the BIC score.\"\"\"\\n    # Make it negative since GridSearchCV expects a score to maximize\\n    return -estimator.bic(X)'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================\\nGaussian Mixture Model Selection\\n================================\\n\\nThis example shows that model selection can be performed with Gaussian Mixture\\nModels (GMM) using :ref:`information-theory criteria <aic_bic>`. Model selection\\nconcerns both the covariance type and the number of components in the model.\\n\\nIn this case, both the Akaike Information Criterion (AIC) and the Bayes\\nInformation Criterion (BIC) provide the right result, but we only demo the\\nlatter as BIC is better suited to identify the true model among a set of\\ncandidates. Unlike Bayesian procedures, such inferences are prior-free.\\n\\n\"\"\"\\n\\n# %%\\n# Data generation\\n# ---------------\\n#\\n# We generate two components (each one containing `n_samples`) by randomly\\n# sampling the standard normal distribution as returned by `numpy.random.randn`.\\n# One component is kept spherical yet shifted and re-scaled. The other one is\\n# deformed to have a more general covariance matrix.\\n\\nimport numpy as np\\n\\nn_samples = 500\\nnp.random.seed(0)\\nC = np.array([[0.0, -0.1], [1.7, 0.4]])\\ncomponent_1 = np.dot(np.random.randn(n_samples, 2), C)  # general\\ncomponent_2 = 0.7 * np.random.randn(n_samples, 2) + np.array([-4, 1])  # spherical\\n\\nX = np.concatenate([component_1, component_2])\\n\\n# %%\\n# We can visualize the different components:\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.scatter(component_1[:, 0], component_1[:, 1], s=0.8)\\nplt.scatter(component_2[:, 0], component_2[:, 1], s=0.8)\\nplt.title(\"Gaussian Mixture components\")\\nplt.axis(\"equal\")\\nplt.show()\\n\\n# %%\\n# Model training and selection\\n# ----------------------------\\n#\\n# We vary the number of components from 1 to 6 and the type of covariance\\n# parameters to use:\\n#\\n# - `\"full\"`: each component has its own general covariance matrix.\\n# - `\"tied\"`: all components share the same general covariance matrix.\\n# - `\"diag\"`: each component has its own diagonal covariance matrix.\\n# - `\"spherical\"`: each component has its own single variance.\\n#\\n# We score the different models and keep the best model (the lowest BIC). This\\n# is done by using :class:`~sklearn.model_selection.GridSearchCV` and a\\n# user-defined score function which returns the negative BIC score, as\\n# :class:`~sklearn.model_selection.GridSearchCV` is designed to **maximize** a\\n# score (maximizing the negative BIC is equivalent to minimizing the BIC).\\n#\\n# The best set of parameters and estimator are stored in `best_parameters_` and\\n# `best_estimator_`, respectively.\\n\\nfrom sklearn.mixture import GaussianMixture\\nfrom sklearn.model_selection import GridSearchCV\\n\\n\\n# Code for: def gmm_bic_score(estimator, X):\\n\\n\\nparam_grid = {\\n    \"n_components\": range(1, 7),\\n    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\\n}\\ngrid_search = GridSearchCV(\\n    GaussianMixture(), param_grid=param_grid, scoring=gmm_bic_score\\n)\\ngrid_search.fit(X)'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_selection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def gmm_bic_score(estimator, X):\\n\\n\\nparam_grid = {\\n    \"n_components\": range(1, 7),\\n    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\\n}\\ngrid_search = GridSearchCV(\\n    GaussianMixture(), param_grid=param_grid, scoring=gmm_bic_score\\n)\\ngrid_search.fit(X)\\n\\n# %%\\n# Plot the BIC scores\\n# -------------------\\n#\\n# To ease the plotting we can create a `pandas.DataFrame` from the results of\\n# the cross-validation done by the grid search. We re-inverse the sign of the\\n# BIC score to show the effect of minimizing it.\\n\\nimport pandas as pd\\n\\ndf = pd.DataFrame(grid_search.cv_results_)[\\n    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\\n]\\ndf[\"mean_test_score\"] = -df[\"mean_test_score\"]\\ndf = df.rename(\\n    columns={\\n        \"param_n_components\": \"Number of components\",\\n        \"param_covariance_type\": \"Type of covariance\",\\n        \"mean_test_score\": \"BIC score\",\\n    }\\n)\\ndf.sort_values(by=\"BIC score\").head()\\n\\n# %%\\nimport seaborn as sns\\n\\nsns.catplot(\\n    data=df,\\n    kind=\"bar\",\\n    x=\"Number of components\",\\n    y=\"BIC score\",\\n    hue=\"Type of covariance\",\\n)\\nplt.show()\\n\\n# %%\\n# In the present case, the model with 2 components and full covariance (which\\n# corresponds to the true generative model) has the lowest BIC score and is\\n# therefore selected by the grid search.\\n#\\n# Plot the best model\\n# -------------------\\n#\\n# We plot an ellipse to show each Gaussian component of the selected model. For\\n# such purpose, one needs to find the eigenvalues of the covariance matrices as\\n# returned by the `covariances_` attribute. The shape of such matrices depends\\n# on the `covariance_type`:\\n#\\n# - `\"full\"`: (`n_components`, `n_features`, `n_features`)\\n# - `\"tied\"`: (`n_features`, `n_features`)\\n# - `\"diag\"`: (`n_components`, `n_features`)\\n# - `\"spherical\"`: (`n_components`,)\\n\\nfrom matplotlib.patches import Ellipse\\nfrom scipy import linalg\\n\\ncolor_iter = sns.color_palette(\"tab10\", 2)[::-1]\\nY_ = grid_search.predict(X)\\n\\nfig, ax = plt.subplots()\\n\\nfor i, (mean, cov, color) in enumerate(\\n    zip(\\n        grid_search.best_estimator_.means_,\\n        grid_search.best_estimator_.covariances_,\\n        color_iter,\\n    )\\n):\\n    v, w = linalg.eigh(cov)\\n    if not np.any(Y_ == i):\\n        continue\\n    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], 0.8, color=color)\\n\\n    angle = np.arctan2(w[0][1], w[0][0])\\n    angle = 180.0 * angle / np.pi  # convert to degrees\\n    v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\\n    ellipse = Ellipse(mean, v[0], v[1], angle=180.0 + angle, color=color)\\n    ellipse.set_clip_box(fig.bbox)\\n    ellipse.set_alpha(0.5)\\n    ax.add_artist(ellipse)\\n\\nplt.title(\\n    f\"Selected GMM: {grid_search.best_params_[\\'covariance_type\\']} model, \"\\n    f\"{grid_search.best_params_[\\'n_components\\']} components\"\\n)\\nplt.axis(\"equal\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_init.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_initial_means(X, init_params, r):\\n    # Run a GaussianMixture with max_iter=0 to output the initialization means\\n    gmm = GaussianMixture(\\n        n_components=4, init_params=init_params, tol=1e-9, max_iter=0, random_state=r\\n    ).fit(X)\\n    return gmm.means_'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_init.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================\\nGMM Initialization Methods\\n==========================\\n\\nExamples of the different methods of initialization in Gaussian Mixture Models\\n\\nSee :ref:`gmm` for more information on the estimator.\\n\\nHere we generate some sample data with four easy to identify clusters. The\\npurpose of this example is to show the four different methods for the\\ninitialization parameter *init_param*.\\n\\nThe four initializations are *kmeans* (default), *random*, *random_from_data* and\\n*k-means++*.\\n\\nOrange diamonds represent the initialization centers for the gmm generated by\\nthe *init_param*. The rest of the data is represented as crosses and the\\ncolouring represents the eventual associated classification after the GMM has\\nfinished.\\n\\nThe numbers in the top right of each subplot represent the number of\\niterations taken for the GaussianMixture to converge and the relative time\\ntaken for the initialization part of the algorithm to run. The shorter\\ninitialization times tend to have a greater number of iterations to converge.\\n\\nThe initialization time is the ratio of the time taken for that method versus\\nthe time taken for the default *kmeans* method. As you can see all three\\nalternative methods take less time to initialize when compared to *kmeans*.\\n\\nIn this example, when initialized with *random_from_data* or *random* the model takes\\nmore iterations to converge. Here *k-means++* does a good job of both low\\ntime to initialize and low number of GaussianMixture iterations to converge.\\n\"\"\"\\n\\n# Author: Gordon Walsh <gordon.p.walsh@gmail.com>\\n# Data generation code from Jake Vanderplas <vanderplas@astro.washington.edu>\\n\\nfrom timeit import default_timer as timer\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets._samples_generator import make_blobs\\nfrom sklearn.mixture import GaussianMixture\\nfrom sklearn.utils.extmath import row_norms\\n\\nprint(__doc__)\\n\\n# Generate some data\\n\\nX, y_true = make_blobs(n_samples=4000, centers=4, cluster_std=0.60, random_state=0)\\nX = X[:, ::-1]\\n\\nn_samples = 4000\\nn_components = 4\\nx_squared_norms = row_norms(X, squared=True)\\n\\n\\n# Code for: def get_initial_means(X, init_params, r):\\n\\n\\nmethods = [\"kmeans\", \"random_from_data\", \"k-means++\", \"random\"]\\ncolors = [\"navy\", \"turquoise\", \"cornflowerblue\", \"darkorange\"]\\ntimes_init = {}\\nrelative_times = {}\\n\\nplt.figure(figsize=(4 * len(methods) // 2, 6))\\nplt.subplots_adjust(\\n    bottom=0.1, top=0.9, hspace=0.15, wspace=0.05, left=0.05, right=0.95\\n)\\n\\nfor n, method in enumerate(methods):\\n    r = np.random.RandomState(seed=1234)\\n    plt.subplot(2, len(methods) // 2, n + 1)\\n\\n    start = timer()\\n    ini = get_initial_means(X, method, r)\\n    end = timer()\\n    init_time = end - start\\n\\n    gmm = GaussianMixture(\\n        n_components=4, means_init=ini, tol=1e-9, max_iter=2000, random_state=r\\n    ).fit(X)\\n\\n    times_init[method] = init_time\\n    for i, color in enumerate(colors):\\n        data = X[gmm.predict(X) == i]\\n        plt.scatter(data[:, 0], data[:, 1], color=color, marker=\"x\")'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_init.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='times_init[method] = init_time\\n    for i, color in enumerate(colors):\\n        data = X[gmm.predict(X) == i]\\n        plt.scatter(data[:, 0], data[:, 1], color=color, marker=\"x\")\\n\\n    plt.scatter(\\n        ini[:, 0], ini[:, 1], s=75, marker=\"D\", c=\"orange\", lw=1.5, edgecolors=\"black\"\\n    )\\n    relative_times[method] = times_init[method] / times_init[methods[0]]\\n\\n    plt.xticks(())\\n    plt.yticks(())\\n    plt.title(method, loc=\"left\", fontsize=12)\\n    plt.title(\\n        \"Iter %i | Init Time %.2fx\" % (gmm.n_iter_, relative_times[method]),\\n        loc=\"right\",\\n        fontsize=10,\\n    )\\nplt.suptitle(\"GMM iterations and relative time taken to initialize\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_covariances.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def make_ellipses(gmm, ax):\\n    for n, color in enumerate(colors):\\n        if gmm.covariance_type == \"full\":\\n            covariances = gmm.covariances_[n][:2, :2]\\n        elif gmm.covariance_type == \"tied\":\\n            covariances = gmm.covariances_[:2, :2]\\n        elif gmm.covariance_type == \"diag\":\\n            covariances = np.diag(gmm.covariances_[n][:2])\\n        elif gmm.covariance_type == \"spherical\":\\n            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\\n        v, w = np.linalg.eigh(covariances)\\n        u = w[0] / np.linalg.norm(w[0])\\n        angle = np.arctan2(u[1], u[0])\\n        angle = 180 * angle / np.pi  # convert to degrees\\n        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\\n        ell = mpl.patches.Ellipse(\\n            gmm.means_[n, :2], v[0], v[1], angle=180 + angle, color=color\\n        )\\n        ell.set_clip_box(ax.bbox)\\n        ell.set_alpha(0.5)\\n        ax.add_artist(ell)\\n        ax.set_aspect(\"equal\", \"datalim\")'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_covariances.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============\\nGMM covariances\\n===============\\n\\nDemonstration of several covariances types for Gaussian mixture models.\\n\\nSee :ref:`gmm` for more information on the estimator.\\n\\nAlthough GMM are often used for clustering, we can compare the obtained\\nclusters with the actual classes from the dataset. We initialize the means\\nof the Gaussians with the means of the classes from the training set to make\\nthis comparison valid.\\n\\nWe plot predicted labels on both training and held out test data using a\\nvariety of GMM covariance types on the iris dataset.\\nWe compare GMMs with spherical, diagonal, full, and tied covariance\\nmatrices in increasing order of performance. Although one would\\nexpect full covariance to perform best in general, it is prone to\\noverfitting on small datasets and does not generalize well to held out\\ntest data.\\n\\nOn the plots, train data is shown as dots, while test data is shown as\\ncrosses. The iris dataset is four-dimensional. Only the first two\\ndimensions are shown here, and thus some points are separated in other\\ndimensions.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.mixture import GaussianMixture\\nfrom sklearn.model_selection import StratifiedKFold\\n\\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\\n\\n\\n# Code for: def make_ellipses(gmm, ax):\\n\\n\\niris = datasets.load_iris()\\n\\n# Break up the dataset into non-overlapping training (75%) and testing\\n# (25%) sets.\\nskf = StratifiedKFold(n_splits=4)\\n# Only take the first fold.\\ntrain_index, test_index = next(iter(skf.split(iris.data, iris.target)))\\n\\n\\nX_train = iris.data[train_index]\\ny_train = iris.target[train_index]\\nX_test = iris.data[test_index]\\ny_test = iris.target[test_index]\\n\\nn_classes = len(np.unique(y_train))\\n\\n# Try GMMs using different types of covariances.\\nestimators = {\\n    cov_type: GaussianMixture(\\n        n_components=n_classes, covariance_type=cov_type, max_iter=20, random_state=0\\n    )\\n    for cov_type in [\"spherical\", \"diag\", \"tied\", \"full\"]\\n}\\n\\nn_estimators = len(estimators)\\n\\nplt.figure(figsize=(3 * n_estimators // 2, 6))\\nplt.subplots_adjust(\\n    bottom=0.01, top=0.95, hspace=0.15, wspace=0.05, left=0.01, right=0.99\\n)\\n\\n\\nfor index, (name, estimator) in enumerate(estimators.items()):\\n    # Since we have class labels for the training data, we can\\n    # initialize the GMM parameters in a supervised manner.\\n    estimator.means_init = np.array(\\n        [X_train[y_train == i].mean(axis=0) for i in range(n_classes)]\\n    )\\n\\n    # Train the other parameters using the EM algorithm.\\n    estimator.fit(X_train)\\n\\n    h = plt.subplot(2, n_estimators // 2, index + 1)\\n    make_ellipses(estimator, h)'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_covariances.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Train the other parameters using the EM algorithm.\\n    estimator.fit(X_train)\\n\\n    h = plt.subplot(2, n_estimators // 2, index + 1)\\n    make_ellipses(estimator, h)\\n\\n    for n, color in enumerate(colors):\\n        data = iris.data[iris.target == n]\\n        plt.scatter(\\n            data[:, 0], data[:, 1], s=0.8, color=color, label=iris.target_names[n]\\n        )\\n    # Plot the test data with crosses\\n    for n, color in enumerate(colors):\\n        data = X_test[y_test == n]\\n        plt.scatter(data[:, 0], data[:, 1], marker=\"x\", color=color)\\n\\n    y_train_pred = estimator.predict(X_train)\\n    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\\n    plt.text(0.05, 0.9, \"Train accuracy: %.1f\" % train_accuracy, transform=h.transAxes)\\n\\n    y_test_pred = estimator.predict(X_test)\\n    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\\n    plt.text(0.05, 0.8, \"Test accuracy: %.1f\" % test_accuracy, transform=h.transAxes)\\n\\n    plt.xticks(())\\n    plt.yticks(())\\n    plt.title(name)\\n\\nplt.legend(scatterpoints=1, loc=\"lower right\", prop=dict(size=12))\\n\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content=\"def plot_results(X, Y_, means, covariances, index, title):\\n    splot = plt.subplot(2, 1, 1 + index)\\n    for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):\\n        v, w = linalg.eigh(covar)\\n        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\\n        u = w[0] / linalg.norm(w[0])\\n        # as the DP will not use every component it has access to\\n        # unless it needs it, we shouldn't plot the redundant\\n        # components.\\n        if not np.any(Y_ == i):\\n            continue\\n        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], 0.8, color=color)\\n\\n        # Plot an ellipse to show the Gaussian component\\n        angle = np.arctan(u[1] / u[0])\\n        angle = 180.0 * angle / np.pi  # convert to degrees\\n        ell = mpl.patches.Ellipse(mean, v[0], v[1], angle=180.0 + angle, color=color)\\n        ell.set_clip_box(splot.bbox)\\n        ell.set_alpha(0.5)\\n        splot.add_artist(ell)\\n\\n    plt.xlim(-9.0, 5.0)\\n    plt.ylim(-3.0, 6.0)\\n    plt.xticks(())\\n    plt.yticks(())\\n    plt.title(title)\"), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================\\nGaussian Mixture Model Ellipsoids\\n=================================\\n\\nPlot the confidence ellipsoids of a mixture of two Gaussians\\nobtained with Expectation Maximisation (``GaussianMixture`` class) and\\nVariational Inference (``BayesianGaussianMixture`` class models with\\na Dirichlet process prior).\\n\\nBoth models have access to five components with which to fit the data. Note\\nthat the Expectation Maximisation model will necessarily use all five\\ncomponents while the Variational Inference model will effectively only use as\\nmany as are needed for a good fit. Here we can see that the Expectation\\nMaximisation model splits some components arbitrarily, because it is trying to\\nfit too many components, while the Dirichlet Process model adapts it number of\\nstate automatically.\\n\\nThis example doesn\\'t show it, as we\\'re in a low-dimensional space, but\\nanother advantage of the Dirichlet process model is that it can fit\\nfull covariance matrices effectively even when there are less examples\\nper cluster than there are dimensions in the data, due to\\nregularization properties of the inference algorithm.\\n\\n\"\"\"\\n\\nimport itertools\\n\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom scipy import linalg\\n\\nfrom sklearn import mixture\\n\\ncolor_iter = itertools.cycle([\"navy\", \"c\", \"cornflowerblue\", \"gold\", \"darkorange\"])\\n\\n\\n# Code for: def plot_results(X, Y_, means, covariances, index, title):\\n\\n\\n# Number of samples per component\\nn_samples = 500\\n\\n# Generate random sample, two components\\nnp.random.seed(0)\\nC = np.array([[0.0, -0.1], [1.7, 0.4]])\\nX = np.r_[\\n    np.dot(np.random.randn(n_samples, 2), C),\\n    0.7 * np.random.randn(n_samples, 2) + np.array([-6, 3]),\\n]\\n\\n# Fit a Gaussian mixture with EM using five components\\ngmm = mixture.GaussianMixture(n_components=5, covariance_type=\"full\").fit(X)\\nplot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0, \"Gaussian Mixture\")\\n\\n# Fit a Dirichlet process Gaussian mixture using five components\\ndpgmm = mixture.BayesianGaussianMixture(n_components=5, covariance_type=\"full\").fit(X)\\nplot_results(\\n    X,\\n    dpgmm.predict(X),\\n    dpgmm.means_,\\n    dpgmm.covariances_,\\n    1,\\n    \"Bayesian Gaussian Mixture with a Dirichlet process prior\",\\n)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_sin.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content=\"def plot_results(X, Y, means, covariances, index, title):\\n    splot = plt.subplot(5, 1, 1 + index)\\n    for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):\\n        v, w = linalg.eigh(covar)\\n        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\\n        u = w[0] / linalg.norm(w[0])\\n        # as the DP will not use every component it has access to\\n        # unless it needs it, we shouldn't plot the redundant\\n        # components.\\n        if not np.any(Y == i):\\n            continue\\n        plt.scatter(X[Y == i, 0], X[Y == i, 1], 0.8, color=color)\\n\\n        # Plot an ellipse to show the Gaussian component\\n        angle = np.arctan(u[1] / u[0])\\n        angle = 180.0 * angle / np.pi  # convert to degrees\\n        ell = mpl.patches.Ellipse(mean, v[0], v[1], angle=180.0 + angle, color=color)\\n        ell.set_clip_box(splot.bbox)\\n        ell.set_alpha(0.5)\\n        splot.add_artist(ell)\\n\\n    plt.xlim(-6.0, 4.0 * np.pi - 6.0)\\n    plt.ylim(-5.0, 5.0)\\n    plt.title(title)\\n    plt.xticks(())\\n    plt.yticks(())\"), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_sin.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content=\"def plot_samples(X, Y, n_components, index, title):\\n    plt.subplot(5, 1, 4 + index)\\n    for i, color in zip(range(n_components), color_iter):\\n        # as the DP will not use every component it has access to\\n        # unless it needs it, we shouldn't plot the redundant\\n        # components.\\n        if not np.any(Y == i):\\n            continue\\n        plt.scatter(X[Y == i, 0], X[Y == i, 1], 0.8, color=color)\\n\\n    plt.xlim(-6.0, 4.0 * np.pi - 6.0)\\n    plt.ylim(-5.0, 5.0)\\n    plt.title(title)\\n    plt.xticks(())\\n    plt.yticks(())\"), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_sin.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================\\nGaussian Mixture Model Sine Curve\\n=================================\\n\\nThis example demonstrates the behavior of Gaussian mixture models fit on data\\nthat was not sampled from a mixture of Gaussian random variables. The dataset\\nis formed by 100 points loosely spaced following a noisy sine curve. There is\\ntherefore no ground truth value for the number of Gaussian components.\\n\\nThe first model is a classical Gaussian Mixture Model with 10 components fit\\nwith the Expectation-Maximization algorithm.\\n\\nThe second model is a Bayesian Gaussian Mixture Model with a Dirichlet process\\nprior fit with variational inference. The low value of the concentration prior\\nmakes the model favor a lower number of active components. This models\\n\"decides\" to focus its modeling power on the big picture of the structure of\\nthe dataset: groups of points with alternating directions modeled by\\nnon-diagonal covariance matrices. Those alternating directions roughly capture\\nthe alternating nature of the original sine signal.\\n\\nThe third model is also a Bayesian Gaussian mixture model with a Dirichlet\\nprocess prior but this time the value of the concentration prior is higher\\ngiving the model more liberty to model the fine-grained structure of the data.\\nThe result is a mixture with a larger number of active components that is\\nsimilar to the first model where we arbitrarily decided to fix the number of\\ncomponents to 10.\\n\\nWhich model is the best is a matter of subjective judgment: do we want to\\nfavor models that only capture the big picture to summarize and explain most of\\nthe structure of the data while ignoring the details or do we prefer models\\nthat closely follow the high density regions of the signal?\\n\\nThe last two panels show how we can sample from the last two models. The\\nresulting samples distributions do not look exactly like the original data\\ndistribution. The difference primarily stems from the approximation error we\\nmade by using a model that assumes that the data was generated by a finite\\nnumber of Gaussian components instead of a continuous noisy sine curve.\\n\\n\"\"\"\\n\\nimport itertools\\n\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom scipy import linalg\\n\\nfrom sklearn import mixture\\n\\ncolor_iter = itertools.cycle([\"navy\", \"c\", \"cornflowerblue\", \"gold\", \"darkorange\"])\\n\\n\\n# Code for: def plot_results(X, Y, means, covariances, index, title):\\n\\n\\n# Code for: def plot_samples(X, Y, n_components, index, title):\\n\\n\\n# Parameters\\nn_samples = 100\\n\\n# Generate random sample following a sine curve\\nnp.random.seed(0)\\nX = np.zeros((n_samples, 2))\\nstep = 4.0 * np.pi / n_samples\\n\\nfor i in range(X.shape[0]):\\n    x = i * step - 6.0\\n    X[i, 0] = x + np.random.normal(0, 0.1)\\n    X[i, 1] = 3.0 * (np.sin(x) + np.random.normal(0, 0.2))\\n\\nplt.figure(figsize=(10, 10))\\nplt.subplots_adjust(\\n    bottom=0.04, top=0.95, hspace=0.2, wspace=0.05, left=0.03, right=0.97\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/mixture/plot_gmm_sin.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for i in range(X.shape[0]):\\n    x = i * step - 6.0\\n    X[i, 0] = x + np.random.normal(0, 0.1)\\n    X[i, 1] = 3.0 * (np.sin(x) + np.random.normal(0, 0.2))\\n\\nplt.figure(figsize=(10, 10))\\nplt.subplots_adjust(\\n    bottom=0.04, top=0.95, hspace=0.2, wspace=0.05, left=0.03, right=0.97\\n)\\n\\n# Fit a Gaussian mixture with EM using ten components\\ngmm = mixture.GaussianMixture(\\n    n_components=10, covariance_type=\"full\", max_iter=100\\n).fit(X)\\nplot_results(\\n    X, gmm.predict(X), gmm.means_, gmm.covariances_, 0, \"Expectation-maximization\"\\n)\\n\\ndpgmm = mixture.BayesianGaussianMixture(\\n    n_components=10,\\n    covariance_type=\"full\",\\n    weight_concentration_prior=1e-2,\\n    weight_concentration_prior_type=\"dirichlet_process\",\\n    mean_precision_prior=1e-2,\\n    covariance_prior=1e0 * np.eye(2),\\n    init_params=\"random\",\\n    max_iter=100,\\n    random_state=2,\\n).fit(X)\\nplot_results(\\n    X,\\n    dpgmm.predict(X),\\n    dpgmm.means_,\\n    dpgmm.covariances_,\\n    1,\\n    \"Bayesian Gaussian mixture models with a Dirichlet process prior \"\\n    r\"for $\\\\gamma_0=0.01$.\",\\n)\\n\\nX_s, y_s = dpgmm.sample(n_samples=2000)\\nplot_samples(\\n    X_s,\\n    y_s,\\n    dpgmm.n_components,\\n    0,\\n    \"Gaussian mixture with a Dirichlet process prior \"\\n    r\"for $\\\\gamma_0=0.01$ sampled with $2000$ samples.\",\\n)\\n\\ndpgmm = mixture.BayesianGaussianMixture(\\n    n_components=10,\\n    covariance_type=\"full\",\\n    weight_concentration_prior=1e2,\\n    weight_concentration_prior_type=\"dirichlet_process\",\\n    mean_precision_prior=1e-2,\\n    covariance_prior=1e0 * np.eye(2),\\n    init_params=\"kmeans\",\\n    max_iter=100,\\n    random_state=2,\\n).fit(X)\\nplot_results(\\n    X,\\n    dpgmm.predict(X),\\n    dpgmm.means_,\\n    dpgmm.covariances_,\\n    2,\\n    \"Bayesian Gaussian mixture models with a Dirichlet process prior \"\\n    r\"for $\\\\gamma_0=100$\",\\n)\\n\\nX_s, y_s = dpgmm.sample(n_samples=2000)\\nplot_samples(\\n    X_s,\\n    y_s,\\n    dpgmm.n_components,\\n    1,\\n    \"Gaussian mixture with a Dirichlet process prior \"\\n    r\"for $\\\\gamma_0=100$ sampled with $2000$ samples.\",\\n)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/developing_estimators/sklearn_is_fitted.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class CustomEstimator(BaseEstimator, ClassifierMixin):\\n    def __init__(self, parameter=1):\\n        self.parameter = parameter\\n\\n    def fit(self, X, y):\\n        \"\"\"\\n        Fit the estimator to the training data.\\n        \"\"\"\\n        self.classes_ = sorted(set(y))\\n        # Custom attribute to track if the estimator is fitted\\n        self._is_fitted = True\\n        return self\\n\\n    def predict(self, X):\\n        \"\"\"\\n        Perform Predictions\\n\\n        If the estimator is not fitted, then raise NotFittedError\\n        \"\"\"\\n        check_is_fitted(self)\\n        # Perform prediction logic\\n        predictions = [self.classes_[0]] * len(X)\\n        return predictions\\n\\n    def score(self, X, y):\\n        \"\"\"\\n        Calculate Score\\n\\n        If the estimator is not fitted, then raise NotFittedError\\n        \"\"\"\\n        check_is_fitted(self)\\n        # Perform scoring logic\\n        return 0.5\\n\\n    def __sklearn_is_fitted__(self):\\n        \"\"\"\\n        Check fitted status and return a Boolean value.\\n        \"\"\"\\n        return hasattr(self, \"_is_fitted\") and self._is_fitted'), Document(metadata={'source': '/content/local_copy_repo/examples/developing_estimators/sklearn_is_fitted.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n========================================\\n`__sklearn_is_fitted__` as Developer API\\n========================================\\n\\nThe `__sklearn_is_fitted__` method is a convention used in scikit-learn for\\nchecking whether an estimator object has been fitted or not. This method is\\ntypically implemented in custom estimator classes that are built on top of\\nscikit-learn\\'s base classes like `BaseEstimator` or its subclasses.\\n\\nDevelopers should use :func:`~sklearn.utils.validation.check_is_fitted`\\nat the beginning of all methods except `fit`. If they need to customize or\\nspeed-up the check, they can implement the `__sklearn_is_fitted__` method as\\nshown below.\\n\\nIn this example the custom estimator showcases the usage of the\\n`__sklearn_is_fitted__` method and the `check_is_fitted` utility function\\nas developer APIs. The `__sklearn_is_fitted__` method checks fitted status\\nby verifying the presence of the `_is_fitted` attribute.\\n\"\"\"\\n\\n# %%\\n# An example custom estimator implementing a simple classifier\\n# ------------------------------------------------------------\\n# This code snippet defines a custom estimator class called `CustomEstimator`\\n# that extends both the `BaseEstimator` and `ClassifierMixin` classes from\\n# scikit-learn and showcases the usage of the `__sklearn_is_fitted__` method\\n# and the `check_is_fitted` utility function.\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom sklearn.base import BaseEstimator, ClassifierMixin\\nfrom sklearn.utils.validation import check_is_fitted\\n\\n\\n# Code for: class CustomEstimator(BaseEstimator, ClassifierMixin):'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_column_transformer.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def subject_body_extractor(posts):\\n    # construct object dtype array with two columns\\n    # first column = \\'subject\\' and second column = \\'body\\'\\n    features = np.empty(shape=(len(posts), 2), dtype=object)\\n    for i, text in enumerate(posts):\\n        # temporary variable `_` stores \\'\\\\n\\\\n\\'\\n        headers, _, body = text.partition(\"\\\\n\\\\n\")\\n        # store body text in second column\\n        features[i, 1] = body\\n\\n        prefix = \"Subject:\"\\n        sub = \"\"\\n        # save text after \\'Subject:\\' in first column\\n        for line in headers.split(\"\\\\n\"):\\n            if line.startswith(prefix):\\n                sub = line[len(prefix) :]\\n                break\\n        features[i, 0] = sub\\n\\n    return features'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_column_transformer.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def text_stats(posts):\\n    return [{\"length\": len(text), \"num_sentences\": text.count(\".\")} for text in posts]'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_column_transformer.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================================================\\nColumn Transformer with Heterogeneous Data Sources\\n==================================================\\n\\nDatasets can often contain components that require different feature\\nextraction and processing pipelines. This scenario might occur when:\\n\\n1. your dataset consists of heterogeneous data types (e.g. raster images and\\n   text captions),\\n2. your dataset is stored in a :class:`pandas.DataFrame` and different columns\\n   require different processing pipelines.\\n\\nThis example demonstrates how to use\\n:class:`~sklearn.compose.ColumnTransformer` on a dataset containing\\ndifferent types of features. The choice of features is not particularly\\nhelpful, but serves to illustrate the technique.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport numpy as np\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.datasets import fetch_20newsgroups\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.feature_extraction import DictVectorizer\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import FunctionTransformer\\nfrom sklearn.svm import LinearSVC\\n\\n##############################################################################\\n# 20 newsgroups dataset\\n# ---------------------\\n#\\n# We will use the :ref:`20 newsgroups dataset <20newsgroups_dataset>`, which\\n# comprises posts from newsgroups on 20 topics. This dataset is split\\n# into train and test subsets based on messages posted before and after\\n# a specific date. We will only use posts from 2 categories to speed up running\\n# time.\\n\\ncategories = [\"sci.med\", \"sci.space\"]\\nX_train, y_train = fetch_20newsgroups(\\n    random_state=1,\\n    subset=\"train\",\\n    categories=categories,\\n    remove=(\"footers\", \"quotes\"),\\n    return_X_y=True,\\n)\\nX_test, y_test = fetch_20newsgroups(\\n    random_state=1,\\n    subset=\"test\",\\n    categories=categories,\\n    remove=(\"footers\", \"quotes\"),\\n    return_X_y=True,\\n)\\n\\n##############################################################################\\n# Each feature comprises meta information about that post, such as the subject,\\n# and the body of the news post.\\n\\nprint(X_train[0])\\n\\n##############################################################################\\n# Creating transformers\\n# ---------------------\\n#\\n# First, we would like a transformer that extracts the subject and\\n# body of each post. Since this is a stateless transformation (does not\\n# require state information from training data), we can define a function that\\n# performs the data transformation then use\\n# :class:`~sklearn.preprocessing.FunctionTransformer` to create a scikit-learn\\n# transformer.\\n\\n\\n# Code for: def subject_body_extractor(posts):\\n\\n\\nsubject_body_transformer = FunctionTransformer(subject_body_extractor)'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_column_transformer.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def subject_body_extractor(posts):\\n\\n\\nsubject_body_transformer = FunctionTransformer(subject_body_extractor)\\n\\n##############################################################################\\n# We will also create a transformer that extracts the\\n# length of the text and the number of sentences.\\n\\n\\n# Code for: def text_stats(posts):\\n\\n\\ntext_stats_transformer = FunctionTransformer(text_stats)\\n\\n##############################################################################\\n# Classification pipeline\\n# -----------------------\\n#\\n# The pipeline below extracts the subject and body from each post using\\n# ``SubjectBodyExtractor``, producing a (n_samples, 2) array. This array is\\n# then used to compute standard bag-of-words features for the subject and body\\n# as well as text length and number of sentences on the body, using\\n# ``ColumnTransformer``. We combine them, with weights, then train a\\n# classifier on the combined set of features.\\n\\npipeline = Pipeline(\\n    [\\n        # Extract subject & body\\n        (\"subjectbody\", subject_body_transformer),\\n        # Use ColumnTransformer to combine the subject and body features\\n        (\\n            \"union\",\\n            ColumnTransformer(\\n                [\\n                    # bag-of-words for subject (col 0)\\n                    (\"subject\", TfidfVectorizer(min_df=50), 0),\\n                    # bag-of-words with decomposition for body (col 1)\\n                    (\\n                        \"body_bow\",\\n                        Pipeline(\\n                            [\\n                                (\"tfidf\", TfidfVectorizer()),\\n                                (\"best\", PCA(n_components=50, svd_solver=\"arpack\")),\\n                            ]\\n                        ),\\n                        1,\\n                    ),\\n                    # Pipeline for pulling text stats from post\\'s body\\n                    (\\n                        \"body_stats\",\\n                        Pipeline(\\n                            [\\n                                (\\n                                    \"stats\",\\n                                    text_stats_transformer,\\n                                ),  # returns a list of dicts\\n                                (\\n                                    \"vect\",\\n                                    DictVectorizer(),\\n                                ),  # list of dicts -> feature matrix\\n                            ]\\n                        ),\\n                        1,\\n                    ),\\n                ],\\n                # weight above ColumnTransformer features\\n                transformer_weights={\\n                    \"subject\": 0.8,\\n                    \"body_bow\": 0.5,\\n                    \"body_stats\": 1.0,\\n                },\\n            ),\\n        ),\\n        # Use a SVC classifier on the combined features\\n        (\"svc\", LinearSVC(dual=False)),\\n    ],\\n    verbose=True,\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_column_transformer.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='##############################################################################\\n# Finally, we fit our pipeline on the training data and use it to predict\\n# topics for ``X_test``. Performance metrics of our pipeline are then printed.\\n\\npipeline.fit(X_train, y_train)\\ny_pred = pipeline.predict(X_test)\\nprint(\"Classification report:\\\\n\\\\n{}\".format(classification_report(y_test, y_pred)))'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_column_transformer_mixed_types.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================\\nColumn Transformer with Mixed Types\\n===================================\\n\\n.. currentmodule:: sklearn\\n\\nThis example illustrates how to apply different preprocessing and feature\\nextraction pipelines to different subsets of features, using\\n:class:`~compose.ColumnTransformer`. This is particularly handy for the\\ncase of datasets that contain heterogeneous data types, since we may want to\\nscale the numeric features and one-hot encode the categorical ones.\\n\\nIn this example, the numeric data is standard-scaled after mean-imputation. The\\ncategorical data is one-hot encoded via ``OneHotEncoder``, which\\ncreates a new category for missing values. We further reduce the dimensionality\\nby selecting categories using a chi-squared test.\\n\\nIn addition, we show two different ways to dispatch the columns to the\\nparticular pre-processor: by column names and by column data types.\\n\\nFinally, the preprocessing pipeline is integrated in a full prediction pipeline\\nusing :class:`~pipeline.Pipeline`, together with a simple classification\\nmodel.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\nimport numpy as np\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.feature_selection import SelectPercentile, chi2\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\nnp.random.seed(0)\\n\\n# %%\\n# Load data from https://www.openml.org/d/40945\\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\\n\\n# Alternatively X and y can be obtained directly from the frame attribute:\\n# X = titanic.frame.drop(\\'survived\\', axis=1)\\n# y = titanic.frame[\\'survived\\']\\n\\n# %%\\n# Use ``ColumnTransformer`` by selecting column by names\\n#\\n# We will train our classifier with the following features:\\n#\\n# Numeric Features:\\n#\\n# * ``age``: float;\\n# * ``fare``: float.\\n#\\n# Categorical Features:\\n#\\n# * ``embarked``: categories encoded as strings ``{\\'C\\', \\'S\\', \\'Q\\'}``;\\n# * ``sex``: categories encoded as strings ``{\\'female\\', \\'male\\'}``;\\n# * ``pclass``: ordinal integers ``{1, 2, 3}``.\\n#\\n# We create the preprocessing pipelines for both numeric and categorical data.\\n# Note that ``pclass`` could either be treated as a categorical or numeric\\n# feature.\\n\\nnumeric_features = [\"age\", \"fare\"]\\nnumeric_transformer = Pipeline(\\n    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_column_transformer_mixed_types.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='numeric_features = [\"age\", \"fare\"]\\nnumeric_transformer = Pipeline(\\n    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\\n)\\n\\ncategorical_features = [\"embarked\", \"sex\", \"pclass\"]\\ncategorical_transformer = Pipeline(\\n    steps=[\\n        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\\n        (\"selector\", SelectPercentile(chi2, percentile=50)),\\n    ]\\n)\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\"num\", numeric_transformer, numeric_features),\\n        (\"cat\", categorical_transformer, categorical_features),\\n    ]\\n)\\n\\n# %%\\n# Append classifier to preprocessing pipeline.\\n# Now we have a full prediction pipeline.\\nclf = Pipeline(\\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\\n)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\n\\nclf.fit(X_train, y_train)\\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\\n\\n# %%\\n# HTML representation of ``Pipeline`` (display diagram)\\n#\\n# When the ``Pipeline`` is printed out in a jupyter notebook an HTML\\n# representation of the estimator is displayed:\\nclf\\n\\n# %%\\n# Use ``ColumnTransformer`` by selecting column by data types\\n#\\n# When dealing with a cleaned dataset, the preprocessing can be automatic by\\n# using the data types of the column to decide whether to treat a column as a\\n# numerical or categorical feature.\\n# :func:`sklearn.compose.make_column_selector` gives this possibility.\\n# First, let\\'s only select a subset of columns to simplify our\\n# example.\\n\\nsubset_feature = [\"embarked\", \"sex\", \"pclass\", \"age\", \"fare\"]\\nX_train, X_test = X_train[subset_feature], X_test[subset_feature]\\n\\n# %%\\n# Then, we introspect the information regarding each column data type.\\n\\nX_train.info()\\n\\n# %%\\n# We can observe that the `embarked` and `sex` columns were tagged as\\n# `category` columns when loading the data with ``fetch_openml``. Therefore, we\\n# can use this information to dispatch the categorical columns to the\\n# ``categorical_transformer`` and the remaining columns to the\\n# ``numerical_transformer``.\\n\\n# %%\\n# .. note:: In practice, you will have to handle yourself the column data type.\\n#    If you want some columns to be considered as `category`, you will have to\\n#    convert them into categorical columns. If you are using pandas, you can\\n#    refer to their documentation regarding `Categorical data\\n#    <https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html>`_.\\n\\nfrom sklearn.compose import make_column_selector as selector\\n\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\\n        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\\n    ]\\n)\\nclf = Pipeline(\\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\\n)\\n\\n\\nclf.fit(X_train, y_train)\\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\\nclf'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_column_transformer_mixed_types.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='clf.fit(X_train, y_train)\\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\\nclf\\n\\n# %%\\n# The resulting score is not exactly the same as the one from the previous\\n# pipeline because the dtype-based selector treats the ``pclass`` column as\\n# a numeric feature instead of a categorical feature as previously:\\n\\nselector(dtype_exclude=\"category\")(X_train)\\n\\n# %%\\n\\nselector(dtype_include=\"category\")(X_train)\\n\\n# %%\\n# Using the prediction pipeline in a grid search\\n#\\n# Grid search can also be performed on the different preprocessing steps\\n# defined in the ``ColumnTransformer`` object, together with the classifier\\'s\\n# hyperparameters as part of the ``Pipeline``.\\n# We will search for both the imputer strategy of the numeric preprocessing\\n# and the regularization parameter of the logistic regression using\\n# :class:`~sklearn.model_selection.RandomizedSearchCV`. This\\n# hyperparameter search randomly selects a fixed number of parameter\\n# settings configured by `n_iter`. Alternatively, one can use\\n# :class:`~sklearn.model_selection.GridSearchCV` but the cartesian product of\\n# the parameter space will be evaluated.\\n\\nparam_grid = {\\n    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\\n    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\\n    \"classifier__C\": [0.1, 1.0, 10, 100],\\n}\\n\\nsearch_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\\nsearch_cv\\n\\n# %%\\n# Calling \\'fit\\' triggers the cross-validated search for the best\\n# hyper-parameters combination:\\n#\\nsearch_cv.fit(X_train, y_train)\\n\\nprint(\"Best params:\")\\nprint(search_cv.best_params_)\\n\\n# %%\\n# The internal cross-validation scores obtained by those parameters is:\\nprint(f\"Internal CV score: {search_cv.best_score_:.3f}\")\\n\\n# %%\\n# We can also introspect the top grid search results as a pandas dataframe:\\nimport pandas as pd\\n\\ncv_results = pd.DataFrame(search_cv.cv_results_)\\ncv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\\ncv_results[\\n    [\\n        \"mean_test_score\",\\n        \"std_test_score\",\\n        \"param_preprocessor__num__imputer__strategy\",\\n        \"param_preprocessor__cat__selector__percentile\",\\n        \"param_classifier__C\",\\n    ]\\n].head(5)\\n\\n# %%\\n# The best hyper-parameters have be used to re-fit a final model on the full\\n# training set. We can evaluate that final model on held out test data that was\\n# not used for hyperparameter tuning.\\n#\\nprint(\\n    \"accuracy of the best model from randomized search: \"\\n    f\"{search_cv.score(X_test, y_test):.3f}\"\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_feature_union.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================\\nConcatenating multiple feature extraction methods\\n=================================================\\n\\nIn many real-world examples, there are many ways to extract features from a\\ndataset. Often it is beneficial to combine several methods to obtain good\\nperformance. This example shows how to use ``FeatureUnion`` to combine\\nfeatures obtained by PCA and univariate selection.\\n\\nCombining features using this transformer has the benefit that it allows\\ncross validation and grid searches over the whole process.\\n\\nThe combination used in this example is not particularly helpful on this\\ndataset and is only used to illustrate the usage of FeatureUnion.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import FeatureUnion, Pipeline\\nfrom sklearn.svm import SVC\\n\\niris = load_iris()\\n\\nX, y = iris.data, iris.target\\n\\n# This dataset is way too high-dimensional. Better do PCA:\\npca = PCA(n_components=2)\\n\\n# Maybe some original features were good, too?\\nselection = SelectKBest(k=1)\\n\\n# Build estimator from PCA and Univariate selection:\\n\\ncombined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\\n\\n# Use combined features to transform dataset:\\nX_features = combined_features.fit(X, y).transform(X)\\nprint(\"Combined space has\", X_features.shape[1], \"features\")\\n\\nsvm = SVC(kernel=\"linear\")\\n\\n# Do grid search over k, n_components and C:\\n\\npipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\\n\\nparam_grid = dict(\\n    features__pca__n_components=[1, 2, 3],\\n    features__univ_select__k=[1, 2],\\n    svm__C=[0.1, 1, 10],\\n)\\n\\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\\ngrid_search.fit(X, y)\\nprint(grid_search.best_estimator_)'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_digits_pipe.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================================================\\nPipelining: chaining a PCA and a logistic regression\\n=========================================================\\n\\nThe PCA does an unsupervised dimensionality reduction, while the logistic\\nregression does the prediction.\\n\\nWe use a GridSearchCV to set the dimensionality of the PCA\\n\\n\"\"\"\\n\\n# Code source: Ga√´l Varoquaux\\n# Modified for documentation by Jaques Grobler\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport polars as pl\\n\\nfrom sklearn import datasets\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Define a pipeline to search for the best combination of PCA truncation\\n# and classifier regularization.\\npca = PCA()\\n# Define a Standard Scaler to normalize inputs\\nscaler = StandardScaler()\\n\\n# set the tolerance to a large value to make the example faster\\nlogistic = LogisticRegression(max_iter=10000, tol=0.1)\\npipe = Pipeline(steps=[(\"scaler\", scaler), (\"pca\", pca), (\"logistic\", logistic)])\\n\\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\\n# Parameters of pipelines can be set using \\'__\\' separated parameter names:\\nparam_grid = {\\n    \"pca__n_components\": [5, 15, 30, 45, 60],\\n    \"logistic__C\": np.logspace(-4, 4, 4),\\n}\\nsearch = GridSearchCV(pipe, param_grid, n_jobs=2)\\nsearch.fit(X_digits, y_digits)\\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\\nprint(search.best_params_)\\n\\n# Plot the PCA spectrum\\npca.fit(X_digits)\\n\\nfig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\\nax0.plot(\\n    np.arange(1, pca.n_components_ + 1), pca.explained_variance_ratio_, \"+\", linewidth=2\\n)\\nax0.set_ylabel(\"PCA explained variance ratio\")\\n\\nax0.axvline(\\n    search.best_estimator_.named_steps[\"pca\"].n_components,\\n    linestyle=\":\",\\n    label=\"n_components chosen\",\\n)\\nax0.legend(prop=dict(size=12))\\n\\n# For each number of components, find the best classifier results\\ncomponents_col = \"param_pca__n_components\"\\nis_max_test_score = pl.col(\"mean_test_score\") == pl.col(\"mean_test_score\").max()\\nbest_clfs = (\\n    pl.LazyFrame(search.cv_results_)\\n    .filter(is_max_test_score.over(components_col))\\n    .unique(components_col)\\n    .sort(components_col)\\n    .collect()\\n)\\nax1.errorbar(\\n    best_clfs[components_col],\\n    best_clfs[\"mean_test_score\"],\\n    yerr=best_clfs[\"std_test_score\"],\\n)\\nax1.set_ylabel(\"Classification accuracy (val)\")\\nax1.set_xlabel(\"n_components\")\\n\\nplt.xlim(-1, 70)\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_transformed_target.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def compute_score(y_true, y_pred):\\n    return {\\n        \"R2\": f\"{r2_score(y_true, y_pred):.3f}\",\\n        \"MedAE\": f\"{median_absolute_error(y_true, y_pred):.3f}\",\\n    }'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_transformed_target.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================================\\nEffect of transforming the targets in regression model\\n======================================================\\n\\nIn this example, we give an overview of\\n:class:`~sklearn.compose.TransformedTargetRegressor`. We use two examples\\nto illustrate the benefit of transforming the targets before learning a linear\\nregression model. The first example uses synthetic data while the second\\nexample is based on the Ames housing data set.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nprint(__doc__)\\n\\n# %%\\n# Synthetic example\\n###################\\n#\\n# A synthetic random regression dataset is generated. The targets ``y`` are\\n# modified by:\\n#\\n#   1. translating all targets such that all entries are\\n#      non-negative (by adding the absolute value of the lowest ``y``) and\\n#   2. applying an exponential function to obtain non-linear\\n#      targets which cannot be fitted using a simple linear model.\\n#\\n# Therefore, a logarithmic (`np.log1p`) and an exponential function\\n# (`np.expm1`) will be used to transform the targets before training a linear\\n# regression model and using it for prediction.\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_regression\\n\\nX, y = make_regression(n_samples=10_000, noise=100, random_state=0)\\ny = np.expm1((y + abs(y.min())) / 200)\\ny_trans = np.log1p(y)\\n\\n# %%\\n# Below we plot the probability density functions of the target\\n# before and after applying the logarithmic functions.\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.model_selection import train_test_split\\n\\nf, (ax0, ax1) = plt.subplots(1, 2)\\n\\nax0.hist(y, bins=100, density=True)\\nax0.set_xlim([0, 2000])\\nax0.set_ylabel(\"Probability\")\\nax0.set_xlabel(\"Target\")\\nax0.set_title(\"Target distribution\")\\n\\nax1.hist(y_trans, bins=100, density=True)\\nax1.set_ylabel(\"Probability\")\\nax1.set_xlabel(\"Target\")\\nax1.set_title(\"Transformed target distribution\")\\n\\nf.suptitle(\"Synthetic data\", y=1.05)\\nplt.tight_layout()\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\n# %%\\n# At first, a linear model will be applied on the original targets. Due to the\\n# non-linearity, the model trained will not be precise during\\n# prediction. Subsequently, a logarithmic function is used to linearize the\\n# targets, allowing better prediction even with a similar linear model as\\n# reported by the median absolute error (MedAE).\\nfrom sklearn.metrics import median_absolute_error, r2_score\\n\\n\\n# Code for: def compute_score(y_true, y_pred):\\n\\n\\n# %%\\nfrom sklearn.compose import TransformedTargetRegressor\\nfrom sklearn.linear_model import RidgeCV\\nfrom sklearn.metrics import PredictionErrorDisplay\\n\\nf, (ax0, ax1) = plt.subplots(1, 2, sharey=True)\\n\\nridge_cv = RidgeCV().fit(X_train, y_train)\\ny_pred_ridge = ridge_cv.predict(X_test)'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_transformed_target.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\nfrom sklearn.compose import TransformedTargetRegressor\\nfrom sklearn.linear_model import RidgeCV\\nfrom sklearn.metrics import PredictionErrorDisplay\\n\\nf, (ax0, ax1) = plt.subplots(1, 2, sharey=True)\\n\\nridge_cv = RidgeCV().fit(X_train, y_train)\\ny_pred_ridge = ridge_cv.predict(X_test)\\n\\nridge_cv_with_trans_target = TransformedTargetRegressor(\\n    regressor=RidgeCV(), func=np.log1p, inverse_func=np.expm1\\n).fit(X_train, y_train)\\ny_pred_ridge_with_trans_target = ridge_cv_with_trans_target.predict(X_test)\\n\\nPredictionErrorDisplay.from_predictions(\\n    y_test,\\n    y_pred_ridge,\\n    kind=\"actual_vs_predicted\",\\n    ax=ax0,\\n    scatter_kwargs={\"alpha\": 0.5},\\n)\\nPredictionErrorDisplay.from_predictions(\\n    y_test,\\n    y_pred_ridge_with_trans_target,\\n    kind=\"actual_vs_predicted\",\\n    ax=ax1,\\n    scatter_kwargs={\"alpha\": 0.5},\\n)\\n\\n# Add the score in the legend of each axis\\nfor ax, y_pred in zip([ax0, ax1], [y_pred_ridge, y_pred_ridge_with_trans_target]):\\n    for name, score in compute_score(y_test, y_pred).items():\\n        ax.plot([], [], \" \", label=f\"{name}={score}\")\\n    ax.legend(loc=\"upper left\")\\n\\nax0.set_title(\"Ridge regression \\\\n without target transformation\")\\nax1.set_title(\"Ridge regression \\\\n with target transformation\")\\nf.suptitle(\"Synthetic data\", y=1.05)\\nplt.tight_layout()\\n\\n# %%\\n# Real-world data set\\n#####################\\n#\\n# In a similar manner, the Ames housing data set is used to show the impact\\n# of transforming the targets before learning a model. In this example, the\\n# target to be predicted is the selling price of each house.\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.preprocessing import quantile_transform\\n\\names = fetch_openml(name=\"house_prices\", as_frame=True)\\n# Keep only numeric columns\\nX = ames.data.select_dtypes(np.number)\\n# Remove columns with NaN or Inf values\\nX = X.drop(columns=[\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"])\\n# Let the price be in k$\\ny = ames.target / 1000\\ny_trans = quantile_transform(\\n    y.to_frame(), n_quantiles=900, output_distribution=\"normal\", copy=True\\n).squeeze()\\n\\n# %%\\n# A :class:`~sklearn.preprocessing.QuantileTransformer` is used to normalize\\n# the target distribution before applying a\\n# :class:`~sklearn.linear_model.RidgeCV` model.\\nf, (ax0, ax1) = plt.subplots(1, 2)\\n\\nax0.hist(y, bins=100, density=True)\\nax0.set_ylabel(\"Probability\")\\nax0.set_xlabel(\"Target\")\\nax0.set_title(\"Target distribution\")\\n\\nax1.hist(y_trans, bins=100, density=True)\\nax1.set_ylabel(\"Probability\")\\nax1.set_xlabel(\"Target\")\\nax1.set_title(\"Transformed target distribution\")\\n\\nf.suptitle(\"Ames housing data: selling price\", y=1.05)\\nplt.tight_layout()\\n\\n# %%\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_transformed_target.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='f.suptitle(\"Ames housing data: selling price\", y=1.05)\\nplt.tight_layout()\\n\\n# %%\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\\n\\n# %%\\n# The effect of the transformer is weaker than on the synthetic data. However,\\n# the transformation results in an increase in :math:`R^2` and large decrease\\n# of the MedAE. The residual plot (predicted target - true target vs predicted\\n# target) without target transformation takes on a curved, \\'reverse smile\\'\\n# shape due to residual values that vary depending on the value of predicted\\n# target. With target transformation, the shape is more linear indicating\\n# better model fit.\\nfrom sklearn.preprocessing import QuantileTransformer\\n\\nf, (ax0, ax1) = plt.subplots(2, 2, sharey=\"row\", figsize=(6.5, 8))\\n\\nridge_cv = RidgeCV().fit(X_train, y_train)\\ny_pred_ridge = ridge_cv.predict(X_test)\\n\\nridge_cv_with_trans_target = TransformedTargetRegressor(\\n    regressor=RidgeCV(),\\n    transformer=QuantileTransformer(n_quantiles=900, output_distribution=\"normal\"),\\n).fit(X_train, y_train)\\ny_pred_ridge_with_trans_target = ridge_cv_with_trans_target.predict(X_test)\\n\\n# plot the actual vs predicted values\\nPredictionErrorDisplay.from_predictions(\\n    y_test,\\n    y_pred_ridge,\\n    kind=\"actual_vs_predicted\",\\n    ax=ax0[0],\\n    scatter_kwargs={\"alpha\": 0.5},\\n)\\nPredictionErrorDisplay.from_predictions(\\n    y_test,\\n    y_pred_ridge_with_trans_target,\\n    kind=\"actual_vs_predicted\",\\n    ax=ax0[1],\\n    scatter_kwargs={\"alpha\": 0.5},\\n)\\n\\n# Add the score in the legend of each axis\\nfor ax, y_pred in zip([ax0[0], ax0[1]], [y_pred_ridge, y_pred_ridge_with_trans_target]):\\n    for name, score in compute_score(y_test, y_pred).items():\\n        ax.plot([], [], \" \", label=f\"{name}={score}\")\\n    ax.legend(loc=\"upper left\")\\n\\nax0[0].set_title(\"Ridge regression \\\\n without target transformation\")\\nax0[1].set_title(\"Ridge regression \\\\n with target transformation\")\\n\\n# plot the residuals vs the predicted values\\nPredictionErrorDisplay.from_predictions(\\n    y_test,\\n    y_pred_ridge,\\n    kind=\"residual_vs_predicted\",\\n    ax=ax1[0],\\n    scatter_kwargs={\"alpha\": 0.5},\\n)\\nPredictionErrorDisplay.from_predictions(\\n    y_test,\\n    y_pred_ridge_with_trans_target,\\n    kind=\"residual_vs_predicted\",\\n    ax=ax1[1],\\n    scatter_kwargs={\"alpha\": 0.5},\\n)\\nax1[0].set_title(\"Ridge regression \\\\n without target transformation\")\\nax1[1].set_title(\"Ridge regression \\\\n with target transformation\")\\n\\nf.suptitle(\"Ames housing data: selling price\", y=1.05)\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_compare_reduction.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================================\\nSelecting dimensionality reduction with Pipeline and GridSearchCV\\n=================================================================\\n\\nThis example constructs a pipeline that does dimensionality\\nreduction followed by prediction with a support vector\\nclassifier. It demonstrates the use of ``GridSearchCV`` and\\n``Pipeline`` to optimize over different classes of estimators in a\\nsingle CV run -- unsupervised ``PCA`` and ``NMF`` dimensionality\\nreductions are compared to univariate feature selection during\\nthe grid search.\\n\\nAdditionally, ``Pipeline`` can be instantiated with the ``memory``\\nargument to memoize the transformers within the pipeline, avoiding to fit\\nagain the same transformers over and over.\\n\\nNote that the use of ``memory`` to enable caching becomes interesting when the\\nfitting of a transformer is costly.\\n\\n\"\"\"\\n\\n# Authors: Robert McGibbon\\n#          Joel Nothman\\n#          Guillaume Lemaitre\\n\\n# %%\\n# Illustration of ``Pipeline`` and ``GridSearchCV``\\n###############################################################################\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.decomposition import NMF, PCA\\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.svm import LinearSVC\\n\\nX, y = load_digits(return_X_y=True)\\n\\npipe = Pipeline(\\n    [\\n        (\"scaling\", MinMaxScaler()),\\n        # the reduce_dim stage is populated by the param_grid\\n        (\"reduce_dim\", \"passthrough\"),\\n        (\"classify\", LinearSVC(dual=False, max_iter=10000)),\\n    ]\\n)\\n\\nN_FEATURES_OPTIONS = [2, 4, 8]\\nC_OPTIONS = [1, 10, 100, 1000]\\nparam_grid = [\\n    {\\n        \"reduce_dim\": [PCA(iterated_power=7), NMF(max_iter=1_000)],\\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\\n        \"classify__C\": C_OPTIONS,\\n    },\\n    {\\n        \"reduce_dim\": [SelectKBest(mutual_info_classif)],\\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\\n        \"classify__C\": C_OPTIONS,\\n    },\\n]\\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\\n\\ngrid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)\\ngrid.fit(X, y)\\n\\n# %%\\nimport pandas as pd\\n\\nmean_scores = np.array(grid.cv_results_[\"mean_test_score\"])\\n# scores are in the order of param_grid iteration, which is alphabetical\\nmean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\\n# select score for best C\\nmean_scores = mean_scores.max(axis=0)\\n# create a dataframe to ease plotting\\nmean_scores = pd.DataFrame(\\n    mean_scores.T, index=N_FEATURES_OPTIONS, columns=reducer_labels\\n)\\n\\nax = mean_scores.plot.bar()\\nax.set_title(\"Comparing feature reduction techniques\")\\nax.set_xlabel(\"Reduced number of features\")\\nax.set_ylabel(\"Digit classification accuracy\")\\nax.set_ylim((0, 1))\\nax.legend(loc=\"upper left\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/compose/plot_compare_reduction.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='ax = mean_scores.plot.bar()\\nax.set_title(\"Comparing feature reduction techniques\")\\nax.set_xlabel(\"Reduced number of features\")\\nax.set_ylabel(\"Digit classification accuracy\")\\nax.set_ylim((0, 1))\\nax.legend(loc=\"upper left\")\\n\\nplt.show()\\n\\n# %%\\n# Caching transformers within a ``Pipeline``\\n###############################################################################\\n# It is sometimes worthwhile storing the state of a specific transformer\\n# since it could be used again. Using a pipeline in ``GridSearchCV`` triggers\\n# such situations. Therefore, we use the argument ``memory`` to enable caching.\\n#\\n# .. warning::\\n#     Note that this example is, however, only an illustration since for this\\n#     specific case fitting PCA is not necessarily slower than loading the\\n#     cache. Hence, use the ``memory`` constructor parameter when the fitting\\n#     of a transformer is costly.\\n\\nfrom shutil import rmtree\\n\\nfrom joblib import Memory\\n\\n# Create a temporary folder to store the transformers of the pipeline\\nlocation = \"cachedir\"\\nmemory = Memory(location=location, verbose=10)\\ncached_pipe = Pipeline(\\n    [(\"reduce_dim\", PCA()), (\"classify\", LinearSVC(dual=False, max_iter=10000))],\\n    memory=memory,\\n)\\n\\n# This time, a cached pipeline will be used within the grid search\\n\\n\\n# Delete the temporary cache before exiting\\nmemory.clear(warn=False)\\nrmtree(location)\\n\\n# %%\\n# The ``PCA`` fitting is only computed at the evaluation of the first\\n# configuration of the ``C`` parameter of the ``LinearSVC`` classifier. The\\n# other configurations of ``C`` will trigger the loading of the cached ``PCA``\\n# estimator data, leading to save processing time. Therefore, the use of\\n# caching the pipeline using ``memory`` is highly beneficial when fitting\\n# a transformer is costly.'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_label_propagation_digits_active_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n========================================\\nLabel Propagation digits active learning\\n========================================\\n\\nDemonstrates an active learning technique to learn handwritten digits\\nusing label propagation.\\n\\nWe start by training a label propagation model with only 10 labeled points,\\nthen we select the top five most uncertain points to label. Next, we train\\nwith 15 labeled points (original 10 + 5 new ones). We repeat this process\\nfour times to have a model trained with 30 labeled examples. Note you can\\nincrease this to label more than 30 by changing `max_iterations`. Labeling\\nmore than 30 can be useful to get a sense for the speed of convergence of\\nthis active learning technique.\\n\\nA plot will appear showing the top 5 most uncertain digits for each iteration\\nof training. These may or may not contain mistakes, but we will train the next\\nmodel with their true labels.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom scipy import stats\\n\\nfrom sklearn import datasets\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nfrom sklearn.semi_supervised import LabelSpreading\\n\\ndigits = datasets.load_digits()\\nrng = np.random.RandomState(0)\\nindices = np.arange(len(digits.data))\\nrng.shuffle(indices)\\n\\nX = digits.data[indices[:330]]\\ny = digits.target[indices[:330]]\\nimages = digits.images[indices[:330]]\\n\\nn_total_samples = len(y)\\nn_labeled_points = 40\\nmax_iterations = 5\\n\\nunlabeled_indices = np.arange(n_total_samples)[n_labeled_points:]\\nf = plt.figure()\\n\\nfor i in range(max_iterations):\\n    if len(unlabeled_indices) == 0:\\n        print(\"No unlabeled items left to label.\")\\n        break\\n    y_train = np.copy(y)\\n    y_train[unlabeled_indices] = -1\\n\\n    lp_model = LabelSpreading(gamma=0.25, max_iter=20)\\n    lp_model.fit(X, y_train)\\n\\n    predicted_labels = lp_model.transduction_[unlabeled_indices]\\n    true_labels = y[unlabeled_indices]\\n\\n    cm = confusion_matrix(true_labels, predicted_labels, labels=lp_model.classes_)\\n\\n    print(\"Iteration %i %s\" % (i, 70 * \"_\"))\\n    print(\\n        \"Label Spreading model: %d labeled & %d unlabeled (%d total)\"\\n        % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples)\\n    )\\n\\n    print(classification_report(true_labels, predicted_labels))\\n\\n    print(\"Confusion matrix\")\\n    print(cm)\\n\\n    # compute the entropies of transduced label distributions\\n    pred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)\\n\\n    # select up to 5 digit examples that the classifier is most uncertain about\\n    uncertainty_index = np.argsort(pred_entropies)[::-1]\\n    uncertainty_index = uncertainty_index[\\n        np.isin(uncertainty_index, unlabeled_indices)\\n    ][:5]\\n\\n    # keep track of indices that we get labels for\\n    delete_indices = np.array([], dtype=int)'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_label_propagation_digits_active_learning.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# keep track of indices that we get labels for\\n    delete_indices = np.array([], dtype=int)\\n\\n    # for more than 5 iterations, visualize the gain only on the first 5\\n    if i < 5:\\n        f.text(\\n            0.05,\\n            (1 - (i + 1) * 0.183),\\n            \"model %d\\\\n\\\\nfit with\\\\n%d labels\" % ((i + 1), i * 5 + 10),\\n            size=10,\\n        )\\n    for index, image_index in enumerate(uncertainty_index):\\n        image = images[image_index]\\n\\n        # for more than 5 iterations, visualize the gain only on the first 5\\n        if i < 5:\\n            sub = f.add_subplot(5, 5, index + 1 + (5 * i))\\n            sub.imshow(image, cmap=plt.cm.gray_r, interpolation=\"none\")\\n            sub.set_title(\\n                \"predict: %i\\\\ntrue: %i\"\\n                % (lp_model.transduction_[image_index], y[image_index]),\\n                size=10,\\n            )\\n            sub.axis(\"off\")\\n\\n        # labeling 5 points, remote from labeled set\\n        (delete_index,) = np.where(unlabeled_indices == image_index)\\n        delete_indices = np.concatenate((delete_indices, delete_index))\\n\\n    unlabeled_indices = np.delete(unlabeled_indices, delete_indices)\\n    n_labeled_points += len(uncertainty_index)\\n\\nf.suptitle(\\n    (\\n        \"Active learning with Label Propagation.\\\\nRows show 5 most \"\\n        \"uncertain labels to learn with the next model.\"\\n    ),\\n    y=1.15,\\n)\\nplt.subplots_adjust(left=0.2, bottom=0.03, right=0.9, top=0.9, wspace=0.2, hspace=0.85)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_semi_supervised_newsgroups.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\\n    print(\"Number of training samples:\", len(X_train))\\n    print(\"Unlabeled samples in training set:\", sum(1 for x in y_train if x == -1))\\n    clf.fit(X_train, y_train)\\n    y_pred = clf.predict(X_test)\\n    print(\\n        \"Micro-averaged F1 score on test set: %0.3f\"\\n        % f1_score(y_test, y_pred, average=\"micro\")\\n    )\\n    print(\"-\" * 10)\\n    print()'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_semi_supervised_newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================================\\nSemi-supervised Classification on a Text Dataset\\n================================================\\n\\nIn this example, semi-supervised classifiers are trained on the 20 newsgroups\\ndataset (which will be automatically downloaded).\\n\\nYou can adjust the number of categories by giving their names to the dataset\\nloader or setting them to `None` to get all 20 of them.\\n\\n\"\"\"\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_20newsgroups\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import FunctionTransformer\\nfrom sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\\n\\n# Loading dataset containing first five categories\\ndata = fetch_20newsgroups(\\n    subset=\"train\",\\n    categories=[\\n        \"alt.atheism\",\\n        \"comp.graphics\",\\n        \"comp.os.ms-windows.misc\",\\n        \"comp.sys.ibm.pc.hardware\",\\n        \"comp.sys.mac.hardware\",\\n    ],\\n)\\nprint(\"%d documents\" % len(data.filenames))\\nprint(\"%d categories\" % len(data.target_names))\\nprint()\\n\\n# Parameters\\nsdg_params = dict(alpha=1e-5, penalty=\"l2\", loss=\"log_loss\")\\nvectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\\n\\n# Supervised Pipeline\\npipeline = Pipeline(\\n    [\\n        (\"vect\", CountVectorizer(**vectorizer_params)),\\n        (\"tfidf\", TfidfTransformer()),\\n        (\"clf\", SGDClassifier(**sdg_params)),\\n    ]\\n)\\n# SelfTraining Pipeline\\nst_pipeline = Pipeline(\\n    [\\n        (\"vect\", CountVectorizer(**vectorizer_params)),\\n        (\"tfidf\", TfidfTransformer()),\\n        (\"clf\", SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True)),\\n    ]\\n)\\n# LabelSpreading Pipeline\\nls_pipeline = Pipeline(\\n    [\\n        (\"vect\", CountVectorizer(**vectorizer_params)),\\n        (\"tfidf\", TfidfTransformer()),\\n        # LabelSpreading does not support dense matrices\\n        (\"toarray\", FunctionTransformer(lambda x: x.toarray())),\\n        (\"clf\", LabelSpreading()),\\n    ]\\n)\\n\\n\\n# Code for: def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\\n\\n\\nif __name__ == \"__main__\":\\n    X, y = data.data, data.target\\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\\n\\n    print(\"Supervised SGDClassifier on 100% of the data:\")\\n    eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\\n\\n    # select a mask of 20% of the train dataset\\n    y_mask = np.random.rand(len(y_train)) < 0.2\\n\\n    # X_20 and y_20 are the subset of the train dataset indicated by the mask\\n    X_20, y_20 = map(\\n        list, zip(*((x, y) for x, y, m in zip(X_train, y_train, y_mask) if m))\\n    )\\n    print(\"Supervised SGDClassifier on 20% of the training data:\")\\n    eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_semi_supervised_newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# set the non-masked subset to be unlabeled\\n    y_train[~y_mask] = -1\\n    print(\"SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\")\\n    eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\\n\\n    print(\"LabelSpreading on 20% of the data (rest is unlabeled):\")\\n    eval_and_print_metrics(ls_pipeline, X_train, y_train, X_test, y_test)'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_self_training_varying_threshold.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================\\nEffect of varying threshold for self-training\\n=============================================\\n\\nThis example illustrates the effect of a varying threshold on self-training.\\nThe `breast_cancer` dataset is loaded, and labels are deleted such that only 50\\nout of 569 samples have labels. A `SelfTrainingClassifier` is fitted on this\\ndataset, with varying thresholds.\\n\\nThe upper graph shows the amount of labeled samples that the classifier has\\navailable by the end of fit, and the accuracy of the classifier. The lower\\ngraph shows the last iteration in which a sample was labeled. All values are\\ncross validated with 3 folds.\\n\\nAt low thresholds (in [0.4, 0.5]), the classifier learns from samples that were\\nlabeled with a low confidence. These low-confidence samples are likely have\\nincorrect predicted labels, and as a result, fitting on these incorrect labels\\nproduces a poor accuracy. Note that the classifier labels almost all of the\\nsamples, and only takes one iteration.\\n\\nFor very high thresholds (in [0.9, 1)) we observe that the classifier does not\\naugment its dataset (the amount of self-labeled samples is 0). As a result, the\\naccuracy achieved with a threshold of 0.9999 is the same as a normal supervised\\nclassifier would achieve.\\n\\nThe optimal accuracy lies in between both of these extremes at a threshold of\\naround 0.7.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import StratifiedKFold\\nfrom sklearn.semi_supervised import SelfTrainingClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.utils import shuffle\\n\\nn_splits = 3\\n\\nX, y = datasets.load_breast_cancer(return_X_y=True)\\nX, y = shuffle(X, y, random_state=42)\\ny_true = y.copy()\\ny[50:] = -1\\ntotal_samples = y.shape[0]\\n\\nbase_classifier = SVC(probability=True, gamma=0.001, random_state=42)\\n\\nx_values = np.arange(0.4, 1.05, 0.05)\\nx_values = np.append(x_values, 0.99999)\\nscores = np.empty((x_values.shape[0], n_splits))\\namount_labeled = np.empty((x_values.shape[0], n_splits))\\namount_iterations = np.empty((x_values.shape[0], n_splits))\\n\\nfor i, threshold in enumerate(x_values):\\n    self_training_clf = SelfTrainingClassifier(base_classifier, threshold=threshold)\\n\\n    # We need manual cross validation so that we don\\'t treat -1 as a separate\\n    # class when computing accuracy\\n    skfolds = StratifiedKFold(n_splits=n_splits)\\n    for fold, (train_index, test_index) in enumerate(skfolds.split(X, y)):\\n        X_train = X[train_index]\\n        y_train = y[train_index]\\n        X_test = X[test_index]\\n        y_test = y[test_index]\\n        y_test_true = y_true[test_index]\\n\\n        self_training_clf.fit(X_train, y_train)'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_self_training_varying_threshold.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='self_training_clf.fit(X_train, y_train)\\n\\n        # The amount of labeled samples that at the end of fitting\\n        amount_labeled[i, fold] = (\\n            total_samples\\n            - np.unique(self_training_clf.labeled_iter_, return_counts=True)[1][0]\\n        )\\n        # The last iteration the classifier labeled a sample in\\n        amount_iterations[i, fold] = np.max(self_training_clf.labeled_iter_)\\n\\n        y_pred = self_training_clf.predict(X_test)\\n        scores[i, fold] = accuracy_score(y_test_true, y_pred)\\n\\n\\nax1 = plt.subplot(211)\\nax1.errorbar(\\n    x_values, scores.mean(axis=1), yerr=scores.std(axis=1), capsize=2, color=\"b\"\\n)\\nax1.set_ylabel(\"Accuracy\", color=\"b\")\\nax1.tick_params(\"y\", colors=\"b\")\\n\\nax2 = ax1.twinx()\\nax2.errorbar(\\n    x_values,\\n    amount_labeled.mean(axis=1),\\n    yerr=amount_labeled.std(axis=1),\\n    capsize=2,\\n    color=\"g\",\\n)\\nax2.set_ylim(bottom=0)\\nax2.set_ylabel(\"Amount of labeled samples\", color=\"g\")\\nax2.tick_params(\"y\", colors=\"g\")\\n\\nax3 = plt.subplot(212, sharex=ax1)\\nax3.errorbar(\\n    x_values,\\n    amount_iterations.mean(axis=1),\\n    yerr=amount_iterations.std(axis=1),\\n    capsize=2,\\n    color=\"b\",\\n)\\nax3.set_ylim(bottom=0)\\nax3.set_ylabel(\"Amount of iterations\")\\nax3.set_xlabel(\"Threshold\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_label_propagation_structure.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================\\nLabel Propagation learning a complex structure\\n==============================================\\n\\nExample of LabelPropagation learning a complex internal structure\\nto demonstrate \"manifold learning\". The outer circle should be\\nlabeled \"red\" and the inner circle \"blue\". Because both label groups\\nlie inside their own distinct shape, we can see that the labels\\npropagate correctly around the circle.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# We generate a dataset with two concentric circles. In addition, a label\\n# is associated with each sample of the dataset that is: 0 (belonging to\\n# the outer circle), 1 (belonging to the inner circle), and -1 (unknown).\\n# Here, all labels but two are tagged as unknown.\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_circles\\n\\nn_samples = 200\\nX, y = make_circles(n_samples=n_samples, shuffle=False)\\nouter, inner = 0, 1\\nlabels = np.full(n_samples, -1.0)\\nlabels[0] = outer\\nlabels[-1] = inner\\n\\n# %%\\n# Plot raw data\\nimport matplotlib.pyplot as plt\\n\\nplt.figure(figsize=(4, 4))\\nplt.scatter(\\n    X[labels == outer, 0],\\n    X[labels == outer, 1],\\n    color=\"navy\",\\n    marker=\"s\",\\n    lw=0,\\n    label=\"outer labeled\",\\n    s=10,\\n)\\nplt.scatter(\\n    X[labels == inner, 0],\\n    X[labels == inner, 1],\\n    color=\"c\",\\n    marker=\"s\",\\n    lw=0,\\n    label=\"inner labeled\",\\n    s=10,\\n)\\nplt.scatter(\\n    X[labels == -1, 0],\\n    X[labels == -1, 1],\\n    color=\"darkorange\",\\n    marker=\".\",\\n    label=\"unlabeled\",\\n)\\nplt.legend(scatterpoints=1, shadow=False, loc=\"center\")\\n_ = plt.title(\"Raw data (2 classes=outer and inner)\")\\n\\n# %%\\n#\\n# The aim of :class:`~sklearn.semi_supervised.LabelSpreading` is to associate\\n# a label to sample where the label is initially unknown.\\nfrom sklearn.semi_supervised import LabelSpreading\\n\\nlabel_spread = LabelSpreading(kernel=\"knn\", alpha=0.8)\\nlabel_spread.fit(X, labels)\\n\\n# %%\\n# Now, we can check which labels have been associated with each sample\\n# when the label was unknown.\\noutput_labels = label_spread.transduction_\\noutput_label_array = np.asarray(output_labels)\\nouter_numbers = np.where(output_label_array == outer)[0]\\ninner_numbers = np.where(output_label_array == inner)[0]\\n\\nplt.figure(figsize=(4, 4))\\nplt.scatter(\\n    X[outer_numbers, 0],\\n    X[outer_numbers, 1],\\n    color=\"navy\",\\n    marker=\"s\",\\n    lw=0,\\n    s=10,\\n    label=\"outer learned\",\\n)\\nplt.scatter(\\n    X[inner_numbers, 0],\\n    X[inner_numbers, 1],\\n    color=\"c\",\\n    marker=\"s\",\\n    lw=0,\\n    s=10,\\n    label=\"inner learned\",\\n)\\nplt.legend(scatterpoints=1, shadow=False, loc=\"center\")\\nplt.title(\"Labels learned with Label Spreading (KNN)\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_semi_supervised_versus_svm_iris.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================================================================\\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\\n===============================================================================\\n\\nA comparison for the decision boundaries generated on the iris dataset\\nby Label Spreading, Self-training and SVM.\\n\\nThis example demonstrates that Label Spreading and Self-training can learn\\ngood boundaries even when small amounts of labeled data are available.\\n\\nNote that Self-training with 100% of the data is omitted as it is functionally\\nidentical to training the SVC on 100% of the data.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\\nfrom sklearn.svm import SVC\\n\\niris = datasets.load_iris()\\n\\nX = iris.data[:, :2]\\ny = iris.target\\n\\n# step size in the mesh\\nh = 0.02\\n\\nrng = np.random.RandomState(0)\\ny_rand = rng.rand(y.shape[0])\\ny_30 = np.copy(y)\\ny_30[y_rand < 0.3] = -1  # set random samples to be unlabeled\\ny_50 = np.copy(y)\\ny_50[y_rand < 0.5] = -1\\n# we create an instance of SVM and fit out data. We do not scale our\\n# data since we want to plot the support vectors\\nls30 = (LabelSpreading().fit(X, y_30), y_30, \"Label Spreading 30% data\")\\nls50 = (LabelSpreading().fit(X, y_50), y_50, \"Label Spreading 50% data\")\\nls100 = (LabelSpreading().fit(X, y), y, \"Label Spreading 100% data\")\\n\\n# the base classifier for self-training is identical to the SVC\\nbase_classifier = SVC(kernel=\"rbf\", gamma=0.5, probability=True)\\nst30 = (\\n    SelfTrainingClassifier(base_classifier).fit(X, y_30),\\n    y_30,\\n    \"Self-training 30% data\",\\n)\\nst50 = (\\n    SelfTrainingClassifier(base_classifier).fit(X, y_50),\\n    y_50,\\n    \"Self-training 50% data\",\\n)\\n\\nrbf_svc = (SVC(kernel=\"rbf\", gamma=0.5).fit(X, y), y, \"SVC with rbf kernel\")\\n\\n# create a mesh to plot in\\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n\\ncolor_map = {-1: (1, 1, 1), 0: (0, 0, 0.9), 1: (1, 0, 0), 2: (0.8, 0.6, 0)}\\n\\nclassifiers = (ls30, st30, ls50, st50, ls100, rbf_svc)\\nfor i, (clf, y_train, title) in enumerate(classifiers):\\n    # Plot the decision boundary. For that, we will assign a color to each\\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\\n    plt.subplot(3, 2, i + 1)\\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n\\n    # Put the result into a color plot\\n    Z = Z.reshape(xx.shape)\\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\\n    plt.axis(\"off\")\\n\\n    # Plot also the training points\\n    colors = [color_map[y] for y in y_train]\\n    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors=\"black\")\\n\\n    plt.title(title)\\n\\nplt.suptitle(\"Unlabeled points are colored white\", y=0.1)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_label_propagation_digits.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================================\\nLabel Propagation digits: Demonstrating performance\\n===================================================\\n\\nThis example demonstrates the power of semisupervised learning by\\ntraining a Label Spreading model to classify handwritten digits\\nwith sets of very few labels.\\n\\nThe handwritten digit dataset has 1797 total points. The model will\\nbe trained using all points, but only 30 will be labeled. Results\\nin the form of a confusion matrix and a series of metrics over each'), Document(metadata={'source': '/content/local_copy_repo/examples/semi_supervised/plot_label_propagation_digits.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='class will be very good.\\n\\nAt the end, the top 10 most uncertain predictions will be shown.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Data generation\\n# ---------------\\n#\\n# We use the digits dataset. We only use a subset of randomly selected samples.\\nimport numpy as np\\n\\nfrom sklearn import datasets\\n\\ndigits = datasets.load_digits()\\nrng = np.random.RandomState(2)\\nindices = np.arange(len(digits.data))\\nrng.shuffle(indices)\\n\\n# %%\\n#\\n# We selected 340 samples of which only 40 will be associated with a known label.\\n# Therefore, we store the indices of the 300 other samples for which we are not\\n# supposed to know their labels.\\nX = digits.data[indices[:340]]\\ny = digits.target[indices[:340]]\\nimages = digits.images[indices[:340]]\\n\\nn_total_samples = len(y)\\nn_labeled_points = 40\\n\\nindices = np.arange(n_total_samples)\\n\\nunlabeled_set = indices[n_labeled_points:]\\n\\n# %%\\n# Shuffle everything around\\ny_train = np.copy(y)\\ny_train[unlabeled_set] = -1\\n\\n# %%\\n# Semi-supervised learning\\n# ------------------------\\n#\\n# We fit a :class:`~sklearn.semi_supervised.LabelSpreading` and use it to predict\\n# the unknown labels.\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.semi_supervised import LabelSpreading\\n\\nlp_model = LabelSpreading(gamma=0.25, max_iter=20)\\nlp_model.fit(X, y_train)\\npredicted_labels = lp_model.transduction_[unlabeled_set]\\ntrue_labels = y[unlabeled_set]\\n\\nprint(\\n    \"Label Spreading model: %d labeled & %d unlabeled points (%d total)\"\\n    % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples)\\n)\\n\\n# %%\\n# Classification report\\nprint(classification_report(true_labels, predicted_labels))\\n\\n# %%\\n# Confusion matrix\\nfrom sklearn.metrics import ConfusionMatrixDisplay\\n\\nConfusionMatrixDisplay.from_predictions(\\n    true_labels, predicted_labels, labels=lp_model.classes_\\n)\\n\\n# %%\\n# Plot the most uncertain predictions\\n# -----------------------------------\\n#\\n# Here, we will pick and show the 10 most uncertain predictions.\\nfrom scipy import stats\\n\\npred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)\\n\\n# %%\\n# Pick the top 10 most uncertain labels\\nuncertainty_index = np.argsort(pred_entropies)[-10:]\\n\\n# %%\\n# Plot\\nimport matplotlib.pyplot as plt\\n\\nf = plt.figure(figsize=(7, 5))\\nfor index, image_index in enumerate(uncertainty_index):\\n    image = images[image_index]\\n\\n    sub = f.add_subplot(2, 5, index + 1)\\n    sub.imshow(image, cmap=plt.cm.gray_r)\\n    plt.xticks([])\\n    plt.yticks([])\\n    sub.set_title(\\n        \"predict: %i\\\\ntrue: %i\" % (lp_model.transduction_[image_index], y[image_index])\\n    )\\n\\nf.suptitle(\"Learning with small amount of labeled data\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_clustering.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def fit_and_evaluate(km, X, name=None, n_runs=5):\\n    name = km.__class__.__name__ if name is None else name\\n\\n    train_times = []\\n    scores = defaultdict(list)\\n    for seed in range(n_runs):\\n        km.set_params(random_state=seed)\\n        t0 = time()\\n        km.fit(X)\\n        train_times.append(time() - t0)\\n        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\\n        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\\n        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\\n        scores[\"Adjusted Rand-Index\"].append(\\n            metrics.adjusted_rand_score(labels, km.labels_)\\n        )\\n        scores[\"Silhouette Coefficient\"].append(\\n            metrics.silhouette_score(X, km.labels_, sample_size=2000)\\n        )\\n    train_times = np.asarray(train_times)\\n\\n    print(f\"clustering done in {train_times.mean():.2f} ¬± {train_times.std():.2f} s \")\\n    evaluation = {\\n        \"estimator\": name,\\n        \"train_time\": train_times.mean(),\\n    }\\n    evaluation_std = {\\n        \"estimator\": name,\\n        \"train_time\": train_times.std(),\\n    }\\n    for score_name, score_values in scores.items():\\n        mean_score, std_score = np.mean(score_values), np.std(score_values)\\n        print(f\"{score_name}: {mean_score:.3f} ¬± {std_score:.3f}\")\\n        evaluation[score_name] = mean_score\\n        evaluation_std[score_name] = std_score\\n    evaluations.append(evaluation)\\n    evaluations_std.append(evaluation_std)'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_clustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=======================================\\nClustering text documents using k-means\\n=======================================\\n\\nThis is an example showing how the scikit-learn API can be used to cluster\\ndocuments by topics using a `Bag of Words approach\\n<https://en.wikipedia.org/wiki/Bag-of-words_model>`_.\\n\\nTwo algorithms are demonstrated, namely :class:`~sklearn.cluster.KMeans` and its more\\nscalable variant, :class:`~sklearn.cluster.MiniBatchKMeans`. Additionally,\\nlatent semantic analysis is used to reduce dimensionality and discover latent\\npatterns in the data.\\n\\nThis example uses two different text vectorizers: a\\n:class:`~sklearn.feature_extraction.text.TfidfVectorizer` and a\\n:class:`~sklearn.feature_extraction.text.HashingVectorizer`. See the example\\nnotebook :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`\\nfor more information on vectorizers and a comparison of their processing times.\\n\\nFor document analysis via a supervised learning approach, see the example script\\n:ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Loading text data\\n# =================\\n#\\n# We load data from :ref:`20newsgroups_dataset`, which comprises around 18,000\\n# newsgroups posts on 20 topics. For illustrative purposes and to reduce the\\n# computational cost, we select a subset of 4 topics only accounting for around\\n# 3,400 documents. See the example\\n# :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\\n# to gain intuition on the overlap of such topics.\\n#\\n# Notice that, by default, the text samples contain some message metadata such\\n# as `\"headers\"`, `\"footers\"` (signatures) and `\"quotes\"` to other posts. We use\\n# the `remove` parameter from :func:`~sklearn.datasets.fetch_20newsgroups` to\\n# strip those features and have a more sensible clustering problem.\\n\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_20newsgroups\\n\\ncategories = [\\n    \"alt.atheism\",\\n    \"talk.religion.misc\",\\n    \"comp.graphics\",\\n    \"sci.space\",\\n]\\n\\ndataset = fetch_20newsgroups(\\n    remove=(\"headers\", \"footers\", \"quotes\"),\\n    subset=\"all\",\\n    categories=categories,\\n    shuffle=True,\\n    random_state=42,\\n)\\n\\nlabels = dataset.target\\nunique_labels, category_sizes = np.unique(labels, return_counts=True)\\ntrue_k = unique_labels.shape[0]\\n\\nprint(f\"{len(dataset.data)} documents - {true_k} categories\")'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_clustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='labels = dataset.target\\nunique_labels, category_sizes = np.unique(labels, return_counts=True)\\ntrue_k = unique_labels.shape[0]\\n\\nprint(f\"{len(dataset.data)} documents - {true_k} categories\")\\n\\n# %%\\n# Quantifying the quality of clustering results\\n# =============================================\\n#\\n# In this section we define a function to score different clustering pipelines\\n# using several metrics.\\n#\\n# Clustering algorithms are fundamentally unsupervised learning methods.\\n# However, since we happen to have class labels for this specific dataset, it is\\n# possible to use evaluation metrics that leverage this \"supervised\" ground\\n# truth information to quantify the quality of the resulting clusters. Examples\\n# of such metrics are the following:\\n#\\n# - homogeneity, which quantifies how much clusters contain only members of a\\n#   single class;\\n#\\n# - completeness, which quantifies how much members of a given class are\\n#   assigned to the same clusters;\\n#\\n# - V-measure, the harmonic mean of completeness and homogeneity;\\n#\\n# - Rand-Index, which measures how frequently pairs of data points are grouped\\n#   consistently according to the result of the clustering algorithm and the\\n#   ground truth class assignment;\\n#\\n# - Adjusted Rand-Index, a chance-adjusted Rand-Index such that random cluster\\n#   assignment have an ARI of 0.0 in expectation.\\n#\\n# If the ground truth labels are not known, evaluation can only be performed\\n# using the model results itself. In that case, the Silhouette Coefficient comes in\\n# handy. See :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`\\n# for an example on how to do it.\\n#\\n# For more reference, see :ref:`clustering_evaluation`.\\n\\nfrom collections import defaultdict\\nfrom time import time\\n\\nfrom sklearn import metrics\\n\\nevaluations = []\\nevaluations_std = []\\n\\n\\n# Code for: def fit_and_evaluate(km, X, name=None, n_runs=5):'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_clustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from collections import defaultdict\\nfrom time import time\\n\\nfrom sklearn import metrics\\n\\nevaluations = []\\nevaluations_std = []\\n\\n\\n# Code for: def fit_and_evaluate(km, X, name=None, n_runs=5):\\n\\n\\n# %%\\n# K-means clustering on text features\\n# ===================================\\n#\\n# Two feature extraction methods are used in this example:\\n#\\n# - :class:`~sklearn.feature_extraction.text.TfidfVectorizer` uses an in-memory\\n#   vocabulary (a Python dict) to map the most frequent words to features\\n#   indices and hence compute a word occurrence frequency (sparse) matrix. The\\n#   word frequencies are then reweighted using the Inverse Document Frequency\\n#   (IDF) vector collected feature-wise over the corpus.\\n#\\n# - :class:`~sklearn.feature_extraction.text.HashingVectorizer` hashes word\\n#   occurrences to a fixed dimensional space, possibly with collisions. The word\\n#   count vectors are then normalized to each have l2-norm equal to one\\n#   (projected to the euclidean unit-sphere) which seems to be important for\\n#   k-means to work in high dimensional space.\\n#\\n# Furthermore it is possible to post-process those extracted features using\\n# dimensionality reduction. We will explore the impact of those choices on the\\n# clustering quality in the following.\\n#\\n# Feature Extraction using TfidfVectorizer\\n# ----------------------------------------\\n#\\n# We first benchmark the estimators using a dictionary vectorizer along with an\\n# IDF normalization as provided by\\n# :class:`~sklearn.feature_extraction.text.TfidfVectorizer`.\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\nvectorizer = TfidfVectorizer(\\n    max_df=0.5,\\n    min_df=5,\\n    stop_words=\"english\",\\n)\\nt0 = time()\\nX_tfidf = vectorizer.fit_transform(dataset.data)\\n\\nprint(f\"vectorization done in {time() - t0:.3f} s\")\\nprint(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")\\n\\n# %%\\n# After ignoring terms that appear in more than 50% of the documents (as set by\\n# `max_df=0.5`) and terms that are not present in at least 5 documents (set by\\n# `min_df=5`), the resulting number of unique terms `n_features` is around\\n# 8,000. We can additionally quantify the sparsity of the `X_tfidf` matrix as\\n# the fraction of non-zero entries divided by the total number of elements.\\n\\nprint(f\"{X_tfidf.nnz / np.prod(X_tfidf.shape):.3f}\")'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_clustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='print(f\"{X_tfidf.nnz / np.prod(X_tfidf.shape):.3f}\")\\n\\n# %%\\n# We find that around 0.7% of the entries of the `X_tfidf` matrix are non-zero.\\n#\\n# .. _kmeans_sparse_high_dim:\\n#\\n# Clustering sparse data with k-means\\n# -----------------------------------\\n#\\n# As both :class:`~sklearn.cluster.KMeans` and\\n# :class:`~sklearn.cluster.MiniBatchKMeans` optimize a non-convex objective\\n# function, their clustering is not guaranteed to be optimal for a given random\\n# init. Even further, on sparse high-dimensional data such as text vectorized\\n# using the Bag of Words approach, k-means can initialize centroids on extremely\\n# isolated data points. Those data points can stay their own centroids all\\n# along.\\n#\\n# The following code illustrates how the previous phenomenon can sometimes lead\\n# to highly imbalanced clusters, depending on the random initialization:\\n\\nfrom sklearn.cluster import KMeans\\n\\nfor seed in range(5):\\n    kmeans = KMeans(\\n        n_clusters=true_k,\\n        max_iter=100,\\n        n_init=1,\\n        random_state=seed,\\n    ).fit(X_tfidf)\\n    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\\n    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\\nprint()\\nprint(\\n    \"True number of documents in each category according to the class labels: \"\\n    f\"{category_sizes}\"\\n)\\n\\n# %%\\n# To avoid this problem, one possibility is to increase the number of runs with\\n# independent random initiations `n_init`. In such case the clustering with the\\n# best inertia (objective function of k-means) is chosen.\\n\\nkmeans = KMeans(\\n    n_clusters=true_k,\\n    max_iter=100,\\n    n_init=5,\\n)\\n\\nfit_and_evaluate(kmeans, X_tfidf, name=\"KMeans\\\\non tf-idf vectors\")\\n\\n# %%\\n# All those clustering evaluation metrics have a maximum value of 1.0 (for a\\n# perfect clustering result). Higher values are better. Values of the Adjusted\\n# Rand-Index close to 0.0 correspond to a random labeling. Notice from the\\n# scores above that the cluster assignment is indeed well above chance level,\\n# but the overall quality can certainly improve.\\n#\\n# Keep in mind that the class labels may not reflect accurately the document\\n# topics and therefore metrics that use labels are not necessarily the best to\\n# evaluate the quality of our clustering pipeline.\\n#\\n# Performing dimensionality reduction using LSA\\n# ---------------------------------------------\\n#\\n# A `n_init=1` can still be used as long as the dimension of the vectorized\\n# space is reduced first to make k-means more stable. For such purpose we use\\n# :class:`~sklearn.decomposition.TruncatedSVD`, which works on term count/tf-idf\\n# matrices. Since SVD results are not normalized, we redo the normalization to\\n# improve the :class:`~sklearn.cluster.KMeans` result. Using SVD to reduce the\\n# dimensionality of TF-IDF document vectors is often known as `latent semantic\\n# analysis <https://en.wikipedia.org/wiki/Latent_semantic_analysis>`_ (LSA) in\\n# the information retrieval and text mining literature.'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_clustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.decomposition import TruncatedSVD\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import Normalizer\\n\\nlsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))\\nt0 = time()\\nX_lsa = lsa.fit_transform(X_tfidf)\\nexplained_variance = lsa[0].explained_variance_ratio_.sum()\\n\\nprint(f\"LSA done in {time() - t0:.3f} s\")\\nprint(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")\\n\\n# %%\\n# Using a single initialization means the processing time will be reduced for\\n# both :class:`~sklearn.cluster.KMeans` and\\n# :class:`~sklearn.cluster.MiniBatchKMeans`.\\n\\nkmeans = KMeans(\\n    n_clusters=true_k,\\n    max_iter=100,\\n    n_init=1,\\n)\\n\\nfit_and_evaluate(kmeans, X_lsa, name=\"KMeans\\\\nwith LSA on tf-idf vectors\")\\n\\n# %%\\n# We can observe that clustering on the LSA representation of the document is\\n# significantly faster (both because of `n_init=1` and because the\\n# dimensionality of the LSA feature space is much smaller). Furthermore, all the\\n# clustering evaluation metrics have improved. We repeat the experiment with\\n# :class:`~sklearn.cluster.MiniBatchKMeans`.\\n\\nfrom sklearn.cluster import MiniBatchKMeans\\n\\nminibatch_kmeans = MiniBatchKMeans(\\n    n_clusters=true_k,\\n    n_init=1,\\n    init_size=1000,\\n    batch_size=1000,\\n)\\n\\nfit_and_evaluate(\\n    minibatch_kmeans,\\n    X_lsa,\\n    name=\"MiniBatchKMeans\\\\nwith LSA on tf-idf vectors\",\\n)\\n\\n# %%\\n# Top terms per cluster\\n# ---------------------\\n#\\n# Since :class:`~sklearn.feature_extraction.text.TfidfVectorizer` can be\\n# inverted we can identify the cluster centers, which provide an intuition of\\n# the most influential words **for each cluster**. See the example script\\n# :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\\n# for a comparison with the most predictive words **for each target class**.\\n\\noriginal_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\\norder_centroids = original_space_centroids.argsort()[:, ::-1]\\nterms = vectorizer.get_feature_names_out()\\n\\nfor i in range(true_k):\\n    print(f\"Cluster {i}: \", end=\"\")\\n    for ind in order_centroids[i, :10]:\\n        print(f\"{terms[ind]} \", end=\"\")\\n    print()\\n\\n# %%\\n# HashingVectorizer\\n# -----------------\\n# An alternative vectorization can be done using a\\n# :class:`~sklearn.feature_extraction.text.HashingVectorizer` instance, which\\n# does not provide IDF weighting as this is a stateless model (the fit method\\n# does nothing). When IDF weighting is needed it can be added by pipelining the\\n# :class:`~sklearn.feature_extraction.text.HashingVectorizer` output to a\\n# :class:`~sklearn.feature_extraction.text.TfidfTransformer` instance. In this\\n# case we also add LSA to the pipeline to reduce the dimension and sparcity of\\n# the hashed vector space.\\n\\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_clustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\\n\\nlsa_vectorizer = make_pipeline(\\n    HashingVectorizer(stop_words=\"english\", n_features=50_000),\\n    TfidfTransformer(),\\n    TruncatedSVD(n_components=100, random_state=0),\\n    Normalizer(copy=False),\\n)\\n\\nt0 = time()\\nX_hashed_lsa = lsa_vectorizer.fit_transform(dataset.data)\\nprint(f\"vectorization done in {time() - t0:.3f} s\")\\n\\n# %%\\n# One can observe that the LSA step takes a relatively long time to fit,\\n# especially with hashed vectors. The reason is that a hashed space is typically\\n# large (set to `n_features=50_000` in this example). One can try lowering the\\n# number of features at the expense of having a larger fraction of features with\\n# hash collisions as shown in the example notebook\\n# :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\\n#\\n# We now fit and evaluate the `kmeans` and `minibatch_kmeans` instances on this\\n# hashed-lsa-reduced data:\\n\\nfit_and_evaluate(kmeans, X_hashed_lsa, name=\"KMeans\\\\nwith LSA on hashed vectors\")\\n\\n# %%\\nfit_and_evaluate(\\n    minibatch_kmeans,\\n    X_hashed_lsa,\\n    name=\"MiniBatchKMeans\\\\nwith LSA on hashed vectors\",\\n)\\n\\n# %%\\n# Both methods lead to good results that are similar to running the same models\\n# on the traditional LSA vectors (without hashing).\\n#\\n# Clustering evaluation summary\\n# ==============================\\n\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\nfig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16, 6), sharey=True)\\n\\ndf = pd.DataFrame(evaluations[::-1]).set_index(\"estimator\")\\ndf_std = pd.DataFrame(evaluations_std[::-1]).set_index(\"estimator\")\\n\\ndf.drop(\\n    [\"train_time\"],\\n    axis=\"columns\",\\n).plot.barh(ax=ax0, xerr=df_std)\\nax0.set_xlabel(\"Clustering scores\")\\nax0.set_ylabel(\"\")\\n\\ndf[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\\nax1.set_xlabel(\"Clustering time (s)\")\\nplt.tight_layout()'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_clustering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='df.drop(\\n    [\"train_time\"],\\n    axis=\"columns\",\\n).plot.barh(ax=ax0, xerr=df_std)\\nax0.set_xlabel(\"Clustering scores\")\\nax0.set_ylabel(\"\")\\n\\ndf[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\\nax1.set_xlabel(\"Clustering time (s)\")\\nplt.tight_layout()\\n\\n# %%\\n# :class:`~sklearn.cluster.KMeans` and :class:`~sklearn.cluster.MiniBatchKMeans`\\n# suffer from the phenomenon called the `Curse of Dimensionality\\n# <https://en.wikipedia.org/wiki/Curse_of_dimensionality>`_ for high dimensional\\n# datasets such as text data. That is the reason why the overall scores improve\\n# when using LSA. Using LSA reduced data also improves the stability and\\n# requires lower clustering time, though keep in mind that the LSA step itself\\n# takes a long time, especially with hashed vectors.\\n#\\n# The Silhouette Coefficient is defined between 0 and 1. In all cases we obtain\\n# values close to 0 (even if they improve a bit after using LSA) because its\\n# definition requires measuring distances, in contrast with other evaluation\\n# metrics such as the V-measure and the Adjusted Rand Index which are only based\\n# on cluster assignments rather than distances. Notice that strictly speaking,\\n# one should not compare the Silhouette Coefficient between spaces of different\\n# dimension, due to the different notions of distance they imply.\\n#\\n# The homogeneity, completeness and hence v-measure metrics do not yield a\\n# baseline with regards to random labeling: this means that depending on the\\n# number of samples, clusters and ground truth classes, a completely random\\n# labeling will not always yield the same values. In particular random labeling\\n# won\\'t yield zero scores, especially when the number of clusters is large. This\\n# problem can safely be ignored when the number of samples is more than a\\n# thousand and the number of clusters is less than 10, which is the case of the\\n# present example. For smaller sample sizes or larger number of clusters it is\\n# safer to use an adjusted index such as the Adjusted Rand Index (ARI). See the\\n# example\\n# :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py` for\\n# a demo on the effect of random labeling.\\n#\\n# The size of the error bars show that :class:`~sklearn.cluster.MiniBatchKMeans`\\n# is less stable than :class:`~sklearn.cluster.KMeans` for this relatively small\\n# dataset. It is more interesting to use when the number of samples is much\\n# bigger, but it can come at the expense of a small degradation in clustering\\n# quality compared to the traditional k-means algorithm.'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_classification_20newsgroups.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def size_mb(docs):\\n    return sum(len(s.encode(\"utf-8\")) for s in docs) / 1e6'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_classification_20newsgroups.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def load_dataset(verbose=False, remove=()):\\n    \"\"\"Load and vectorize the 20 newsgroups dataset.\"\"\"\\n\\n    data_train = fetch_20newsgroups(\\n        subset=\"train\",\\n        categories=categories,\\n        shuffle=True,\\n        random_state=42,\\n        remove=remove,\\n    )\\n\\n    data_test = fetch_20newsgroups(\\n        subset=\"test\",\\n        categories=categories,\\n        shuffle=True,\\n        random_state=42,\\n        remove=remove,\\n    )\\n\\n    # order of labels in `target_names` can be different from `categories`\\n    target_names = data_train.target_names\\n\\n    # split target in a training set and a test set\\n    y_train, y_test = data_train.target, data_test.target\\n\\n    # Extracting features from the training data using a sparse vectorizer\\n    t0 = time()\\n    vectorizer = TfidfVectorizer(\\n        sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\"\\n    )\\n    X_train = vectorizer.fit_transform(data_train.data)\\n    duration_train = time() - t0\\n\\n    # Extracting features from the test data using the same vectorizer\\n    t0 = time()\\n    X_test = vectorizer.transform(data_test.data)\\n    duration_test = time() - t0\\n\\n    feature_names = vectorizer.get_feature_names_out()\\n\\n    if verbose:\\n        # compute size of loaded data\\n        data_train_size_mb = size_mb(data_train.data)\\n        data_test_size_mb = size_mb(data_test.data)\\n\\n        print(\\n            f\"{len(data_train.data)} documents - \"\\n            f\"{data_train_size_mb:.2f}MB (training set)\"\\n        )\\n        print(f\"{len(data_test.data)} documents - {data_test_size_mb:.2f}MB (test set)\")\\n        print(f\"{len(target_names)} categories\")\\n        print(\\n            f\"vectorize training done in {duration_train:.3f}s \"\\n            f\"at {data_train_size_mb / duration_train:.3f}MB/s\"\\n        )\\n        print(f\"n_samples: {X_train.shape[0]}, n_features: {X_train.shape[1]}\")\\n        print(\\n            f\"vectorize testing done in {duration_test:.3f}s \"\\n            f\"at {data_test_size_mb / duration_test:.3f}MB/s\"\\n        )\\n        print(f\"n_samples: {X_test.shape[0]}, n_features: {X_test.shape[1]}\")\\n\\n    return X_train, X_test, y_train, y_test, feature_names, target_names'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_classification_20newsgroups.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_feature_effects():\\n    # learned coefficients weighted by frequency of appearance\\n    average_feature_effects = clf.coef_ * np.asarray(X_train.mean(axis=0)).ravel()\\n\\n    for i, label in enumerate(target_names):\\n        top5 = np.argsort(average_feature_effects[i])[-5:][::-1]\\n        if i == 0:\\n            top = pd.DataFrame(feature_names[top5], columns=[label])\\n            top_indices = top5\\n        else:\\n            top[label] = feature_names[top5]\\n            top_indices = np.concatenate((top_indices, top5), axis=None)\\n    top_indices = np.unique(top_indices)\\n    predictive_words = feature_names[top_indices]\\n\\n    # plot feature effects\\n    bar_size = 0.25\\n    padding = 0.75\\n    y_locs = np.arange(len(top_indices)) * (4 * bar_size + padding)\\n\\n    fig, ax = plt.subplots(figsize=(10, 8))\\n    for i, label in enumerate(target_names):\\n        ax.barh(\\n            y_locs + (i - 2) * bar_size,\\n            average_feature_effects[i, top_indices],\\n            height=bar_size,\\n            label=label,\\n        )\\n    ax.set(\\n        yticks=y_locs,\\n        yticklabels=predictive_words,\\n        ylim=[\\n            0 - 4 * bar_size,\\n            len(top_indices) * (4 * bar_size + padding) - 4 * bar_size,\\n        ],\\n    )\\n    ax.legend(loc=\"lower right\")\\n\\n    print(\"top 5 keywords per class:\")\\n    print(top)\\n\\n    return ax'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_classification_20newsgroups.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def benchmark(clf, custom_name=False):\\n    print(\"_\" * 80)\\n    print(\"Training: \")\\n    print(clf)\\n    t0 = time()\\n    clf.fit(X_train, y_train)\\n    train_time = time() - t0\\n    print(f\"train time: {train_time:.3}s\")\\n\\n    t0 = time()\\n    pred = clf.predict(X_test)\\n    test_time = time() - t0\\n    print(f\"test time:  {test_time:.3}s\")\\n\\n    score = metrics.accuracy_score(y_test, pred)\\n    print(f\"accuracy:   {score:.3}\")\\n\\n    if hasattr(clf, \"coef_\"):\\n        print(f\"dimensionality: {clf.coef_.shape[1]}\")\\n        print(f\"density: {density(clf.coef_)}\")\\n        print()\\n\\n    print()\\n    if custom_name:\\n        clf_descr = str(custom_name)\\n    else:\\n        clf_descr = clf.__class__.__name__\\n    return clf_descr, score, train_time, test_time'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_classification_20newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================================\\nClassification of text documents using sparse features\\n======================================================\\n\\nThis is an example showing how scikit-learn can be used to classify documents by\\ntopics using a `Bag of Words approach\\n<https://en.wikipedia.org/wiki/Bag-of-words_model>`_. This example uses a\\nTf-idf-weighted document-term sparse matrix to encode the features and\\ndemonstrates various classifiers that can efficiently handle sparse matrices.\\n\\nFor document analysis via an unsupervised learning approach, see the example\\nscript :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n\\n# %%\\n# Loading and vectorizing the 20 newsgroups text dataset\\n# ======================================================\\n#\\n# We define a function to load data from :ref:`20newsgroups_dataset`, which\\n# comprises around 18,000 newsgroups posts on 20 topics split in two subsets:\\n# one for training (or development) and the other one for testing (or for\\n# performance evaluation). Note that, by default, the text samples contain some\\n# message metadata such as `\\'headers\\'`, `\\'footers\\'` (signatures) and `\\'quotes\\'`\\n# to other posts. The `fetch_20newsgroups` function therefore accepts a\\n# parameter named `remove` to attempt stripping such information that can make\\n# the classification problem \"too easy\". This is achieved using simple\\n# heuristics that are neither perfect nor standard, hence disabled by default.\\n\\nfrom time import time\\n\\nfrom sklearn.datasets import fetch_20newsgroups\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\ncategories = [\\n    \"alt.atheism\",\\n    \"talk.religion.misc\",\\n    \"comp.graphics\",\\n    \"sci.space\",\\n]\\n\\n\\n# Code for: def size_mb(docs):\\n\\n\\n# Code for: def load_dataset(verbose=False, remove=()):\\n\\n\\n# %%\\n# Analysis of a bag-of-words document classifier\\n# ==============================================\\n#\\n# We will now train a classifier twice, once on the text samples including\\n# metadata and once after stripping the metadata. For both cases we will analyze\\n# the classification errors on a test set using a confusion matrix and inspect\\n# the coefficients that define the classification function of the trained\\n# models.\\n#\\n# Model without metadata stripping\\n# --------------------------------\\n#\\n# We start by using the custom function `load_dataset` to load the data without\\n# metadata stripping.\\n\\nX_train, X_test, y_train, y_test, feature_names, target_names = load_dataset(\\n    verbose=True\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_classification_20newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X_train, X_test, y_train, y_test, feature_names, target_names = load_dataset(\\n    verbose=True\\n)\\n\\n# %%\\n# Our first model is an instance of the\\n# :class:`~sklearn.linear_model.RidgeClassifier` class. This is a linear\\n# classification model that uses the mean squared error on {-1, 1} encoded\\n# targets, one for each possible class. Contrary to\\n# :class:`~sklearn.linear_model.LogisticRegression`,\\n# :class:`~sklearn.linear_model.RidgeClassifier` does not\\n# provide probabilistic predictions (no `predict_proba` method),\\n# but it is often faster to train.\\n\\nfrom sklearn.linear_model import RidgeClassifier\\n\\nclf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\\nclf.fit(X_train, y_train)\\npred = clf.predict(X_test)\\n\\n# %%\\n# We plot the confusion matrix of this classifier to find if there is a pattern\\n# in the classification errors.\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.metrics import ConfusionMatrixDisplay\\n\\nfig, ax = plt.subplots(figsize=(10, 5))\\nConfusionMatrixDisplay.from_predictions(y_test, pred, ax=ax)\\nax.xaxis.set_ticklabels(target_names)\\nax.yaxis.set_ticklabels(target_names)\\n_ = ax.set_title(\\n    f\"Confusion Matrix for {clf.__class__.__name__}\\\\non the original documents\"\\n)\\n\\n# %%\\n# The confusion matrix highlights that documents of the `alt.atheism` class are\\n# often confused with documents with the class `talk.religion.misc` class and\\n# vice-versa which is expected since the topics are semantically related.\\n#\\n# We also observe that some documents of the `sci.space` class can be misclassified as\\n# `comp.graphics` while the converse is much rarer. A manual inspection of those\\n# badly classified documents would be required to get some insights on this\\n# asymmetry. It could be the case that the vocabulary of the space topic could\\n# be more specific than the vocabulary for computer graphics.\\n#\\n# We can gain a deeper understanding of how this classifier makes its decisions\\n# by looking at the words with the highest average feature effects:\\n\\nimport numpy as np\\nimport pandas as pd\\n\\n\\n# Code for: def plot_feature_effects():\\n\\n\\n_ = plot_feature_effects().set_title(\"Average feature effect on the original data\")'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_classification_20newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='import numpy as np\\nimport pandas as pd\\n\\n\\n# Code for: def plot_feature_effects():\\n\\n\\n_ = plot_feature_effects().set_title(\"Average feature effect on the original data\")\\n\\n# %%\\n# We can observe that the most predictive words are often strongly positively\\n# associated with a single class and negatively associated with all the other\\n# classes. Most of those positive associations are quite easy to interpret.\\n# However, some words such as `\"god\"` and `\"people\"` are positively associated to\\n# both `\"talk.misc.religion\"` and `\"alt.atheism\"` as those two classes expectedly\\n# share some common vocabulary. Notice however that there are also words such as\\n# `\"christian\"` and `\"morality\"` that are only positively associated with\\n# `\"talk.misc.religion\"`. Furthermore, in this version of the dataset, the word\\n# `\"caltech\"` is one of the top predictive features for atheism due to pollution\\n# in the dataset coming from some sort of metadata such as the email addresses\\n# of the sender of previous emails in the discussion as can be seen below:\\n\\ndata_train = fetch_20newsgroups(\\n    subset=\"train\", categories=categories, shuffle=True, random_state=42\\n)\\n\\nfor doc in data_train.data:\\n    if \"caltech\" in doc:\\n        print(doc)\\n        break\\n\\n# %%\\n# Such headers, signature footers (and quoted metadata from previous messages)\\n# can be considered side information that artificially reveals the newsgroup by\\n# identifying the registered members and one would rather want our text\\n# classifier to only learn from the \"main content\" of each text document instead\\n# of relying on the leaked identity of the writers.\\n#\\n# Model with metadata stripping\\n# -----------------------------\\n#\\n# The `remove` option of the 20 newsgroups dataset loader in scikit-learn allows\\n# to heuristically attempt to filter out some of this unwanted metadata that\\n# makes the classification problem artificially easier. Be aware that such\\n# filtering of the text contents is far from perfect.\\n#\\n# Let us try to leverage this option to train a text classifier that does not\\n# rely too much on this kind of metadata to make its decisions:\\n(\\n    X_train,\\n    X_test,\\n    y_train,\\n    y_test,\\n    feature_names,\\n    target_names,\\n) = load_dataset(remove=(\"headers\", \"footers\", \"quotes\"))\\n\\nclf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\\nclf.fit(X_train, y_train)\\npred = clf.predict(X_test)\\n\\nfig, ax = plt.subplots(figsize=(10, 5))\\nConfusionMatrixDisplay.from_predictions(y_test, pred, ax=ax)\\nax.xaxis.set_ticklabels(target_names)\\nax.yaxis.set_ticklabels(target_names)\\n_ = ax.set_title(\\n    f\"Confusion Matrix for {clf.__class__.__name__}\\\\non filtered documents\"\\n)\\n\\n# %%\\n# By looking at the confusion matrix, it is more evident that the scores of the\\n# model trained with metadata were over-optimistic. The classification problem\\n# without access to the metadata is less accurate but more representative of the\\n# intended text classification problem.'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_classification_20newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# By looking at the confusion matrix, it is more evident that the scores of the\\n# model trained with metadata were over-optimistic. The classification problem\\n# without access to the metadata is less accurate but more representative of the\\n# intended text classification problem.\\n\\n_ = plot_feature_effects().set_title(\"Average feature effects on filtered documents\")\\n\\n# %%\\n# In the next section we keep the dataset without metadata to compare several\\n# classifiers.\\n\\n# %%\\n# Benchmarking classifiers\\n# ========================\\n#\\n# Scikit-learn provides many different kinds of classification algorithms. In\\n# this section we will train a selection of those classifiers on the same text\\n# classification problem and measure both their generalization performance\\n# (accuracy on the test set) and their computation performance (speed), both at\\n# training time and testing time. For such purpose we define the following\\n# benchmarking utilities:\\n\\nfrom sklearn import metrics\\nfrom sklearn.utils.extmath import density\\n\\n\\n# Code for: def benchmark(clf, custom_name=False):\\n\\n\\n# %%\\n# We now train and test the datasets with 8 different classification models and\\n# get performance results for each model. The goal of this study is to highlight\\n# the computation/accuracy tradeoffs of different types of classifiers for\\n# such a multi-class text classification problem.\\n#\\n# Notice that the most important hyperparameters values were tuned using a grid\\n# search procedure not shown in this notebook for the sake of simplicity. See\\n# the example script\\n# :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`  # noqa: E501\\n# for a demo on how such tuning can be done.\\n\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\\nfrom sklearn.naive_bayes import ComplementNB\\nfrom sklearn.neighbors import KNeighborsClassifier, NearestCentroid\\nfrom sklearn.svm import LinearSVC\\n\\nresults = []\\nfor clf, name in (\\n    (LogisticRegression(C=5, max_iter=1000), \"Logistic Regression\"),\\n    (RidgeClassifier(alpha=1.0, solver=\"sparse_cg\"), \"Ridge Classifier\"),\\n    (KNeighborsClassifier(n_neighbors=100), \"kNN\"),\\n    (RandomForestClassifier(), \"Random Forest\"),\\n    # L2 penalty Linear SVC\\n    (LinearSVC(C=0.1, dual=False, max_iter=1000), \"Linear SVC\"),\\n    # L2 penalty Linear SGD\\n    (\\n        SGDClassifier(\\n            loss=\"log_loss\", alpha=1e-4, n_iter_no_change=3, early_stopping=True\\n        ),\\n        \"log-loss SGD\",\\n    ),\\n    # NearestCentroid (aka Rocchio classifier)\\n    (NearestCentroid(), \"NearestCentroid\"),\\n    # Sparse naive Bayes classifier\\n    (ComplementNB(alpha=0.1), \"Complement naive Bayes\"),\\n):\\n    print(\"=\" * 80)\\n    print(name)\\n    results.append(benchmark(clf, name))'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_document_classification_20newsgroups.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Plot accuracy, training and test time of each classifier\\n# ========================================================\\n#\\n# The scatter plots show the trade-off between the test accuracy and the\\n# training and testing time of each classifier.\\n\\nindices = np.arange(len(results))\\n\\nresults = [[x[i] for x in results] for i in range(4)]\\n\\nclf_names, score, training_time, test_time = results\\ntraining_time = np.array(training_time)\\ntest_time = np.array(test_time)\\n\\nfig, ax1 = plt.subplots(figsize=(10, 8))\\nax1.scatter(score, training_time, s=60)\\nax1.set(\\n    title=\"Score-training time trade-off\",\\n    yscale=\"log\",\\n    xlabel=\"test accuracy\",\\n    ylabel=\"training time (s)\",\\n)\\nfig, ax2 = plt.subplots(figsize=(10, 8))\\nax2.scatter(score, test_time, s=60)\\nax2.set(\\n    title=\"Score-test time trade-off\",\\n    yscale=\"log\",\\n    xlabel=\"test accuracy\",\\n    ylabel=\"test time (s)\",\\n)\\n\\nfor i, txt in enumerate(clf_names):\\n    ax1.annotate(txt, (score[i], training_time[i]))\\n    ax2.annotate(txt, (score[i], test_time[i]))\\n\\n# %%\\n# The naive Bayes model has the best trade-off between score and\\n# training/testing time, while Random Forest is both slow to train, expensive to\\n# predict and has a comparatively bad accuracy. This is expected: for\\n# high-dimensional prediction problems, linear models are often better suited as\\n# most problems become linearly separable when the feature space has 10,000\\n# dimensions or more.\\n#\\n# The difference in training speed and accuracy of the linear models can be\\n# explained by the choice of the loss function they optimize and the kind of\\n# regularization they use. Be aware that some linear models with the same loss\\n# but a different solver or regularization configuration may yield different\\n# fitting times and test accuracy. We can observe on the second plot that once\\n# trained, all linear models have approximately the same prediction speed which\\n# is expected because they all implement the same prediction function.\\n#\\n# KNeighborsClassifier has a relatively low accuracy and has the highest testing\\n# time. The long prediction time is also expected: for each prediction the model\\n# has to compute the pairwise distances between the testing sample and each\\n# document in the training set, which is computationally expensive. Furthermore,\\n# the \"curse of dimensionality\" harms the ability of this model to yield\\n# competitive accuracy in the high dimensional feature space of text\\n# classification problems.'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_hashing_vs_dict_vectorizer.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def tokenize(doc):\\n    \"\"\"Extract tokens from doc.\\n\\n    This uses a simple regex that matches word characters to break strings\\n    into tokens. For a more principled approach, see CountVectorizer or\\n    TfidfVectorizer.\\n    \"\"\"\\n    return (tok.lower() for tok in re.findall(r\"\\\\w+\", doc))'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_hashing_vs_dict_vectorizer.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def token_freqs(doc):\\n    \"\"\"Extract a dict mapping tokens from doc to their occurrences.\"\"\"\\n\\n    freq = defaultdict(int)\\n    for tok in tokenize(doc):\\n        freq[tok] += 1\\n    return freq'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_hashing_vs_dict_vectorizer.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def n_nonzero_columns(X):\\n    \"\"\"Number of columns with at least one non-zero value in a CSR matrix.\\n\\n    This is useful to count the number of features columns that are effectively\\n    active when using the FeatureHasher.\\n    \"\"\"\\n    return len(np.unique(X.nonzero()[1]))'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_hashing_vs_dict_vectorizer.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================\\nFeatureHasher and DictVectorizer Comparison\\n===========================================\\n\\nIn this example we illustrate text vectorization, which is the process of\\nrepresenting non-numerical input data (such as dictionaries or text documents)\\nas vectors of real numbers.\\n\\nWe first compare :func:`~sklearn.feature_extraction.FeatureHasher` and\\n:func:`~sklearn.feature_extraction.DictVectorizer` by using both methods to\\nvectorize text documents that are preprocessed (tokenized) with the help of a\\ncustom Python function.\\n\\nLater we introduce and analyze the text-specific vectorizers\\n:func:`~sklearn.feature_extraction.text.HashingVectorizer`,\\n:func:`~sklearn.feature_extraction.text.CountVectorizer` and\\n:func:`~sklearn.feature_extraction.text.TfidfVectorizer` that handle both the\\ntokenization and the assembling of the feature matrix within a single class.\\n\\nThe objective of the example is to demonstrate the usage of text vectorization\\nAPI and to compare their processing time. See the example scripts\\n:ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\\nand :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py` for actual\\nlearning on text documents.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Load Data\\n# ---------\\n#\\n# We load data from :ref:`20newsgroups_dataset`, which comprises around\\n# 18000 newsgroups posts on 20 topics split in two subsets: one for training and\\n# one for testing. For the sake of simplicity and reducing the computational\\n# cost, we select a subset of 7 topics and use the training set only.\\n\\nfrom sklearn.datasets import fetch_20newsgroups\\n\\ncategories = [\\n    \"alt.atheism\",\\n    \"comp.graphics\",\\n    \"comp.sys.ibm.pc.hardware\",\\n    \"misc.forsale\",\\n    \"rec.autos\",\\n    \"sci.space\",\\n    \"talk.religion.misc\",\\n]\\n\\nprint(\"Loading 20 newsgroups training data\")\\nraw_data, _ = fetch_20newsgroups(subset=\"train\", categories=categories, return_X_y=True)\\ndata_size_mb = sum(len(s.encode(\"utf-8\")) for s in raw_data) / 1e6\\nprint(f\"{len(raw_data)} documents - {data_size_mb:.3f}MB\")\\n\\n# %%\\n# Define preprocessing functions\\n# ------------------------------\\n#\\n# A token may be a word, part of a word or anything comprised between spaces or\\n# symbols in a string. Here we define a function that extracts the tokens using\\n# a simple regular expression (regex) that matches Unicode word characters. This\\n# includes most characters that can be part of a word in any language, as well\\n# as numbers and the underscore:\\n\\nimport re\\n\\n\\n# Code for: def tokenize(doc):\\n\\n\\nlist(tokenize(\"This is a simple example, isn\\'t it?\"))\\n\\n# %%\\n# We define an additional function that counts the (frequency of) occurrence of\\n# each token in a given document. It returns a frequency dictionary to be used\\n# by the vectorizers.\\n\\nfrom collections import defaultdict\\n\\n\\n# Code for: def token_freqs(doc):\\n\\n\\ntoken_freqs(\"That is one example, but this is another one\")'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_hashing_vs_dict_vectorizer.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from collections import defaultdict\\n\\n\\n# Code for: def token_freqs(doc):\\n\\n\\ntoken_freqs(\"That is one example, but this is another one\")\\n\\n# %%\\n# Observe in particular that the repeated token `\"is\"` is counted twice for\\n# instance.\\n#\\n# Breaking a text document into word tokens, potentially losing the order\\n# information between the words in a sentence is often called a `Bag of Words\\n# representation <https://en.wikipedia.org/wiki/Bag-of-words_model>`_.\\n\\n# %%\\n# DictVectorizer\\n# --------------\\n#\\n# First we benchmark the :func:`~sklearn.feature_extraction.DictVectorizer`,\\n# then we compare it to :func:`~sklearn.feature_extraction.FeatureHasher` as\\n# both of them receive dictionaries as input.\\n\\nfrom time import time\\n\\nfrom sklearn.feature_extraction import DictVectorizer\\n\\ndict_count_vectorizers = defaultdict(list)\\n\\nt0 = time()\\nvectorizer = DictVectorizer()\\nvectorizer.fit_transform(token_freqs(d) for d in raw_data)\\nduration = time() - t0\\ndict_count_vectorizers[\"vectorizer\"].append(\\n    vectorizer.__class__.__name__ + \"\\\\non freq dicts\"\\n)\\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\\nprint(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")\\n\\n# %%\\n# The actual mapping from text token to column index is explicitly stored in\\n# the `.vocabulary_` attribute which is a potentially very large Python\\n# dictionary:\\ntype(vectorizer.vocabulary_)\\n\\n# %%\\nlen(vectorizer.vocabulary_)\\n\\n# %%\\nvectorizer.vocabulary_[\"example\"]\\n\\n# %%\\n# FeatureHasher\\n# -------------\\n#\\n# Dictionaries take up a large amount of storage space and grow in size as the\\n# training set grows. Instead of growing the vectors along with a dictionary,\\n# feature hashing builds a vector of pre-defined length by applying a hash\\n# function `h` to the features (e.g., tokens), then using the hash values\\n# directly as feature indices and updating the resulting vector at those\\n# indices. When the feature space is not large enough, hashing functions tend to\\n# map distinct values to the same hash code (hash collisions). As a result, it\\n# is impossible to determine what object generated any particular hash code.\\n#\\n# Because of the above it is impossible to recover the original tokens from the\\n# feature matrix and the best approach to estimate the number of unique terms in\\n# the original dictionary is to count the number of active columns in the\\n# encoded feature matrix. For such a purpose we define the following function:\\n\\nimport numpy as np\\n\\n\\n# Code for: def n_nonzero_columns(X):\\n\\n\\n# %%\\n# The default number of features for the\\n# :func:`~sklearn.feature_extraction.FeatureHasher` is 2**20. Here we set\\n# `n_features = 2**18` to illustrate hash collisions.\\n#\\n# **FeatureHasher on frequency dictionaries**\\n\\nfrom sklearn.feature_extraction import FeatureHasher'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_hashing_vs_dict_vectorizer.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# The default number of features for the\\n# :func:`~sklearn.feature_extraction.FeatureHasher` is 2**20. Here we set\\n# `n_features = 2**18` to illustrate hash collisions.\\n#\\n# **FeatureHasher on frequency dictionaries**\\n\\nfrom sklearn.feature_extraction import FeatureHasher\\n\\nt0 = time()\\nhasher = FeatureHasher(n_features=2**18)\\nX = hasher.transform(token_freqs(d) for d in raw_data)\\nduration = time() - t0\\ndict_count_vectorizers[\"vectorizer\"].append(\\n    hasher.__class__.__name__ + \"\\\\non freq dicts\"\\n)\\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\\nprint(f\"Found {n_nonzero_columns(X)} unique tokens\")\\n\\n# %%\\n# The number of unique tokens when using the\\n# :func:`~sklearn.feature_extraction.FeatureHasher` is lower than those obtained\\n# using the :func:`~sklearn.feature_extraction.DictVectorizer`. This is due to\\n# hash collisions.\\n#\\n# The number of collisions can be reduced by increasing the feature space.\\n# Notice that the speed of the vectorizer does not change significantly when\\n# setting a large number of features, though it causes larger coefficient\\n# dimensions and then requires more memory usage to store them, even if a\\n# majority of them is inactive.\\n\\nt0 = time()\\nhasher = FeatureHasher(n_features=2**22)\\nX = hasher.transform(token_freqs(d) for d in raw_data)\\nduration = time() - t0\\n\\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\\nprint(f\"Found {n_nonzero_columns(X)} unique tokens\")\\n\\n# %%\\n# We confirm that the number of unique tokens gets closer to the number of\\n# unique terms found by the :func:`~sklearn.feature_extraction.DictVectorizer`.\\n#\\n# **FeatureHasher on raw tokens**\\n#\\n# Alternatively, one can set `input_type=\"string\"` in the\\n# :func:`~sklearn.feature_extraction.FeatureHasher` to vectorize the strings\\n# output directly from the customized `tokenize` function. This is equivalent to\\n# passing a dictionary with an implied frequency of 1 for each feature name.\\n\\nt0 = time()\\nhasher = FeatureHasher(n_features=2**18, input_type=\"string\")\\nX = hasher.transform(tokenize(d) for d in raw_data)\\nduration = time() - t0\\ndict_count_vectorizers[\"vectorizer\"].append(\\n    hasher.__class__.__name__ + \"\\\\non raw tokens\"\\n)\\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\\nprint(f\"Found {n_nonzero_columns(X)} unique tokens\")\\n\\n# %%\\n# We now plot the speed of the above methods for vectorizing.\\n\\nimport matplotlib.pyplot as plt\\n\\nfig, ax = plt.subplots(figsize=(12, 6))\\n\\ny_pos = np.arange(len(dict_count_vectorizers[\"vectorizer\"]))\\nax.barh(y_pos, dict_count_vectorizers[\"speed\"], align=\"center\")\\nax.set_yticks(y_pos)\\nax.set_yticklabels(dict_count_vectorizers[\"vectorizer\"])\\nax.invert_yaxis()\\n_ = ax.set_xlabel(\"speed (MB/s)\")'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_hashing_vs_dict_vectorizer.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig, ax = plt.subplots(figsize=(12, 6))\\n\\ny_pos = np.arange(len(dict_count_vectorizers[\"vectorizer\"]))\\nax.barh(y_pos, dict_count_vectorizers[\"speed\"], align=\"center\")\\nax.set_yticks(y_pos)\\nax.set_yticklabels(dict_count_vectorizers[\"vectorizer\"])\\nax.invert_yaxis()\\n_ = ax.set_xlabel(\"speed (MB/s)\")\\n\\n# %%\\n# In both cases :func:`~sklearn.feature_extraction.FeatureHasher` is\\n# approximately twice as fast as\\n# :func:`~sklearn.feature_extraction.DictVectorizer`. This is handy when dealing\\n# with large amounts of data, with the downside of losing the invertibility of\\n# the transformation, which in turn makes the interpretation of a model a more\\n# complex task.\\n#\\n# The `FeatureHeasher` with `input_type=\"string\"` is slightly faster than the\\n# variant that works on frequency dict because it does not count repeated\\n# tokens: each token is implicitly counted once, even if it was repeated.\\n# Depending on the downstream machine learning task, it can be a limitation or\\n# not.\\n#\\n# Comparison with special purpose text vectorizers\\n# ------------------------------------------------\\n#\\n# :func:`~sklearn.feature_extraction.text.CountVectorizer` accepts raw data as\\n# it internally implements tokenization and occurrence counting. It is similar\\n# to the :func:`~sklearn.feature_extraction.DictVectorizer` when used along with\\n# the customized function `token_freqs` as done in the previous section. The\\n# difference being that :func:`~sklearn.feature_extraction.text.CountVectorizer`\\n# is more flexible. In particular it accepts various regex patterns through the\\n# `token_pattern` parameter.\\n\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n\\nt0 = time()\\nvectorizer = CountVectorizer()\\nvectorizer.fit_transform(raw_data)\\nduration = time() - t0\\ndict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\\nprint(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")\\n\\n# %%\\n# We see that using the :func:`~sklearn.feature_extraction.text.CountVectorizer`\\n# implementation is approximately twice as fast as using the\\n# :func:`~sklearn.feature_extraction.DictVectorizer` along with the simple\\n# function we defined for mapping the tokens. The reason is that\\n# :func:`~sklearn.feature_extraction.text.CountVectorizer` is optimized by\\n# reusing a compiled regular expression for the full training set instead of\\n# creating one per document as done in our naive tokenize function.\\n#\\n# Now we make a similar experiment with the\\n# :func:`~sklearn.feature_extraction.text.HashingVectorizer`, which is\\n# equivalent to combining the \"hashing trick\" implemented by the\\n# :func:`~sklearn.feature_extraction.FeatureHasher` class and the text\\n# preprocessing and tokenization of the\\n# :func:`~sklearn.feature_extraction.text.CountVectorizer`.\\n\\nfrom sklearn.feature_extraction.text import HashingVectorizer'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_hashing_vs_dict_vectorizer.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.feature_extraction.text import HashingVectorizer\\n\\nt0 = time()\\nvectorizer = HashingVectorizer(n_features=2**18)\\nvectorizer.fit_transform(raw_data)\\nduration = time() - t0\\ndict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\\n\\n# %%\\n# We can observe that this is the fastest text tokenization strategy so far,\\n# assuming that the downstream machine learning task can tolerate a few\\n# collisions.\\n#\\n# TfidfVectorizer\\n# ---------------\\n#\\n# In a large text corpus, some words appear with higher frequency (e.g. \"the\",\\n# \"a\", \"is\" in English) and do not carry meaningful information about the actual\\n# contents of a document. If we were to feed the word count data directly to a\\n# classifier, those very common terms would shadow the frequencies of rarer yet\\n# more informative terms. In order to re-weight the count features into floating\\n# point values suitable for usage by a classifier it is very common to use the\\n# tf-idf transform as implemented by the\\n# :func:`~sklearn.feature_extraction.text.TfidfTransformer`. TF stands for\\n# \"term-frequency\" while \"tf-idf\" means term-frequency times inverse\\n# document-frequency.\\n#\\n# We now benchmark the :func:`~sklearn.feature_extraction.text.TfidfVectorizer`,\\n# which is equivalent to combining the tokenization and occurrence counting of\\n# the :func:`~sklearn.feature_extraction.text.CountVectorizer` along with the\\n# normalizing and weighting from a\\n# :func:`~sklearn.feature_extraction.text.TfidfTransformer`.\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\nt0 = time()\\nvectorizer = TfidfVectorizer()\\nvectorizer.fit_transform(raw_data)\\nduration = time() - t0\\ndict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\\nprint(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")\\n\\n# %%\\n# Summary\\n# -------\\n# Let\\'s conclude this notebook by summarizing all the recorded processing speeds\\n# in a single plot:\\n\\nfig, ax = plt.subplots(figsize=(12, 6))\\n\\ny_pos = np.arange(len(dict_count_vectorizers[\"vectorizer\"]))\\nax.barh(y_pos, dict_count_vectorizers[\"speed\"], align=\"center\")\\nax.set_yticks(y_pos)\\nax.set_yticklabels(dict_count_vectorizers[\"vectorizer\"])\\nax.invert_yaxis()\\n_ = ax.set_xlabel(\"speed (MB/s)\")'), Document(metadata={'source': '/content/local_copy_repo/examples/text/plot_hashing_vs_dict_vectorizer.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig, ax = plt.subplots(figsize=(12, 6))\\n\\ny_pos = np.arange(len(dict_count_vectorizers[\"vectorizer\"]))\\nax.barh(y_pos, dict_count_vectorizers[\"speed\"], align=\"center\")\\nax.set_yticks(y_pos)\\nax.set_yticklabels(dict_count_vectorizers[\"vectorizer\"])\\nax.invert_yaxis()\\n_ = ax.set_xlabel(\"speed (MB/s)\")\\n\\n# %%\\n# Notice from the plot that\\n# :func:`~sklearn.feature_extraction.text.TfidfVectorizer` is slightly slower\\n# than :func:`~sklearn.feature_extraction.text.CountVectorizer` because of the\\n# extra operation induced by the\\n# :func:`~sklearn.feature_extraction.text.TfidfTransformer`.\\n#\\n# Also notice that, by setting the number of features `n_features = 2**18`, the\\n# :func:`~sklearn.feature_extraction.text.HashingVectorizer` performs better\\n# than the :func:`~sklearn.feature_extraction.text.CountVectorizer` at the\\n# expense of inversibility of the transformation due to hash collisions.\\n#\\n# We highlight that :func:`~sklearn.feature_extraction.text.CountVectorizer` and\\n# :func:`~sklearn.feature_extraction.text.HashingVectorizer` perform better than\\n# their equivalent :func:`~sklearn.feature_extraction.DictVectorizer` and\\n# :func:`~sklearn.feature_extraction.FeatureHasher` on manually tokenized\\n# documents since the internal tokenization step of the former vectorizers\\n# compiles a regular expression once and then reuses it for all the documents.'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/wikipedia_principal_eigenvector.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def index(redirects, index_map, k):\\n    \"\"\"Find the index of an article name after redirect resolution\"\"\"\\n    k = redirects.get(k, k)\\n    return index_map.setdefault(k, len(index_map))'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/wikipedia_principal_eigenvector.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def short_name(nt_uri):\\n    \"\"\"Remove the < and > URI markers and the common URI prefix\"\"\"\\n    return nt_uri[SHORTNAME_SLICE]'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/wikipedia_principal_eigenvector.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_redirects(redirects_filename):\\n    \"\"\"Parse the redirections and build a transitively closed map out of it\"\"\"\\n    redirects = {}\\n    print(\"Parsing the NT redirect file\")\\n    for l, line in enumerate(BZ2File(redirects_filename)):\\n        split = line.split()\\n        if len(split) != 4:\\n            print(\"ignoring malformed line: \" + line)\\n            continue\\n        redirects[short_name(split[0])] = short_name(split[2])\\n        if l % 1000000 == 0:\\n            print(\"[%s] line: %08d\" % (datetime.now().isoformat(), l))\\n\\n    # compute the transitive closure\\n    print(\"Computing the transitive closure of the redirect relation\")\\n    for l, source in enumerate(redirects.keys()):\\n        transitive_target = None\\n        target = redirects[source]\\n        seen = {source}\\n        while True:\\n            transitive_target = target\\n            target = redirects.get(target)\\n            if target is None or target in seen:\\n                break\\n            seen.add(target)\\n        redirects[source] = transitive_target\\n        if l % 1000000 == 0:\\n            print(\"[%s] line: %08d\" % (datetime.now().isoformat(), l))\\n\\n    return redirects'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/wikipedia_principal_eigenvector.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_adjacency_matrix(redirects_filename, page_links_filename, limit=None):\\n    \"\"\"Extract the adjacency graph as a scipy sparse matrix\\n\\n    Redirects are resolved first.\\n\\n    Returns X, the scipy sparse adjacency matrix, redirects as python\\n    dict from article names to article names and index_map a python dict\\n    from article names to python int (article indexes).\\n    \"\"\"\\n\\n    print(\"Computing the redirect map\")\\n    redirects = get_redirects(redirects_filename)\\n\\n    print(\"Computing the integer index map\")\\n    index_map = dict()\\n    links = list()\\n    for l, line in enumerate(BZ2File(page_links_filename)):\\n        split = line.split()\\n        if len(split) != 4:\\n            print(\"ignoring malformed line: \" + line)\\n            continue\\n        i = index(redirects, index_map, short_name(split[0]))\\n        j = index(redirects, index_map, short_name(split[2]))\\n        links.append((i, j))\\n        if l % 1000000 == 0:\\n            print(\"[%s] line: %08d\" % (datetime.now().isoformat(), l))\\n\\n        if limit is not None and l >= limit - 1:\\n            break\\n\\n    print(\"Computing the adjacency matrix\")\\n    X = sparse.lil_matrix((len(index_map), len(index_map)), dtype=np.float32)\\n    for i, j in links:\\n        X[i, j] = 1.0\\n    del links\\n    print(\"Converting to CSR representation\")\\n    X = X.tocsr()\\n    print(\"CSR conversion done\")\\n    return X, redirects, index_map'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/wikipedia_principal_eigenvector.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def centrality_scores(X, alpha=0.85, max_iter=100, tol=1e-10):\\n    \"\"\"Power iteration computation of the principal eigenvector\\n\\n    This method is also known as Google PageRank and the implementation\\n    is based on the one from the NetworkX project (BSD licensed too)\\n    with copyrights by:\\n\\n      Aric Hagberg <hagberg@lanl.gov>\\n      Dan Schult <dschult@colgate.edu>\\n      Pieter Swart <swart@lanl.gov>\\n    \"\"\"\\n    n = X.shape[0]\\n    X = X.copy()\\n    incoming_counts = np.asarray(X.sum(axis=1)).ravel()\\n\\n    print(\"Normalizing the graph\")\\n    for i in incoming_counts.nonzero()[0]:\\n        X.data[X.indptr[i] : X.indptr[i + 1]] *= 1.0 / incoming_counts[i]\\n    dangle = np.asarray(np.where(np.isclose(X.sum(axis=1), 0), 1.0 / n, 0)).ravel()\\n\\n    scores = np.full(n, 1.0 / n, dtype=np.float32)  # initial guess\\n    for i in range(max_iter):\\n        print(\"power iteration #%d\" % i)\\n        prev_scores = scores\\n        scores = (\\n            alpha * (scores * X + np.dot(dangle, prev_scores))\\n            + (1 - alpha) * prev_scores.sum() / n\\n        )\\n        # check convergence: normalized l_inf norm\\n        scores_max = np.abs(scores).max()\\n        if scores_max == 0.0:\\n            scores_max = 1.0\\n        err = np.abs(scores - prev_scores).max() / scores_max\\n        print(\"error: %0.6f\" % err)\\n        if err < n * tol:\\n            return scores\\n\\n    return scores'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/wikipedia_principal_eigenvector.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================\\nWikipedia principal eigenvector\\n===============================\\n\\nA classical way to assert the relative importance of vertices in a\\ngraph is to compute the principal eigenvector of the adjacency matrix\\nso as to assign to each vertex the values of the components of the first\\neigenvector as a centrality score:\\n\\n    https://en.wikipedia.org/wiki/Eigenvector_centrality\\n\\nOn the graph of webpages and links those values are called the PageRank\\nscores by Google.\\n\\nThe goal of this example is to analyze the graph of links inside\\nwikipedia articles to rank articles by relative importance according to\\nthis eigenvector centrality.\\n\\nThe traditional way to compute the principal eigenvector is to use the\\npower iteration method:\\n\\n    https://en.wikipedia.org/wiki/Power_iteration\\n\\nHere the computation is achieved thanks to Martinsson\\'s Randomized SVD\\nalgorithm implemented in scikit-learn.\\n\\nThe graph data is fetched from the DBpedia dumps. DBpedia is an extraction\\nof the latent structured data of the Wikipedia content.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport os\\nfrom bz2 import BZ2File\\nfrom datetime import datetime\\nfrom pprint import pprint\\nfrom time import time\\nfrom urllib.request import urlopen\\n\\nimport numpy as np\\nfrom scipy import sparse\\n\\nfrom sklearn.decomposition import randomized_svd\\n\\n# %%\\n# Download data, if not already on disk\\n# -------------------------------------\\nredirects_url = \"http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2\"\\nredirects_filename = redirects_url.rsplit(\"/\", 1)[1]\\n\\npage_links_url = \"http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2\"\\npage_links_filename = page_links_url.rsplit(\"/\", 1)[1]\\n\\nresources = [\\n    (redirects_url, redirects_filename),\\n    (page_links_url, page_links_filename),\\n]\\n\\nfor url, filename in resources:\\n    if not os.path.exists(filename):\\n        print(\"Downloading data from \\'%s\\', please wait...\" % url)\\n        opener = urlopen(url)\\n        with open(filename, \"wb\") as f:\\n            f.write(opener.read())\\n        print()\\n\\n\\n# %%\\n# Loading the redirect files\\n# --------------------------\\n# Code for: def index(redirects, index_map, k):\\n\\n\\nDBPEDIA_RESOURCE_PREFIX_LEN = len(\"http://dbpedia.org/resource/\")\\nSHORTNAME_SLICE = slice(DBPEDIA_RESOURCE_PREFIX_LEN + 1, -1)\\n\\n\\n# Code for: def short_name(nt_uri):\\n\\n\\n# Code for: def get_redirects(redirects_filename):\\n\\n\\n# %%\\n# Computing the Adjacency matrix\\n# ------------------------------\\n# Code for: def get_adjacency_matrix(redirects_filename, page_links_filename, limit=None):\\n\\n\\n# stop after 5M links to make it possible to work in RAM\\nX, redirects, index_map = get_adjacency_matrix(\\n    redirects_filename, page_links_filename, limit=5000000\\n)\\nnames = {i: name for name, i in index_map.items()}'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/wikipedia_principal_eigenvector.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# stop after 5M links to make it possible to work in RAM\\nX, redirects, index_map = get_adjacency_matrix(\\n    redirects_filename, page_links_filename, limit=5000000\\n)\\nnames = {i: name for name, i in index_map.items()}\\n\\n\\n# %%\\n# Computing Principal Singular Vector using Randomized SVD\\n# --------------------------------------------------------\\nprint(\"Computing the principal singular vectors using randomized_svd\")\\nt0 = time()\\nU, s, V = randomized_svd(X, 5, n_iter=3)\\nprint(\"done in %0.3fs\" % (time() - t0))\\n\\n# print the names of the wikipedia related strongest components of the\\n# principal singular vector which should be similar to the highest eigenvector\\nprint(\"Top wikipedia pages according to principal singular vectors\")\\npprint([names[i] for i in np.abs(U.T[0]).argsort()[-10:]])\\npprint([names[i] for i in np.abs(V[0]).argsort()[-10:]])\\n\\n\\n# %%\\n# Computing Centrality scores\\n# ---------------------------\\n# Code for: def centrality_scores(X, alpha=0.85, max_iter=100, tol=1e-10):\\n\\n\\nprint(\"Computing principal eigenvector score using a power iteration method\")\\nt0 = time()\\nscores = centrality_scores(X, max_iter=100)\\nprint(\"done in %0.3fs\" % (time() - t0))\\npprint([names[i] for i in np.abs(scores).argsort()[-10:]])'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_tomography_l1_reconstruction.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def _weights(x, dx=1, orig=0):\\n    x = np.ravel(x)\\n    floor_x = np.floor((x - orig) / dx).astype(np.int64)\\n    alpha = (x - orig - floor_x * dx) / dx\\n    return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_tomography_l1_reconstruction.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def _generate_center_coordinates(l_x):\\n    X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)\\n    center = l_x / 2.0\\n    X += 0.5 - center\\n    Y += 0.5 - center\\n    return X, Y'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_tomography_l1_reconstruction.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def build_projection_operator(l_x, n_dir):\\n    \"\"\"Compute the tomography design matrix.\\n\\n    Parameters\\n    ----------\\n\\n    l_x : int\\n        linear size of image array\\n\\n    n_dir : int\\n        number of angles at which projections are acquired.\\n\\n    Returns\\n    -------\\n    p : sparse matrix of shape (n_dir l_x, l_x**2)\\n    \"\"\"\\n    X, Y = _generate_center_coordinates(l_x)\\n    angles = np.linspace(0, np.pi, n_dir, endpoint=False)\\n    data_inds, weights, camera_inds = [], [], []\\n    data_unravel_indices = np.arange(l_x**2)\\n    data_unravel_indices = np.hstack((data_unravel_indices, data_unravel_indices))\\n    for i, angle in enumerate(angles):\\n        Xrot = np.cos(angle) * X - np.sin(angle) * Y\\n        inds, w = _weights(Xrot, dx=1, orig=X.min())\\n        mask = np.logical_and(inds >= 0, inds < l_x)\\n        weights += list(w[mask])\\n        camera_inds += list(inds[mask] + i * l_x)\\n        data_inds += list(data_unravel_indices[mask])\\n    proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds)))\\n    return proj_operator'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_tomography_l1_reconstruction.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def generate_synthetic_data():\\n    \"\"\"Synthetic binary data\"\"\"\\n    rs = np.random.RandomState(0)\\n    n_pts = 36\\n    x, y = np.ogrid[0:l, 0:l]\\n    mask_outer = (x - l / 2.0) ** 2 + (y - l / 2.0) ** 2 < (l / 2.0) ** 2\\n    mask = np.zeros((l, l))\\n    points = l * rs.rand(2, n_pts)\\n    mask[(points[0]).astype(int), (points[1]).astype(int)] = 1\\n    mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)\\n    res = np.logical_and(mask > mask.mean(), mask_outer)\\n    return np.logical_xor(res, ndimage.binary_erosion(res))'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_tomography_l1_reconstruction.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================================================\\nCompressive sensing: tomography reconstruction with L1 prior (Lasso)\\n======================================================================\\n\\nThis example shows the reconstruction of an image from a set of parallel\\nprojections, acquired along different angles. Such a dataset is acquired in\\n**computed tomography** (CT).\\n\\nWithout any prior information on the sample, the number of projections\\nrequired to reconstruct the image is of the order of the linear size\\n``l`` of the image (in pixels). For simplicity we consider here a sparse\\nimage, where only pixels on the boundary of objects have a non-zero\\nvalue. Such data could correspond for example to a cellular material.\\nNote however that most images are sparse in a different basis, such as\\nthe Haar wavelets. Only ``l/7`` projections are acquired, therefore it is\\nnecessary to use prior information available on the sample (its\\nsparsity): this is an example of **compressive sensing**.\\n\\nThe tomography projection operation is a linear transformation. In\\naddition to the data-fidelity term corresponding to a linear regression,\\nwe penalize the L1 norm of the image to account for its sparsity. The\\nresulting optimization problem is called the :ref:`lasso`. We use the'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_tomography_l1_reconstruction.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='class :class:`~sklearn.linear_model.Lasso`, that uses the coordinate descent\\nalgorithm. Importantly, this implementation is more computationally efficient\\non a sparse matrix, than the projection operator used here.\\n\\nThe reconstruction with L1 penalization gives a result with zero error\\n(all pixels are successfully labeled with 0 or 1), even if noise was\\nadded to the projections. In comparison, an L2 penalization\\n(:class:`~sklearn.linear_model.Ridge`) produces a large number of labeling\\nerrors for the pixels. Important artifacts are observed on the\\nreconstructed image, contrary to the L1 penalization. Note in particular\\nthe circular artifact separating the pixels in the corners, that have\\ncontributed to fewer projections than the central disk.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom scipy import ndimage, sparse\\n\\nfrom sklearn.linear_model import Lasso, Ridge\\n\\n\\n# Code for: def _weights(x, dx=1, orig=0):\\n\\n\\n# Code for: def _generate_center_coordinates(l_x):\\n\\n\\n# Code for: def build_projection_operator(l_x, n_dir):\\n\\n\\n# Code for: def generate_synthetic_data():\\n\\n\\n# Generate synthetic images, and projections\\nl = 128\\nproj_operator = build_projection_operator(l, l // 7)\\ndata = generate_synthetic_data()\\nproj = proj_operator @ data.ravel()[:, np.newaxis]\\nproj += 0.15 * np.random.randn(*proj.shape)\\n\\n# Reconstruction with L2 (Ridge) penalization\\nrgr_ridge = Ridge(alpha=0.2)\\nrgr_ridge.fit(proj_operator, proj.ravel())\\nrec_l2 = rgr_ridge.coef_.reshape(l, l)\\n\\n# Reconstruction with L1 (Lasso) penalization\\n# the best value of alpha was determined using cross validation\\n# with LassoCV\\nrgr_lasso = Lasso(alpha=0.001)\\nrgr_lasso.fit(proj_operator, proj.ravel())\\nrec_l1 = rgr_lasso.coef_.reshape(l, l)\\n\\nplt.figure(figsize=(8, 3.3))\\nplt.subplot(131)\\nplt.imshow(data, cmap=plt.cm.gray, interpolation=\"nearest\")\\nplt.axis(\"off\")\\nplt.title(\"original image\")\\nplt.subplot(132)\\nplt.imshow(rec_l2, cmap=plt.cm.gray, interpolation=\"nearest\")\\nplt.title(\"L2 penalization\")\\nplt.axis(\"off\")\\nplt.subplot(133)\\nplt.imshow(rec_l1, cmap=plt.cm.gray, interpolation=\"nearest\")\\nplt.title(\"L1 penalization\")\\nplt.axis(\"off\")\\n\\nplt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0, right=1)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def evaluate(model, X, y, cv, model_prop=None, model_step=None):\\n    cv_results = cross_validate(\\n        model,\\n        X,\\n        y,\\n        cv=cv,\\n        scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\\n        return_estimator=model_prop is not None,\\n    )\\n    if model_prop is not None:\\n        if model_step is not None:\\n            values = [\\n                getattr(m[model_step], model_prop) for m in cv_results[\"estimator\"]\\n            ]\\n        else:\\n            values = [getattr(m, model_prop) for m in cv_results[\"estimator\"]]\\n        print(f\"Mean model.{model_prop} = {np.mean(values)}\")\\n    mae = -cv_results[\"test_neg_mean_absolute_error\"]\\n    rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\\n    print(\\n        f\"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\\\\n\"\\n        f\"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}\"\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def sin_transformer(period):\\n    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def cos_transformer(period):\\n    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def periodic_spline_transformer(period, n_splines=None, degree=3):\\n    if n_splines is None:\\n        n_splines = period\\n    n_knots = n_splines + 1  # periodic and include_bias is True\\n    return SplineTransformer(\\n        degree=degree,\\n        n_knots=n_knots,\\n        knots=np.linspace(0, period, n_knots).reshape(n_knots, 1),\\n        extrapolation=\"periodic\",\\n        include_bias=True,\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================\\nTime-related feature engineering\\n================================\\n\\nThis notebook introduces different strategies to leverage time-related features\\nfor a bike sharing demand regression task that is highly dependent on business\\ncycles (days, weeks, months) and yearly season cycles.\\n\\nIn the process, we introduce how to perform periodic feature engineering using\\nthe :class:`sklearn.preprocessing.SplineTransformer` class and its\\n`extrapolation=\"periodic\"` option.\\n\\n\"\"\"\\n\\n# %%\\n# Data exploration on the Bike Sharing Demand dataset\\n# ---------------------------------------------------\\n#\\n# We start by loading the data from the OpenML repository.\\nfrom sklearn.datasets import fetch_openml\\n\\nbike_sharing = fetch_openml(\"Bike_Sharing_Demand\", version=2, as_frame=True)\\ndf = bike_sharing.frame\\n\\n# %%\\n# To get a quick understanding of the periodic patterns of the data, let us\\n# have a look at the average demand per hour during a week.\\n#\\n# Note that the week starts on a Sunday, during the weekend. We can clearly\\n# distinguish the commute patterns in the morning and evenings of the work days\\n# and the leisure use of the bikes on the weekends with a more spread peak\\n# demand around the middle of the days:\\nimport matplotlib.pyplot as plt\\n\\nfig, ax = plt.subplots(figsize=(12, 4))\\naverage_week_demand = df.groupby([\"weekday\", \"hour\"])[\"count\"].mean()\\naverage_week_demand.plot(ax=ax)\\n_ = ax.set(\\n    title=\"Average hourly bike demand during the week\",\\n    xticks=[i * 24 for i in range(7)],\\n    xticklabels=[\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"],\\n    xlabel=\"Time of the week\",\\n    ylabel=\"Number of bike rentals\",\\n)\\n\\n# %%\\n#\\n# The target of the prediction problem is the absolute count of bike rentals on\\n# a hourly basis:\\ndf[\"count\"].max()\\n\\n# %%\\n#\\n# Let us rescale the target variable (number of hourly bike rentals) to predict\\n# a relative demand so that the mean absolute error is more easily interpreted\\n# as a fraction of the maximum demand.\\n#\\n# .. note::\\n#\\n#     The fit method of the models used in this notebook all minimize the\\n#     mean squared error to estimate the conditional mean.\\n#     The absolute error, however, would estimate the conditional median.\\n#\\n#     Nevertheless, when reporting performance measures on the test set in\\n#     the discussion, we choose to focus on the mean absolute error instead\\n#     of the (root) mean squared error because it is more intuitive to\\n#     interpret. Note, however, that in this study the best models for one\\n#     metric are also the best ones in terms of the other metric.\\ny = df[\"count\"] / df[\"count\"].max()\\n\\n# %%\\nfig, ax = plt.subplots(figsize=(12, 4))\\ny.hist(bins=30, ax=ax)\\n_ = ax.set(\\n    xlabel=\"Fraction of rented fleet demand\",\\n    ylabel=\"Number of hours\",\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\nfig, ax = plt.subplots(figsize=(12, 4))\\ny.hist(bins=30, ax=ax)\\n_ = ax.set(\\n    xlabel=\"Fraction of rented fleet demand\",\\n    ylabel=\"Number of hours\",\\n)\\n\\n# %%\\n# The input feature data frame is a time annotated hourly log of variables\\n# describing the weather conditions. It includes both numerical and categorical\\n# variables. Note that the time information has already been expanded into\\n# several complementary columns.\\n#\\nX = df.drop(\"count\", axis=\"columns\")\\nX\\n\\n# %%\\n# .. note::\\n#\\n#    If the time information was only present as a date or datetime column, we\\n#    could have expanded it into hour-in-the-day, day-in-the-week,\\n#    day-in-the-month, month-in-the-year using pandas:\\n#    https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-date-components\\n#\\n# We now introspect the distribution of the categorical variables, starting\\n# with `\"weather\"`:\\n#\\nX[\"weather\"].value_counts()\\n\\n# %%\\n# Since there are only 3 `\"heavy_rain\"` events, we cannot use this category to\\n# train machine learning models with cross validation. Instead, we simplify the\\n# representation by collapsing those into the `\"rain\"` category.\\n#\\nX[\"weather\"] = (\\n    X[\"weather\"]\\n    .astype(object)\\n    .replace(to_replace=\"heavy_rain\", value=\"rain\")\\n    .astype(\"category\")\\n)\\n\\n# %%\\nX[\"weather\"].value_counts()\\n\\n# %%\\n# As expected, the `\"season\"` variable is well balanced:\\n#\\nX[\"season\"].value_counts()\\n\\n# %%\\n# Time-based cross-validation\\n# ---------------------------\\n#\\n# Since the dataset is a time-ordered event log (hourly demand), we will use a\\n# time-sensitive cross-validation splitter to evaluate our demand forecasting\\n# model as realistically as possible. We use a gap of 2 days between the train\\n# and test side of the splits. We also limit the training set size to make the\\n# performance of the CV folds more stable.\\n#\\n# 1000 test datapoints should be enough to quantify the performance of the\\n# model. This represents a bit less than a month and a half of contiguous test\\n# data:\\n\\nfrom sklearn.model_selection import TimeSeriesSplit\\n\\nts_cv = TimeSeriesSplit(\\n    n_splits=5,\\n    gap=48,\\n    max_train_size=10000,\\n    test_size=1000,\\n)\\n\\n# %%\\n# Let us manually inspect the various splits to check that the\\n# `TimeSeriesSplit` works as we expect, starting with the first split:\\nall_splits = list(ts_cv.split(X, y))\\ntrain_0, test_0 = all_splits[0]\\n\\n# %%\\nX.iloc[test_0]\\n\\n# %%\\nX.iloc[train_0]\\n\\n# %%\\n# We now inspect the last split:\\ntrain_4, test_4 = all_splits[4]\\n\\n# %%\\nX.iloc[test_4]\\n\\n# %%\\nX.iloc[train_4]'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\nX.iloc[test_0]\\n\\n# %%\\nX.iloc[train_0]\\n\\n# %%\\n# We now inspect the last split:\\ntrain_4, test_4 = all_splits[4]\\n\\n# %%\\nX.iloc[test_4]\\n\\n# %%\\nX.iloc[train_4]\\n\\n# %%\\n# All is well. We are now ready to do some predictive modeling!\\n#\\n# Gradient Boosting\\n# -----------------\\n#\\n# Gradient Boosting Regression with decision trees is often flexible enough to\\n# efficiently handle heterogeneous tabular data with a mix of categorical and\\n# numerical features as long as the number of samples is large enough.\\n#\\n# Here, we use the modern\\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor` with native support\\n# for categorical features. Therefore, we only need to set\\n# `categorical_features=\"from_dtype\"` such that features with categorical dtype\\n# are considered categorical features. For reference, we extract the categorical\\n# features from the dataframe based on the dtype. The internal trees use a dedicated\\n# tree splitting rule for these features.\\n#\\n# The numerical variables need no preprocessing and, for the sake of simplicity,\\n# we only try the default hyper-parameters for this model:\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\nfrom sklearn.model_selection import cross_validate\\nfrom sklearn.pipeline import make_pipeline\\n\\ngbrt = HistGradientBoostingRegressor(categorical_features=\"from_dtype\", random_state=42)\\ncategorical_columns = X.columns[X.dtypes == \"category\"]\\nprint(\"Categorical features:\", categorical_columns.tolist())\\n\\n# %%\\n#\\n# Lets evaluate our gradient boosting model with the mean absolute error of the\\n# relative demand averaged across our 5 time-based cross-validation splits:\\nimport numpy as np\\n\\n\\n# Code for: def evaluate(model, X, y, cv, model_prop=None, model_step=None):\\n\\n\\nevaluate(gbrt, X, y, cv=ts_cv, model_prop=\"n_iter_\")\\n\\n# %%\\n# We see that we set `max_iter` large enough such that early stopping took place.\\n#\\n# This model has an average error around 4 to 5% of the maximum demand. This is\\n# quite good for a first trial without any hyper-parameter tuning! We just had\\n# to make the categorical variables explicit. Note that the time related\\n# features are passed as is, i.e. without processing them. But this is not much\\n# of a problem for tree-based models as they can learn a non-monotonic\\n# relationship between ordinal input features and the target.\\n#\\n# This is not the case for linear regression models as we will see in the\\n# following.\\n#\\n# Naive linear regression\\n# -----------------------\\n#\\n# As usual for linear models, categorical variables need to be one-hot encoded.\\n# For consistency, we scale the numerical features to the same 0-1 range using\\n# :class:`~sklearn.preprocessing.MinMaxScaler`, although in this case it does not\\n# impact the results much because they are already on comparable scales:\\nfrom sklearn.linear_model import RidgeCV\\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\\nalphas = np.logspace(-6, 6, 25)\\nnaive_linear_pipeline = make_pipeline(\\n    ColumnTransformer(\\n        transformers=[\\n            (\"categorical\", one_hot_encoder, categorical_columns),\\n        ],\\n        remainder=MinMaxScaler(),\\n    ),\\n    RidgeCV(alphas=alphas),\\n)\\n\\n\\nevaluate(\\n    naive_linear_pipeline, X, y, cv=ts_cv, model_prop=\"alpha_\", model_step=\"ridgecv\"\\n)\\n\\n\\n# %%\\n# It is affirmative to see that the selected `alpha_` is in our specified\\n# range.\\n#\\n# The performance is not good: the average error is around 14% of the maximum\\n# demand. This is more than three times higher than the average error of the\\n# gradient boosting model. We can suspect that the naive original encoding\\n# (merely min-max scaled) of the periodic time-related features might prevent\\n# the linear regression model to properly leverage the time information: linear\\n# regression does not automatically model non-monotonic relationships between\\n# the input features and the target. Non-linear terms have to be engineered in\\n# the input.\\n#\\n# For example, the raw numerical encoding of the `\"hour\"` feature prevents the\\n# linear model from recognizing that an increase of hour in the morning from 6\\n# to 8 should have a strong positive impact on the number of bike rentals while\\n# an increase of similar magnitude in the evening from 18 to 20 should have a\\n# strong negative impact on the predicted number of bike rentals.\\n#\\n# Time-steps as categories\\n# ------------------------\\n#\\n# Since the time features are encoded in a discrete manner using integers (24\\n# unique values in the \"hours\" feature), we could decide to treat those as\\n# categorical variables using a one-hot encoding and thereby ignore any\\n# assumption implied by the ordering of the hour values.\\n#\\n# Using one-hot encoding for the time features gives the linear model a lot\\n# more flexibility as we introduce one additional feature per discrete time\\n# level.\\none_hot_linear_pipeline = make_pipeline(\\n    ColumnTransformer(\\n        transformers=[\\n            (\"categorical\", one_hot_encoder, categorical_columns),\\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\\n        ],\\n        remainder=MinMaxScaler(),\\n    ),\\n    RidgeCV(alphas=alphas),\\n)\\n\\nevaluate(one_hot_linear_pipeline, X, y, cv=ts_cv)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='evaluate(one_hot_linear_pipeline, X, y, cv=ts_cv)\\n\\n# %%\\n# The average error rate of this model is 10% which is much better than using\\n# the original (ordinal) encoding of the time feature, confirming our intuition\\n# that the linear regression model benefits from the added flexibility to not\\n# treat time progression in a monotonic manner.\\n#\\n# However, this introduces a very large number of new features. If the time of\\n# the day was represented in minutes since the start of the day instead of\\n# hours, one-hot encoding would have introduced 1440 features instead of 24.\\n# This could cause some significant overfitting. To avoid this we could use\\n# :func:`sklearn.preprocessing.KBinsDiscretizer` instead to re-bin the number\\n# of levels of fine-grained ordinal or numerical variables while still\\n# benefitting from the non-monotonic expressivity advantages of one-hot\\n# encoding.\\n#\\n# Finally, we also observe that one-hot encoding completely ignores the\\n# ordering of the hour levels while this could be an interesting inductive bias\\n# to preserve to some level. In the following we try to explore smooth,\\n# non-monotonic encoding that locally preserves the relative ordering of time\\n# features.\\n#\\n# Trigonometric features\\n# ----------------------\\n#\\n# As a first attempt, we can try to encode each of those periodic features\\n# using a sine and cosine transformation with the matching period.\\n#\\n# Each ordinal time feature is transformed into 2 features that together encode\\n# equivalent information in a non-monotonic way, and more importantly without\\n# any jump between the first and the last value of the periodic range.\\nfrom sklearn.preprocessing import FunctionTransformer\\n\\n\\n# Code for: def sin_transformer(period):\\n\\n\\n# Code for: def cos_transformer(period):\\n\\n\\n# %%\\n#\\n# Let us visualize the effect of this feature expansion on some synthetic hour\\n# data with a bit of extrapolation beyond hour=23:\\nimport pandas as pd\\n\\nhour_df = pd.DataFrame(\\n    np.arange(26).reshape(-1, 1),\\n    columns=[\"hour\"],\\n)\\nhour_df[\"hour_sin\"] = sin_transformer(24).fit_transform(hour_df)[\"hour\"]\\nhour_df[\"hour_cos\"] = cos_transformer(24).fit_transform(hour_df)[\"hour\"]\\nhour_df.plot(x=\"hour\")\\n_ = plt.title(\"Trigonometric encoding for the \\'hour\\' feature\")\\n\\n# %%\\n#\\n# Let\\'s use a 2D scatter plot with the hours encoded as colors to better see\\n# how this representation maps the 24 hours of the day to a 2D space, akin to\\n# some sort of a 24 hour version of an analog clock. Note that the \"25th\" hour\\n# is mapped back to the 1st hour because of the periodic nature of the\\n# sine/cosine representation.\\nfig, ax = plt.subplots(figsize=(7, 5))\\nsp = ax.scatter(hour_df[\"hour_sin\"], hour_df[\"hour_cos\"], c=hour_df[\"hour\"])\\nax.set(\\n    xlabel=\"sin(hour)\",\\n    ylabel=\"cos(hour)\",\\n)\\n_ = fig.colorbar(sp)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n#\\n# We can now build a feature extraction pipeline using this strategy:\\ncyclic_cossin_transformer = ColumnTransformer(\\n    transformers=[\\n        (\"categorical\", one_hot_encoder, categorical_columns),\\n        (\"month_sin\", sin_transformer(12), [\"month\"]),\\n        (\"month_cos\", cos_transformer(12), [\"month\"]),\\n        (\"weekday_sin\", sin_transformer(7), [\"weekday\"]),\\n        (\"weekday_cos\", cos_transformer(7), [\"weekday\"]),\\n        (\"hour_sin\", sin_transformer(24), [\"hour\"]),\\n        (\"hour_cos\", cos_transformer(24), [\"hour\"]),\\n    ],\\n    remainder=MinMaxScaler(),\\n)\\ncyclic_cossin_linear_pipeline = make_pipeline(\\n    cyclic_cossin_transformer,\\n    RidgeCV(alphas=alphas),\\n)\\nevaluate(cyclic_cossin_linear_pipeline, X, y, cv=ts_cv)\\n\\n\\n# %%\\n#\\n# The performance of our linear regression model with this simple feature\\n# engineering is a bit better than using the original ordinal time features but\\n# worse than using the one-hot encoded time features. We will further analyze\\n# possible reasons for this disappointing outcome at the end of this notebook.\\n#\\n# Periodic spline features\\n# ------------------------\\n#\\n# We can try an alternative encoding of the periodic time-related features\\n# using spline transformations with a large enough number of splines, and as a\\n# result a larger number of expanded features compared to the sine/cosine\\n# transformation:\\nfrom sklearn.preprocessing import SplineTransformer\\n\\n\\n# Code for: def periodic_spline_transformer(period, n_splines=None, degree=3):\\n\\n\\n# %%\\n#\\n# Again, let us visualize the effect of this feature expansion on some\\n# synthetic hour data with a bit of extrapolation beyond hour=23:\\nhour_df = pd.DataFrame(\\n    np.linspace(0, 26, 1000).reshape(-1, 1),\\n    columns=[\"hour\"],\\n)\\nsplines = periodic_spline_transformer(24, n_splines=12).fit_transform(hour_df)\\nsplines_df = pd.DataFrame(\\n    splines,\\n    columns=[f\"spline_{i}\" for i in range(splines.shape[1])],\\n)\\npd.concat([hour_df, splines_df], axis=\"columns\").plot(x=\"hour\", cmap=plt.cm.tab20b)\\n_ = plt.title(\"Periodic spline-based encoding for the \\'hour\\' feature\")'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Thanks to the use of the `extrapolation=\"periodic\"` parameter, we observe\\n# that the feature encoding stays smooth when extrapolating beyond midnight.\\n#\\n# We can now build a predictive pipeline using this alternative periodic\\n# feature engineering strategy.\\n#\\n# It is possible to use fewer splines than discrete levels for those ordinal\\n# values. This makes spline-based encoding more efficient than one-hot encoding\\n# while preserving most of the expressivity:\\ncyclic_spline_transformer = ColumnTransformer(\\n    transformers=[\\n        (\"categorical\", one_hot_encoder, categorical_columns),\\n        (\"cyclic_month\", periodic_spline_transformer(12, n_splines=6), [\"month\"]),\\n        (\"cyclic_weekday\", periodic_spline_transformer(7, n_splines=3), [\"weekday\"]),\\n        (\"cyclic_hour\", periodic_spline_transformer(24, n_splines=12), [\"hour\"]),\\n    ],\\n    remainder=MinMaxScaler(),\\n)\\ncyclic_spline_linear_pipeline = make_pipeline(\\n    cyclic_spline_transformer,\\n    RidgeCV(alphas=alphas),\\n)\\nevaluate(cyclic_spline_linear_pipeline, X, y, cv=ts_cv)\\n\\n# %%\\n# Spline features make it possible for the linear model to successfully\\n# leverage the periodic time-related features and reduce the error from ~14% to\\n# ~10% of the maximum demand, which is similar to what we observed with the\\n# one-hot encoded features.\\n#\\n# Qualitative analysis of the impact of features on linear model predictions\\n# --------------------------------------------------------------------------\\n#\\n# Here, we want to visualize the impact of the feature engineering choices on\\n# the time related shape of the predictions.\\n#\\n# To do so we consider an arbitrary time-based split to compare the predictions\\n# on a range of held out data points.\\nnaive_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\\nnaive_linear_predictions = naive_linear_pipeline.predict(X.iloc[test_0])\\n\\none_hot_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\\none_hot_linear_predictions = one_hot_linear_pipeline.predict(X.iloc[test_0])\\n\\ncyclic_cossin_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\\ncyclic_cossin_linear_predictions = cyclic_cossin_linear_pipeline.predict(X.iloc[test_0])\\n\\ncyclic_spline_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\\ncyclic_spline_linear_predictions = cyclic_spline_linear_pipeline.predict(X.iloc[test_0])'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='cyclic_spline_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\\ncyclic_spline_linear_predictions = cyclic_spline_linear_pipeline.predict(X.iloc[test_0])\\n\\n# %%\\n# We visualize those predictions by zooming on the last 96 hours (4 days) of\\n# the test set to get some qualitative insights:\\nlast_hours = slice(-96, None)\\nfig, ax = plt.subplots(figsize=(12, 4))\\nfig.suptitle(\"Predictions by linear models\")\\nax.plot(\\n    y.iloc[test_0].values[last_hours],\\n    \"x-\",\\n    alpha=0.2,\\n    label=\"Actual demand\",\\n    color=\"black\",\\n)\\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\\nax.plot(\\n    cyclic_cossin_linear_predictions[last_hours],\\n    \"x-\",\\n    label=\"Trigonometric time features\",\\n)\\nax.plot(\\n    cyclic_spline_linear_predictions[last_hours],\\n    \"x-\",\\n    label=\"Spline-based time features\",\\n)\\nax.plot(\\n    one_hot_linear_predictions[last_hours],\\n    \"x-\",\\n    label=\"One-hot time features\",\\n)\\n_ = ax.legend()\\n\\n# %%\\n# We can draw the following conclusions from the above plot:\\n#\\n# - The **raw ordinal time-related features** are problematic because they do\\n#   not capture the natural periodicity: we observe a big jump in the\\n#   predictions at the end of each day when the hour features goes from 23 back\\n#   to 0. We can expect similar artifacts at the end of each week or each year.\\n#\\n# - As expected, the **trigonometric features** (sine and cosine) do not have\\n#   these discontinuities at midnight, but the linear regression model fails to\\n#   leverage those features to properly model intra-day variations.\\n#   Using trigonometric features for higher harmonics or additional\\n#   trigonometric features for the natural period with different phases could\\n#   potentially fix this problem.\\n#\\n# - the **periodic spline-based features** fix those two problems at once: they\\n#   give more expressivity to the linear model by making it possible to focus\\n#   on specific hours thanks to the use of 12 splines. Furthermore the\\n#   `extrapolation=\"periodic\"` option enforces a smooth representation between\\n#   `hour=23` and `hour=0`.\\n#\\n# - The **one-hot encoded features** behave similarly to the periodic\\n#   spline-based features but are more spiky: for instance they can better\\n#   model the morning peak during the week days since this peak lasts shorter\\n#   than an hour. However, we will see in the following that what can be an\\n#   advantage for linear models is not necessarily one for more expressive\\n#   models.\\n\\n# %%\\n# We can also compare the number of features extracted by each feature\\n# engineering pipeline:\\nnaive_linear_pipeline[:-1].transform(X).shape\\n\\n# %%\\none_hot_linear_pipeline[:-1].transform(X).shape\\n\\n# %%\\ncyclic_cossin_linear_pipeline[:-1].transform(X).shape\\n\\n# %%\\ncyclic_spline_linear_pipeline[:-1].transform(X).shape'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\none_hot_linear_pipeline[:-1].transform(X).shape\\n\\n# %%\\ncyclic_cossin_linear_pipeline[:-1].transform(X).shape\\n\\n# %%\\ncyclic_spline_linear_pipeline[:-1].transform(X).shape\\n\\n# %%\\n# This confirms that the one-hot encoding and the spline encoding strategies\\n# create a lot more features for the time representation than the alternatives,\\n# which in turn gives the downstream linear model more flexibility (degrees of\\n# freedom) to avoid underfitting.\\n#\\n# Finally, we observe that none of the linear models can approximate the true\\n# bike rentals demand, especially for the peaks that can be very sharp at rush\\n# hours during the working days but much flatter during the week-ends: the most\\n# accurate linear models based on splines or one-hot encoding tend to forecast\\n# peaks of commuting-related bike rentals even on the week-ends and\\n# under-estimate the commuting-related events during the working days.\\n#\\n# These systematic prediction errors reveal a form of under-fitting and can be\\n# explained by the lack of interactions terms between features, e.g.\\n# \"workingday\" and features derived from \"hours\". This issue will be addressed\\n# in the following section.\\n\\n# %%\\n# Modeling pairwise interactions with splines and polynomial features\\n# -------------------------------------------------------------------\\n#\\n# Linear models do not automatically capture interaction effects between input\\n# features. It does not help that some features are marginally non-linear as is\\n# the case with features constructed by `SplineTransformer` (or one-hot\\n# encoding or binning).\\n#\\n# However, it is possible to use the `PolynomialFeatures` class on coarse\\n# grained spline encoded hours to model the \"workingday\"/\"hours\" interaction\\n# explicitly without introducing too many new variables:\\nfrom sklearn.pipeline import FeatureUnion\\nfrom sklearn.preprocessing import PolynomialFeatures\\n\\nhour_workday_interaction = make_pipeline(\\n    ColumnTransformer(\\n        [\\n            (\"cyclic_hour\", periodic_spline_transformer(24, n_splines=8), [\"hour\"]),\\n            (\"workingday\", FunctionTransformer(lambda x: x == \"True\"), [\"workingday\"]),\\n        ]\\n    ),\\n    PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\\n)\\n\\n# %%\\n# Those features are then combined with the ones already computed in the\\n# previous spline-base pipeline. We can observe a nice performance improvement\\n# by modeling this pairwise interaction explicitly:\\n\\ncyclic_spline_interactions_pipeline = make_pipeline(\\n    FeatureUnion(\\n        [\\n            (\"marginal\", cyclic_spline_transformer),\\n            (\"interactions\", hour_workday_interaction),\\n        ]\\n    ),\\n    RidgeCV(alphas=alphas),\\n)\\nevaluate(cyclic_spline_interactions_pipeline, X, y, cv=ts_cv)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='cyclic_spline_interactions_pipeline = make_pipeline(\\n    FeatureUnion(\\n        [\\n            (\"marginal\", cyclic_spline_transformer),\\n            (\"interactions\", hour_workday_interaction),\\n        ]\\n    ),\\n    RidgeCV(alphas=alphas),\\n)\\nevaluate(cyclic_spline_interactions_pipeline, X, y, cv=ts_cv)\\n\\n# %%\\n# Modeling non-linear feature interactions with kernels\\n# -----------------------------------------------------\\n#\\n# The previous analysis highlighted the need to model the interactions between\\n# `\"workingday\"` and `\"hours\"`. Another example of a such a non-linear\\n# interaction that we would like to model could be the impact of the rain that\\n# might not be the same during the working days and the week-ends and holidays\\n# for instance.\\n#\\n# To model all such interactions, we could either use a polynomial expansion on\\n# all marginal features at once, after their spline-based expansion. However,\\n# this would create a quadratic number of features which can cause overfitting\\n# and computational tractability issues.\\n#\\n# Alternatively, we can use the Nystr√∂m method to compute an approximate\\n# polynomial kernel expansion. Let us try the latter:\\nfrom sklearn.kernel_approximation import Nystroem\\n\\ncyclic_spline_poly_pipeline = make_pipeline(\\n    cyclic_spline_transformer,\\n    Nystroem(kernel=\"poly\", degree=2, n_components=300, random_state=0),\\n    RidgeCV(alphas=alphas),\\n)\\nevaluate(cyclic_spline_poly_pipeline, X, y, cv=ts_cv)\\n\\n# %%\\n#\\n# We observe that this model can almost rival the performance of the gradient\\n# boosted trees with an average error around 5% of the maximum demand.\\n#\\n# Note that while the final step of this pipeline is a linear regression model,\\n# the intermediate steps such as the spline feature extraction and the Nystr√∂m\\n# kernel approximation are highly non-linear. As a result the compound pipeline\\n# is much more expressive than a simple linear regression model with raw features.\\n#\\n# For the sake of completeness, we also evaluate the combination of one-hot\\n# encoding and kernel approximation:\\n\\none_hot_poly_pipeline = make_pipeline(\\n    ColumnTransformer(\\n        transformers=[\\n            (\"categorical\", one_hot_encoder, categorical_columns),\\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\\n        ],\\n        remainder=\"passthrough\",\\n    ),\\n    Nystroem(kernel=\"poly\", degree=2, n_components=300, random_state=0),\\n    RidgeCV(alphas=alphas),\\n)\\nevaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# While one-hot encoded features were competitive with spline-based features\\n# when using linear models, this is no longer the case when using a low-rank\\n# approximation of a non-linear kernel: this can be explained by the fact that\\n# spline features are smoother and allow the kernel approximation to find a\\n# more expressive decision function.\\n#\\n# Let us now have a qualitative look at the predictions of the kernel models\\n# and of the gradient boosted trees that should be able to better model\\n# non-linear interactions between features:\\ngbrt.fit(X.iloc[train_0], y.iloc[train_0])\\ngbrt_predictions = gbrt.predict(X.iloc[test_0])\\n\\none_hot_poly_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\\none_hot_poly_predictions = one_hot_poly_pipeline.predict(X.iloc[test_0])\\n\\ncyclic_spline_poly_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\\ncyclic_spline_poly_predictions = cyclic_spline_poly_pipeline.predict(X.iloc[test_0])\\n\\n# %%\\n# Again we zoom on the last 4 days of the test set:\\n\\nlast_hours = slice(-96, None)\\nfig, ax = plt.subplots(figsize=(12, 4))\\nfig.suptitle(\"Predictions by non-linear regression models\")\\nax.plot(\\n    y.iloc[test_0].values[last_hours],\\n    \"x-\",\\n    alpha=0.2,\\n    label=\"Actual demand\",\\n    color=\"black\",\\n)\\nax.plot(\\n    gbrt_predictions[last_hours],\\n    \"x-\",\\n    label=\"Gradient Boosted Trees\",\\n)\\nax.plot(\\n    one_hot_poly_predictions[last_hours],\\n    \"x-\",\\n    label=\"One-hot + polynomial kernel\",\\n)\\nax.plot(\\n    cyclic_spline_poly_predictions[last_hours],\\n    \"x-\",\\n    label=\"Splines + polynomial kernel\",\\n)\\n_ = ax.legend()\\n\\n\\n# %%\\n# First, note that trees can naturally model non-linear feature interactions\\n# since, by default, decision trees are allowed to grow beyond a depth of 2\\n# levels.\\n#\\n# Here, we can observe that the combinations of spline features and non-linear\\n# kernels works quite well and can almost rival the accuracy of the gradient\\n# boosting regression trees.\\n#\\n# On the contrary, one-hot encoded time features do not perform that well with\\n# the low rank kernel model. In particular, they significantly over-estimate\\n# the low demand hours more than the competing models.\\n#\\n# We also observe that none of the models can successfully predict some of the\\n# peak rentals at the rush hours during the working days. It is possible that\\n# access to additional features would be required to further improve the\\n# accuracy of the predictions. For instance, it could be useful to have access\\n# to the geographical repartition of the fleet at any point in time or the\\n# fraction of bikes that are immobilized because they need servicing.\\n#\\n# Let us finally get a more quantitative look at the prediction errors of those\\n# three models using the true vs predicted demand scatter plots:\\nfrom sklearn.metrics import PredictionErrorDisplay'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(13, 7), sharex=True, sharey=\"row\")\\nfig.suptitle(\"Non-linear regression models\", y=1.0)\\npredictions = [\\n    one_hot_poly_predictions,\\n    cyclic_spline_poly_predictions,\\n    gbrt_predictions,\\n]\\nlabels = [\\n    \"One hot +\\\\npolynomial kernel\",\\n    \"Splines +\\\\npolynomial kernel\",\\n    \"Gradient Boosted\\\\nTrees\",\\n]\\nplot_kinds = [\"actual_vs_predicted\", \"residual_vs_predicted\"]\\nfor axis_idx, kind in enumerate(plot_kinds):\\n    for ax, pred, label in zip(axes[axis_idx], predictions, labels):\\n        disp = PredictionErrorDisplay.from_predictions(\\n            y_true=y.iloc[test_0],\\n            y_pred=pred,\\n            kind=kind,\\n            scatter_kwargs={\"alpha\": 0.3},\\n            ax=ax,\\n        )\\n        ax.set_xticks(np.linspace(0, 1, num=5))\\n        if axis_idx == 0:\\n            ax.set_yticks(np.linspace(0, 1, num=5))\\n            ax.legend(\\n                [\"Best model\", label],\\n                loc=\"upper center\",\\n                bbox_to_anchor=(0.5, 1.3),\\n                ncol=2,\\n            )\\n        ax.set_aspect(\"equal\", adjustable=\"box\")\\nplt.show()\\n# %%\\n# This visualization confirms the conclusions we draw on the previous plot.\\n#\\n# All models under-estimate the high demand events (working day rush hours),\\n# but gradient boosting a bit less so. The low demand events are well predicted\\n# on average by gradient boosting while the one-hot polynomial regression\\n# pipeline seems to systematically over-estimate demand in that regime. Overall\\n# the predictions of the gradient boosted trees are closer to the diagonal than\\n# for the kernel models.\\n#\\n# Concluding remarks\\n# ------------------\\n#\\n# We note that we could have obtained slightly better results for kernel models\\n# by using more components (higher rank kernel approximation) at the cost of\\n# longer fit and prediction durations. For large values of `n_components`, the\\n# performance of the one-hot encoded features would even match the spline\\n# features.\\n#\\n# The `Nystroem` + `RidgeCV` regressor could also have been replaced by\\n# :class:`~sklearn.neural_network.MLPRegressor` with one or two hidden layers\\n# and we would have obtained quite similar results.\\n#\\n# The dataset we used in this case study is sampled on a hourly basis. However\\n# cyclic spline-based features could model time-within-day or time-within-week\\n# very efficiently with finer-grained time resolutions (for instance with\\n# measurements taken every minute instead of every hours) without introducing\\n# more features. One-hot encoding time representations would not offer this\\n# flexibility.\\n#\\n# Finally, in this notebook we used `RidgeCV` because it is very efficient from\\n# a computational point of view. However, it models the target variable as a\\n# Gaussian random variable with constant variance. For positive regression\\n# problems, it is likely that using a Poisson or Gamma distribution would make\\n# more sense. This could be achieved by using'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_cyclical_feature_engineering.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# a computational point of view. However, it models the target variable as a\\n# Gaussian random variable with constant variance. For positive regression\\n# problems, it is likely that using a Poisson or Gamma distribution would make\\n# more sense. This could be achieved by using\\n# `GridSearchCV(TweedieRegressor(power=2), param_grid({\"alpha\": alphas}))`\\n# instead of `RidgeCV`.'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_time_series_lagged_features.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def consolidate_scores(cv_results, scores, metric):\\n    if metric == \"MAPE\":\\n        scores[metric].append(f\"{value.mean():.2f} ¬± {value.std():.2f}\")\\n    else:\\n        scores[metric].append(f\"{value.mean():.1f} ¬± {value.std():.1f}\")\\n\\n    return scores'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_time_series_lagged_features.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def min_arg(col):\\n    col_split = pl.col(col).str.split(\" \")\\n    return pl.arg_sort_by(\\n        col_split.list.get(0).cast(pl.Float64),\\n        col_split.list.get(2).cast(pl.Float64),\\n    ).first()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_time_series_lagged_features.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===========================================\\nLagged features for time series forecasting\\n===========================================\\n\\nThis example demonstrates how Polars-engineered lagged features can be used\\nfor time series forecasting with\\n:class:`~sklearn.ensemble.HistGradientBoostingRegressor` on the Bike Sharing\\nDemand dataset.\\n\\nSee the example on\\n:ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`\\nfor some data exploration on this dataset and a demo on periodic feature\\nengineering.\\n\\n\"\"\"\\n\\n# %%\\n# Analyzing the Bike Sharing Demand dataset\\n# -----------------------------------------\\n#\\n# We start by loading the data from the OpenML repository as a raw parquet file\\n# to illustrate how to work with an arbitrary parquet file instead of hiding this\\n# step in a convenience tool such as `sklearn.datasets.fetch_openml`.\\n#\\n# The URL of the parquet file can be found in the JSON description of the\\n# Bike Sharing Demand dataset with id 44063 on openml.org\\n# (https://openml.org/search?type=data&status=active&id=44063).\\n#\\n# The `sha256` hash of the file is also provided to ensure the integrity of the\\n# downloaded file.\\nimport numpy as np\\nimport polars as pl\\n\\nfrom sklearn.datasets import fetch_file\\n\\npl.Config.set_fmt_str_lengths(20)\\n\\nbike_sharing_data_file = fetch_file(\\n    \"https://openml1.win.tue.nl/datasets/0004/44063/dataset_44063.pq\",\\n    sha256=\"d120af76829af0d256338dc6dd4be5df4fd1f35bf3a283cab66a51c1c6abd06a\",\\n)\\nbike_sharing_data_file\\n\\n# %%\\n# We load the parquet file with Polars for feature engineering. Polars\\n# automatically caches common subexpressions which are reused in multiple\\n# expressions (like `pl.col(\"count\").shift(1)` below). See\\n# https://docs.pola.rs/user-guide/lazy/optimizations/ for more information.\\n\\ndf = pl.read_parquet(bike_sharing_data_file)\\n\\n# %%\\n# Next, we take a look at the statistical summary of the dataset\\n# so that we can better understand the data that we are working with.\\nimport polars.selectors as cs\\n\\nsummary = df.select(cs.numeric()).describe()\\nsummary\\n\\n# %%\\n# Let us look at the count of the seasons `\"fall\"`, `\"spring\"`, `\"summer\"`\\n# and `\"winter\"` present in the dataset to confirm they are balanced.\\n\\nimport matplotlib.pyplot as plt\\n\\ndf[\"season\"].value_counts()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_time_series_lagged_features.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='summary = df.select(cs.numeric()).describe()\\nsummary\\n\\n# %%\\n# Let us look at the count of the seasons `\"fall\"`, `\"spring\"`, `\"summer\"`\\n# and `\"winter\"` present in the dataset to confirm they are balanced.\\n\\nimport matplotlib.pyplot as plt\\n\\ndf[\"season\"].value_counts()\\n\\n\\n# %%\\n# Generating Polars-engineered lagged features\\n# --------------------------------------------\\n# Let\\'s consider the problem of predicting the demand at the\\n# next hour given past demands. Since the demand is a continuous\\n# variable, one could intuitively use any regression model. However, we do\\n# not have the usual `(X_train, y_train)` dataset. Instead, we just have\\n# the `y_train` demand data sequentially organized by time.\\nlagged_df = df.select(\\n    \"count\",\\n    *[pl.col(\"count\").shift(i).alias(f\"lagged_count_{i}h\") for i in [1, 2, 3]],\\n    lagged_count_1d=pl.col(\"count\").shift(24),\\n    lagged_count_1d_1h=pl.col(\"count\").shift(24 + 1),\\n    lagged_count_7d=pl.col(\"count\").shift(7 * 24),\\n    lagged_count_7d_1h=pl.col(\"count\").shift(7 * 24 + 1),\\n    lagged_mean_24h=pl.col(\"count\").shift(1).rolling_mean(24),\\n    lagged_max_24h=pl.col(\"count\").shift(1).rolling_max(24),\\n    lagged_min_24h=pl.col(\"count\").shift(1).rolling_min(24),\\n    lagged_mean_7d=pl.col(\"count\").shift(1).rolling_mean(7 * 24),\\n    lagged_max_7d=pl.col(\"count\").shift(1).rolling_max(7 * 24),\\n    lagged_min_7d=pl.col(\"count\").shift(1).rolling_min(7 * 24),\\n)\\nlagged_df.tail(10)\\n\\n# %%\\n# Watch out however, the first lines have undefined values because their own\\n# past is unknown. This depends on how much lag we used:\\nlagged_df.head(10)\\n\\n# %%\\n# We can now separate the lagged features in a matrix `X` and the target variable\\n# (the counts to predict) in an array of the same first dimension `y`.\\nlagged_df = lagged_df.drop_nulls()\\nX = lagged_df.drop(\"count\")\\ny = lagged_df[\"count\"]\\nprint(\"X shape: {}\\\\ny shape: {}\".format(X.shape, y.shape))\\n\\n# %%\\n# Naive evaluation of the next hour bike demand regression\\n# --------------------------------------------------------\\n# Let\\'s randomly split our tabularized dataset to train a gradient\\n# boosting regression tree (GBRT) model and evaluate it using Mean\\n# Absolute Percentage Error (MAPE). If our model is aimed at forecasting\\n# (i.e., predicting future data from past data), we should not use training\\n# data that are ulterior to the testing data. In time series machine learning\\n# the \"i.i.d\" (independent and identically distributed) assumption does not\\n# hold true as the data points are not independent and have a temporal\\n# relationship.\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, random_state=42\\n)\\n\\nmodel = HistGradientBoostingRegressor().fit(X_train, y_train)\\n\\n# %%\\n# Taking a look at the performance of the model.\\nfrom sklearn.metrics import mean_absolute_percentage_error'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_time_series_lagged_features.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='X_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, random_state=42\\n)\\n\\nmodel = HistGradientBoostingRegressor().fit(X_train, y_train)\\n\\n# %%\\n# Taking a look at the performance of the model.\\nfrom sklearn.metrics import mean_absolute_percentage_error\\n\\ny_pred = model.predict(X_test)\\nmean_absolute_percentage_error(y_test, y_pred)\\n\\n# %%\\n# Proper next hour forecasting evaluation\\n# ---------------------------------------\\n# Let\\'s use a proper evaluation splitting strategies that takes into account\\n# the temporal structure of the dataset to evaluate our model\\'s ability to\\n# predict data points in the future (to avoid cheating by reading values from\\n# the lagged features in the training set).\\nfrom sklearn.model_selection import TimeSeriesSplit\\n\\nts_cv = TimeSeriesSplit(\\n    n_splits=3,  # to keep the notebook fast enough on common laptops\\n    gap=48,  # 2 days data gap between train and test\\n    max_train_size=10000,  # keep train sets of comparable sizes\\n    test_size=3000,  # for 2 or 3 digits of precision in scores\\n)\\nall_splits = list(ts_cv.split(X, y))\\n\\n# %%\\n# Training the model and evaluating its performance based on MAPE.\\ntrain_idx, test_idx = all_splits[0]\\nX_train, X_test = X[train_idx, :], X[test_idx, :]\\ny_train, y_test = y[train_idx], y[test_idx]\\n\\nmodel = HistGradientBoostingRegressor().fit(X_train, y_train)\\ny_pred = model.predict(X_test)\\nmean_absolute_percentage_error(y_test, y_pred)\\n\\n# %%\\n# The generalization error measured via a shuffled trained test split\\n# is too optimistic. The generalization via a time-based split is likely to\\n# be more representative of the true performance of the regression model.\\n# Let\\'s assess this variability of our error evaluation with proper\\n# cross-validation:\\nfrom sklearn.model_selection import cross_val_score\\n\\ncv_mape_scores = -cross_val_score(\\n    model, X, y, cv=ts_cv, scoring=\"neg_mean_absolute_percentage_error\"\\n)\\ncv_mape_scores\\n\\n# %%\\n# The variability across splits is quite large! In a real life setting\\n# it would be advised to use more splits to better assess the variability.\\n# Let\\'s report the mean CV scores and their standard deviation from now on.\\nprint(f\"CV MAPE: {cv_mape_scores.mean():.3f} ¬± {cv_mape_scores.std():.3f}\")\\n\\n# %%\\n# We can compute several combinations of evaluation metrics and loss functions,\\n# which are reported a bit below.\\nfrom collections import defaultdict\\n\\nfrom sklearn.metrics import (\\n    make_scorer,\\n    mean_absolute_error,\\n    mean_pinball_loss,\\n    root_mean_squared_error,\\n)\\nfrom sklearn.model_selection import cross_validate\\n\\n\\n# Code for: def consolidate_scores(cv_results, scores, metric):'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_time_series_lagged_features.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn.metrics import (\\n    make_scorer,\\n    mean_absolute_error,\\n    mean_pinball_loss,\\n    root_mean_squared_error,\\n)\\nfrom sklearn.model_selection import cross_validate\\n\\n\\n# Code for: def consolidate_scores(cv_results, scores, metric):\\n\\n\\nscoring = {\\n    \"MAPE\": make_scorer(mean_absolute_percentage_error),\\n    \"RMSE\": make_scorer(root_mean_squared_error),\\n    \"MAE\": make_scorer(mean_absolute_error),\\n    \"pinball_loss_05\": make_scorer(mean_pinball_loss, alpha=0.05),\\n    \"pinball_loss_50\": make_scorer(mean_pinball_loss, alpha=0.50),\\n    \"pinball_loss_95\": make_scorer(mean_pinball_loss, alpha=0.95),\\n}\\nloss_functions = [\"squared_error\", \"poisson\", \"absolute_error\"]\\nscores = defaultdict(list)\\nfor loss_func in loss_functions:\\n    model = HistGradientBoostingRegressor(loss=loss_func)\\n    cv_results = cross_validate(\\n        model,\\n        X,\\n        y,\\n        cv=ts_cv,\\n        scoring=scoring,\\n        n_jobs=2,\\n    )\\n    time = cv_results[\"fit_time\"]\\n    scores[\"loss\"].append(loss_func)\\n    scores[\"fit_time\"].append(f\"{time.mean():.2f} ¬± {time.std():.2f} s\")\\n\\n    for key, value in cv_results.items():\\n        if key.startswith(\"test_\"):\\n            metric = key.split(\"test_\")[1]\\n            scores = consolidate_scores(cv_results, scores, metric)\\n\\n\\n# %%\\n# Modeling predictive uncertainty via quantile regression\\n# -------------------------------------------------------\\n# Instead of modeling the expected value of the distribution of\\n# :math:`Y|X` like the least squares and Poisson losses do, one could try to\\n# estimate quantiles of the conditional distribution.\\n#\\n# :math:`Y|X=x_i` is expected to be a random variable for a given data point\\n# :math:`x_i` because we expect that the number of rentals cannot be 100%\\n# accurately predicted from the features. It can be influenced by other\\n# variables not properly captured by the existing lagged features. For\\n# instance whether or not it will rain in the next hour cannot be fully\\n# anticipated from the past hours bike rental data. This is what we\\n# call aleatoric uncertainty.\\n#\\n# Quantile regression makes it possible to give a finer description of that\\n# distribution without making strong assumptions on its shape.\\nquantile_list = [0.05, 0.5, 0.95]\\n\\nfor quantile in quantile_list:\\n    model = HistGradientBoostingRegressor(loss=\"quantile\", quantile=quantile)\\n    cv_results = cross_validate(\\n        model,\\n        X,\\n        y,\\n        cv=ts_cv,\\n        scoring=scoring,\\n        n_jobs=2,\\n    )\\n    time = cv_results[\"fit_time\"]\\n    scores[\"fit_time\"].append(f\"{time.mean():.2f} ¬± {time.std():.2f} s\")\\n\\n    scores[\"loss\"].append(f\"quantile {int(quantile*100)}\")\\n    for key, value in cv_results.items():\\n        if key.startswith(\"test_\"):\\n            metric = key.split(\"test_\")[1]\\n            scores = consolidate_scores(cv_results, scores, metric)\\n\\nscores_df = pl.DataFrame(scores)\\nscores_df\\n\\n\\n# %%\\n# Let us take a look at the losses that minimise each metric.\\n# Code for: def min_arg(col):'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_time_series_lagged_features.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='scores_df = pl.DataFrame(scores)\\nscores_df\\n\\n\\n# %%\\n# Let us take a look at the losses that minimise each metric.\\n# Code for: def min_arg(col):\\n\\n\\nscores_df.select(\\n    pl.col(\"loss\").get(min_arg(col_name)).alias(col_name)\\n    for col_name in scores_df.columns\\n    if col_name != \"loss\"\\n)\\n\\n# %%\\n# Even if the score distributions overlap due to the variance in the dataset,\\n# it is true that the average RMSE is lower when `loss=\"squared_error\"`, whereas\\n# the average MAPE is lower when `loss=\"absolute_error\"` as expected. That is\\n# also the case for the Mean Pinball Loss with the quantiles 5 and 95. The score\\n# corresponding to the 50 quantile loss is overlapping with the score obtained\\n# by minimizing other loss functions, which is also the case for the MAE.\\n#\\n# A qualitative look at the predictions\\n# -------------------------------------\\n# We can now visualize the performance of the model with regards\\n# to the 5th percentile, median and the 95th percentile:\\nall_splits = list(ts_cv.split(X, y))\\ntrain_idx, test_idx = all_splits[0]\\n\\nX_train, X_test = X[train_idx, :], X[test_idx, :]\\ny_train, y_test = y[train_idx], y[test_idx]\\n\\nmax_iter = 50\\ngbrt_mean_poisson = HistGradientBoostingRegressor(loss=\"poisson\", max_iter=max_iter)\\ngbrt_mean_poisson.fit(X_train, y_train)\\nmean_predictions = gbrt_mean_poisson.predict(X_test)\\n\\ngbrt_median = HistGradientBoostingRegressor(\\n    loss=\"quantile\", quantile=0.5, max_iter=max_iter\\n)\\ngbrt_median.fit(X_train, y_train)\\nmedian_predictions = gbrt_median.predict(X_test)\\n\\ngbrt_percentile_5 = HistGradientBoostingRegressor(\\n    loss=\"quantile\", quantile=0.05, max_iter=max_iter\\n)\\ngbrt_percentile_5.fit(X_train, y_train)\\npercentile_5_predictions = gbrt_percentile_5.predict(X_test)\\n\\ngbrt_percentile_95 = HistGradientBoostingRegressor(\\n    loss=\"quantile\", quantile=0.95, max_iter=max_iter\\n)\\ngbrt_percentile_95.fit(X_train, y_train)\\npercentile_95_predictions = gbrt_percentile_95.predict(X_test)\\n\\n# %%\\n# We can now take a look at the predictions made by the regression models:\\nlast_hours = slice(-96, None)\\nfig, ax = plt.subplots(figsize=(15, 7))\\nplt.title(\"Predictions by regression models\")\\nax.plot(\\n    y_test[last_hours],\\n    \"x-\",\\n    alpha=0.2,\\n    label=\"Actual demand\",\\n    color=\"black\",\\n)\\nax.plot(\\n    median_predictions[last_hours],\\n    \"^-\",\\n    label=\"GBRT median\",\\n)\\nax.plot(\\n    mean_predictions[last_hours],\\n    \"x-\",\\n    label=\"GBRT mean (Poisson)\",\\n)\\nax.fill_between(\\n    np.arange(96),\\n    percentile_5_predictions[last_hours],\\n    percentile_95_predictions[last_hours],\\n    alpha=0.3,\\n    label=\"GBRT 90% interval\",\\n)\\n_ = ax.legend()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_time_series_lagged_features.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Here it\\'s interesting to notice that the blue area between the 5% and 95%\\n# percentile estimators has a width that varies with the time of the day:\\n#\\n# - At night, the blue band is much narrower: the pair of models is quite\\n#   certain that there will be a small number of bike rentals. And furthermore\\n#   these seem correct in the sense that the actual demand stays in that blue\\n#   band.\\n# - During the day, the blue band is much wider: the uncertainty grows, probably\\n#   because of the variability of the weather that can have a very large impact,\\n#   especially on week-ends.\\n# - We can also see that during week-days, the commute pattern is still visible in\\n#   the 5% and 95% estimations.\\n# - Finally, it is expected that 10% of the time, the actual demand does not lie\\n#   between the 5% and 95% percentile estimates. On this test span, the actual\\n#   demand seems to be higher, especially during the rush hours. It might reveal that\\n#   our 95% percentile estimator underestimates the demand peaks. This could be be\\n#   quantitatively confirmed by computing empirical coverage numbers as done in\\n#   the :ref:`calibration of confidence intervals <calibration-section>`.\\n#\\n# Looking at the performance of non-linear regression models vs\\n# the best models:\\nfrom sklearn.metrics import PredictionErrorDisplay\\n\\nfig, axes = plt.subplots(ncols=3, figsize=(15, 6), sharey=True)\\nfig.suptitle(\"Non-linear regression models\")\\npredictions = [\\n    median_predictions,\\n    percentile_5_predictions,\\n    percentile_95_predictions,\\n]\\nlabels = [\\n    \"Median\",\\n    \"5th percentile\",\\n    \"95th percentile\",\\n]\\nfor ax, pred, label in zip(axes, predictions, labels):\\n    PredictionErrorDisplay.from_predictions(\\n        y_true=y_test,\\n        y_pred=pred,\\n        kind=\"residual_vs_predicted\",\\n        scatter_kwargs={\"alpha\": 0.3},\\n        ax=ax,\\n    )\\n    ax.set(xlabel=\"Predicted demand\", ylabel=\"True demand\")\\n    ax.legend([\"Best model\", label])\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_time_series_lagged_features.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.show()\\n\\n# %%\\n# Conclusion\\n# ----------\\n# Through this example we explored time series forecasting using lagged\\n# features. We compared a naive regression (using the standardized\\n# :class:`~sklearn.model_selection.train_test_split`) with a proper time\\n# series evaluation strategy using\\n# :class:`~sklearn.model_selection.TimeSeriesSplit`. We observed that the\\n# model trained using :class:`~sklearn.model_selection.train_test_split`,\\n# having a default value of `shuffle` set to `True` produced an overly\\n# optimistic Mean Average Percentage Error (MAPE). The results\\n# produced from the time-based split better represent the performance\\n# of our time-series regression model. We also analyzed the predictive uncertainty\\n# of our model via Quantile Regression. Predictions based on the 5th and\\n# 95th percentile using `loss=\"quantile\"` provide us with a quantitative estimate\\n# of the uncertainty of the forecasts made by our time series regression model.\\n# Uncertainty estimation can also be performed\\n# using `MAPIE <https://mapie.readthedocs.io/en/latest/index.html>`_,\\n# that provides an implementation based on recent work on conformal prediction\\n# methods and estimates both aleatoric and epistemic uncertainty at the same time.\\n# Furthermore, functionalities provided\\n# by `sktime <https://www.sktime.net/en/latest/users.html>`_\\n# can be used to extend scikit-learn estimators by making use of recursive time\\n# series forecasting, that enables dynamic predictions of future values.'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def _not_in_sphinx():\\n    # Hack to detect whether we are running by the sphinx builder\\n    return \"__file__\" in globals()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class ReutersParser(HTMLParser):\\n    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\\n\\n    def __init__(self, encoding=\"latin-1\"):\\n        HTMLParser.__init__(self)\\n        self._reset()\\n        self.encoding = encoding\\n\\n    def handle_starttag(self, tag, attrs):\\n        method = \"start_\" + tag\\n        getattr(self, method, lambda x: None)(attrs)\\n\\n    def handle_endtag(self, tag):\\n        method = \"end_\" + tag\\n        getattr(self, method, lambda: None)()\\n\\n    def _reset(self):\\n        self.in_title = 0\\n        self.in_body = 0\\n        self.in_topics = 0\\n        self.in_topic_d = 0\\n        self.title = \"\"\\n        self.body = \"\"\\n        self.topics = []\\n        self.topic_d = \"\"\\n\\n    def parse(self, fd):\\n        self.docs = []\\n        for chunk in fd:\\n            self.feed(chunk.decode(self.encoding))\\n            for doc in self.docs:\\n                yield doc\\n            self.docs = []\\n        self.close()\\n\\n    def handle_data(self, data):\\n        if self.in_body:\\n            self.body += data\\n        elif self.in_title:\\n            self.title += data\\n        elif self.in_topic_d:\\n            self.topic_d += data\\n\\n    def start_reuters(self, attributes):\\n        pass\\n\\n    def end_reuters(self):\\n        self.body = re.sub(r\"\\\\s+\", r\" \", self.body)\\n        self.docs.append(\\n            {\"title\": self.title, \"body\": self.body, \"topics\": self.topics}\\n        )\\n        self._reset()\\n\\n    def start_title(self, attributes):\\n        self.in_title = 1\\n\\n    def end_title(self):\\n        self.in_title = 0\\n\\n    def start_body(self, attributes):\\n        self.in_body = 1\\n\\n    def end_body(self):\\n        self.in_body = 0\\n\\n    def start_topics(self, attributes):\\n        self.in_topics = 1\\n\\n    def end_topics(self):\\n        self.in_topics = 0\\n\\n    def start_d(self, attributes):\\n        self.in_topic_d = 1\\n\\n    def end_d(self):\\n        self.in_topic_d = 0\\n        self.topics.append(self.topic_d)\\n        self.topic_d = \"\"'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def stream_reuters_documents(data_path=None):\\n    \"\"\"Iterate over documents of the Reuters dataset.\\n\\n    The Reuters archive will automatically be downloaded and uncompressed if\\n    the `data_path` directory does not exist.\\n\\n    Documents are represented as dictionaries with \\'body\\' (str),\\n    \\'title\\' (str), \\'topics\\' (list(str)) keys.\\n\\n    \"\"\"\\n\\n    DOWNLOAD_URL = (\\n        \"http://archive.ics.uci.edu/ml/machine-learning-databases/\"\\n        \"reuters21578-mld/reuters21578.tar.gz\"\\n    )\\n    ARCHIVE_SHA256 = \"3bae43c9b14e387f76a61b6d82bf98a4fb5d3ef99ef7e7075ff2ccbcf59f9d30\"\\n    ARCHIVE_FILENAME = \"reuters21578.tar.gz\"\\n\\n    if data_path is None:\\n        data_path = Path(get_data_home()) / \"reuters\"\\n    else:\\n        data_path = Path(data_path)\\n    if not data_path.exists():\\n        \"\"\"Download the dataset.\"\"\"\\n        print(\"downloading dataset (once and for all) into %s\" % data_path)\\n        data_path.mkdir(parents=True, exist_ok=True)\\n\\n        def progress(blocknum, bs, size):\\n            total_sz_mb = \"%.2f MB\" % (size / 1e6)\\n            current_sz_mb = \"%.2f MB\" % ((blocknum * bs) / 1e6)\\n            if _not_in_sphinx():\\n                sys.stdout.write(\"\\\\rdownloaded %s / %s\" % (current_sz_mb, total_sz_mb))\\n\\n        archive_path = data_path / ARCHIVE_FILENAME\\n\\n        urlretrieve(DOWNLOAD_URL, filename=archive_path, reporthook=progress)\\n        if _not_in_sphinx():\\n            sys.stdout.write(\"\\\\r\")\\n\\n        # Check that the archive was not tampered:\\n        assert sha256(archive_path.read_bytes()).hexdigest() == ARCHIVE_SHA256\\n\\n        print(\"untarring Reuters dataset...\")\\n        with tarfile.open(archive_path, \"r:gz\") as fp:\\n            fp.extractall(data_path, filter=\"data\")\\n        print(\"done.\")\\n\\n    parser = ReutersParser()\\n    for filename in data_path.glob(\"*.sgm\"):\\n        for doc in parser.parse(open(filename, \"rb\")):\\n            yield doc'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def get_minibatch(doc_iter, size, pos_class=positive_class):\\n    \"\"\"Extract a minibatch of examples, return a tuple X_text, y.\\n\\n    Note: size is before excluding invalid docs with no topics assigned.\\n\\n    \"\"\"\\n    data = [\\n        (\"{title}\\\\n\\\\n{body}\".format(**doc), pos_class in doc[\"topics\"])\\n        for doc in itertools.islice(doc_iter, size)\\n        if doc[\"topics\"]\\n    ]\\n    if not len(data):\\n        return np.asarray([], dtype=int), np.asarray([], dtype=int)\\n    X_text, y = zip(*data)\\n    return X_text, np.asarray(y, dtype=int)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def iter_minibatches(doc_iter, minibatch_size):\\n    \"\"\"Generator of minibatches.\"\"\"\\n    X_text, y = get_minibatch(doc_iter, minibatch_size)\\n    while len(X_text):\\n        yield X_text, y\\n        X_text, y = get_minibatch(doc_iter, minibatch_size)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def progress(cls_name, stats):\\n    \"\"\"Report progress information, return a string.\"\"\"\\n    duration = time.time() - stats[\"t0\"]\\n    s = \"%20s classifier : \\\\t\" % cls_name\\n    s += \"%(n_train)6d train docs (%(n_train_pos)6d positive) \" % stats\\n    s += \"%(n_test)6d test docs (%(n_test_pos)6d positive) \" % test_stats\\n    s += \"accuracy: %(accuracy).3f \" % stats\\n    s += \"in %.2fs (%5d docs/s)\" % (duration, stats[\"n_train\"] / duration)\\n    return s'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_accuracy(x, y, x_legend):\\n    \"\"\"Plot accuracy as a function of x.\"\"\"\\n    x = np.array(x)\\n    y = np.array(y)\\n    plt.title(\"Classification accuracy as a function of %s\" % x_legend)\\n    plt.xlabel(\"%s\" % x_legend)\\n    plt.ylabel(\"Accuracy\")\\n    plt.grid(True)\\n    plt.plot(x, y)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def autolabel(rectangles):\\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\\n    for rect in rectangles:\\n        height = rect.get_height()\\n        ax.text(\\n            rect.get_x() + rect.get_width() / 2.0,\\n            1.05 * height,\\n            \"%.4f\" % height,\\n            ha=\"center\",\\n            va=\"bottom\",\\n        )\\n        plt.setp(plt.xticks()[1], rotation=30)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n======================================================\\nOut-of-core classification of text documents\\n======================================================\\n\\nThis is an example showing how scikit-learn can be used for classification\\nusing an out-of-core approach: learning from data that doesn\\'t fit into main\\nmemory. We make use of an online classifier, i.e., one that supports the\\npartial_fit method, that will be fed with batches of examples. To guarantee\\nthat the features space remains the same over time we leverage a\\nHashingVectorizer that will project each example into the same feature space.\\nThis is especially useful in the case of text classification where new\\nfeatures (words) may appear in each batch.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport itertools\\nimport re\\nimport sys\\nimport tarfile\\nimport time\\nfrom hashlib import sha256\\nfrom html.parser import HTMLParser\\nfrom pathlib import Path\\nfrom urllib.request import urlretrieve\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib import rcParams\\n\\nfrom sklearn.datasets import get_data_home\\nfrom sklearn.feature_extraction.text import HashingVectorizer\\nfrom sklearn.linear_model import PassiveAggressiveClassifier, Perceptron, SGDClassifier\\nfrom sklearn.naive_bayes import MultinomialNB\\n\\n\\n# Code for: def _not_in_sphinx():\\n\\n\\n# %%\\n# Reuters Dataset related routines\\n# --------------------------------\\n#\\n# The dataset used in this example is Reuters-21578 as provided by the UCI ML\\n# repository. It will be automatically downloaded and uncompressed on first\\n# run.\\n\\n\\n# Code for: class ReutersParser(HTMLParser):\\n\\n\\n# Code for: def stream_reuters_documents(data_path=None):\\n\\n\\n# %%\\n# Main\\n# ----\\n#\\n# Create the vectorizer and limit the number of features to a reasonable\\n# maximum\\n\\nvectorizer = HashingVectorizer(\\n    decode_error=\"ignore\", n_features=2**18, alternate_sign=False\\n)\\n\\n\\n# Iterator over parsed Reuters SGML files.\\ndata_stream = stream_reuters_documents()\\n\\n# We learn a binary classification between the \"acq\" class and all the others.\\n# \"acq\" was chosen as it is more or less evenly distributed in the Reuters\\n# files. For other datasets, one should take care of creating a test set with\\n# a realistic portion of positive instances.\\nall_classes = np.array([0, 1])\\npositive_class = \"acq\"\\n\\n# Here are some classifiers that support the `partial_fit` method\\npartial_fit_classifiers = {\\n    \"SGD\": SGDClassifier(max_iter=5),\\n    \"Perceptron\": Perceptron(),\\n    \"NB Multinomial\": MultinomialNB(alpha=0.01),\\n    \"Passive-Aggressive\": PassiveAggressiveClassifier(),\\n}\\n\\n\\n# Code for: def get_minibatch(doc_iter, size, pos_class=positive_class):\\n\\n\\n# Code for: def iter_minibatches(doc_iter, minibatch_size):\\n\\n\\n# test data statistics\\ntest_stats = {\"n_test\": 0, \"n_test_pos\": 0}'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def get_minibatch(doc_iter, size, pos_class=positive_class):\\n\\n\\n# Code for: def iter_minibatches(doc_iter, minibatch_size):\\n\\n\\n# test data statistics\\ntest_stats = {\"n_test\": 0, \"n_test_pos\": 0}\\n\\n# First we hold out a number of examples to estimate accuracy\\nn_test_documents = 1000\\ntick = time.time()\\nX_test_text, y_test = get_minibatch(data_stream, 1000)\\nparsing_time = time.time() - tick\\ntick = time.time()\\nX_test = vectorizer.transform(X_test_text)\\nvectorizing_time = time.time() - tick\\ntest_stats[\"n_test\"] += len(y_test)\\ntest_stats[\"n_test_pos\"] += sum(y_test)\\nprint(\"Test set is %d documents (%d positive)\" % (len(y_test), sum(y_test)))\\n\\n\\n# Code for: def progress(cls_name, stats):\\n\\n\\ncls_stats = {}\\n\\nfor cls_name in partial_fit_classifiers:\\n    stats = {\\n        \"n_train\": 0,\\n        \"n_train_pos\": 0,\\n        \"accuracy\": 0.0,\\n        \"accuracy_history\": [(0, 0)],\\n        \"t0\": time.time(),\\n        \"runtime_history\": [(0, 0)],\\n        \"total_fit_time\": 0.0,\\n    }\\n    cls_stats[cls_name] = stats\\n\\nget_minibatch(data_stream, n_test_documents)\\n# Discard test set\\n\\n# We will feed the classifier with mini-batches of 1000 documents; this means\\n# we have at most 1000 docs in memory at any time.  The smaller the document\\n# batch, the bigger the relative overhead of the partial fit methods.\\nminibatch_size = 1000\\n\\n# Create the data_stream that parses Reuters SGML files and iterates on\\n# documents as a stream.\\nminibatch_iterators = iter_minibatches(data_stream, minibatch_size)\\ntotal_vect_time = 0.0\\n\\n# Main loop : iterate on mini-batches of examples\\nfor i, (X_train_text, y_train) in enumerate(minibatch_iterators):\\n    tick = time.time()\\n    X_train = vectorizer.transform(X_train_text)\\n    total_vect_time += time.time() - tick\\n\\n    for cls_name, cls in partial_fit_classifiers.items():\\n        tick = time.time()\\n        # update estimator with examples in the current mini-batch\\n        cls.partial_fit(X_train, y_train, classes=all_classes)\\n\\n        # accumulate test accuracy stats\\n        cls_stats[cls_name][\"total_fit_time\"] += time.time() - tick\\n        cls_stats[cls_name][\"n_train\"] += X_train.shape[0]\\n        cls_stats[cls_name][\"n_train_pos\"] += sum(y_train)\\n        tick = time.time()\\n        cls_stats[cls_name][\"accuracy\"] = cls.score(X_test, y_test)\\n        cls_stats[cls_name][\"prediction_time\"] = time.time() - tick\\n        acc_history = (cls_stats[cls_name][\"accuracy\"], cls_stats[cls_name][\"n_train\"])\\n        cls_stats[cls_name][\"accuracy_history\"].append(acc_history)\\n        run_history = (\\n            cls_stats[cls_name][\"accuracy\"],\\n            total_vect_time + cls_stats[cls_name][\"total_fit_time\"],\\n        )\\n        cls_stats[cls_name][\"runtime_history\"].append(run_history)\\n\\n        if i % 3 == 0:\\n            print(progress(cls_name, cls_stats[cls_name]))\\n    if i % 3 == 0:\\n        print(\"\\\\n\")'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_out_of_core_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='if i % 3 == 0:\\n            print(progress(cls_name, cls_stats[cls_name]))\\n    if i % 3 == 0:\\n        print(\"\\\\n\")\\n\\n\\n# %%\\n# Plot results\\n# ------------\\n#\\n# The plot represents the learning curve of the classifier: the evolution\\n# of classification accuracy over the course of the mini-batches. Accuracy is\\n# measured on the first 1000 samples, held out as a validation set.\\n#\\n# To limit the memory consumption, we queue examples up to a fixed amount\\n# before feeding them to the learner.\\n\\n\\n# Code for: def plot_accuracy(x, y, x_legend):\\n\\n\\nrcParams[\"legend.fontsize\"] = 10\\ncls_names = list(sorted(cls_stats.keys()))\\n\\n# Plot accuracy evolution\\nplt.figure()\\nfor _, stats in sorted(cls_stats.items()):\\n    # Plot accuracy evolution with #examples\\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\\n    ax = plt.gca()\\n    ax.set_ylim((0.8, 1))\\nplt.legend(cls_names, loc=\"best\")\\n\\nplt.figure()\\nfor _, stats in sorted(cls_stats.items()):\\n    # Plot accuracy evolution with runtime\\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\\n    ax = plt.gca()\\n    ax.set_ylim((0.8, 1))\\nplt.legend(cls_names, loc=\"best\")\\n\\n# Plot fitting times\\nplt.figure()\\nfig = plt.gcf()\\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\\n\\ncls_runtime.append(total_vect_time)\\ncls_names.append(\"Vectorization\")\\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\\n\\nax = plt.subplot(111)\\nrectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\\n\\nax.set_xticks(np.linspace(0, len(cls_names) - 1, len(cls_names)))\\nax.set_xticklabels(cls_names, fontsize=10)\\nymax = max(cls_runtime) * 1.2\\nax.set_ylim((0, ymax))\\nax.set_ylabel(\"runtime (s)\")\\nax.set_title(\"Training Times\")\\n\\n\\n# Code for: def autolabel(rectangles):\\n\\n\\nautolabel(rectangles)\\nplt.tight_layout()\\nplt.show()\\n\\n# Plot prediction times\\nplt.figure()\\ncls_runtime = []\\ncls_names = list(sorted(cls_stats.keys()))\\nfor cls_name, stats in sorted(cls_stats.items()):\\n    cls_runtime.append(stats[\"prediction_time\"])\\ncls_runtime.append(parsing_time)\\ncls_names.append(\"Read/Parse\\\\n+Feat.Extr.\")\\ncls_runtime.append(vectorizing_time)\\ncls_names.append(\"Hashing\\\\n+Vect.\")\\n\\nax = plt.subplot(111)\\nrectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\\n\\nax.set_xticks(np.linspace(0, len(cls_names) - 1, len(cls_names)))\\nax.set_xticklabels(cls_names, fontsize=8)\\nplt.setp(plt.xticks()[1], rotation=30)\\nymax = max(cls_runtime) * 1.2\\nax.set_ylim((0, ymax))\\nax.set_ylabel(\"runtime (s)\")\\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\\nautolabel(rectangles)\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_digits_denoising.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_digits(X, title):\\n    \"\"\"Small helper function to plot 100 digits.\"\"\"\\n    fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(8, 8))\\n    for img, ax in zip(X, axs.ravel()):\\n        ax.imshow(img.reshape((16, 16)), cmap=\"Greys\")\\n        ax.axis(\"off\")\\n    fig.suptitle(title, fontsize=24)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_digits_denoising.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================\\nImage denoising using kernel PCA\\n================================\\n\\nThis example shows how to use :class:`~sklearn.decomposition.KernelPCA` to\\ndenoise images. In short, we take advantage of the approximation function\\nlearned during `fit` to reconstruct the original image.\\n\\nWe will compare the results with an exact reconstruction using\\n:class:`~sklearn.decomposition.PCA`.\\n\\nWe will use USPS digits dataset to reproduce presented in Sect. 4 of [1]_.\\n\\n.. rubric:: References\\n\\n.. [1] `Bakƒ±r, G√∂khan H., Jason Weston, and Bernhard Sch√∂lkopf.\\n    \"Learning to find pre-images.\"\\n    Advances in neural information processing systems 16 (2004): 449-456.\\n    <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_\\n\\n\"\"\"\\n\\n# Authors: Guillaume Lemaitre <guillaume.lemaitre@inria.fr>\\n# Licence: BSD 3 clause\\n\\n# %%\\n# Load the dataset via OpenML\\n# ---------------------------\\n#\\n# The USPS digits datasets is available in OpenML. We use\\n# :func:`~sklearn.datasets.fetch_openml` to get this dataset. In addition, we\\n# normalize the dataset such that all pixel values are in the range (0, 1).\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\nX, y = fetch_openml(data_id=41082, as_frame=False, return_X_y=True)\\nX = MinMaxScaler().fit_transform(X)\\n\\n# %%\\n# The idea will be to learn a PCA basis (with and without a kernel) on\\n# noisy images and then use these models to reconstruct and denoise these\\n# images.\\n#\\n# Thus, we split our dataset into a training and testing set composed of 1,000\\n# samples for the training and 100 samples for testing. These images are\\n# noise-free and we will use them to evaluate the efficiency of the denoising\\n# approaches. In addition, we create a copy of the original dataset and add a\\n# Gaussian noise.\\n#\\n# The idea of this application, is to show that we can denoise corrupted images\\n# by learning a PCA basis on some uncorrupted images. We will use both a PCA\\n# and a kernel-based PCA to solve this problem.\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, stratify=y, random_state=0, train_size=1_000, test_size=100\\n)\\n\\nrng = np.random.RandomState(0)\\nnoise = rng.normal(scale=0.25, size=X_test.shape)\\nX_test_noisy = X_test + noise\\n\\nnoise = rng.normal(scale=0.25, size=X_train.shape)\\nX_train_noisy = X_train + noise\\n\\n# %%\\n# In addition, we will create a helper function to qualitatively assess the\\n# image reconstruction by plotting the test images.\\nimport matplotlib.pyplot as plt\\n\\n\\n# Code for: def plot_digits(X, title):'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_digits_denoising.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='noise = rng.normal(scale=0.25, size=X_train.shape)\\nX_train_noisy = X_train + noise\\n\\n# %%\\n# In addition, we will create a helper function to qualitatively assess the\\n# image reconstruction by plotting the test images.\\nimport matplotlib.pyplot as plt\\n\\n\\n# Code for: def plot_digits(X, title):\\n\\n\\n# %%\\n# In addition, we will use the mean squared error (MSE) to quantitatively\\n# assess the image reconstruction.\\n#\\n# Let\\'s first have a look to see the difference between noise-free and noisy\\n# images. We will check the test set in this regard.\\nplot_digits(X_test, \"Uncorrupted test images\")\\nplot_digits(\\n    X_test_noisy, f\"Noisy test images\\\\nMSE: {np.mean((X_test - X_test_noisy) ** 2):.2f}\"\\n)\\n\\n# %%\\n# Learn the `PCA` basis\\n# ---------------------\\n#\\n# We can now learn our PCA basis using both a linear PCA and a kernel PCA that\\n# uses a radial basis function (RBF) kernel.\\nfrom sklearn.decomposition import PCA, KernelPCA\\n\\npca = PCA(n_components=32, random_state=42)\\nkernel_pca = KernelPCA(\\n    n_components=400,\\n    kernel=\"rbf\",\\n    gamma=1e-3,\\n    fit_inverse_transform=True,\\n    alpha=5e-3,\\n    random_state=42,\\n)\\n\\npca.fit(X_train_noisy)\\n_ = kernel_pca.fit(X_train_noisy)\\n\\n# %%\\n# Reconstruct and denoise test images\\n# -----------------------------------\\n#\\n# Now, we can transform and reconstruct the noisy test set. Since we used less\\n# components than the number of original features, we will get an approximation\\n# of the original set. Indeed, by dropping the components explaining variance\\n# in PCA the least, we hope to remove noise. Similar thinking happens in kernel\\n# PCA; however, we expect a better reconstruction because we use a non-linear\\n# kernel to learn the PCA basis and a kernel ridge to learn the mapping\\n# function.\\nX_reconstructed_kernel_pca = kernel_pca.inverse_transform(\\n    kernel_pca.transform(X_test_noisy)\\n)\\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test_noisy))\\n\\n# %%\\nplot_digits(X_test, \"Uncorrupted test images\")\\nplot_digits(\\n    X_reconstructed_pca,\\n    f\"PCA reconstruction\\\\nMSE: {np.mean((X_test - X_reconstructed_pca) ** 2):.2f}\",\\n)\\nplot_digits(\\n    X_reconstructed_kernel_pca,\\n    (\\n        \"Kernel PCA reconstruction\\\\n\"\\n        f\"MSE: {np.mean((X_test - X_reconstructed_kernel_pca) ** 2):.2f}\"\\n    ),\\n)\\n\\n# %%\\n# PCA has a lower MSE than kernel PCA. However, the qualitative analysis might\\n# not favor PCA instead of kernel PCA. We observe that kernel PCA is able to\\n# remove background noise and provide a smoother image.\\n#\\n# However, it should be noted that the results of the denoising with kernel PCA\\n# will depend of the parameters `n_components`, `gamma`, and `alpha`.'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_model_complexity_influence.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def generate_data(case):\\n    \"\"\"Generate regression/classification data.\"\"\"\\n    if case == \"regression\":\\n        X, y = datasets.load_diabetes(return_X_y=True)\\n        train_size = 0.8\\n    elif case == \"classification\":\\n        X, y = datasets.fetch_20newsgroups_vectorized(subset=\"all\", return_X_y=True)\\n        train_size = 0.4  # to make the example run faster\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, train_size=train_size, random_state=0\\n    )\\n\\n    data = {\"X_train\": X_train, \"X_test\": X_test, \"y_train\": y_train, \"y_test\": y_test}\\n    return data'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_model_complexity_influence.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def benchmark_influence(conf):\\n    \"\"\"\\n    Benchmark influence of `changing_param` on both MSE and latency.\\n    \"\"\"\\n    prediction_times = []\\n    prediction_powers = []\\n    complexities = []\\n    for param_value in conf[\"changing_param_values\"]:\\n        conf[\"tuned_params\"][conf[\"changing_param\"]] = param_value\\n        estimator = conf[\"estimator\"](**conf[\"tuned_params\"])\\n\\n        print(\"Benchmarking %s\" % estimator)\\n        estimator.fit(conf[\"data\"][\"X_train\"], conf[\"data\"][\"y_train\"])\\n        conf[\"postfit_hook\"](estimator)\\n        complexity = conf[\"complexity_computer\"](estimator)\\n        complexities.append(complexity)\\n        start_time = time.time()\\n        for _ in range(conf[\"n_samples\"]):\\n            y_pred = estimator.predict(conf[\"data\"][\"X_test\"])\\n        elapsed_time = (time.time() - start_time) / float(conf[\"n_samples\"])\\n        prediction_times.append(elapsed_time)\\n        pred_score = conf[\"prediction_performance_computer\"](\\n            conf[\"data\"][\"y_test\"], y_pred\\n        )\\n        prediction_powers.append(pred_score)\\n        print(\\n            \"Complexity: %d | %s: %.4f | Pred. Time: %fs\\\\n\"\\n            % (\\n                complexity,\\n                conf[\"prediction_performance_label\"],\\n                pred_score,\\n                elapsed_time,\\n            )\\n        )\\n    return prediction_powers, prediction_times, complexities'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_model_complexity_influence.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def _count_nonzero_coefficients(estimator):\\n    a = estimator.coef_.toarray()\\n    return np.count_nonzero(a)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_model_complexity_influence.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_influence(conf, mse_values, prediction_times, complexities):\\n    \"\"\"\\n    Plot influence of model complexity on both accuracy and latency.\\n    \"\"\"\\n\\n    fig = plt.figure()\\n    fig.subplots_adjust(right=0.75)\\n\\n    # first axes (prediction error)\\n    ax1 = fig.add_subplot(111)\\n    line1 = ax1.plot(complexities, mse_values, c=\"tab:blue\", ls=\"-\")[0]\\n    ax1.set_xlabel(\"Model Complexity (%s)\" % conf[\"complexity_label\"])\\n    y1_label = conf[\"prediction_performance_label\"]\\n    ax1.set_ylabel(y1_label)\\n\\n    ax1.spines[\"left\"].set_color(line1.get_color())\\n    ax1.yaxis.label.set_color(line1.get_color())\\n    ax1.tick_params(axis=\"y\", colors=line1.get_color())\\n\\n    # second axes (latency)\\n    ax2 = fig.add_subplot(111, sharex=ax1, frameon=False)\\n    line2 = ax2.plot(complexities, prediction_times, c=\"tab:orange\", ls=\"-\")[0]\\n    ax2.yaxis.tick_right()\\n    ax2.yaxis.set_label_position(\"right\")\\n    y2_label = \"Time (s)\"\\n    ax2.set_ylabel(y2_label)\\n    ax1.spines[\"right\"].set_color(line2.get_color())\\n    ax2.yaxis.label.set_color(line2.get_color())\\n    ax2.tick_params(axis=\"y\", colors=line2.get_color())\\n\\n    plt.legend(\\n        (line1, line2), (\"prediction error\", \"prediction latency\"), loc=\"upper center\"\\n    )\\n\\n    plt.title(\\n        \"Influence of varying \\'%s\\' on %s\"\\n        % (conf[\"changing_param\"], conf[\"estimator\"].__name__)\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_model_complexity_influence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==========================\\nModel Complexity Influence\\n==========================\\n\\nDemonstrate how model complexity influences both prediction accuracy and\\ncomputational performance.\\n\\nWe will be using two datasets:\\n    - :ref:`diabetes_dataset` for regression.\\n      This dataset consists of 10 measurements taken from diabetes patients.\\n      The task is to predict disease progression;\\n    - :ref:`20newsgroups_dataset` for classification. This dataset consists of\\n      newsgroup posts. The task is to predict on which topic (out of 20 topics)\\n      the post is written about.\\n\\nWe will model the complexity influence on three different estimators:\\n    - :class:`~sklearn.linear_model.SGDClassifier` (for classification data)\\n      which implements stochastic gradient descent learning;\\n\\n    - :class:`~sklearn.svm.NuSVR` (for regression data) which implements\\n      Nu support vector regression;\\n\\n    - :class:`~sklearn.ensemble.GradientBoostingRegressor` builds an additive\\n      model in a forward stage-wise fashion. Notice that\\n      :class:`~sklearn.ensemble.HistGradientBoostingRegressor` is much faster\\n      than :class:`~sklearn.ensemble.GradientBoostingRegressor` starting with\\n      intermediate datasets (`n_samples >= 10_000`), which is not the case for\\n      this example.\\n\\n\\nWe make the model complexity vary through the choice of relevant model\\nparameters in each of our selected models. Next, we will measure the influence\\non both computational performance (latency) and predictive power (MSE or\\nHamming Loss).\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.metrics import hamming_loss, mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import NuSVR\\n\\n# Initialize random generator\\nnp.random.seed(0)\\n\\n##############################################################################\\n# Load the data\\n# -------------\\n#\\n# First we load both datasets.\\n#\\n# .. note:: We are using\\n#    :func:`~sklearn.datasets.fetch_20newsgroups_vectorized` to download 20\\n#    newsgroups dataset. It returns ready-to-use features.\\n#\\n# .. note:: ``X`` of the 20 newsgroups dataset is a sparse matrix while ``X``\\n#    of diabetes dataset is a numpy array.\\n#\\n\\n\\n# Code for: def generate_data(case):\\n\\n\\nregression_data = generate_data(\"regression\")\\nclassification_data = generate_data(\"classification\")'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_model_complexity_influence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def generate_data(case):\\n\\n\\nregression_data = generate_data(\"regression\")\\nclassification_data = generate_data(\"classification\")\\n\\n\\n##############################################################################\\n# Benchmark influence\\n# -------------------\\n# Next, we can calculate the influence of the parameters on the given\\n# estimator. In each round, we will set the estimator with the new value of\\n# ``changing_param`` and we will be collecting the prediction times, prediction\\n# performance and complexities to see how those changes affect the estimator.\\n# We will calculate the complexity using ``complexity_computer`` passed as a\\n# parameter.\\n#\\n\\n\\n# Code for: def benchmark_influence(conf):\\n\\n\\n##############################################################################\\n# Choose parameters\\n# -----------------\\n#\\n# We choose the parameters for each of our estimators by making\\n# a dictionary with all the necessary values.\\n# ``changing_param`` is the name of the parameter which will vary in each\\n# estimator.\\n# Complexity will be defined by the ``complexity_label`` and calculated using\\n# `complexity_computer`.\\n# Also note that depending on the estimator type we are passing\\n# different data.\\n#\\n\\n\\n# Code for: def _count_nonzero_coefficients(estimator):'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_model_complexity_influence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def _count_nonzero_coefficients(estimator):\\n\\n\\nconfigurations = [\\n    {\\n        \"estimator\": SGDClassifier,\\n        \"tuned_params\": {\\n            \"penalty\": \"elasticnet\",\\n            \"alpha\": 0.001,\\n            \"loss\": \"modified_huber\",\\n            \"fit_intercept\": True,\\n            \"tol\": 1e-1,\\n            \"n_iter_no_change\": 2,\\n        },\\n        \"changing_param\": \"l1_ratio\",\\n        \"changing_param_values\": [0.25, 0.5, 0.75, 0.9],\\n        \"complexity_label\": \"non_zero coefficients\",\\n        \"complexity_computer\": _count_nonzero_coefficients,\\n        \"prediction_performance_computer\": hamming_loss,\\n        \"prediction_performance_label\": \"Hamming Loss (Misclassification Ratio)\",\\n        \"postfit_hook\": lambda x: x.sparsify(),\\n        \"data\": classification_data,\\n        \"n_samples\": 5,\\n    },\\n    {\\n        \"estimator\": NuSVR,\\n        \"tuned_params\": {\"C\": 1e3, \"gamma\": 2**-15},\\n        \"changing_param\": \"nu\",\\n        \"changing_param_values\": [0.05, 0.1, 0.2, 0.35, 0.5],\\n        \"complexity_label\": \"n_support_vectors\",\\n        \"complexity_computer\": lambda x: len(x.support_vectors_),\\n        \"data\": regression_data,\\n        \"postfit_hook\": lambda x: x,\\n        \"prediction_performance_computer\": mean_squared_error,\\n        \"prediction_performance_label\": \"MSE\",\\n        \"n_samples\": 15,\\n    },\\n    {\\n        \"estimator\": GradientBoostingRegressor,\\n        \"tuned_params\": {\\n            \"loss\": \"squared_error\",\\n            \"learning_rate\": 0.05,\\n            \"max_depth\": 2,\\n        },\\n        \"changing_param\": \"n_estimators\",\\n        \"changing_param_values\": [10, 25, 50, 75, 100],\\n        \"complexity_label\": \"n_trees\",\\n        \"complexity_computer\": lambda x: x.n_estimators,\\n        \"data\": regression_data,\\n        \"postfit_hook\": lambda x: x,\\n        \"prediction_performance_computer\": mean_squared_error,\\n        \"prediction_performance_label\": \"MSE\",\\n        \"n_samples\": 15,\\n    },\\n]\\n\\n\\n##############################################################################\\n# Run the code and plot the results\\n# ---------------------------------\\n#\\n# We defined all the functions required to run our benchmark. Now, we will loop\\n# over the different configurations that we defined previously. Subsequently,\\n# we can analyze the plots obtained from the benchmark:\\n# Relaxing the `L1` penalty in the SGD classifier reduces the prediction error\\n# but leads to an increase in the training time.\\n# We can draw a similar analysis regarding the training time which increases\\n# with the number of support vectors with a Nu-SVR. However, we observed that\\n# there is an optimal number of support vectors which reduces the prediction\\n# error. Indeed, too few support vectors lead to an under-fitted model while\\n# too many support vectors lead to an over-fitted model.\\n# The exact same conclusion can be drawn for the gradient-boosting model. The\\n# only the difference with the Nu-SVR is that having too many trees in the\\n# ensemble is not as detrimental.\\n#'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_model_complexity_influence.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Code for: def plot_influence(conf, mse_values, prediction_times, complexities):\\n\\n\\nfor conf in configurations:\\n    prediction_performances, prediction_times, complexities = benchmark_influence(conf)\\n    plot_influence(conf, prediction_performances, prediction_times, complexities)\\nplt.show()\\n\\n##############################################################################\\n# Conclusion\\n# ----------\\n#\\n# As a conclusion, we can deduce the following insights:\\n#\\n# * a model which is more complex (or expressive) will require a larger\\n#   training time;\\n# * a more complex model does not guarantee to reduce the prediction error.\\n#\\n# These aspects are related to model generalization and avoiding model\\n# under-fitting or over-fitting.'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_face_recognition.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\\n    plt.subplots_adjust(bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\\n    for i in range(n_row * n_col):\\n        plt.subplot(n_row, n_col, i + 1)\\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\\n        plt.title(titles[i], size=12)\\n        plt.xticks(())\\n        plt.yticks(())'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_face_recognition.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def title(y_pred, y_test, target_names, i):\\n    pred_name = target_names[y_pred[i]].rsplit(\" \", 1)[-1]\\n    true_name = target_names[y_test[i]].rsplit(\" \", 1)[-1]\\n    return \"predicted: %s\\\\ntrue:      %s\" % (pred_name, true_name)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_face_recognition.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================================\\nFaces recognition example using eigenfaces and SVMs\\n===================================================\\n\\nThe dataset used in this example is a preprocessed excerpt of the\\n\"Labeled Faces in the Wild\", aka LFW_:\\n\\n  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\\n\\n.. _LFW: http://vis-www.cs.umass.edu/lfw/\\n\\n\"\"\"\\n\\n# %%\\nfrom time import time\\n\\nimport matplotlib.pyplot as plt\\nfrom scipy.stats import loguniform\\n\\nfrom sklearn.datasets import fetch_lfw_people\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report\\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\n\\n# %%\\n# Download the data, if not already on disk and load it as numpy arrays\\n\\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\\n\\n# introspect the images arrays to find the shapes (for plotting)\\nn_samples, h, w = lfw_people.images.shape\\n\\n# for machine learning we use the 2 data directly (as relative pixel\\n# positions info is ignored by this model)\\nX = lfw_people.data\\nn_features = X.shape[1]\\n\\n# the label to predict is the id of the person\\ny = lfw_people.target\\ntarget_names = lfw_people.target_names\\nn_classes = target_names.shape[0]\\n\\nprint(\"Total dataset size:\")\\nprint(\"n_samples: %d\" % n_samples)\\nprint(\"n_features: %d\" % n_features)\\nprint(\"n_classes: %d\" % n_classes)\\n\\n\\n# %%\\n# Split into a training set and a test and keep 25% of the data for testing.\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.25, random_state=42\\n)\\n\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\n# %%\\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\\n# dataset): unsupervised feature extraction / dimensionality reduction\\n\\nn_components = 150\\n\\nprint(\\n    \"Extracting the top %d eigenfaces from %d faces\" % (n_components, X_train.shape[0])\\n)\\nt0 = time()\\npca = PCA(n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(X_train)\\nprint(\"done in %0.3fs\" % (time() - t0))\\n\\neigenfaces = pca.components_.reshape((n_components, h, w))\\n\\nprint(\"Projecting the input data on the eigenfaces orthonormal basis\")\\nt0 = time()\\nX_train_pca = pca.transform(X_train)\\nX_test_pca = pca.transform(X_test)\\nprint(\"done in %0.3fs\" % (time() - t0))\\n\\n\\n# %%\\n# Train a SVM classification model\\n\\nprint(\"Fitting the classifier to the training set\")\\nt0 = time()\\nparam_grid = {\\n    \"C\": loguniform(1e3, 1e5),\\n    \"gamma\": loguniform(1e-4, 1e-1),\\n}\\nclf = RandomizedSearchCV(\\n    SVC(kernel=\"rbf\", class_weight=\"balanced\"), param_grid, n_iter=10\\n)\\nclf = clf.fit(X_train_pca, y_train)\\nprint(\"done in %0.3fs\" % (time() - t0))\\nprint(\"Best estimator found by grid search:\")\\nprint(clf.best_estimator_)\\n\\n\\n# %%\\n# Quantitative evaluation of the model quality on the test set'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_face_recognition.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Quantitative evaluation of the model quality on the test set\\n\\nprint(\"Predicting people\\'s names on the test set\")\\nt0 = time()\\ny_pred = clf.predict(X_test_pca)\\nprint(\"done in %0.3fs\" % (time() - t0))\\n\\nprint(classification_report(y_test, y_pred, target_names=target_names))\\nConfusionMatrixDisplay.from_estimator(\\n    clf, X_test_pca, y_test, display_labels=target_names, xticks_rotation=\"vertical\"\\n)\\nplt.tight_layout()\\nplt.show()\\n\\n\\n# %%\\n# Qualitative evaluation of the predictions using matplotlib\\n\\n\\n# Code for: def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\\n\\n\\n# %%\\n# plot the result of the prediction on a portion of the test set\\n\\n\\n# Code for: def title(y_pred, y_test, target_names, i):\\n\\n\\nprediction_titles = [\\n    title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])\\n]\\n\\nplot_gallery(X_test, prediction_titles, h, w)\\n# %%\\n# plot the gallery of the most significative eigenfaces\\n\\neigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\\nplot_gallery(eigenfaces, eigenface_titles, h, w)\\n\\nplt.show()\\n\\n# %%\\n# Face recognition problem would be much more effectively solved by training\\n# convolutional neural networks but this family of models is outside of the scope of\\n# the scikit-learn library. Interested readers should instead try to use pytorch or\\n# tensorflow to implement such models.'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_stock_market.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=======================================\\nVisualizing the stock market structure\\n=======================================\\n\\nThis example employs several unsupervised learning techniques to extract\\nthe stock market structure from variations in historical quotes.\\n\\nThe quantity that we use is the daily variation in quote price: quotes\\nthat are linked tend to fluctuate in relation to each other during a day.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Retrieve the data from Internet\\n# -------------------------------\\n#\\n# The data is from 2003 - 2008. This is reasonably calm: (not too long ago so\\n# that we get high-tech firms, and before the 2008 crash). This kind of\\n# historical data can be obtained from APIs like the\\n# `data.nasdaq.com <https://data.nasdaq.com/>`_ and\\n# `alphavantage.co <https://www.alphavantage.co/>`_.\\n\\nimport sys\\n\\nimport numpy as np\\nimport pandas as pd\\n\\nsymbol_dict = {\\n    \"TOT\": \"Total\",\\n    \"XOM\": \"Exxon\",\\n    \"CVX\": \"Chevron\",\\n    \"COP\": \"ConocoPhillips\",\\n    \"VLO\": \"Valero Energy\",\\n    \"MSFT\": \"Microsoft\",\\n    \"IBM\": \"IBM\",\\n    \"TWX\": \"Time Warner\",\\n    \"CMCSA\": \"Comcast\",\\n    \"CVC\": \"Cablevision\",\\n    \"YHOO\": \"Yahoo\",\\n    \"DELL\": \"Dell\",\\n    \"HPQ\": \"HP\",\\n    \"AMZN\": \"Amazon\",\\n    \"TM\": \"Toyota\",\\n    \"CAJ\": \"Canon\",\\n    \"SNE\": \"Sony\",\\n    \"F\": \"Ford\",\\n    \"HMC\": \"Honda\",\\n    \"NAV\": \"Navistar\",\\n    \"NOC\": \"Northrop Grumman\",\\n    \"BA\": \"Boeing\",\\n    \"KO\": \"Coca Cola\",\\n    \"MMM\": \"3M\",\\n    \"MCD\": \"McDonald\\'s\",\\n    \"PEP\": \"Pepsi\",\\n    \"K\": \"Kellogg\",\\n    \"UN\": \"Unilever\",\\n    \"MAR\": \"Marriott\",\\n    \"PG\": \"Procter Gamble\",\\n    \"CL\": \"Colgate-Palmolive\",\\n    \"GE\": \"General Electrics\",\\n    \"WFC\": \"Wells Fargo\",\\n    \"JPM\": \"JPMorgan Chase\",\\n    \"AIG\": \"AIG\",\\n    \"AXP\": \"American express\",\\n    \"BAC\": \"Bank of America\",\\n    \"GS\": \"Goldman Sachs\",\\n    \"AAPL\": \"Apple\",\\n    \"SAP\": \"SAP\",\\n    \"CSCO\": \"Cisco\",\\n    \"TXN\": \"Texas Instruments\",\\n    \"XRX\": \"Xerox\",\\n    \"WMT\": \"Wal-Mart\",\\n    \"HD\": \"Home Depot\",\\n    \"GSK\": \"GlaxoSmithKline\",\\n    \"PFE\": \"Pfizer\",\\n    \"SNY\": \"Sanofi-Aventis\",\\n    \"NVS\": \"Novartis\",\\n    \"KMB\": \"Kimberly-Clark\",\\n    \"R\": \"Ryder\",\\n    \"GD\": \"General Dynamics\",\\n    \"RTN\": \"Raytheon\",\\n    \"CVS\": \"CVS\",\\n    \"CAT\": \"Caterpillar\",\\n    \"DD\": \"DuPont de Nemours\",\\n}\\n\\n\\nsymbols, names = np.array(sorted(symbol_dict.items())).T\\n\\nquotes = []\\n\\nfor symbol in symbols:\\n    print(\"Fetching quote history for %r\" % symbol, file=sys.stderr)\\n    url = (\\n        \"https://raw.githubusercontent.com/scikit-learn/examples-data/\"\\n        \"master/financial-data/{}.csv\"\\n    )\\n    quotes.append(pd.read_csv(url.format(symbol)))\\n\\nclose_prices = np.vstack([q[\"close\"] for q in quotes])\\nopen_prices = np.vstack([q[\"open\"] for q in quotes])\\n\\n# The daily variations of the quotes are what carry the most information\\nvariation = close_prices - open_prices'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_stock_market.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='close_prices = np.vstack([q[\"close\"] for q in quotes])\\nopen_prices = np.vstack([q[\"open\"] for q in quotes])\\n\\n# The daily variations of the quotes are what carry the most information\\nvariation = close_prices - open_prices\\n\\n# %%\\n# .. _stock_market:\\n#\\n# Learning a graph structure\\n# --------------------------\\n#\\n# We use sparse inverse covariance estimation to find which quotes are\\n# correlated conditionally on the others. Specifically, sparse inverse\\n# covariance gives us a graph, that is a list of connections. For each\\n# symbol, the symbols that it is connected to are those useful to explain\\n# its fluctuations.\\n\\nfrom sklearn import covariance\\n\\nalphas = np.logspace(-1.5, 1, num=10)\\nedge_model = covariance.GraphicalLassoCV(alphas=alphas)\\n\\n# standardize the time series: using correlations rather than covariance\\n# former is more efficient for structure recovery\\nX = variation.copy().T\\nX /= X.std(axis=0)\\nedge_model.fit(X)\\n\\n# %%\\n# Clustering using affinity propagation\\n# -------------------------------------\\n#\\n# We use clustering to group together quotes that behave similarly. Here,\\n# amongst the :ref:`various clustering techniques <clustering>` available\\n# in the scikit-learn, we use :ref:`affinity_propagation` as it does\\n# not enforce equal-size clusters, and it can choose automatically the\\n# number of clusters from the data.\\n#\\n# Note that this gives us a different indication than the graph, as the\\n# graph reflects conditional relations between variables, while the\\n# clustering reflects marginal properties: variables clustered together can\\n# be considered as having a similar impact at the level of the full stock\\n# market.\\n\\nfrom sklearn import cluster\\n\\n_, labels = cluster.affinity_propagation(edge_model.covariance_, random_state=0)\\nn_labels = labels.max()\\n\\nfor i in range(n_labels + 1):\\n    print(f\"Cluster {i + 1}: {\\', \\'.join(names[labels == i])}\")\\n\\n# %%\\n# Embedding in 2D space\\n# ---------------------\\n#\\n# For visualization purposes, we need to lay out the different symbols on a\\n# 2D canvas. For this we use :ref:`manifold` techniques to retrieve 2D\\n# embedding.\\n# We use a dense eigen_solver to achieve reproducibility (arpack is initiated\\n# with the random vectors that we don\\'t control). In addition, we use a large\\n# number of neighbors to capture the large-scale structure.\\n\\n# Finding a low-dimension embedding for visualization: find the best position of\\n# the nodes (the stocks) on a 2D plane\\n\\nfrom sklearn import manifold\\n\\nnode_position_model = manifold.LocallyLinearEmbedding(\\n    n_components=2, eigen_solver=\"dense\", n_neighbors=6\\n)\\n\\nembedding = node_position_model.fit_transform(X.T).T'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_stock_market.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='from sklearn import manifold\\n\\nnode_position_model = manifold.LocallyLinearEmbedding(\\n    n_components=2, eigen_solver=\"dense\", n_neighbors=6\\n)\\n\\nembedding = node_position_model.fit_transform(X.T).T\\n\\n# %%\\n# Visualization\\n# -------------\\n#\\n# The output of the 3 models are combined in a 2D graph where nodes\\n# represents the stocks and edges the:\\n#\\n# - cluster labels are used to define the color of the nodes\\n# - the sparse covariance model is used to display the strength of the edges\\n# - the 2D embedding is used to position the nodes in the plan\\n#\\n# This example has a fair amount of visualization-related code, as\\n# visualization is crucial here to display the graph. One of the challenge\\n# is to position the labels minimizing overlap. For this we use an\\n# heuristic based on the direction of the nearest neighbor along each\\n# axis.\\n\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.collections import LineCollection\\n\\nplt.figure(1, facecolor=\"w\", figsize=(10, 8))\\nplt.clf()\\nax = plt.axes([0.0, 0.0, 1.0, 1.0])\\nplt.axis(\"off\")\\n\\n# Plot the graph of partial correlations\\npartial_correlations = edge_model.precision_.copy()\\nd = 1 / np.sqrt(np.diag(partial_correlations))\\npartial_correlations *= d\\npartial_correlations *= d[:, np.newaxis]\\nnon_zero = np.abs(np.triu(partial_correlations, k=1)) > 0.02\\n\\n# Plot the nodes using the coordinates of our embedding\\nplt.scatter(\\n    embedding[0], embedding[1], s=100 * d**2, c=labels, cmap=plt.cm.nipy_spectral\\n)\\n\\n# Plot the edges\\nstart_idx, end_idx = np.where(non_zero)\\n# a sequence of (*line0*, *line1*, *line2*), where::\\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\\nsegments = [\\n    [embedding[:, start], embedding[:, stop]] for start, stop in zip(start_idx, end_idx)\\n]\\nvalues = np.abs(partial_correlations[non_zero])\\nlc = LineCollection(\\n    segments, zorder=0, cmap=plt.cm.hot_r, norm=plt.Normalize(0, 0.7 * values.max())\\n)\\nlc.set_array(values)\\nlc.set_linewidths(15 * values)\\nax.add_collection(lc)\\n\\n# Add a label to each node. The challenge here is that we want to\\n# position the labels to avoid overlap with other labels\\nfor index, (name, label, (x, y)) in enumerate(zip(names, labels, embedding.T)):\\n    dx = x - embedding[0]\\n    dx[index] = 1\\n    dy = y - embedding[1]\\n    dy[index] = 1\\n    this_dx = dx[np.argmin(np.abs(dy))]\\n    this_dy = dy[np.argmin(np.abs(dx))]\\n    if this_dx > 0:\\n        horizontalalignment = \"left\"\\n        x = x + 0.002\\n    else:\\n        horizontalalignment = \"right\"\\n        x = x - 0.002\\n    if this_dy > 0:\\n        verticalalignment = \"bottom\"\\n        y = y + 0.002\\n    else:\\n        verticalalignment = \"top\"\\n        y = y - 0.002\\n    plt.text(\\n        x,\\n        y,\\n        name,\\n        size=10,\\n        horizontalalignment=horizontalalignment,\\n        verticalalignment=verticalalignment,\\n        bbox=dict(\\n            facecolor=\"w\",\\n            edgecolor=plt.cm.nipy_spectral(label / float(n_labels)),\\n            alpha=0.6,\\n        ),\\n    )'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_stock_market.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.xlim(\\n    embedding[0].min() - 0.15 * np.ptp(embedding[0]),\\n    embedding[0].max() + 0.10 * np.ptp(embedding[0]),\\n)\\nplt.ylim(\\n    embedding[1].min() - 0.03 * np.ptp(embedding[1]),\\n    embedding[1].max() + 0.03 * np.ptp(embedding[1]),\\n)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_species_distribution_modeling.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def construct_grids(batch):\\n    \"\"\"Construct the map grid from the batch object\\n\\n    Parameters\\n    ----------\\n    batch : Batch object\\n        The object returned by :func:`fetch_species_distributions`\\n\\n    Returns\\n    -------\\n    (xgrid, ygrid) : 1-D arrays\\n        The grid corresponding to the values in batch.coverages\\n    \"\"\"\\n    # x,y coordinates for corner cells\\n    xmin = batch.x_left_lower_corner + batch.grid_size\\n    xmax = xmin + (batch.Nx * batch.grid_size)\\n    ymin = batch.y_left_lower_corner + batch.grid_size\\n    ymax = ymin + (batch.Ny * batch.grid_size)\\n\\n    # x coordinates of the grid cells\\n    xgrid = np.arange(xmin, xmax, batch.grid_size)\\n    # y coordinates of the grid cells\\n    ygrid = np.arange(ymin, ymax, batch.grid_size)\\n\\n    return (xgrid, ygrid)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_species_distribution_modeling.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def create_species_bunch(species_name, train, test, coverages, xgrid, ygrid):\\n    \"\"\"Create a bunch with information about a particular organism\\n\\n    This will use the test/train record arrays to extract the\\n    data specific to the given species name.\\n    \"\"\"\\n    bunch = Bunch(name=\" \".join(species_name.split(\"_\")[:2]))\\n    species_name = species_name.encode(\"ascii\")\\n    points = dict(test=test, train=train)\\n\\n    for label, pts in points.items():\\n        # choose points associated with the desired species\\n        pts = pts[pts[\"species\"] == species_name]\\n        bunch[\"pts_%s\" % label] = pts\\n\\n        # determine coverage values for each of the training & testing points\\n        ix = np.searchsorted(xgrid, pts[\"dd long\"])\\n        iy = np.searchsorted(ygrid, pts[\"dd lat\"])\\n        bunch[\"cov_%s\" % label] = coverages[:, -iy, ix].T\\n\\n    return bunch'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_species_distribution_modeling.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_species_distribution(\\n    species=(\"bradypus_variegatus_0\", \"microryzomys_minutus_0\")\\n):\\n    \"\"\"\\n    Plot the species distribution.\\n    \"\"\"\\n    if len(species) > 2:\\n        print(\\n            \"Note: when more than two species are provided,\"\\n            \" only the first two will be used\"\\n        )\\n\\n    t0 = time()\\n\\n    # Load the compressed data\\n    data = fetch_species_distributions()\\n\\n    # Set up the data grid\\n    xgrid, ygrid = construct_grids(data)\\n\\n    # The grid in x,y coordinates\\n    X, Y = np.meshgrid(xgrid, ygrid[::-1])\\n\\n    # create a bunch for each species\\n    BV_bunch = create_species_bunch(\\n        species[0], data.train, data.test, data.coverages, xgrid, ygrid\\n    )\\n    MM_bunch = create_species_bunch(\\n        species[1], data.train, data.test, data.coverages, xgrid, ygrid\\n    )\\n\\n    # background points (grid coordinates) for evaluation\\n    np.random.seed(13)\\n    background_points = np.c_[\\n        np.random.randint(low=0, high=data.Ny, size=10000),\\n        np.random.randint(low=0, high=data.Nx, size=10000),\\n    ].T\\n\\n    # We\\'ll make use of the fact that coverages[6] has measurements at all\\n    # land points.  This will help us decide between land and water.\\n    land_reference = data.coverages[6]\\n\\n    # Fit, predict, and plot for each species.\\n    for i, species in enumerate([BV_bunch, MM_bunch]):\\n        print(\"_\" * 80)\\n        print(\"Modeling distribution of species \\'%s\\'\" % species.name)\\n\\n        # Standardize features\\n        mean = species.cov_train.mean(axis=0)\\n        std = species.cov_train.std(axis=0)\\n        train_cover_std = (species.cov_train - mean) / std\\n\\n        # Fit OneClassSVM\\n        print(\" - fit OneClassSVM ... \", end=\"\")\\n        clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.5)\\n        clf.fit(train_cover_std)\\n        print(\"done.\")\\n\\n        # Plot map of South America\\n        plt.subplot(1, 2, i + 1)\\n        if basemap:\\n            print(\" - plot coastlines using basemap\")\\n            m = Basemap(\\n                projection=\"cyl\",\\n                llcrnrlat=Y.min(),\\n                urcrnrlat=Y.max(),\\n                llcrnrlon=X.min(),\\n                urcrnrlon=X.max(),\\n                resolution=\"c\",\\n            )\\n            m.drawcoastlines()\\n            m.drawcountries()\\n        else:\\n            print(\" - plot coastlines from coverage\")\\n            plt.contour(\\n                X, Y, land_reference, levels=[-9998], colors=\"k\", linestyles=\"solid\"\\n            )\\n            plt.xticks([])\\n            plt.yticks([])\\n\\n        print(\" - predict species distribution\")\\n\\n        # Predict species distribution using the training data\\n        Z = np.ones((data.Ny, data.Nx), dtype=np.float64)\\n\\n        # We\\'ll predict only for the land points.\\n        idx = np.where(land_reference > -9999)\\n        coverages_land = data.coverages[:, idx[0], idx[1]].T\\n\\n        pred = clf.decision_function((coverages_land - mean) / std)\\n        Z *= pred.min()\\n        Z[idx[0], idx[1]] = pred'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_species_distribution_modeling.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='# We\\'ll predict only for the land points.\\n        idx = np.where(land_reference > -9999)\\n        coverages_land = data.coverages[:, idx[0], idx[1]].T\\n\\n        pred = clf.decision_function((coverages_land - mean) / std)\\n        Z *= pred.min()\\n        Z[idx[0], idx[1]] = pred\\n\\n        levels = np.linspace(Z.min(), Z.max(), 25)\\n        Z[land_reference == -9999] = -9999\\n\\n        # plot contours of the prediction\\n        plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\\n        plt.colorbar(format=\"%.2f\")\\n\\n        # scatter training/testing points\\n        plt.scatter(\\n            species.pts_train[\"dd long\"],\\n            species.pts_train[\"dd lat\"],\\n            s=2**2,\\n            c=\"black\",\\n            marker=\"^\",\\n            label=\"train\",\\n        )\\n        plt.scatter(\\n            species.pts_test[\"dd long\"],\\n            species.pts_test[\"dd lat\"],\\n            s=2**2,\\n            c=\"black\",\\n            marker=\"x\",\\n            label=\"test\",\\n        )\\n        plt.legend()\\n        plt.title(species.name)\\n        plt.axis(\"equal\")\\n\\n        # Compute AUC with regards to background points\\n        pred_background = Z[background_points[0], background_points[1]]\\n        pred_test = clf.decision_function((species.cov_test - mean) / std)\\n        scores = np.r_[pred_test, pred_background]\\n        y = np.r_[np.ones(pred_test.shape), np.zeros(pred_background.shape)]\\n        fpr, tpr, thresholds = metrics.roc_curve(y, scores)\\n        roc_auc = metrics.auc(fpr, tpr)\\n        plt.text(-35, -70, \"AUC: %.3f\" % roc_auc, ha=\"right\")\\n        print(\"\\\\n Area under the ROC curve : %f\" % roc_auc)\\n\\n    print(\"\\\\ntime elapsed: %.2fs\" % (time() - t0))'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_species_distribution_modeling.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================\\nSpecies distribution modeling\\n=============================\\n\\nModeling species\\' geographic distributions is an important\\nproblem in conservation biology. In this example, we\\nmodel the geographic distribution of two South American\\nmammals given past observations and 14 environmental\\nvariables. Since we have only positive examples (there are\\nno unsuccessful observations), we cast this problem as a\\ndensity estimation problem and use the :class:`~sklearn.svm.OneClassSVM`\\nas our modeling tool. The dataset is provided by Phillips et. al. (2006).\\nIf available, the example uses\\n`basemap <https://matplotlib.org/basemap/>`_\\nto plot the coast lines and national boundaries of South America.\\n\\nThe two species are:\\n\\n - `\"Bradypus variegatus\"\\n   <http://www.iucnredlist.org/details/3038/0>`_ ,\\n   the Brown-throated Sloth.\\n\\n - `\"Microryzomys minutus\"\\n   <http://www.iucnredlist.org/details/13408/0>`_ ,\\n   also known as the Forest Small Rice Rat, a rodent that lives in Peru,\\n   Colombia, Ecuador, Peru, and Venezuela.\\n\\nReferences\\n----------\\n\\n * `\"Maximum entropy modeling of species geographic distributions\"\\n   <http://rob.schapire.net/papers/ecolmod.pdf>`_\\n   S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\\n   190:231-259, 2006.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom time import time\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import metrics, svm\\nfrom sklearn.datasets import fetch_species_distributions\\nfrom sklearn.utils import Bunch\\n\\n# if basemap is available, we\\'ll use it.\\n# otherwise, we\\'ll improvise later...\\ntry:\\n    from mpl_toolkits.basemap import Basemap\\n\\n    basemap = True\\nexcept ImportError:\\n    basemap = False\\n\\n\\n# Code for: def construct_grids(batch):\\n\\n\\n# Code for: def create_species_bunch(species_name, train, test, coverages, xgrid, ygrid):\\n\\n\\n# Code for: def plot_species_distribution(\\n\\n\\nplot_species_distribution()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def _not_in_sphinx():\\n    # Hack to detect whether we are running by the sphinx builder\\n    return \"__file__\" in globals()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def atomic_benchmark_estimator(estimator, X_test, verbose=False):\\n    \"\"\"Measure runtime prediction of each instance.\"\"\"\\n    n_instances = X_test.shape[0]\\n    runtimes = np.zeros(n_instances, dtype=float)\\n    for i in range(n_instances):\\n        instance = X_test[[i], :]\\n        start = time.time()\\n        estimator.predict(instance)\\n        runtimes[i] = time.time() - start\\n    if verbose:\\n        print(\\n            \"atomic_benchmark runtimes:\",\\n            min(runtimes),\\n            np.percentile(runtimes, 50),\\n            max(runtimes),\\n        )\\n    return runtimes'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):\\n    \"\"\"Measure runtime prediction of the whole input.\"\"\"\\n    n_instances = X_test.shape[0]\\n    runtimes = np.zeros(n_bulk_repeats, dtype=float)\\n    for i in range(n_bulk_repeats):\\n        start = time.time()\\n        estimator.predict(X_test)\\n        runtimes[i] = time.time() - start\\n    runtimes = np.array(list(map(lambda x: x / float(n_instances), runtimes)))\\n    if verbose:\\n        print(\\n            \"bulk_benchmark runtimes:\",\\n            min(runtimes),\\n            np.percentile(runtimes, 50),\\n            max(runtimes),\\n        )\\n    return runtimes'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def benchmark_estimator(estimator, X_test, n_bulk_repeats=30, verbose=False):\\n    \"\"\"\\n    Measure runtimes of prediction in both atomic and bulk mode.\\n\\n    Parameters\\n    ----------\\n    estimator : already trained estimator supporting `predict()`\\n    X_test : test input\\n    n_bulk_repeats : how many times to repeat when evaluating bulk mode\\n\\n    Returns\\n    -------\\n    atomic_runtimes, bulk_runtimes : a pair of `np.array` which contain the\\n    runtimes in seconds.\\n\\n    \"\"\"\\n    atomic_runtimes = atomic_benchmark_estimator(estimator, X_test, verbose)\\n    bulk_runtimes = bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose)\\n    return atomic_runtimes, bulk_runtimes'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def generate_dataset(n_train, n_test, n_features, noise=0.1, verbose=False):\\n    \"\"\"Generate a regression dataset with the given parameters.\"\"\"\\n    if verbose:\\n        print(\"generating dataset...\")\\n\\n    X, y, coef = make_regression(\\n        n_samples=n_train + n_test, n_features=n_features, noise=noise, coef=True\\n    )\\n\\n    random_seed = 13\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, train_size=n_train, test_size=n_test, random_state=random_seed\\n    )\\n    X_train, y_train = shuffle(X_train, y_train, random_state=random_seed)\\n\\n    X_scaler = StandardScaler()\\n    X_train = X_scaler.fit_transform(X_train)\\n    X_test = X_scaler.transform(X_test)\\n\\n    y_scaler = StandardScaler()\\n    y_train = y_scaler.fit_transform(y_train[:, None])[:, 0]\\n    y_test = y_scaler.transform(y_test[:, None])[:, 0]\\n\\n    gc.collect()\\n    if verbose:\\n        print(\"ok\")\\n    return X_train, y_train, X_test, y_test'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def boxplot_runtimes(runtimes, pred_type, configuration):\\n    \"\"\"\\n    Plot a new `Figure` with boxplots of prediction runtimes.\\n\\n    Parameters\\n    ----------\\n    runtimes : list of `np.array` of latencies in micro-seconds\\n    cls_names : list of estimator class names that generated the runtimes\\n    pred_type : \\'bulk\\' or \\'atomic\\'\\n\\n    \"\"\"\\n\\n    fig, ax1 = plt.subplots(figsize=(10, 6))\\n    bp = plt.boxplot(\\n        runtimes,\\n    )\\n\\n    cls_infos = [\\n        \"%s\\\\n(%d %s)\"\\n        % (\\n            estimator_conf[\"name\"],\\n            estimator_conf[\"complexity_computer\"](estimator_conf[\"instance\"]),\\n            estimator_conf[\"complexity_label\"],\\n        )\\n        for estimator_conf in configuration[\"estimators\"]\\n    ]\\n    plt.setp(ax1, xticklabels=cls_infos)\\n    plt.setp(bp[\"boxes\"], color=\"black\")\\n    plt.setp(bp[\"whiskers\"], color=\"black\")\\n    plt.setp(bp[\"fliers\"], color=\"red\", marker=\"+\")\\n\\n    ax1.yaxis.grid(True, linestyle=\"-\", which=\"major\", color=\"lightgrey\", alpha=0.5)\\n\\n    ax1.set_axisbelow(True)\\n    ax1.set_title(\\n        \"Prediction Time per Instance - %s, %d feats.\"\\n        % (pred_type.capitalize(), configuration[\"n_features\"])\\n    )\\n    ax1.set_ylabel(\"Prediction Time (us)\")\\n\\n    plt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def benchmark(configuration):\\n    \"\"\"Run the whole benchmark.\"\"\"\\n    X_train, y_train, X_test, y_test = generate_dataset(\\n        configuration[\"n_train\"], configuration[\"n_test\"], configuration[\"n_features\"]\\n    )\\n\\n    stats = {}\\n    for estimator_conf in configuration[\"estimators\"]:\\n        print(\"Benchmarking\", estimator_conf[\"instance\"])\\n        estimator_conf[\"instance\"].fit(X_train, y_train)\\n        gc.collect()\\n        a, b = benchmark_estimator(estimator_conf[\"instance\"], X_test)\\n        stats[estimator_conf[\"name\"]] = {\"atomic\": a, \"bulk\": b}\\n\\n    cls_names = [\\n        estimator_conf[\"name\"] for estimator_conf in configuration[\"estimators\"]\\n    ]\\n    runtimes = [1e6 * stats[clf_name][\"atomic\"] for clf_name in cls_names]\\n    boxplot_runtimes(runtimes, \"atomic\", configuration)\\n    runtimes = [1e6 * stats[clf_name][\"bulk\"] for clf_name in cls_names]\\n    boxplot_runtimes(runtimes, \"bulk (%d)\" % configuration[\"n_test\"], configuration)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def n_feature_influence(estimators, n_train, n_test, n_features, percentile):\\n    \"\"\"\\n    Estimate influence of the number of features on prediction time.\\n\\n    Parameters\\n    ----------\\n\\n    estimators : dict of (name (str), estimator) to benchmark\\n    n_train : nber of training instances (int)\\n    n_test : nber of testing instances (int)\\n    n_features : list of feature-space dimensionality to test (int)\\n    percentile : percentile at which to measure the speed (int [0-100])\\n\\n    Returns:\\n    --------\\n\\n    percentiles : dict(estimator_name,\\n                       dict(n_features, percentile_perf_in_us))\\n\\n    \"\"\"\\n    percentiles = defaultdict(defaultdict)\\n    for n in n_features:\\n        print(\"benchmarking with %d features\" % n)\\n        X_train, y_train, X_test, y_test = generate_dataset(n_train, n_test, n)\\n        for cls_name, estimator in estimators.items():\\n            estimator.fit(X_train, y_train)\\n            gc.collect()\\n            runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)\\n            percentiles[cls_name][n] = 1e6 * np.percentile(runtimes, percentile)\\n    return percentiles'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_n_features_influence(percentiles, percentile):\\n    fig, ax1 = plt.subplots(figsize=(10, 6))\\n    colors = [\"r\", \"g\", \"b\"]\\n    for i, cls_name in enumerate(percentiles.keys()):\\n        x = np.array(sorted(percentiles[cls_name].keys()))\\n        y = np.array([percentiles[cls_name][n] for n in x])\\n        plt.plot(\\n            x,\\n            y,\\n            color=colors[i],\\n        )\\n    ax1.yaxis.grid(True, linestyle=\"-\", which=\"major\", color=\"lightgrey\", alpha=0.5)\\n    ax1.set_axisbelow(True)\\n    ax1.set_title(\"Evolution of Prediction Time with #Features\")\\n    ax1.set_xlabel(\"#Features\")\\n    ax1.set_ylabel(\"Prediction Time at %d%%-ile (us)\" % percentile)\\n    plt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def benchmark_throughputs(configuration, duration_secs=0.1):\\n    \"\"\"benchmark throughput for different estimators.\"\"\"\\n    X_train, y_train, X_test, y_test = generate_dataset(\\n        configuration[\"n_train\"], configuration[\"n_test\"], configuration[\"n_features\"]\\n    )\\n    throughputs = dict()\\n    for estimator_config in configuration[\"estimators\"]:\\n        estimator_config[\"instance\"].fit(X_train, y_train)\\n        start_time = time.time()\\n        n_predictions = 0\\n        while (time.time() - start_time) < duration_secs:\\n            estimator_config[\"instance\"].predict(X_test[[0]])\\n            n_predictions += 1\\n        throughputs[estimator_config[\"name\"]] = n_predictions / duration_secs\\n    return throughputs'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_benchmark_throughput(throughputs, configuration):\\n    fig, ax = plt.subplots(figsize=(10, 6))\\n    colors = [\"r\", \"g\", \"b\"]\\n    cls_infos = [\\n        \"%s\\\\n(%d %s)\"\\n        % (\\n            estimator_conf[\"name\"],\\n            estimator_conf[\"complexity_computer\"](estimator_conf[\"instance\"]),\\n            estimator_conf[\"complexity_label\"],\\n        )\\n        for estimator_conf in configuration[\"estimators\"]\\n    ]\\n    cls_values = [\\n        throughputs[estimator_conf[\"name\"]]\\n        for estimator_conf in configuration[\"estimators\"]\\n    ]\\n    plt.bar(range(len(throughputs)), cls_values, width=0.5, color=colors)\\n    ax.set_xticks(np.linspace(0.25, len(throughputs) - 0.75, len(throughputs)))\\n    ax.set_xticklabels(cls_infos, fontsize=10)\\n    ymax = max(cls_values) * 1.2\\n    ax.set_ylim((0, ymax))\\n    ax.set_ylabel(\"Throughput (predictions/sec)\")\\n    ax.set_title(\\n        \"Prediction Throughput for different estimators (%d features)\"\\n        % configuration[\"n_features\"]\\n    )\\n    plt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==================\\nPrediction Latency\\n==================\\n\\nThis is an example showing the prediction latency of various scikit-learn\\nestimators.\\n\\nThe goal is to measure the latency one can expect when doing predictions\\neither in bulk or atomic (i.e. one by one) mode.\\n\\nThe plots represent the distribution of the prediction latency as a boxplot.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport gc\\nimport time\\nfrom collections import defaultdict\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.linear_model import Ridge, SGDRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVR\\nfrom sklearn.utils import shuffle\\n\\n\\n# Code for: def _not_in_sphinx():\\n\\n\\n# %%\\n# Benchmark and plot helper functions\\n# -----------------------------------\\n\\n\\n# Code for: def atomic_benchmark_estimator(estimator, X_test, verbose=False):\\n\\n\\n# Code for: def bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):\\n\\n\\n# Code for: def benchmark_estimator(estimator, X_test, n_bulk_repeats=30, verbose=False):\\n\\n\\n# Code for: def generate_dataset(n_train, n_test, n_features, noise=0.1, verbose=False):\\n\\n\\n# Code for: def boxplot_runtimes(runtimes, pred_type, configuration):\\n\\n\\n# Code for: def benchmark(configuration):\\n\\n\\n# Code for: def n_feature_influence(estimators, n_train, n_test, n_features, percentile):\\n\\n\\n# Code for: def plot_n_features_influence(percentiles, percentile):\\n\\n\\n# Code for: def benchmark_throughputs(configuration, duration_secs=0.1):\\n\\n\\n# Code for: def plot_benchmark_throughput(throughputs, configuration):\\n\\n\\n# %%\\n# Benchmark bulk/atomic prediction speed for various regressors\\n# -------------------------------------------------------------\\n\\nconfiguration = {\\n    \"n_train\": int(1e3),\\n    \"n_test\": int(1e2),\\n    \"n_features\": int(1e2),\\n    \"estimators\": [\\n        {\\n            \"name\": \"Linear Model\",\\n            \"instance\": SGDRegressor(\\n                penalty=\"elasticnet\", alpha=0.01, l1_ratio=0.25, tol=1e-4\\n            ),\\n            \"complexity_label\": \"non-zero coefficients\",\\n            \"complexity_computer\": lambda clf: np.count_nonzero(clf.coef_),\\n        },\\n        {\\n            \"name\": \"RandomForest\",\\n            \"instance\": RandomForestRegressor(),\\n            \"complexity_label\": \"estimators\",\\n            \"complexity_computer\": lambda clf: clf.n_estimators,\\n        },\\n        {\\n            \"name\": \"SVR\",\\n            \"instance\": SVR(kernel=\"rbf\"),\\n            \"complexity_label\": \"support vectors\",\\n            \"complexity_computer\": lambda clf: len(clf.support_vectors_),\\n        },\\n    ],\\n}\\nbenchmark(configuration)\\n\\n# %%\\n# Benchmark n_features influence on prediction speed\\n# --------------------------------------------------'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_prediction_latency.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Benchmark n_features influence on prediction speed\\n# --------------------------------------------------\\n\\npercentile = 90\\npercentiles = n_feature_influence(\\n    {\"ridge\": Ridge()},\\n    configuration[\"n_train\"],\\n    configuration[\"n_test\"],\\n    [100, 250, 500],\\n    percentile,\\n)\\nplot_n_features_influence(percentiles, percentile)\\n\\n# %%\\n# Benchmark throughput\\n# --------------------\\n\\nthroughputs = benchmark_throughputs(configuration)\\nplot_benchmark_throughput(throughputs, configuration)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_outlier_detection_wine.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n====================================\\nOutlier detection on a real data set\\n====================================\\n\\nThis example illustrates the need for robust covariance estimation\\non a real data set. It is useful both for outlier detection and for\\na better understanding of the data structure.\\n\\nWe selected two sets of two variables from the Wine data set\\nas an illustration of what kind of analysis can be done with several\\noutlier detection tools. For the purpose of visualization, we are working\\nwith two-dimensional examples, but one should be aware that things are\\nnot so trivial in high-dimension, as it will be pointed out.\\n\\nIn both examples below, the main result is that the empirical covariance\\nestimate, as a non-robust one, is highly influenced by the heterogeneous\\nstructure of the observations. Although the robust covariance estimate is\\nable to focus on the main mode of the data distribution, it sticks to the\\nassumption that the data should be Gaussian distributed, yielding some biased\\nestimation of the data structure, but yet accurate to some extent.\\nThe One-Class SVM does not assume any parametric form of the data distribution\\nand can therefore model the complex shape of the data much better.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# First example\\n# -------------\\n#\\n# The first example illustrates how the Minimum Covariance Determinant\\n# robust estimator can help concentrate on a relevant cluster when outlying\\n# points exist. Here the empirical covariance estimation is skewed by points\\n# outside of the main cluster. Of course, some screening tools would have pointed\\n# out the presence of two clusters (Support Vector Machines, Gaussian Mixture\\n# Models, univariate outlier detection, ...). But had it been a high-dimensional\\n# example, none of these could be applied that easily.\\nfrom sklearn.covariance import EllipticEnvelope\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.svm import OneClassSVM\\n\\nestimators = {\\n    \"Empirical Covariance\": EllipticEnvelope(support_fraction=1.0, contamination=0.25),\\n    \"Robust Covariance (Minimum Covariance Determinant)\": EllipticEnvelope(\\n        contamination=0.25\\n    ),\\n    \"OCSVM\": OneClassSVM(nu=0.25, gamma=0.35),\\n}\\n\\n# %%\\nimport matplotlib.lines as mlines\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import load_wine\\n\\nX = load_wine()[\"data\"][:, [1, 2]]  # two clusters\\n\\nfig, ax = plt.subplots()\\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:red\"]\\n# Learn a frontier for outlier detection with several classifiers\\nlegend_lines = []\\nfor color, (name, estimator) in zip(colors, estimators.items()):\\n    estimator.fit(X)\\n    DecisionBoundaryDisplay.from_estimator(\\n        estimator,\\n        X,\\n        response_method=\"decision_function\",\\n        plot_method=\"contour\",\\n        levels=[0],\\n        colors=color,\\n        ax=ax,\\n    )\\n    legend_lines.append(mlines.Line2D([], [], color=color, label=name))'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_outlier_detection_wine.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='ax.scatter(X[:, 0], X[:, 1], color=\"black\")\\nbbox_args = dict(boxstyle=\"round\", fc=\"0.8\")\\narrow_args = dict(arrowstyle=\"->\")\\nax.annotate(\\n    \"outlying points\",\\n    xy=(4, 2),\\n    xycoords=\"data\",\\n    textcoords=\"data\",\\n    xytext=(3, 1.25),\\n    bbox=bbox_args,\\n    arrowprops=arrow_args,\\n)\\nax.legend(handles=legend_lines, loc=\"upper center\")\\n_ = ax.set(\\n    xlabel=\"ash\",\\n    ylabel=\"malic_acid\",\\n    title=\"Outlier detection on a real data set (wine recognition)\",\\n)\\n\\n# %%\\n# Second example\\n# --------------\\n#\\n# The second example shows the ability of the Minimum Covariance Determinant\\n# robust estimator of covariance to concentrate on the main mode of the data\\n# distribution: the location seems to be well estimated, although the\\n# covariance is hard to estimate due to the banana-shaped distribution. Anyway,\\n# we can get rid of some outlying observations. The One-Class SVM is able to\\n# capture the real data structure, but the difficulty is to adjust its kernel\\n# bandwidth parameter so as to obtain a good compromise between the shape of\\n# the data scatter matrix and the risk of over-fitting the data.\\nX = load_wine()[\"data\"][:, [6, 9]]  # \"banana\"-shaped\\n\\nfig, ax = plt.subplots()\\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:red\"]\\n# Learn a frontier for outlier detection with several classifiers\\nlegend_lines = []\\nfor color, (name, estimator) in zip(colors, estimators.items()):\\n    estimator.fit(X)\\n    DecisionBoundaryDisplay.from_estimator(\\n        estimator,\\n        X,\\n        response_method=\"decision_function\",\\n        plot_method=\"contour\",\\n        levels=[0],\\n        colors=color,\\n        ax=ax,\\n    )\\n    legend_lines.append(mlines.Line2D([], [], color=color, label=name))\\n\\n\\nax.scatter(X[:, 0], X[:, 1], color=\"black\")\\nax.legend(handles=legend_lines, loc=\"upper center\")\\nax.set(\\n    xlabel=\"flavanoids\",\\n    ylabel=\"color_intensity\",\\n    title=\"Outlier detection on a real data set (wine recognition)\",\\n)\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_topics_extraction_with_nmf_lda.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def plot_top_words(model, feature_names, n_top_words, title):\\n    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\\n    axes = axes.flatten()\\n    for topic_idx, topic in enumerate(model.components_):\\n        top_features_ind = topic.argsort()[-n_top_words:]\\n        top_features = feature_names[top_features_ind]\\n        weights = topic[top_features_ind]\\n\\n        ax = axes[topic_idx]\\n        ax.barh(top_features, weights, height=0.7)\\n        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\\n        for i in \"top right left\".split():\\n            ax.spines[i].set_visible(False)\\n        fig.suptitle(title, fontsize=40)\\n\\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\\n    plt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_topics_extraction_with_nmf_lda.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=======================================================================================\\nTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\\n=======================================================================================\\n\\nThis is an example of applying :class:`~sklearn.decomposition.NMF` and\\n:class:`~sklearn.decomposition.LatentDirichletAllocation` on a corpus\\nof documents and extract additive models of the topic structure of the\\ncorpus.  The output is a plot of topics, each represented as bar plot\\nusing top few words based on weights.\\n\\nNon-negative Matrix Factorization is applied with two different objective\\nfunctions: the Frobenius norm, and the generalized Kullback-Leibler divergence.\\nThe latter is equivalent to Probabilistic Latent Semantic Indexing.\\n\\nThe default parameters (n_samples / n_features / n_components) should make\\nthe example runnable in a couple of tens of seconds. You can try to\\nincrease the dimensions of the problem, but be aware that the time\\ncomplexity is polynomial in NMF. In LDA, the time complexity is\\nproportional to (n_samples * iterations).\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nfrom time import time\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import fetch_20newsgroups\\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, MiniBatchNMF\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\n\\nn_samples = 2000\\nn_features = 1000\\nn_components = 10\\nn_top_words = 20\\nbatch_size = 128\\ninit = \"nndsvda\"\\n\\n\\n# Code for: def plot_top_words(model, feature_names, n_top_words, title):\\n\\n\\n# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\\n# to filter out useless terms early on: the posts are stripped of headers,\\n# footers and quoted replies, and common English words, words occurring in\\n# only one document or in at least 95% of the documents are removed.\\n\\nprint(\"Loading dataset...\")\\nt0 = time()\\ndata, _ = fetch_20newsgroups(\\n    shuffle=True,\\n    random_state=1,\\n    remove=(\"headers\", \"footers\", \"quotes\"),\\n    return_X_y=True,\\n)\\ndata_samples = data[:n_samples]\\nprint(\"done in %0.3fs.\" % (time() - t0))\\n\\n# Use tf-idf features for NMF.\\nprint(\"Extracting tf-idf features for NMF...\")\\ntfidf_vectorizer = TfidfVectorizer(\\n    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\\n)\\nt0 = time()\\ntfidf = tfidf_vectorizer.fit_transform(data_samples)\\nprint(\"done in %0.3fs.\" % (time() - t0))\\n\\n# Use tf (raw term count) features for LDA.\\nprint(\"Extracting tf features for LDA...\")\\ntf_vectorizer = CountVectorizer(\\n    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\\n)\\nt0 = time()\\ntf = tf_vectorizer.fit_transform(data_samples)\\nprint(\"done in %0.3fs.\" % (time() - t0))\\nprint()'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_topics_extraction_with_nmf_lda.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# Fit the NMF model\\nprint(\\n    \"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\\n    \"n_samples=%d and n_features=%d...\" % (n_samples, n_features)\\n)\\nt0 = time()\\nnmf = NMF(\\n    n_components=n_components,\\n    random_state=1,\\n    init=init,\\n    beta_loss=\"frobenius\",\\n    alpha_W=0.00005,\\n    alpha_H=0.00005,\\n    l1_ratio=1,\\n).fit(tfidf)\\nprint(\"done in %0.3fs.\" % (time() - t0))\\n\\n\\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\\nplot_top_words(\\n    nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model (Frobenius norm)\"\\n)\\n\\n# Fit the NMF model\\nprint(\\n    \"\\\\n\" * 2,\\n    \"Fitting the NMF model (generalized Kullback-Leibler \"\\n    \"divergence) with tf-idf features, n_samples=%d and n_features=%d...\"\\n    % (n_samples, n_features),\\n)\\nt0 = time()\\nnmf = NMF(\\n    n_components=n_components,\\n    random_state=1,\\n    init=init,\\n    beta_loss=\"kullback-leibler\",\\n    solver=\"mu\",\\n    max_iter=1000,\\n    alpha_W=0.00005,\\n    alpha_H=0.00005,\\n    l1_ratio=0.5,\\n).fit(tfidf)\\nprint(\"done in %0.3fs.\" % (time() - t0))\\n\\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\\nplot_top_words(\\n    nmf,\\n    tfidf_feature_names,\\n    n_top_words,\\n    \"Topics in NMF model (generalized Kullback-Leibler divergence)\",\\n)\\n\\n# Fit the MiniBatchNMF model\\nprint(\\n    \"\\\\n\" * 2,\\n    \"Fitting the MiniBatchNMF model (Frobenius norm) with tf-idf \"\\n    \"features, n_samples=%d and n_features=%d, batch_size=%d...\"\\n    % (n_samples, n_features, batch_size),\\n)\\nt0 = time()\\nmbnmf = MiniBatchNMF(\\n    n_components=n_components,\\n    random_state=1,\\n    batch_size=batch_size,\\n    init=init,\\n    beta_loss=\"frobenius\",\\n    alpha_W=0.00005,\\n    alpha_H=0.00005,\\n    l1_ratio=0.5,\\n).fit(tfidf)\\nprint(\"done in %0.3fs.\" % (time() - t0))\\n\\n\\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\\nplot_top_words(\\n    mbnmf,\\n    tfidf_feature_names,\\n    n_top_words,\\n    \"Topics in MiniBatchNMF model (Frobenius norm)\",\\n)\\n\\n# Fit the MiniBatchNMF model\\nprint(\\n    \"\\\\n\" * 2,\\n    \"Fitting the MiniBatchNMF model (generalized Kullback-Leibler \"\\n    \"divergence) with tf-idf features, n_samples=%d and n_features=%d, \"\\n    \"batch_size=%d...\" % (n_samples, n_features, batch_size),\\n)\\nt0 = time()\\nmbnmf = MiniBatchNMF(\\n    n_components=n_components,\\n    random_state=1,\\n    batch_size=batch_size,\\n    init=init,\\n    beta_loss=\"kullback-leibler\",\\n    alpha_W=0.00005,\\n    alpha_H=0.00005,\\n    l1_ratio=0.5,\\n).fit(tfidf)\\nprint(\"done in %0.3fs.\" % (time() - t0))\\n\\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\\nplot_top_words(\\n    mbnmf,\\n    tfidf_feature_names,\\n    n_top_words,\\n    \"Topics in MiniBatchNMF model (generalized Kullback-Leibler divergence)\",\\n)'), Document(metadata={'source': '/content/local_copy_repo/examples/applications/plot_topics_extraction_with_nmf_lda.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\\nplot_top_words(\\n    mbnmf,\\n    tfidf_feature_names,\\n    n_top_words,\\n    \"Topics in MiniBatchNMF model (generalized Kullback-Leibler divergence)\",\\n)\\n\\nprint(\\n    \"\\\\n\" * 2,\\n    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\\n    % (n_samples, n_features),\\n)\\nlda = LatentDirichletAllocation(\\n    n_components=n_components,\\n    max_iter=5,\\n    learning_method=\"online\",\\n    learning_offset=50.0,\\n    random_state=0,\\n)\\nt0 = time()\\nlda.fit(tf)\\nprint(\"done in %0.3fs.\" % (time() - t0))\\n\\ntf_feature_names = tf_vectorizer.get_feature_names_out()\\nplot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_lof_outlier_detection.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def update_legend_marker_size(handle, orig):\\n    \"Customize size of the legend marker\"\\n    handle.update_from(orig)\\n    handle.set_sizes([20])'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_lof_outlier_detection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================\\nOutlier detection with Local Outlier Factor (LOF)\\n=================================================\\n\\nThe Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection\\nmethod which computes the local density deviation of a given data point with\\nrespect to its neighbors. It considers as outliers the samples that have a\\nsubstantially lower density than their neighbors. This example shows how to use\\nLOF for outlier detection which is the default use case of this estimator in\\nscikit-learn. Note that when LOF is used for outlier detection it has no\\n`predict`, `decision_function` and `score_samples` methods. See the :ref:`User\\nGuide <outlier_detection>` for details on the difference between outlier\\ndetection and novelty detection and how to use LOF for novelty detection.\\n\\nThe number of neighbors considered (parameter `n_neighbors`) is typically set 1)\\ngreater than the minimum number of samples a cluster has to contain, so that\\nother samples can be local outliers relative to this cluster, and 2) smaller\\nthan the maximum number of close by samples that can potentially be local\\noutliers. In practice, such information is generally not available, and taking\\n`n_neighbors=20` appears to work well in general.\\n\\n\"\"\"\\n\\n# %%\\n# Generate data with outliers\\n# ---------------------------\\n\\n# %%\\nimport numpy as np\\n\\nnp.random.seed(42)\\n\\nX_inliers = 0.3 * np.random.randn(100, 2)\\nX_inliers = np.r_[X_inliers + 2, X_inliers - 2]\\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\\nX = np.r_[X_inliers, X_outliers]\\n\\nn_outliers = len(X_outliers)\\nground_truth = np.ones(len(X), dtype=int)\\nground_truth[-n_outliers:] = -1\\n\\n# %%\\n# Fit the model for outlier detection (default)\\n# ---------------------------------------------\\n#\\n# Use `fit_predict` to compute the predicted labels of the training samples\\n# (when LOF is used for outlier detection, the estimator has no `predict`,\\n# `decision_function` and `score_samples` methods).\\n\\nfrom sklearn.neighbors import LocalOutlierFactor\\n\\nclf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\\ny_pred = clf.fit_predict(X)\\nn_errors = (y_pred != ground_truth).sum()\\nX_scores = clf.negative_outlier_factor_\\n\\n# %%\\n# Plot results\\n# ------------\\n\\n# %%\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.legend_handler import HandlerPathCollection\\n\\n\\n# Code for: def update_legend_marker_size(handle, orig):'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_lof_outlier_detection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\n# Plot results\\n# ------------\\n\\n# %%\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.legend_handler import HandlerPathCollection\\n\\n\\n# Code for: def update_legend_marker_size(handle, orig):\\n\\n\\nplt.scatter(X[:, 0], X[:, 1], color=\"k\", s=3.0, label=\"Data points\")\\n# plot circles with radius proportional to the outlier scores\\nradius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\\nscatter = plt.scatter(\\n    X[:, 0],\\n    X[:, 1],\\n    s=1000 * radius,\\n    edgecolors=\"r\",\\n    facecolors=\"none\",\\n    label=\"Outlier scores\",\\n)\\nplt.axis(\"tight\")\\nplt.xlim((-5, 5))\\nplt.ylim((-5, 5))\\nplt.xlabel(\"prediction errors: %d\" % (n_errors))\\nplt.legend(\\n    handler_map={scatter: HandlerPathCollection(update_func=update_legend_marker_size)}\\n)\\nplt.title(\"Local Outlier Factor (LOF)\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_nca_illustration.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def link_thickness_i(X, i):\\n    diff_embedded = X[i] - X\\n    dist_embedded = np.einsum(\"ij,ij->i\", diff_embedded, diff_embedded)\\n    dist_embedded[i] = np.inf\\n\\n    # compute exponentiated distances (use the log-sum-exp trick to\\n    # avoid numerical instabilities\\n    exp_dist_embedded = np.exp(-dist_embedded - logsumexp(-dist_embedded))\\n    return exp_dist_embedded'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_nca_illustration.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def relate_point(X, i, ax):\\n    pt_i = X[i]\\n    for j, pt_j in enumerate(X):\\n        thickness = link_thickness_i(X, i)\\n        if i != j:\\n            line = ([pt_i[0], pt_j[0]], [pt_i[1], pt_j[1]])\\n            ax.plot(*line, c=cm.Set1(y[j]), linewidth=5 * thickness[j])'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_nca_illustration.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================\\nNeighborhood Components Analysis Illustration\\n=============================================\\n\\nThis example illustrates a learned distance metric that maximizes\\nthe nearest neighbors classification accuracy. It provides a visual\\nrepresentation of this metric compared to the original point\\nspace. Please refer to the :ref:`User Guide <nca>` for more information.\\n\\n\"\"\"\\n\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib import cm\\nfrom scipy.special import logsumexp\\n\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.neighbors import NeighborhoodComponentsAnalysis\\n\\n# %%\\n# Original points\\n# ---------------\\n# First we create a data set of 9 samples from 3 classes, and plot the points\\n# in the original space. For this example, we focus on the classification of\\n# point no. 3. The thickness of a link between point no. 3 and another point\\n# is proportional to their distance.\\n\\nX, y = make_classification(\\n    n_samples=9,\\n    n_features=2,\\n    n_informative=2,\\n    n_redundant=0,\\n    n_classes=3,\\n    n_clusters_per_class=1,\\n    class_sep=1.0,\\n    random_state=0,\\n)\\n\\nplt.figure(1)\\nax = plt.gca()\\nfor i in range(X.shape[0]):\\n    ax.text(X[i, 0], X[i, 1], str(i), va=\"center\", ha=\"center\")\\n    ax.scatter(X[i, 0], X[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\\n\\nax.set_title(\"Original points\")\\nax.axes.get_xaxis().set_visible(False)\\nax.axes.get_yaxis().set_visible(False)\\nax.axis(\"equal\")  # so that boundaries are displayed correctly as circles\\n\\n\\n# Code for: def link_thickness_i(X, i):\\n\\n\\n# Code for: def relate_point(X, i, ax):\\n\\n\\ni = 3\\nrelate_point(X, i, ax)\\nplt.show()\\n\\n# %%\\n# Learning an embedding\\n# ---------------------\\n# We use :class:`~sklearn.neighbors.NeighborhoodComponentsAnalysis` to learn an\\n# embedding and plot the points after the transformation. We then take the\\n# embedding and find the nearest neighbors.\\n\\nnca = NeighborhoodComponentsAnalysis(max_iter=30, random_state=0)\\nnca = nca.fit(X, y)\\n\\nplt.figure(2)\\nax2 = plt.gca()\\nX_embedded = nca.transform(X)\\nrelate_point(X_embedded, i, ax2)\\n\\nfor i in range(len(X)):\\n    ax2.text(X_embedded[i, 0], X_embedded[i, 1], str(i), va=\"center\", ha=\"center\")\\n    ax2.scatter(X_embedded[i, 0], X_embedded[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\\n\\nax2.set_title(\"NCA embedding\")\\nax2.axes.get_xaxis().set_visible(False)\\nax2.axes.get_yaxis().set_visible(False)\\nax2.axis(\"equal\")\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_species_kde.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def construct_grids(batch):\\n    \"\"\"Construct the map grid from the batch object\\n\\n    Parameters\\n    ----------\\n    batch : Batch object\\n        The object returned by :func:`fetch_species_distributions`\\n\\n    Returns\\n    -------\\n    (xgrid, ygrid) : 1-D arrays\\n        The grid corresponding to the values in batch.coverages\\n    \"\"\"\\n    # x,y coordinates for corner cells\\n    xmin = batch.x_left_lower_corner + batch.grid_size\\n    xmax = xmin + (batch.Nx * batch.grid_size)\\n    ymin = batch.y_left_lower_corner + batch.grid_size\\n    ymax = ymin + (batch.Ny * batch.grid_size)\\n\\n    # x coordinates of the grid cells\\n    xgrid = np.arange(xmin, xmax, batch.grid_size)\\n    # y coordinates of the grid cells\\n    ygrid = np.arange(ymin, ymax, batch.grid_size)\\n\\n    return (xgrid, ygrid)'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_species_kde.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================================\\nKernel Density Estimate of Species Distributions\\n================================================\\nThis shows an example of a neighbors-based query (in particular a kernel\\ndensity estimate) on geospatial data, using a Ball Tree built upon the\\nHaversine distance metric -- i.e. distances over points in latitude/longitude.\\nThe dataset is provided by Phillips et. al. (2006).\\nIf available, the example uses\\n`basemap <https://matplotlib.org/basemap/>`_\\nto plot the coast lines and national boundaries of South America.\\n\\nThis example does not perform any learning over the data\\n(see :ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py` for\\nan example of classification based on the attributes in this dataset).  It\\nsimply shows the kernel density estimate of observed data points in\\ngeospatial coordinates.\\n\\nThe two species are:\\n\\n - `\"Bradypus variegatus\"\\n   <https://www.iucnredlist.org/species/3038/47437046>`_ ,\\n   the Brown-throated Sloth.\\n\\n - `\"Microryzomys minutus\"\\n   <http://www.iucnredlist.org/details/13408/0>`_ ,\\n   also known as the Forest Small Rice Rat, a rodent that lives in Peru,\\n   Colombia, Ecuador, Peru, and Venezuela.\\n\\nReferences\\n----------\\n\\n * `\"Maximum entropy modeling of species geographic distributions\"\\n   <http://rob.schapire.net/papers/ecolmod.pdf>`_\\n   S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\\n   190:231-259, 2006.\\n\"\"\"  # noqa: E501\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import fetch_species_distributions\\nfrom sklearn.neighbors import KernelDensity\\n\\n# if basemap is available, we\\'ll use it.\\n# otherwise, we\\'ll improvise later...\\ntry:\\n    from mpl_toolkits.basemap import Basemap\\n\\n    basemap = True\\nexcept ImportError:\\n    basemap = False\\n\\n\\n# Code for: def construct_grids(batch):\\n\\n\\n# Get matrices/arrays of species IDs and locations\\ndata = fetch_species_distributions()\\nspecies_names = [\"Bradypus Variegatus\", \"Microryzomys Minutus\"]\\n\\nXtrain = np.vstack([data[\"train\"][\"dd lat\"], data[\"train\"][\"dd long\"]]).T\\nytrain = np.array(\\n    [d.decode(\"ascii\").startswith(\"micro\") for d in data[\"train\"][\"species\"]],\\n    dtype=\"int\",\\n)\\nXtrain *= np.pi / 180.0  # Convert lat/long to radians\\n\\n# Set up the data grid for the contour plot\\nxgrid, ygrid = construct_grids(data)\\nX, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\\nland_reference = data.coverages[6][::5, ::5]\\nland_mask = (land_reference > -9999).ravel()\\n\\nxy = np.vstack([Y.ravel(), X.ravel()]).T\\nxy = xy[land_mask]\\nxy *= np.pi / 180.0\\n\\n# Plot map of South America with distributions of each species\\nfig = plt.figure()\\nfig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\\n\\nfor i in range(2):\\n    plt.subplot(1, 2, i + 1)'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_species_kde.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='xy = np.vstack([Y.ravel(), X.ravel()]).T\\nxy = xy[land_mask]\\nxy *= np.pi / 180.0\\n\\n# Plot map of South America with distributions of each species\\nfig = plt.figure()\\nfig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\\n\\nfor i in range(2):\\n    plt.subplot(1, 2, i + 1)\\n\\n    # construct a kernel density estimate of the distribution\\n    print(\" - computing KDE in spherical coordinates\")\\n    kde = KernelDensity(\\n        bandwidth=0.04, metric=\"haversine\", kernel=\"gaussian\", algorithm=\"ball_tree\"\\n    )\\n    kde.fit(Xtrain[ytrain == i])\\n\\n    # evaluate only on the land: -9999 indicates ocean\\n    Z = np.full(land_mask.shape[0], -9999, dtype=\"int\")\\n    Z[land_mask] = np.exp(kde.score_samples(xy))\\n    Z = Z.reshape(X.shape)\\n\\n    # plot contours of the density\\n    levels = np.linspace(0, Z.max(), 25)\\n    plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\\n\\n    if basemap:\\n        print(\" - plot coastlines using basemap\")\\n        m = Basemap(\\n            projection=\"cyl\",\\n            llcrnrlat=Y.min(),\\n            urcrnrlat=Y.max(),\\n            llcrnrlon=X.min(),\\n            urcrnrlon=X.max(),\\n            resolution=\"c\",\\n        )\\n        m.drawcoastlines()\\n        m.drawcountries()\\n    else:\\n        print(\" - plot coastlines from coverage\")\\n        plt.contour(\\n            X, Y, land_reference, levels=[-9998], colors=\"k\", linestyles=\"solid\"\\n        )\\n        plt.xticks([])\\n        plt.yticks([])\\n\\n    plt.title(species_names[i])\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_nca_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=============================================================================\\nComparing Nearest Neighbors with and without Neighborhood Components Analysis\\n=============================================================================\\n\\nAn example comparing nearest neighbors classification with and without\\nNeighborhood Components Analysis.\\n\\nIt will plot the class decision boundaries given by a Nearest Neighbors\\nclassifier when using the Euclidean distance on the original features, versus\\nusing the Euclidean distance after the transformation learned by Neighborhood\\nComponents Analysis. The latter aims to find a linear transformation that\\nmaximises the (stochastic) nearest neighbor classification accuracy on the\\ntraining set.\\n\\n\"\"\"\\n\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import ListedColormap\\n\\nfrom sklearn import datasets\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nn_neighbors = 1\\n\\ndataset = datasets.load_iris()\\nX, y = dataset.data, dataset.target\\n\\n# we only take two features. We could avoid this ugly\\n# slicing by using a two-dim dataset\\nX = X[:, [0, 2]]\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, stratify=y, test_size=0.7, random_state=42\\n)\\n\\nh = 0.05  # step size in the mesh\\n\\n# Create color maps\\ncmap_light = ListedColormap([\"#FFAAAA\", \"#AAFFAA\", \"#AAAAFF\"])\\ncmap_bold = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\\n\\nnames = [\"KNN\", \"NCA, KNN\"]\\n\\nclassifiers = [\\n    Pipeline(\\n        [\\n            (\"scaler\", StandardScaler()),\\n            (\"knn\", KNeighborsClassifier(n_neighbors=n_neighbors)),\\n        ]\\n    ),\\n    Pipeline(\\n        [\\n            (\"scaler\", StandardScaler()),\\n            (\"nca\", NeighborhoodComponentsAnalysis()),\\n            (\"knn\", KNeighborsClassifier(n_neighbors=n_neighbors)),\\n        ]\\n    ),\\n]\\n\\nfor name, clf in zip(names, classifiers):\\n    clf.fit(X_train, y_train)\\n    score = clf.score(X_test, y_test)\\n\\n    _, ax = plt.subplots()\\n    DecisionBoundaryDisplay.from_estimator(\\n        clf,\\n        X,\\n        cmap=cmap_light,\\n        alpha=0.8,\\n        ax=ax,\\n        response_method=\"predict\",\\n        plot_method=\"pcolormesh\",\\n        shading=\"auto\",\\n    )\\n\\n    # Plot also the training and testing points\\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\\n    plt.title(\"{} (k = {})\".format(name, n_neighbors))\\n    plt.text(\\n        0.9,\\n        0.1,\\n        \"{:.2f}\".format(score),\\n        size=15,\\n        ha=\"center\",\\n        va=\"center\",\\n        transform=plt.gca().transAxes,\\n    )\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n================================\\nNearest Neighbors Classification\\n================================\\n\\nThis example shows how to use :class:`~sklearn.neighbors.KNeighborsClassifier`.\\nWe train such a classifier on the iris dataset and observe the difference of the\\ndecision boundary obtained with regards to the parameter `weights`.\\n\"\"\"\\n\\n# %%\\n# Load the data\\n# -------------\\n#\\n# In this example, we use the iris dataset. We split the data into a train and test\\n# dataset.\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\n\\niris = load_iris(as_frame=True)\\nX = iris.data[[\"sepal length (cm)\", \"sepal width (cm)\"]]\\ny = iris.target\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n\\n# %%\\n# K-nearest neighbors classifier\\n# ------------------------------\\n#\\n# We want to use a k-nearest neighbors classifier considering a neighborhood of 11 data\\n# points. Since our k-nearest neighbors model uses euclidean distance to find the\\n# nearest neighbors, it is therefore important to scale the data beforehand. Refer to\\n# the example entitled\\n# :ref:`sphx_glr_auto_examples_preprocessing_plot_scaling_importance.py` for more\\n# detailed information.\\n#\\n# Thus, we use a :class:`~sklearn.pipeline.Pipeline` to chain a scaler before to use\\n# our classifier.\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nclf = Pipeline(\\n    steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=11))]\\n)\\n\\n# %%\\n# Decision boundary\\n# -----------------\\n#\\n# Now, we fit two classifiers with different values of the parameter\\n# `weights`. We plot the decision boundary of each classifier as well as the original\\n# dataset to observe the difference.\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\n\\n_, axs = plt.subplots(ncols=2, figsize=(12, 5))\\n\\nfor ax, weights in zip(axs, (\"uniform\", \"distance\")):\\n    clf.set_params(knn__weights=weights).fit(X_train, y_train)\\n    disp = DecisionBoundaryDisplay.from_estimator(\\n        clf,\\n        X_test,\\n        response_method=\"predict\",\\n        plot_method=\"pcolormesh\",\\n        xlabel=iris.feature_names[0],\\n        ylabel=iris.feature_names[1],\\n        shading=\"auto\",\\n        alpha=0.5,\\n        ax=ax,\\n    )\\n    scatter = disp.ax_.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors=\"k\")\\n    disp.ax_.legend(\\n        scatter.legend_elements()[0],\\n        iris.target_names,\\n        loc=\"lower left\",\\n        title=\"Classes\",\\n    )\\n    _ = disp.ax_.set_title(\\n        f\"3-Class classification\\\\n(k={clf[-1].n_neighbors}, weights={weights!r})\"\\n    )\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_classification.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.show()\\n\\n# %%\\n# Conclusion\\n# ----------\\n#\\n# We observe that the parameter `weights` has an impact on the decision boundary. When\\n# `weights=\"unifom\"` all nearest neighbors will have the same impact on the decision.\\n# Whereas when `weights=\"distance\"` the weight given to each neighbor is proportional\\n# to the inverse of the distance from that neighbor to the query point.\\n#\\n# In some cases, taking the distance into account might improve the model.'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_nca_dim_reduction.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n==============================================================\\nDimensionality Reduction with Neighborhood Components Analysis\\n==============================================================\\n\\nSample usage of Neighborhood Components Analysis for dimensionality reduction.\\n\\nThis example compares different (linear) dimensionality reduction methods\\napplied on the Digits data set. The data set contains images of digits from\\n0 to 9 with approximately 180 samples of each class. Each image is of\\ndimension 8x8 = 64, and is reduced to a two-dimensional data point.\\n\\nPrincipal Component Analysis (PCA) applied to this data identifies the\\ncombination of attributes (principal components, or directions in the\\nfeature space) that account for the most variance in the data. Here we\\nplot the different samples on the 2 first principal components.\\n\\nLinear Discriminant Analysis (LDA) tries to identify attributes that\\naccount for the most variance *between classes*. In particular,\\nLDA, in contrast to PCA, is a supervised method, using known class labels.\\n\\nNeighborhood Components Analysis (NCA) tries to find a feature space such\\nthat a stochastic nearest neighbor algorithm will give the best accuracy.\\nLike LDA, it is a supervised method.\\n\\nOne can see that NCA enforces a clustering of the data that is visually\\nmeaningful despite the large reduction in dimension.\\n\\n\"\"\"\\n\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import datasets\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\nn_neighbors = 3\\nrandom_state = 0\\n\\n# Load Digits dataset\\nX, y = datasets.load_digits(return_X_y=True)\\n\\n# Split into train/test\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.5, stratify=y, random_state=random_state\\n)\\n\\ndim = len(X[0])\\nn_classes = len(np.unique(y))\\n\\n# Reduce dimension to 2 with PCA\\npca = make_pipeline(StandardScaler(), PCA(n_components=2, random_state=random_state))\\n\\n# Reduce dimension to 2 with LinearDiscriminantAnalysis\\nlda = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis(n_components=2))\\n\\n# Reduce dimension to 2 with NeighborhoodComponentAnalysis\\nnca = make_pipeline(\\n    StandardScaler(),\\n    NeighborhoodComponentsAnalysis(n_components=2, random_state=random_state),\\n)\\n\\n# Use a nearest neighbor classifier to evaluate the methods\\nknn = KNeighborsClassifier(n_neighbors=n_neighbors)\\n\\n# Make a list of the methods to be compared\\ndim_reduction_methods = [(\"PCA\", pca), (\"LDA\", lda), (\"NCA\", nca)]\\n\\n# plt.figure()\\nfor i, (name, model) in enumerate(dim_reduction_methods):\\n    plt.figure()\\n    # plt.subplot(1, 3, i + 1, aspect=1)\\n\\n    # Fit the method\\'s model\\n    model.fit(X_train, y_train)'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_nca_dim_reduction.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# plt.figure()\\nfor i, (name, model) in enumerate(dim_reduction_methods):\\n    plt.figure()\\n    # plt.subplot(1, 3, i + 1, aspect=1)\\n\\n    # Fit the method\\'s model\\n    model.fit(X_train, y_train)\\n\\n    # Fit a nearest neighbor classifier on the embedded training set\\n    knn.fit(model.transform(X_train), y_train)\\n\\n    # Compute the nearest neighbor accuracy on the embedded test set\\n    acc_knn = knn.score(model.transform(X_test), y_test)\\n\\n    # Embed the data set in 2 dimensions using the fitted model\\n    X_embedded = model.transform(X)\\n\\n    # Plot the projected points and show the evaluation score\\n    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, s=30, cmap=\"Set1\")\\n    plt.title(\\n        \"{}, KNN (k={})\\\\nTest accuracy = {:.2f}\".format(name, n_neighbors, acc_knn)\\n    )\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_digits_kde_sampling.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================\\nKernel Density Estimation\\n=========================\\n\\nThis example shows how kernel density estimation (KDE), a powerful\\nnon-parametric density estimation technique, can be used to learn\\na generative model for a dataset.  With this generative model in place,\\nnew samples can be drawn.  These new samples reflect the underlying model\\nof the data.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.neighbors import KernelDensity\\n\\n# load the data\\ndigits = load_digits()\\n\\n# project the 64-dimensional data to a lower dimension\\npca = PCA(n_components=15, whiten=False)\\ndata = pca.fit_transform(digits.data)\\n\\n# use grid search cross-validation to optimize the bandwidth\\nparams = {\"bandwidth\": np.logspace(-1, 1, 20)}\\ngrid = GridSearchCV(KernelDensity(), params)\\ngrid.fit(data)\\n\\nprint(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\\n\\n# use the best estimator to compute the kernel density estimate\\nkde = grid.best_estimator_\\n\\n# sample 44 new points from the data\\nnew_data = kde.sample(44, random_state=0)\\nnew_data = pca.inverse_transform(new_data)\\n\\n# turn data into a 4x11 grid\\nnew_data = new_data.reshape((4, 11, -1))\\nreal_data = digits.data[:44].reshape((4, 11, -1))\\n\\n# plot real digits and resampled digits\\nfig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\\nfor j in range(11):\\n    ax[4, j].set_visible(False)\\n    for i in range(4):\\n        im = ax[i, j].imshow(\\n            real_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\\n        )\\n        im.set_clim(0, 16)\\n        im = ax[i + 5, j].imshow(\\n            new_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\\n        )\\n        im.set_clim(0, 16)\\n\\nax[0, 5].set_title(\"Selection from the input data\")\\nax[5, 5].set_title(\\'\"New\" digits drawn from the kernel density model\\')\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_nearest_centroid.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===============================\\nNearest Centroid Classification\\n===============================\\n\\nSample usage of Nearest Centroid classification.\\nIt will plot the decision boundaries for each class.\\n\\n\"\"\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom matplotlib.colors import ListedColormap\\n\\nfrom sklearn import datasets\\nfrom sklearn.inspection import DecisionBoundaryDisplay\\nfrom sklearn.neighbors import NearestCentroid\\n\\n# import some data to play with\\niris = datasets.load_iris()\\n# we only take the first two features. We could avoid this ugly\\n# slicing by using a two-dim dataset\\nX = iris.data[:, :2]\\ny = iris.target\\n\\n# Create color maps\\ncmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\\ncmap_bold = ListedColormap([\"darkorange\", \"c\", \"darkblue\"])\\n\\nfor shrinkage in [None, 0.2]:\\n    # we create an instance of Nearest Centroid Classifier and fit the data.\\n    clf = NearestCentroid(shrink_threshold=shrinkage)\\n    clf.fit(X, y)\\n    y_pred = clf.predict(X)\\n    print(shrinkage, np.mean(y == y_pred))\\n\\n    _, ax = plt.subplots()\\n    DecisionBoundaryDisplay.from_estimator(\\n        clf, X, cmap=cmap_light, ax=ax, response_method=\"predict\"\\n    )\\n\\n    # Plot also the training points\\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\\n    plt.title(\"3-Class classification (shrink_threshold=%r)\" % shrinkage)\\n    plt.axis(\"tight\")\\n\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/approximate_nearest_neighbors.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='class NMSlibTransformer(TransformerMixin, BaseEstimator):\\n    \"\"\"Wrapper for using nmslib as sklearn\\'s KNeighborsTransformer\"\"\"\\n\\n    def __init__(self, n_neighbors=5, metric=\"euclidean\", method=\"sw-graph\", n_jobs=-1):\\n        self.n_neighbors = n_neighbors\\n        self.method = method\\n        self.metric = metric\\n        self.n_jobs = n_jobs\\n\\n    def fit(self, X):\\n        self.n_samples_fit_ = X.shape[0]\\n\\n        # see more metric in the manual\\n        # https://github.com/nmslib/nmslib/tree/master/manual\\n        space = {\\n            \"euclidean\": \"l2\",\\n            \"cosine\": \"cosinesimil\",\\n            \"l1\": \"l1\",\\n            \"l2\": \"l2\",\\n        }[self.metric]\\n\\n        self.nmslib_ = nmslib.init(method=self.method, space=space)\\n        self.nmslib_.addDataPointBatch(X.copy())\\n        self.nmslib_.createIndex()\\n        return self\\n\\n    def transform(self, X):\\n        n_samples_transform = X.shape[0]\\n\\n        # For compatibility reasons, as each sample is considered as its own\\n        # neighbor, one extra neighbor will be computed.\\n        n_neighbors = self.n_neighbors + 1\\n\\n        if self.n_jobs < 0:\\n            # Same handling as done in joblib for negative values of n_jobs:\\n            # in particular, `n_jobs == -1` means \"as many threads as CPUs\".\\n            num_threads = joblib.cpu_count() + self.n_jobs + 1\\n        else:\\n            num_threads = self.n_jobs\\n\\n        results = self.nmslib_.knnQueryBatch(\\n            X.copy(), k=n_neighbors, num_threads=num_threads\\n        )\\n        indices, distances = zip(*results)\\n        indices, distances = np.vstack(indices), np.vstack(distances)\\n\\n        indptr = np.arange(0, n_samples_transform * n_neighbors + 1, n_neighbors)\\n        kneighbors_graph = csr_matrix(\\n            (distances.ravel(), indices.ravel(), indptr),\\n            shape=(n_samples_transform, self.n_samples_fit_),\\n        )\\n\\n        return kneighbors_graph'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/approximate_nearest_neighbors.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def load_mnist(n_samples):\\n    \"\"\"Load MNIST, shuffle the data, and return only n_samples.\"\"\"\\n    mnist = fetch_openml(\"mnist_784\", as_frame=False)\\n    X, y = shuffle(mnist.data, mnist.target, random_state=2)\\n    return X[:n_samples] / 255, y[:n_samples]'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/approximate_nearest_neighbors.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=====================================\\nApproximate nearest neighbors in TSNE\\n=====================================\\n\\nThis example presents how to chain KNeighborsTransformer and TSNE in a pipeline.\\nIt also shows how to wrap the packages `nmslib` and `pynndescent` to replace\\nKNeighborsTransformer and perform approximate nearest neighbors. These packages\\ncan be installed with `pip install nmslib pynndescent`.\\n\\nNote: In KNeighborsTransformer we use the definition which includes each\\ntraining point as its own neighbor in the count of `n_neighbors`, and for\\ncompatibility reasons, one extra neighbor is computed when `mode == \\'distance\\'`.\\nPlease note that we do the same in the proposed `nmslib` wrapper.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# First we try to import the packages and warn the user in case they are\\n# missing.\\nimport sys\\n\\ntry:\\n    import nmslib\\nexcept ImportError:\\n    print(\"The package \\'nmslib\\' is required to run this example.\")\\n    sys.exit()\\n\\ntry:\\n    from pynndescent import PyNNDescentTransformer\\nexcept ImportError:\\n    print(\"The package \\'pynndescent\\' is required to run this example.\")\\n    sys.exit()\\n\\n# %%\\n# We define a wrapper class for implementing the scikit-learn API to the\\n# `nmslib`, as well as a loading function.\\nimport joblib\\nimport numpy as np\\nfrom scipy.sparse import csr_matrix\\n\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nfrom sklearn.datasets import fetch_openml\\nfrom sklearn.utils import shuffle\\n\\n\\n# Code for: class NMSlibTransformer(TransformerMixin, BaseEstimator):\\n\\n\\n# Code for: def load_mnist(n_samples):\\n\\n\\n# %%\\n# We benchmark the different exact/approximate nearest neighbors transformers.\\nimport time\\n\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.neighbors import KNeighborsTransformer\\nfrom sklearn.pipeline import make_pipeline\\n\\ndatasets = [\\n    (\"MNIST_10000\", load_mnist(n_samples=10_000)),\\n    (\"MNIST_20000\", load_mnist(n_samples=20_000)),\\n]\\n\\nn_iter = 500\\nperplexity = 30\\nmetric = \"euclidean\"\\n# TSNE requires a certain number of neighbors which depends on the\\n# perplexity parameter.\\n# Add one since we include each sample as its own neighbor.\\nn_neighbors = int(3.0 * perplexity + 1) + 1\\n\\ntsne_params = dict(\\n    init=\"random\",  # pca not supported for sparse matrices\\n    perplexity=perplexity,\\n    method=\"barnes_hut\",\\n    random_state=42,\\n    n_iter=n_iter,\\n    learning_rate=\"auto\",\\n)\\n\\ntransformers = [\\n    (\\n        \"KNeighborsTransformer\",\\n        KNeighborsTransformer(n_neighbors=n_neighbors, mode=\"distance\", metric=metric),\\n    ),\\n    (\\n        \"NMSlibTransformer\",\\n        NMSlibTransformer(n_neighbors=n_neighbors, metric=metric),\\n    ),\\n    (\\n        \"PyNNDescentTransformer\",\\n        PyNNDescentTransformer(\\n            n_neighbors=n_neighbors, metric=metric, parallel_batch_queries=True\\n        ),\\n    ),\\n]\\n\\nfor dataset_name, (X, y) in datasets:\\n    msg = f\"Benchmarking on {dataset_name}:\"\\n    print(f\"\\\\n{msg}\\\\n\" + str(\"-\" * len(msg)))'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/approximate_nearest_neighbors.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for dataset_name, (X, y) in datasets:\\n    msg = f\"Benchmarking on {dataset_name}:\"\\n    print(f\"\\\\n{msg}\\\\n\" + str(\"-\" * len(msg)))\\n\\n    for transformer_name, transformer in transformers:\\n        longest = np.max([len(name) for name, model in transformers])\\n        start = time.time()\\n        transformer.fit(X)\\n        fit_duration = time.time() - start\\n        print(f\"{transformer_name:<{longest}} {fit_duration:.3f} sec (fit)\")\\n        start = time.time()\\n        Xt = transformer.transform(X)\\n        transform_duration = time.time() - start\\n        print(f\"{transformer_name:<{longest}} {transform_duration:.3f} sec (transform)\")\\n        if transformer_name == \"PyNNDescentTransformer\":\\n            start = time.time()\\n            Xt = transformer.transform(X)\\n            transform_duration = time.time() - start\\n            print(\\n                f\"{transformer_name:<{longest}} {transform_duration:.3f} sec\"\\n                \" (transform)\"\\n            )\\n\\n# %%\\n# Sample output::\\n#\\n#     Benchmarking on MNIST_10000:\\n#     ----------------------------\\n#     KNeighborsTransformer  0.007 sec (fit)\\n#     KNeighborsTransformer  1.139 sec (transform)\\n#     NMSlibTransformer      0.208 sec (fit)\\n#     NMSlibTransformer      0.315 sec (transform)\\n#     PyNNDescentTransformer 4.823 sec (fit)\\n#     PyNNDescentTransformer 4.884 sec (transform)\\n#     PyNNDescentTransformer 0.744 sec (transform)\\n#\\n#     Benchmarking on MNIST_20000:\\n#     ----------------------------\\n#     KNeighborsTransformer  0.011 sec (fit)\\n#     KNeighborsTransformer  5.769 sec (transform)\\n#     NMSlibTransformer      0.733 sec (fit)\\n#     NMSlibTransformer      1.077 sec (transform)\\n#     PyNNDescentTransformer 14.448 sec (fit)\\n#     PyNNDescentTransformer 7.103 sec (transform)\\n#     PyNNDescentTransformer 1.759 sec (transform)\\n#\\n# Notice that the `PyNNDescentTransformer` takes more time during the first\\n# `fit` and the first `transform` due to the overhead of the numba just in time\\n# compiler. But after the first call, the compiled Python code is kept in a\\n# cache by numba and subsequent calls do not suffer from this initial overhead.\\n# Both :class:`~sklearn.neighbors.KNeighborsTransformer` and `NMSlibTransformer`\\n# are only run once here as they would show more stable `fit` and `transform`\\n# times (they don\\'t have the cold start problem of PyNNDescentTransformer).\\n\\n# %%\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.ticker import NullFormatter'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/approximate_nearest_neighbors.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='# %%\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.ticker import NullFormatter\\n\\ntransformers = [\\n    (\"TSNE with internal NearestNeighbors\", TSNE(metric=metric, **tsne_params)),\\n    (\\n        \"TSNE with KNeighborsTransformer\",\\n        make_pipeline(\\n            KNeighborsTransformer(\\n                n_neighbors=n_neighbors, mode=\"distance\", metric=metric\\n            ),\\n            TSNE(metric=\"precomputed\", **tsne_params),\\n        ),\\n    ),\\n    (\\n        \"TSNE with NMSlibTransformer\",\\n        make_pipeline(\\n            NMSlibTransformer(n_neighbors=n_neighbors, metric=metric),\\n            TSNE(metric=\"precomputed\", **tsne_params),\\n        ),\\n    ),\\n]\\n\\n# init the plot\\nnrows = len(datasets)\\nncols = np.sum([1 for name, model in transformers if \"TSNE\" in name])\\nfig, axes = plt.subplots(\\n    nrows=nrows, ncols=ncols, squeeze=False, figsize=(5 * ncols, 4 * nrows)\\n)\\naxes = axes.ravel()\\ni_ax = 0\\n\\nfor dataset_name, (X, y) in datasets:\\n    msg = f\"Benchmarking on {dataset_name}:\"\\n    print(f\"\\\\n{msg}\\\\n\" + str(\"-\" * len(msg)))\\n\\n    for transformer_name, transformer in transformers:\\n        longest = np.max([len(name) for name, model in transformers])\\n        start = time.time()\\n        Xt = transformer.fit_transform(X)\\n        transform_duration = time.time() - start\\n        print(\\n            f\"{transformer_name:<{longest}} {transform_duration:.3f} sec\"\\n            \" (fit_transform)\"\\n        )\\n\\n        # plot TSNE embedding which should be very similar across methods\\n        axes[i_ax].set_title(transformer_name + \"\\\\non \" + dataset_name)\\n        axes[i_ax].scatter(\\n            Xt[:, 0],\\n            Xt[:, 1],\\n            c=y.astype(np.int32),\\n            alpha=0.2,\\n            cmap=plt.cm.viridis,\\n        )\\n        axes[i_ax].xaxis.set_major_formatter(NullFormatter())\\n        axes[i_ax].yaxis.set_major_formatter(NullFormatter())\\n        axes[i_ax].axis(\"tight\")\\n        i_ax += 1\\n\\nfig.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/approximate_nearest_neighbors.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='fig.tight_layout()\\nplt.show()\\n\\n# %%\\n# Sample output::\\n#\\n#     Benchmarking on MNIST_10000:\\n#     ----------------------------\\n#     TSNE with internal NearestNeighbors 24.828 sec (fit_transform)\\n#     TSNE with KNeighborsTransformer     20.111 sec (fit_transform)\\n#     TSNE with NMSlibTransformer         21.757 sec (fit_transform)\\n#\\n#     Benchmarking on MNIST_20000:\\n#     ----------------------------\\n#     TSNE with internal NearestNeighbors 51.955 sec (fit_transform)\\n#     TSNE with KNeighborsTransformer     50.994 sec (fit_transform)\\n#     TSNE with NMSlibTransformer         43.536 sec (fit_transform)\\n#\\n# We can observe that the default :class:`~sklearn.manifold.TSNE` estimator with\\n# its internal :class:`~sklearn.neighbors.NearestNeighbors` implementation is\\n# roughly equivalent to the pipeline with :class:`~sklearn.manifold.TSNE` and\\n# :class:`~sklearn.neighbors.KNeighborsTransformer` in terms of performance.\\n# This is expected because both pipelines rely internally on the same\\n# :class:`~sklearn.neighbors.NearestNeighbors` implementation that performs\\n# exacts neighbors search. The approximate `NMSlibTransformer` is already\\n# slightly faster than the exact search on the smallest dataset but this speed\\n# difference is expected to become more significant on datasets with a larger\\n# number of samples.\\n#\\n# Notice however that not all approximate search methods are guaranteed to\\n# improve the speed of the default exact search method: indeed the exact search\\n# implementation significantly improved since scikit-learn 1.1. Furthermore, the\\n# brute-force exact search method does not require building an index at `fit`\\n# time. So, to get an overall performance improvement in the context of the\\n# :class:`~sklearn.manifold.TSNE` pipeline, the gains of the approximate search\\n# at `transform` need to be larger than the extra time spent to build the\\n# approximate search index at `fit` time.\\n#\\n# Finally, the TSNE algorithm itself is also computationally intensive,\\n# irrespective of the nearest neighbors search. So speeding-up the nearest\\n# neighbors search step by a factor of 5 would not result in a speed up by a\\n# factor of 5 for the overall pipeline.'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_lof_novelty_detection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=================================================\\nNovelty detection with Local Outlier Factor (LOF)\\n=================================================\\n\\nThe Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection\\nmethod which computes the local density deviation of a given data point with\\nrespect to its neighbors. It considers as outliers the samples that have a\\nsubstantially lower density than their neighbors. This example shows how to\\nuse LOF for novelty detection. Note that when LOF is used for novelty\\ndetection you MUST not use predict, decision_function and score_samples on the\\ntraining set as this would lead to wrong results. You must only use these\\nmethods on new unseen data (which are not in the training set). See\\n:ref:`User Guide <outlier_detection>`: for details on the difference between\\noutlier detection and novelty detection and how to use LOF for outlier\\ndetection.\\n\\nThe number of neighbors considered, (parameter n_neighbors) is typically\\nset 1) greater than the minimum number of samples a cluster has to contain,\\nso that other samples can be local outliers relative to this cluster, and 2)\\nsmaller than the maximum number of close by samples that can potentially be\\nlocal outliers.\\nIn practice, such information is generally not available, and taking\\nn_neighbors=20 appears to work well in general.\\n\\n\"\"\"\\n\\nimport matplotlib\\nimport matplotlib.lines as mlines\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn.neighbors import LocalOutlierFactor\\n\\nnp.random.seed(42)\\n\\nxx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\\n# Generate normal (not abnormal) training observations\\nX = 0.3 * np.random.randn(100, 2)\\nX_train = np.r_[X + 2, X - 2]\\n# Generate new normal (not abnormal) observations\\nX = 0.3 * np.random.randn(20, 2)\\nX_test = np.r_[X + 2, X - 2]\\n# Generate some abnormal novel observations\\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\\n\\n# fit the model for novelty detection (novelty=True)\\nclf = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1)\\nclf.fit(X_train)\\n# DO NOT use predict, decision_function and score_samples on X_train as this\\n# would give wrong results but only on new unseen data (not used in X_train),\\n# e.g. X_test, X_outliers or the meshgrid\\ny_pred_test = clf.predict(X_test)\\ny_pred_outliers = clf.predict(X_outliers)\\nn_error_test = y_pred_test[y_pred_test == -1].size\\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\\n\\n# plot the learned frontier, the points, and the nearest vectors to the plane\\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\\nZ = Z.reshape(xx.shape)\\n\\nplt.title(\"Novelty Detection with LOF\")\\nplt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\\na = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\"darkred\")\\nplt.contourf(xx, yy, Z, levels=[0, Z.max()], colors=\"palevioletred\")'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_lof_novelty_detection.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='plt.title(\"Novelty Detection with LOF\")\\nplt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\\na = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\"darkred\")\\nplt.contourf(xx, yy, Z, levels=[0, Z.max()], colors=\"palevioletred\")\\n\\ns = 40\\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\\nplt.axis(\"tight\")\\nplt.xlim((-5, 5))\\nplt.ylim((-5, 5))\\nplt.legend(\\n    [mlines.Line2D([], [], color=\"darkred\"), b1, b2, c],\\n    [\\n        \"learned frontier\",\\n        \"training observations\",\\n        \"new regular observations\",\\n        \"new abnormal observations\",\\n    ],\\n    loc=\"upper left\",\\n    prop=matplotlib.font_manager.FontProperties(size=11),\\n)\\nplt.xlabel(\\n    \"errors novel regular: %d/40 ; errors novel abnormal: %d/40\"\\n    % (n_error_test, n_error_outliers)\\n)\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_regression.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n============================\\nNearest Neighbors regression\\n============================\\n\\nDemonstrate the resolution of a regression problem\\nusing a k-Nearest Neighbor and the interpolation of the\\ntarget using both barycenter and constant weights.\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\n\\n# %%\\n# Generate sample data\\n# --------------------\\n# Here we generate a few data points to use to train the model. We also generate\\n# data in the whole range of the training data to visualize how the model would\\n# react in that whole region.\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom sklearn import neighbors\\n\\nrng = np.random.RandomState(0)\\nX_train = np.sort(5 * rng.rand(40, 1), axis=0)\\nX_test = np.linspace(0, 5, 500)[:, np.newaxis]\\ny = np.sin(X_train).ravel()\\n\\n# Add noise to targets\\ny[::5] += 1 * (0.5 - np.random.rand(8))\\n\\n# %%\\n# Fit regression model\\n# --------------------\\n# Here we train a model and visualize how `uniform` and `distance`\\n# weights in prediction effect predicted values.\\nn_neighbors = 5\\n\\nfor i, weights in enumerate([\"uniform\", \"distance\"]):\\n    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\\n    y_ = knn.fit(X_train, y).predict(X_test)\\n\\n    plt.subplot(2, 1, i + 1)\\n    plt.scatter(X_train, y, color=\"darkorange\", label=\"data\")\\n    plt.plot(X_test, y_, color=\"navy\", label=\"prediction\")\\n    plt.axis(\"tight\")\\n    plt.legend()\\n    plt.title(\"KNeighborsRegressor (k = %i, weights = \\'%s\\')\" % (n_neighbors, weights))\\n\\nplt.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_caching_nearest_neighbors.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n=========================\\nCaching nearest neighbors\\n=========================\\n\\nThis examples demonstrates how to precompute the k nearest neighbors before\\nusing them in KNeighborsClassifier. KNeighborsClassifier can compute the\\nnearest neighbors internally, but precomputing them can have several benefits,\\nsuch as finer parameter control, caching for multiple use, or custom\\nimplementations.\\n\\nHere we use the caching property of pipelines to cache the nearest neighbors\\ngraph between multiple fits of KNeighborsClassifier. The first call is slow\\nsince it computes the neighbors graph, while subsequent call are faster as they\\ndo not need to recompute the graph. Here the durations are small since the\\ndataset is small, but the gain can be more substantial when the dataset grows\\nlarger, or when the grid of parameter to search is large.\\n\\n\"\"\"\\n\\n# Authors: The scikit-learn developers\\n# SPDX-License-Identifier: BSD-3-Clause\\nfrom tempfile import TemporaryDirectory\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsTransformer\\nfrom sklearn.pipeline import Pipeline\\n\\nX, y = load_digits(return_X_y=True)\\nn_neighbors_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\\n\\n# The transformer computes the nearest neighbors graph using the maximum number\\n# of neighbors necessary in the grid search. The classifier model filters the\\n# nearest neighbors graph as required by its own n_neighbors parameter.\\ngraph_model = KNeighborsTransformer(n_neighbors=max(n_neighbors_list), mode=\"distance\")\\nclassifier_model = KNeighborsClassifier(metric=\"precomputed\")\\n\\n# Note that we give `memory` a directory to cache the graph computation\\n# that will be used several times when tuning the hyperparameters of the\\n# classifier.\\nwith TemporaryDirectory(prefix=\"sklearn_graph_cache_\") as tmpdir:\\n    full_model = Pipeline(\\n        steps=[(\"graph\", graph_model), (\"classifier\", classifier_model)], memory=tmpdir\\n    )\\n\\n    param_grid = {\"classifier__n_neighbors\": n_neighbors_list}\\n    grid_model = GridSearchCV(full_model, param_grid)\\n    grid_model.fit(X, y)\\n\\n# Plot the results of the grid search.\\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\\naxes[0].errorbar(\\n    x=n_neighbors_list,\\n    y=grid_model.cv_results_[\"mean_test_score\"],\\n    yerr=grid_model.cv_results_[\"std_test_score\"],\\n)\\naxes[0].set(xlabel=\"n_neighbors\", title=\"Classification accuracy\")\\naxes[1].errorbar(\\n    x=n_neighbors_list,\\n    y=grid_model.cv_results_[\"mean_fit_time\"],\\n    yerr=grid_model.cv_results_[\"std_fit_time\"],\\n    color=\"r\",\\n)\\naxes[1].set(xlabel=\"n_neighbors\", title=\"Fit time (with caching)\")\\nfig.tight_layout()\\nplt.show()'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_kde_1d.py', 'content_type': 'functions_classes', 'language': 'python'}, page_content='def format_func(x, loc):\\n    if x == 0:\\n        return \"0\"\\n    elif x == 1:\\n        return \"h\"\\n    elif x == -1:\\n        return \"-h\"\\n    else:\\n        return \"%ih\" % x'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_kde_1d.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='\"\"\"\\n===================================\\nSimple 1D Kernel Density Estimation\\n===================================\\nThis example uses the :class:`~sklearn.neighbors.KernelDensity` class to\\ndemonstrate the principles of Kernel Density Estimation in one dimension.\\n\\nThe first plot shows one of the problems with using histograms to visualize\\nthe density of points in 1D. Intuitively, a histogram can be thought of as a\\nscheme in which a unit \"block\" is stacked above each point on a regular grid.\\nAs the top two panels show, however, the choice of gridding for these blocks\\ncan lead to wildly divergent ideas about the underlying shape of the density\\ndistribution.  If we instead center each block on the point it represents, we\\nget the estimate shown in the bottom left panel.  This is a kernel density\\nestimation with a \"top hat\" kernel.  This idea can be generalized to other\\nkernel shapes: the bottom-right panel of the first figure shows a Gaussian\\nkernel density estimate over the same distribution.\\n\\nScikit-learn implements efficient kernel density estimation using either\\na Ball Tree or KD Tree structure, through the\\n:class:`~sklearn.neighbors.KernelDensity` estimator.  The available kernels\\nare shown in the second figure of this example.\\n\\nThe third figure compares kernel density estimates for a distribution of 100\\nsamples in 1 dimension.  Though this example uses 1D distributions, kernel\\ndensity estimation is easily and efficiently extensible to higher dimensions\\nas well.\\n\\n\"\"\"\\n\\n# Author: Jake Vanderplas <jakevdp@cs.washington.edu>\\n#\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom scipy.stats import norm\\n\\nfrom sklearn.neighbors import KernelDensity\\n\\n# ----------------------------------------------------------------------\\n# Plot the progression of histograms to kernels\\nnp.random.seed(1)\\nN = 20\\nX = np.concatenate(\\n    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))\\n)[:, np.newaxis]\\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\\nbins = np.linspace(-5, 10, 10)\\n\\nfig, ax = plt.subplots(2, 2, sharex=True, sharey=True)\\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\\n\\n# histogram 1\\nax[0, 0].hist(X[:, 0], bins=bins, fc=\"#AAAAFF\", density=True)\\nax[0, 0].text(-3.5, 0.31, \"Histogram\")\\n\\n# histogram 2\\nax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc=\"#AAAAFF\", density=True)\\nax[0, 1].text(-3.5, 0.31, \"Histogram, bins shifted\")\\n\\n# tophat KDE\\nkde = KernelDensity(kernel=\"tophat\", bandwidth=0.75).fit(X)\\nlog_dens = kde.score_samples(X_plot)\\nax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc=\"#AAAAFF\")\\nax[1, 0].text(-3.5, 0.31, \"Tophat Kernel Density\")\\n\\n# Gaussian KDE\\nkde = KernelDensity(kernel=\"gaussian\", bandwidth=0.75).fit(X)\\nlog_dens = kde.score_samples(X_plot)\\nax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc=\"#AAAAFF\")\\nax[1, 1].text(-3.5, 0.31, \"Gaussian Kernel Density\")\\n\\nfor axi in ax.ravel():\\n    axi.plot(X[:, 0], np.full(X.shape[0], -0.01), \"+k\")\\n    axi.set_xlim(-4, 9)\\n    axi.set_ylim(-0.02, 0.34)'), Document(metadata={'source': '/content/local_copy_repo/examples/neighbors/plot_kde_1d.py', 'content_type': 'simplified_code', 'language': 'python'}, page_content='for axi in ax.ravel():\\n    axi.plot(X[:, 0], np.full(X.shape[0], -0.01), \"+k\")\\n    axi.set_xlim(-4, 9)\\n    axi.set_ylim(-0.02, 0.34)\\n\\nfor axi in ax[:, 0]:\\n    axi.set_ylabel(\"Normalized Density\")\\n\\nfor axi in ax[1, :]:\\n    axi.set_xlabel(\"x\")\\n\\n# ----------------------------------------------------------------------\\n# Plot all available kernels\\nX_plot = np.linspace(-6, 6, 1000)[:, None]\\nX_src = np.zeros((1, 1))\\n\\nfig, ax = plt.subplots(2, 3, sharex=True, sharey=True)\\nfig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\\n\\n\\n# Code for: def format_func(x, loc):\\n\\n\\nfor i, kernel in enumerate(\\n    [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"]\\n):\\n    axi = ax.ravel()[i]\\n    log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(X_plot)\\n    axi.fill(X_plot[:, 0], np.exp(log_dens), \"-k\", fc=\"#AAAAFF\")\\n    axi.text(-2.6, 0.95, kernel)\\n\\n    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\\n    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\\n    axi.yaxis.set_major_locator(plt.NullLocator())\\n\\n    axi.set_ylim(0, 1.05)\\n    axi.set_xlim(-2.9, 2.9)\\n\\nax[0, 1].set_title(\"Available Kernels\")\\n\\n# ----------------------------------------------------------------------\\n# Plot a 1D density example\\nN = 100\\nnp.random.seed(1)\\nX = np.concatenate(\\n    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))\\n)[:, np.newaxis]\\n\\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\\n\\ntrue_dens = 0.3 * norm(0, 1).pdf(X_plot[:, 0]) + 0.7 * norm(5, 1).pdf(X_plot[:, 0])\\n\\nfig, ax = plt.subplots()\\nax.fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"input distribution\")\\ncolors = [\"navy\", \"cornflowerblue\", \"darkorange\"]\\nkernels = [\"gaussian\", \"tophat\", \"epanechnikov\"]\\nlw = 2\\n\\nfor color, kernel in zip(colors, kernels):\\n    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)\\n    log_dens = kde.score_samples(X_plot)\\n    ax.plot(\\n        X_plot[:, 0],\\n        np.exp(log_dens),\\n        color=color,\\n        lw=lw,\\n        linestyle=\"-\",\\n        label=\"kernel = \\'{0}\\'\".format(kernel),\\n    )\\n\\nax.text(6, 0.38, \"N={0} points\".format(N))\\n\\nax.legend(loc=\"upper left\")\\nax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), \"+k\")\\n\\nax.set_xlim(-4, 9)\\nax.set_ylim(-0.02, 0.4)\\nplt.show()')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#We need Language for splitting the code\n",
        "from langchain.text_splitter import Language\n",
        "\n",
        "code_text_splitter = RecursiveCharacterTextSplitter.from_language(chunk_size=3000,\n",
        "                                                                  chunk_overlap=300,\n",
        "                                                                  language=Language.PYTHON)\n",
        "code_chunks = code_text_splitter.split_documents(documents)\n",
        "print(\"code_chunks\", len(code_chunks))\n",
        "print(code_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1OTaydz5NSW",
        "outputId": "edddd881-c845-44f9-a31f-96365bd13b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='\"\"\"\n",
            "================\n",
            "The Iris Dataset\n",
            "================\n",
            "This data sets consists of 3 different types of irises'\n",
            "(Setosa, Versicolour, and Virginica) petal and sepal\n",
            "length, stored in a 150x4 numpy.ndarray\n",
            "\n",
            "The rows being the samples and the columns being:\n",
            "Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
            "\n",
            "The below plot uses the first two features.\n",
            "See `here <https://en.wikipedia.org/wiki/Iris_flower_data_set>`_ for more\n",
            "information on this dataset.\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "# Code source: Ga√´l Varoquaux\n",
            "# Modified for documentation by Jaques Grobler\n",
            "# SPDX-License-Identifier: BSD-3-Clause\n",
            "\n",
            "# %%\n",
            "# Loading the iris dataset\n",
            "# ------------------------\n",
            "from sklearn import datasets\n",
            "\n",
            "iris = datasets.load_iris()\n",
            "\n",
            "\n",
            "# %%\n",
            "# Scatter Plot of the Iris dataset\n",
            "# --------------------------------\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "_, ax = plt.subplots()\n",
            "scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\n",
            "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n",
            "_ = ax.legend(\n",
            "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
            ")\n",
            "\n",
            "# %%\n",
            "# Each point in the scatter plot refers to one of the 150 iris flowers\n",
            "# in the dataset, with the color indicating their respective type\n",
            "# (Setosa, Versicolour, and Virginica).\n",
            "# You can already see a pattern regarding the Setosa type, which is\n",
            "# easily identifiable based on its short and wide sepal. Only\n",
            "# considering these 2 dimensions, sepal width and length, there's still\n",
            "# overlap between the Versicolor and Virginica types.\n",
            "\n",
            "# %%\n",
            "# Plot a PCA representation\n",
            "# -------------------------\n",
            "# Let's apply a Principal Component Analysis (PCA) to the iris dataset\n",
            "# and then plot the irises across the first three PCA dimensions.\n",
            "# This will allow us to better differentiate between the three types!\n",
            "\n",
            "# unused but required import for doing 3d projections with matplotlib < 3.2\n",
            "import mpl_toolkits.mplot3d  # noqa: F401\n",
            "\n",
            "from sklearn.decomposition import PCA\n",
            "\n",
            "fig = plt.figure(1, figsize=(8, 6))\n",
            "ax = fig.add_subplot(111, projection=\"3d\", elev=-150, azim=110)\n",
            "\n",
            "X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
            "ax.scatter(\n",
            "    X_reduced[:, 0],\n",
            "    X_reduced[:, 1],\n",
            "    X_reduced[:, 2],\n",
            "    c=iris.target,\n",
            "    s=40,\n",
            ")\n",
            "\n",
            "ax.set_title(\"First three PCA dimensions\")\n",
            "ax.set_xlabel(\"1st Eigenvector\")\n",
            "ax.xaxis.set_ticklabels([])\n",
            "ax.set_ylabel(\"2nd Eigenvector\")\n",
            "ax.yaxis.set_ticklabels([])\n",
            "ax.set_zlabel(\"3rd Eigenvector\")\n",
            "ax.zaxis.set_ticklabels([])\n",
            "\n",
            "plt.show()\n",
            "\n",
            "# %%\n",
            "# PCA will create 3 new features that are a linear combination of the\n",
            "# 4 original features. In addition, this transform maximizes the variance.\n",
            "# With this transformation, we see that we can identify each species using\n",
            "# only the first feature (i.e. first eigenvalues).' metadata={'source': '/content/local_copy_repo/examples/datasets/plot_iris_dataset.py', 'content_type': 'simplified_code', 'language': 'python'}\n"
          ]
        }
      ],
      "source": [
        "print(code_chunks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EXsOyv28Thu"
      },
      "source": [
        "#Step-4: Embeddings and VectorDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKn8ozQGMDaB"
      },
      "source": [
        "### Instructions to Write the Python Code for Creating Embeddings and Vector Database\n",
        "\n",
        "1. **Set Up the Environment:**\n",
        "   - Import necessary modules and set the `OPENAI_API_KEY` environment variable. Ensure you have the required key stored in Google Colab's `userdata`.\n",
        "\n",
        "\n",
        "2. **Install Required Libraries:**\n",
        "   - Install the `chromadb` and `tiktoken` libraries using pip.\n",
        "\n",
        "\n",
        "3. **Import Necessary Classes:**\n",
        "   - Import the `OpenAIEmbeddings` class from `langchain.embeddings` and the `Chroma` class from `langchain.vectorstores`. Also, import the `tiktoken` library.\n",
        "\n",
        "\n",
        "4. **Create Embeddings:**\n",
        "   - Instantiate the `OpenAIEmbeddings` class to create embeddings for the code chunks.\n",
        "\n",
        "\n",
        "5. **Create Vector Database:**\n",
        "   - Use the `Chroma.from_documents` method to create a vector database from the code chunks. Specify the embeddings and the directory where the database will be persisted.\n",
        "\n",
        "\n",
        "6. **Persist the Database:**\n",
        "   - Persist the vector database to the specified directory.\n",
        "\n",
        "\n",
        "By following these steps, you will be able to create embeddings for the code chunks and store them in a persistent vector database using the `Chroma` vector store and `OpenAIEmbeddings`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U4OSKpt9U3Q",
        "outputId": "2b47ff42-27a5-4e90-9d69-644a3a791880"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import tiktoken\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "code_db = Chroma.from_documents(documents=code_chunks,\n",
        "                                embedding=embeddings,\n",
        "                                persist_directory=\"code_db\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGlI1DeUAeeJ"
      },
      "source": [
        "#Step-5: RAG and Q_A Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z4sXM6hM5WC"
      },
      "source": [
        "### Instructions to Write the Python Code for Retrieval-Augmented Generation (RAG) and Q&A Chain\n",
        "\n",
        "1. **Import Necessary Classes:**\n",
        "   - Import the `RetrievalQA` and `RetrievalQAWithSourcesChain` classes from `langchain.chains`, and the `OpenAI` class from `langchain.llms`.\n",
        "\n",
        "\n",
        "2. **Create a Retriever from the Vector Database:**\n",
        "   - Convert the previously created `code_db` vector store into a retriever object.\n",
        "\n",
        "\n",
        "3. **Initialize the Language Model:**\n",
        "   - Create an instance of the `OpenAI` language model with a specified temperature (controls the randomness of the model's output).\n",
        "\n",
        "\n",
        "4. **Create the RetrievalQA Chain:**\n",
        "   - Use the `RetrievalQA.from_chain_type` method to create a Q&A chain by specifying the language model (`llm`), chain type, and retriever.\n",
        "\n",
        "\n",
        "5. **Run Queries on the Code Repository:**\n",
        "   - Define queries to search for specific code snippets within the repository. Use the `run` method of the `Code_Repo_QandA` chain to get the results.\n",
        "     ```python\n",
        "     query = \"Code for decision trees\"\n",
        "     query = \"Code for Random Forest\"\n",
        "     query = \"Code for Gradient Boosting\"\n",
        "     ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_brseFhDFo8",
        "outputId": "0a400fdd-ead7-4bdf-8274-2e2234333e21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "retriever=code_db.as_retriever()\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "Code_Repo_QandA = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                              chain_type=\"stuff\",\n",
        "                                              retriever=retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHD9qMaABkwI",
        "outputId": "8a315ec4-88ba-46dc-8289-fafa3be3b245"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The code for decision trees can vary depending on the specific implementation or library being used. However, here is an example of code for a decision tree classifier using the scikit-learn library:\n",
            "\n",
            "```\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "\n",
            "# Load data\n",
            "iris = load_iris()\n",
            "X = iris.data\n",
            "y = iris.target\n",
            "\n",
            "# Split data into training and testing sets\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
            "\n",
            "# Create decision tree classifier\n",
            "clf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
            "\n",
            "# Train the classifier on the training data\n",
            "clf.fit(X_train, y_train)\n",
            "\n",
            "# Make predictions on the test data\n",
            "y_pred = clf.predict(X_test)\n",
            "\n",
            "# Evaluate the performance of the classifier\n",
            "accuracy = clf.score(X_test, y_test)\n",
            "print(\"Accuracy:\", accuracy)\n",
            "```\n",
            "\n",
            "This code first imports the `DecisionTreeClassifier` class from the `sklearn.tree` module. Then, it loads the iris dataset and splits it into training and testing sets. Next, it creates an instance of the decision tree classifier with a maximum of 3 leaf nodes and a random state of 0. The classifier is then trained on the training data using\n"
          ]
        }
      ],
      "source": [
        "query=\"Code for decision trees\"\n",
        "result=Code_Repo_QandA.run({\"query\":query})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAjwqUmABqzO",
        "outputId": "3e254998-35f4-4d6d-f8a8-ed91cd2a3502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The code for Random Forest is:\n",
            "\n",
            "```\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "\n",
            "# Create a random forest classifier with default parameters\n",
            "rf = RandomForestClassifier()\n",
            "\n",
            "# Fit the model on your data\n",
            "rf.fit(X, y)\n",
            "\n",
            "# Make predictions on new data\n",
            "y_pred = rf.predict(X_new)\n",
            "```\n",
            "\n",
            "Note: This is just a basic example, and you may need to adjust the parameters and data preprocessing steps based on your specific problem. It's always a good idea to consult the documentation and experiment with different settings to find the best model for your data.\n"
          ]
        }
      ],
      "source": [
        "query=\"Code for Random Forest\"\n",
        "result=Code_Repo_QandA.run({\"query\":query})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TGX3h5uB24v",
        "outputId": "c83e0d1e-dbf6-4d54-9675-62f977f549bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "I'm not sure what you mean by \"code for Gradient Boosting\". The code for Gradient Boosting is already included in the context provided. It is the code for training and evaluating a Gradient Boosting model on the California Housing Prices dataset. Is there something specific you are looking for?\n"
          ]
        }
      ],
      "source": [
        "query=\"Code for Gradient Boosting\"\n",
        "result=Code_Repo_QandA.run({\"query\":query})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZsh8VD3E0uI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
