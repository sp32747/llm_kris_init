{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOvFj4LAlVeR89Zwxwucino"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install langchain --quiet\n","!pip install openai --quiet\n","!pip install cohere --quiet\n","!pip install langchain_community --quiet"],"metadata":{"id":"NLjfYbAfSCbW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#OpenAI Model - Paid Version"],"metadata":{"id":"rMwoNQTOT_QA"}},{"cell_type":"markdown","source":["Get your OpenAI API key here\n","https://platform.openai.com/usage"],"metadata":{"id":"eTzpLBqhVObK"}},{"cell_type":"code","source":["import os\n","os.environ['OPENAI_API_KEY'] = \"Your own OPENAI_API_KEY\"\n","\n","#Better way\n","from google.colab import userdata\n","os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")"],"metadata":{"id":"TLcmCBxxUDx_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.llms import OpenAI\n","\n","llm=OpenAI(temperature=0.9, max_tokens=256)\n","response = llm.invoke(\"Write a 4 line poem on AI\")\n","print(response)\n","\n","# - temperature: Set to 0.9, which controls the randomness of the output.\n","#   A higher temperature results in more varied and unpredictable outputs,\n","#   while a lower temperature produces more deterministic and conservative outputs.\n","#   This is often used in generative tasks to balance between creativity and relevance.\n","\n","# - max_tokens: Set to 256, which specifies the maximum number of tokens (words or pieces of words)\n","#   that the model can generate in a single response.\n"],"metadata":{"id":"XIjGxnjpUD53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm=OpenAI(temperature=0)\n","response = llm.invoke(\"What is overfitting in Machine Learning? Explain it to a layman\")\n","print(response)"],"metadata":{"id":"3SrvJiBPUD-v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Cohere"],"metadata":{"id":"UXYuvS5jQ6Rv"}},{"cell_type":"markdown","source":["Get your Cohere Trail API key here\n","https://dashboard.cohere.com/api-keys"],"metadata":{"id":"BhHtQ_iSQ9EW"}},{"cell_type":"code","source":["os.environ['COHERE_API_KEY'] = \"Your own COHERE_API_KEY\"\n","#Better way\n","os.environ['COHERE_API_KEY'] = userdata.get(\"COHERE_API_KEY\")"],"metadata":{"id":"WEFXkf--Tqn2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8cZpytJ7QsEw"},"outputs":[],"source":["from langchain.llms import Cohere\n","\n","llm = Cohere(temperature=0.9, max_tokens=256)\n","response = llm.invoke(\"Write a 4 line poem on AI\")\n","print(response)"]},{"cell_type":"code","source":["llm=Cohere(temperature=0)\n","response = llm.invoke(\"What is overfitting in Machine Learning? Explain it to a layman\")\n","print(response)"],"metadata":{"id":"mSczTG0-V_tY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Open source models"],"metadata":{"id":"7Aq4hbTgY45z"}},{"cell_type":"markdown","source":["\n","* Mistral Model (Mistral 7B, Mixtral8-7B)\n","* LLama (Llam2, Llama3)\n","* Bloom by Hugging Face\n","* Falcon 180B\n","* Opt 175B\n","* Xgen-7B\n","* Vicuna-13B\n","\n"],"metadata":{"id":"UOi09tmebaTo"}},{"cell_type":"markdown","source":["### Top Open-Source Large Language Models for 2024\n","\n","1. **LLaMA 2**:\n","   - Developed by Meta, LLaMA 2 is a generative text model with 7 to 70 billion parameters, fine-tuned with reinforcement learning from human feedback (RLHF).\n","   - Released for research and commercial use in July 2023.\n","   - Includes versions like LLaMA Chat and Code LLaMA for varied natural language tasks.\n","\n","2. **BLOOM**:\n","   - Launched by Hugging Face in 2022, BLOOM is an autoregressive model with 176 billion parameters.\n","   - Supports 46 languages and 13 programming languages.\n","   - Emphasizes transparency and is available for free through Hugging Face.\n","\n","3. **BERT**:\n","   - Introduced by Google in 2018, BERT is known for its bidirectional encoder representations from transformers.\n","   - Achieved state-of-the-art performance in many NLP tasks and is widely used, including in Google Search.\n","\n","4. **Falcon 180B**:\n","   - Released by the Technology Innovation Institute in the UAE in 2023.\n","   - With 180 billion parameters, it rivals models like LLaMA 2 and GPT-3.5.\n","   - Requires significant computing resources.\n","\n","5. **OPT-175B**:\n","   - Part of Meta's suite of pre-trained transformers, released in 2022.\n","   - Ranges from 125M to 175B parameters.\n","   - Available for research use only due to its non-commercial license.\n","\n","6. **XGen-7B**:\n","   - Launched by Salesforce in July 2023, designed for longer context windows.\n","   - Utilizes only 7 billion parameters.\n","   - Available for commercial and research purposes, with some variants under a non-commercial license.\n","\n","7. **GPT-NeoX and GPT-J**:\n","   - Developed by EleutherAI, GPT-NeoX has 20 billion parameters and GPT-J has 6 billion parameters.\n","   - Available for various NLP tasks via the NLP Cloud API.\n","\n","8. **Vicuna-13B**:\n","   - Fine-tuned from LLaMA 13B, Vicuna-13B is a conversational model.\n","   - Performs well in customer service, healthcare, education, and more.\n","   - Achieves high quality, comparable to ChatGPT and Google Bard.\n","\n","### Choosing the Right Open-Source LLM\n","Consider the following factors:\n","- **Purpose**: Ensure the LLM's licensing fits your use case, especially for commercial purposes.\n","- **Necessity**: Evaluate if an LLM is essential for your goals.\n","- **Accuracy**: Larger models typically offer higher accuracy.\n","- **Investment**: Consider the cost of resources for training and operating the LLM.\n","- **Pre-trained Models**: Leverage existing pre-trained models for specific use cases to save resources."],"metadata":{"id":"MZqC43tme-0v"}},{"cell_type":"markdown","source":["#HuggingFace models"],"metadata":{"id":"yQO2lLhSWQ0z"}},{"cell_type":"markdown","source":["https://huggingface.co/mistralai"],"metadata":{"id":"5NJk5-NNlIZQ"}},{"cell_type":"code","source":["import os\n","os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"Your own HUGGINGFACEHUB_API_TOKEN\"\n","\n","#Better way\n","from google.colab import userdata\n","os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")"],"metadata":{"id":"R8MLcURRcyaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.llms import HuggingFaceHub\n","\n","repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n","\n","llm = HuggingFaceHub(\n","    repo_id=repo_id,\n","    model_kwargs={\"temperature\": 0.9, \"max_length\": 256},\n",")\n","\n","response = llm.invoke(\"Write a 4 line poem on AI\")\n","print(response)"],"metadata":{"id":"ynG1WvTVWS-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n","\n","llm = HuggingFaceHub(\n","    repo_id=repo_id,\n","    model_kwargs={\"temperature\": 0.3, \"max_length\": 1000},\n",")\n","\n","response = llm.invoke(\"How to pick a stock based on Revenue, Profit and profit margin trends?\")\n","print(response)"],"metadata":{"id":"zhnQDbE_c8dI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Llama from Hugging Facehub\n","https://huggingface.co/meta-llama\n","\n","* You need to fill the contact info and wait for the approval.\n","https://huggingface.co/meta-llama/Meta-Llama-3.1-8B"],"metadata":{"id":"3IUaxgdbnOhR"}},{"cell_type":"code","source":["repo_id=\"meta-llama/Meta-Llama-3.1-8B\"\n","#Throws an error\n","#The model meta-llama/Meta-Llama-3.1-8B is too large to be loaded automatically (16GB > 10GB).\n","#Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\n","\n","llm = HuggingFaceHub(\n","    repo_id=repo_id,\n","    model_kwargs={\"temperature\": 0.9},\n",")\n","\n","response = llm.invoke(\"What are some ways to boost creativity?\")\n","print(response)"],"metadata":{"id":"2bJ8n1APujqk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Replicate"],"metadata":{"id":"A6lV6zlWWVsp"}},{"cell_type":"markdown","source":["- Run and fine-tune open-source models with Replicate's API.https://replicate.com/home\n","- Deploy custom models at scale using one line of code.\n","- Avoid managing infrastructure or learning machine learning details.\n","- Use open-source models or package your own.\n","- Choose to make models public or keep them private.\n","- Start with any open-source model with just one line of code.\n"],"metadata":{"id":"qPL03Yfnr7Vx"}},{"cell_type":"markdown","source":["Replciate API Token\n","\n","On top Left >>> Home>>Click on your id>> API Tokens\n","https://replicate.com/account/api-tokens"],"metadata":{"id":"uZMqrpD6sKWy"}},{"cell_type":"code","source":["!pip install replicate"],"metadata":{"id":"UhKCAyesWZlf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"REPLICATE_API_TOKEN\"] = userdata.get(\"REPLICATE_API_TOKEN\")"],"metadata":{"id":"EyQu5O3VsOd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.llms import Replicate\n","\n","replicate_llm = Replicate(\n","    model=\"meta/meta-llama-3.1-405b-instruct\",\n","    model_kwargs={\"temperature\": 0.6},\n",")\n","\n","response = replicate_llm.invoke(\"What are some good strategies for studying?\")\n","print(response)"],"metadata":{"id":"g8cuUoGAs9di"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Groq"],"metadata":{"id":"nZ9G5F5DWTqf"}},{"cell_type":"markdown","source":["* Developed the LPU(Language Processing Unit) chip to run LLMs faster and cheaper.\n","* Offers Groq Cloud to try open-source LLMs like Llama3 or Mixtral.\n","* Allows free use of Llama3 or Mixtral in apps via Groq API Key with rate limits.\n","* Models on Groq https://console.groq.com/docs/models\n","* Get your Groq API key https://console.groq.com/keys\n"],"metadata":{"id":"qq-Y3ae0oX7B"}},{"cell_type":"code","source":["!pip install langchain-groq"],"metadata":{"id":"ilkVBfuso1jK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")"],"metadata":{"id":"mLICC6MpWVLA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_groq import ChatGroq\n","llm=ChatGroq(\n","    model=\"llama3-70b-8192\"\n",")\n","result=llm.invoke(\"what are the top 10 quotes about ignorance?\")\n","print(result)"],"metadata":{"id":"3JCgb4ubpAy6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Many more ways"],"metadata":{"id":"KDcBv9MtlU5Y"}},{"cell_type":"markdown","source":["https://python.langchain.com/v0.1/docs/integrations/llms/"],"metadata":{"id":"Mr1NfzpUlXY4"}},{"cell_type":"code","source":[],"metadata":{"id":"7c4JPXjq1eBY"},"execution_count":null,"outputs":[]}]}